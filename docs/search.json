[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "On this page you’ll find links to all sorts of stuff that I have found useful, including tutorials, books, and general reading on R and Praat, statistics, software, corpora, design, and other stuff."
  },
  {
    "objectID": "resources.html#my-handouts-tutorials-and-workshops",
    "href": "resources.html#my-handouts-tutorials-and-workshops",
    "title": "Resources",
    "section": "My handouts, tutorials, and workshops",
    "text": "My handouts, tutorials, and workshops\n\nR Workshops\nThis is a series of workshops on how to use R which includes a variety of topics. I have included PDFs and additional information on each installment of this series.\n\n\nFormant extraction tutorial\nThis tutorial walks you through writing a praat script that extracts formant measurements from vowels. If you’ve never worked with Praat scripting but want to work with vowels, this might be a good starting point.\n\n\nVowel plots in R tutorials (Part 1 and Part 2)\nThis is a multi-part tutorial on how to make sort of the typical vowel plots in R. Part 1 shows plotting single-point measurements as scatter plots and serves as a mild introduction to ggplot2. Part 2 shows how to plot trajectories, both in the F1-F2 space and in a Praat-like time-Hz space, and is a bit of an introduction to tidyverse as well.\n\n\nMeasuring vowel overlap in R (Part 1 and Part 2)\nThis is a two-part tutorial on calculating Pillai scores and Bhattacharyya’s Affinity in R. The first covers what I consider the bare necessities, culminating custom R functions for each. The second is a bit more in-depth as it looks at ways to make the functions more robust, but it also shows some simple visualizations you can make with the output.\n\n\nMake yourself googleable\nI’m no expert, but I have given a workshop on how grad students can increase their online presence and make themselves more googleable, based in large part to ImpactStory’s fantastic 30-day challenge, which you can read here.\n\n\nAcademic Poster Workshop\nIn response to the need for a “How to Make an Academic Poster” workshop, I put one together last minute. Poster-making is more of an art than a science and this is a very opinionated view on the dos and don’ts of making an academic poster.\n\n\nExcel Workshop\nI once gave a workshop on Excel and ended producing a long handout, that goes from the very basics to relatively tricky techniques. The link above will take you to a blog post that summarizes the workshop, and you can also find the handout itself."
  },
  {
    "objectID": "resources.html#r-resources",
    "href": "resources.html#r-resources",
    "title": "Resources",
    "section": "R Resources",
    "text": "R Resources\nHere is a list of resources I’ve found for R. I’ve gone through some of them and others are on my to-do list. These are in no particular order.\n\nGeneral R Coding\n\nThe website for Tidyverse is a great go-to place for learning how to use dplyr, tidyr, and many other packages.\nR for Data Science by Garrett Grolemund & Hadley Wickham is a fantastic overview of tidyverse functions.\nAdvanced R by Hadley Wickham with the solutions by Malte Grosser, Henning Bumann, Peter Hurford & Robert Krzyzanowski.\nR Packages by Hadley Wickham. Also try Shannon Pileggi’s tutorial called Your first R package in 1 hour to see some of these tools in action.\nHands-On Programming with R by Garrett Grolemund & Hadley Wickham for writing functions and simulations. Haven’t read it, but it looks good.\nr-statistics.co by Selva Prabhakaran which has great tutorials on R itself, ggplot2, and advanced statistical modeling.\nTidymodels is like the Tidyverse suite of packages, but it’s meant for better handling of many statistical models. Also see it’s GitHub page.\nLearn to purrr by Rebecca Barter is the tutorial on purrr that I wish I had.\nModern R with the Tidyverse by Bruno Rodriguez is a work in progress (as of June 2022), but it’s another free eBook that shows R and the Tidyverse.\nEasystats “is a collection of R packages, which aims to provide a unifying and consistent framework to tame, discipline, and harness the scary R statistics and their pesky models.”\nOscar Baruffa’s monstrous Big Book of R is your one-stop resource for open-source R books on pretty much any topic. There are hundreds of books!\n\n\n\nWorking with Text\n\nText Mining with R by Julia Silge & David Robinson. Haven’t read it, but it looks great.\nHandling Strings with R by Gaston Sanchez.\nIf you use the CMU Pronouncing Dictionary, you should look at the phon package. It makes the whole thing searchable and easy to find rhymes. Personally, this’ll make it a lot easier to find potential words for a word list.\nThe ggtext package by Claus O. Wilke makes it a lot easier to work with text if you want to add a little bit of rich text to your plots.\n\n\n\n\nRMarkdown, Bookdown, and Blogdown\nNote: Now that Quarto is available, some of this material may be out of date.\n\nElegant, flexible, and fast dynamic report generation with R by Yihui Xie is a great resource for RMarkdown.\nR Markdown: The Definitive Guide Yihui Xie, J. J. Allaire, and Garrett Grolemund is the comprehensive guide to R Markdown and Bookdown.\n15 Tips on Making Better Use of R Markdown by Yihue Xie offers some very useful and practical tips for getting the most out of RMarkdown. (These are slides from a presentation in 2019.)\nbookdown: Authoring Books and Technical Documents with R Markdown by Yihui Xie. See an introduction to Bookdown by RStudio here.\nIf your love for Zotero is what’s preventing you from using RMarkdown, never fear! Zotero hacks: unlimited synced storage and its smooth use with rmarkdown by Ilya Kashnitsky is the perfect guide to getting those two integrated.\nThis is an excellent blog post by Rebecca Barter about how to start a blog and what kinds of things to do on it. Becoming an R blogger.\n\n\n\nGIS and Spatial Stuff\n\n\nAn Introduction to Spatial Analysis in R by Chris Brown.\n\n\n\nSpatial Data Science by Edzer Pebesma and Roger Bivand.\nSpatial and Spatioteporal Data Analysis in R, a workshop Edzer Pebesma, Roger Bivand, and Angela Li at the useR! 2019 conference on Jul 9, 2019.\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow.\nR for Geospatial Processing by Nicolas Roelandt.\nGIS and Mapping in R: An Introduction to the sf package by Oliver Gimemez.\nI’ve needed to do a bivariate cloropleth before, so Timo Grossenbacher’s blog post was helpful because it illustrates what this is and how you can do it in R.\nI get all my shape files from the National Historical Geographic Information System (NHGIS) website.\nAnd because I haven’t quite gotten the hang of it yet in R, I do all my mapmaking using the QGIS, the open-source, Mac-friendly, and free alternative to ArcGIS. Shout-out to Meagan Duever of UGA Libraries for teaching me everything I know about GIS.\n\n\n\nWorking with Census Data\n\nKyle Walker’s online book Analyzing US Census Data: Methods, Maps, and Models in R.\nA Guide to Working with US Census Data in R by Ari Lamstein and Logan Powell is a nice, brief guide to census data and some places to go if you want to work with it in R.\nThe tidycensus package by Kyle Walker looks really slick and makes it easy to work with census data within the Tidyverse framework. This blog post, Burden of roof: revisiting housing costs with tidycensus, by Austin Wehrwein is a walkthrough of a real-world application with tidycensus.\n\n\n\nMiscelleny\n\ngt or, the “Grammar of Tables,” the is basically the ggplot2 but for tables.\ntidymodels is collection of packages harmoneous with the tidyverse, that mkes it really easy to run models on your data.\nSelf-explanatory tweets:\n\n\n\nAs 2019 comes to a close, I want to thank all of the lovely people in the #rstats world who have made my year a professional success. For each person in this thread, I'm going to tweet one thing they've done that I particularly appreciate.\n\n— David Keyes (@dgkeyes) December 31, 2019"
  },
  {
    "objectID": "resources.html#data-visualization",
    "href": "resources.html#data-visualization",
    "title": "Resources",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nCourses\n\nHere’s an entire open-access course on Data Visualization by Andrew Heiss, based in R and ggplot2.\n\n\n\nBooks\n\nggplot2 by Hadley Wickham is a comprehensive resource for learning all the ins and outs of ggplot2. Version 3 is due in 2020, but you can look through what’s been written so far here.\nA ggplot2 grammar guide by Gina Reynolds is a great online resource for figuring out ggplot2 works!\nData Visualization: A Practical Introduction by Kieran Healy. I haven’t had the time to look through it, but this books looks quite good. It covers data prep, basic plots, visualizing statistical models, maps, and a whole bunch of other stuff.\nFundamentals of Data Visualization by Claus O. Wilke is “meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional.”\nInteractive web-based data visualization with R, plotly, and shiny by Carson Sievert is another free online book on data visualization in in R. This has a good focus on interactivity since it involves plotly and Shiny.\nMastering Shiny by Hadley Wickham is under development and will be released late 2020. I’m looking forward to this comprehensive book on Winston Chang’s shiny package a lot actually, but in the meantime though you and I can peruse the online version for free.\n\n\n\nColors\nI’ve given a workshop on colors in data visualization, which you can view here. In it, I list the following resources, plus a whole bunch of other ones.\n\nUsing colors in data visualization\n\nYour Friendly Guide to Colors in Data Visualisation by Lisa Charlotte Rost is a great overview of using colors in data visualization with lots of links to other sites and resources.\nWhat to consider when choosing colors for data visualization by Lisa Charlotte Rost has great brief tips for color in data visualization. Be sure to see the links at the bottom for more resources!\nWhen you do create your own palettes, be sure to run it through this Color Blindness Simulator to make sure that everyone can see them. Nick Tierney’s blog post also walks you through a way to check this in R.\nStephen Few has a nice guide for using colors and has his own palette you can use.\nMasataka Okabe and Kei Ito have a guide called Color Universal Design that is pretty well-known.\nFabio Crameri, Grace E. Shephard & Philip J. Heron’s article in Nature called The misuse of colour in science communication may help you when choosing a color palette.\n\n\n\nPrepackaged color palettes\n\nA monster compilation of color palettes in R can be found at Emil Hvitfeldt’s Github.\nThe scico package has a bunch of colorblind-safe, perceptually uniform, ggplot2-friendly color palettes for use in visuals. Very cool.\nThe color brewer website, while best for maps, offers great color palettes that are colorblind and sometimes also printer-safe. The have native integration with ggplot2 with the scale_[color|fill]_ [brewer|distiller] functions.\nPaul Tol has come up with some additional color themes, which you can access with scale_color_ptol in the ggthemes package.\nThere is no shortage of color palettes. Here are a handful of ones I’ve seen and liked for one reason or another:\n\nnationalparkcolors: An R package by Katie Jolly with color palettes based on vintage-looking national parks posters.\nearthtones: An R package by Will Cornwell where you give it GPS coordinates and it’ll go to that location in Google Maps and create a color palette based on satellite images. Pretty cool.\nRSkittleBrewer: An R package by Alyssa Frazee that includes color palettes based on Skittles!\npokepalettes.com: A simple webpage that takes a Pokemon name and generates a color palette.\nwesanderson is based on this Tumbler post that has color palettes based on Wes Anderson movies.\n\n\n\ndutchmasters: Instead of coming up with your own colors, why not use ones created by Dutch painters? This is an R package by Edwin Thoen.\nPrettyCols by Nicola Rennie.\n\nColors.css: A nicer color palette for the web look like nice, customizable colors that work great for websites.\n\n\n\nCreating your own color palettes\n\nIf you want to make your own discrete color scale in R, definitely check out Garrick Aden-Buie’s tutorial, Custom Discrete Color Scales for ggplot2.\nCheck out the simplecolors package, by Jake Riley, to find hex codes for consistently-named colors.\nDefinitely check out Adobe’s Color app for some inspiration on color palettes.\nAlso, check out Coolers for more inspiration on color palettes.\nAnd if you have a start and end point, this Colorpicker app can get colors in between those points.\nI’ve needed to do a bivariate cloropleth before, so Timo Grossenbacher’s blog post was helpful because it illustrates what this is and how you can do it in R.\n\n\n\n\nAnimation\n\nThomas Lin Pedersen’s gganimate package has now made it possible to make really cool animations in R. Sometimes you want to add a bit of pizzazz to your presentation, but other times animation really is the best way to visualize something. Either way, this package will help you out a lot.\n\n\n\nRayshader\n\nDefinitely check out Tyler Morgan-Wall’s rayshader package. It makes it pretty simple to make absolutely stunning 3D images of your data in R. You can make 3D maps if you have spatial data, but you can also turn any boring ggplot2 plot into a 3D work of art. Seriously, go try it out.\nLego World Map - Rayshader Walkthrough by Arthur Welle is an awesome walkthrough on rayshader and maps made out of virtual Legos. It’s a lot of fun.\n\n\n\nMaking better plots\n\nEdward Tufte is a statistician known for his series of four books that focus on best practices in the presentation of data: The Visual Display of Quantitative Information, Envisioning Information, Visual Explanations, and Beautiful Evidence. I read them over several months on the bus and they are very cool. As a practical application of them, this page by Lukasz Piwek shows how to implement many of these visualizations in R. You can also use ggthemes to get some of this implementation.\nJoey Cherdarchuk of Darkhorse Analytics has put together some really succinct presentations on how to simplify things you might put in a paper like maps, charts, tables, and reducing the data to ink ratio.\nClaus Wilke’s Practical ggplot2 is a “repository [that] houses a set of step-by-step examples demonstrating how to get the most out of ggplot2, including how to choose and customize scales, how to theme plots, and when and how to use extension packages.”\nMalcom Barrett’s Designing ggplots: Making clear figures that communicate is a great walk-through, with code, on how to really make your plots look professional, with emphasis on telling a story.\nThe Glamour of Graphics, a talk at RStudio::Conf 2020 by William Chase that discusses how to make nice-looking plots.\nA ggplot2 Tutorial for Beautiful Plotting in R by Cédric Scherer.\n\n\n\nMiscellany\n\nThe R Graph Gallery has hundreds of plots, with code, illustrating what the plots are typically used for and different variants of the same plot. Very cool.\nMy friend Andres Karjus has given several workshops on wide range of data visualization topics, collectively called aRt of the figure: explore and visualize your data using R. You should definitely explore his github and check out his materials.\nThis blog post by Jesse Sadler is a great tutorial on how to use R to visualize network data.\nPlotting special characters or unique fonts can be tricky. Yixuan Qiu’s tutorial showtext: Using Fonts More Easily in R Graphs can help you with that.\nGeorge Bailey’s excellent workshop materials for visualizing vowel formant data can be found here.\nNot sure what kind of data visualization you should use, try From Data to Viz to help you find the most appropriate plot for your data."
  },
  {
    "objectID": "resources.html#statistics-resources",
    "href": "resources.html#statistics-resources",
    "title": "Resources",
    "section": "Statistics Resources",
    "text": "Statistics Resources\n\nGeneral Statistics Knowledge\n\nThe American Statistical Association, which is essentially the statistics equivalent in scope and prestige as the the Linguistic Society of America, put out a statement on p-values in 2016. In March of 2019, they followed up with a monster 43-article special issue, Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05, wherein they recommend that the expression “statistically significant” be abandoned. This has potential to be a pivot point in the field of statistics. Why should a linguist care? Well, the first article in that issue says “If you use statistics in research, business, or policymaking but are not a statistician, these articles were indeed written with YOU in mind.” If you use statistics in your research, it might be worth reading through at least the first article of this issue.\nThe book Modern Dive: An Introduction to Statistical and Data Sciences via R by Chester Ismay and Albert Y. Kim is a free eBook available that teachest the basics of R and statistics. See Andrew Heiss’s post about this book for more information.\nSame Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. This went viral in some circles and shows that you can get the exact same summary statistics with wildly different distributions. Very cool.\nHere’s a BuzzFeed article by Stephanie M. Lee about a researcher who made the news because of his unbelieveable amount of p-hacking and using “statistics” to lie about his data.\nHave you learned about tests like t-tests, ANOVA, chi-squared tests? Did you know they’re all just reguression under the hood? Check out this explanation by Jonas Kristoffer Lindeløv called Common statistical tests are linear models. It’s mathy and based in R.\n\n\n\nLinear mixed-effects models\n\nBodo Winter’s mixed-effects modeling tutorials are the best resource I’ve found on using these in linguistics research. It’s a two-part tutorial, so be sure to look through both of them.\nMixed-Effects Regression Models in Linguistics, edited by Dirk Speelman, Kris Heylen, & Dirk Geeraerts and published by Springer is an entire book on mixed-effects models, specifically for linguists.\nMichael Clark’s post called Shrinkage in Mixed Effects Models has some beautiful illustrations that demonstrate shirnkage. In fact, he has written a much larger document explaining what mixed-effects models and how to run them in R.\n\n\n\nReference Collection to push back against “Common Statistical Myths” is a crowdsourced compilation (managed by Andrew Althouse) of articles that may be used to argue against some common statistical myths or no-nos.\nLisa M. DeBruine & Dale J. Barr’s paper “Understanding Mixed-Effects Models Through Data Simulation”, in Advances in Methods and Practices in Psychological Science serves as a nice tutorial to mixed-effects modeling.\nStefano Coretta’s brief blog post, On Random Effects helps explain what a random effect even is.\nNot sure how to actually run a linear mixed effects model? Try this PDF of Standard Operating Procedures For Using Mixed-Effects Models.\n\n\n\nGAM(M)s\nMy dissertation makes heavy use of generalized additive mixed-effects models (GAMMs). Here are some resources that I used to help learn about these.\n\nGeneralised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction by Márton Sóskuthy.\nHow to analyze linguistic change using mixed models, Growth Curve Analysis and Generalized Additive Modeling by Bodo Winter and Martijn Wieling is a tutorial on using GAMs—with one M—and Growth Curve Analysis.\nAnalyzing dynamic phonetic data using generalized additive mixed modeling: A tutorial focusing on articulatory differences between L1 and L2 speakers of English is another tutorial by Martijn Wieling in the Journal of Phonetics.\nIn fact, Martijn Wieling has the slides for a graduate course in statistical methods, including GAMMs, avilable on his website.\nStudying Pronunciation Changes with gamms by Josef Fruehwald.\nOverview GAMM analysis of time series data by Jacolien van Rij. I haven’t had time to go through this one yet, but it’s on my todo list. Actually all of her tutorials look great.\nGAMs in R by Noam Ross is a free interactive course on GAMs in R.\nIntroduction to Generalized Additive Models with R and mgcv by Gavin Simpson.\n\nIf you don’t like the visuals in mgcv, try Gavin Simpson’s R package, gratia with some ggplot2 alternatives.\ntidymv: Tidy Model Visualisation is an R package by Stefano Coretta that lets you visualize GAMMs using tidyverse-friendly code.\n\n\n\n\n\nOther Models\nI know there are other types of models out there but I haven’t had the opportunity to use them. Here are some resources I’ve found that might be good for me down the road.\n\n15 Types of Regression You Should Know is a post on the blog Listen Data that is a nice overview of different kinds of regression and how to implement them in R.\nIntroduction to Generalized Linear Models by Heather Turner\nCourse materials for the generalized nonlinear models (GNM) half-day course at the useR! 2019 conference by Heather Turner. Here’s her full-day version from Zurich R Course series.\n\n\n\nBayesian Statistics\nI have not yet learned about Bayesian stats, but here are some resources I’ve come across that I may use later.\n\nBayes Rules! An Introduction to Bayesian Modeling with R by Alicia A. Johnson, Miles Ott, Mine Dogucu.\nRichard McElreath’s Statistical Rethinking: A Bayesian Course Using R and Stan is an entire course.\nStefano Coretta, Joseph V. Casillas, and Timo Roettger’s learning materials for their Learn Bayesian Analysis for the Speech Sciences workshop.\n\n\n\nStatistics for Linguists\n\n\nBodo Winter’s mixed-effects modeling tutorials are the best resource I’ve found on using linear mixed-effects models in linguistics research.\nGeneralised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction by Márton Sóskuthy is the best resource I’ve found on using generalized additive mixed-effects models in linguistics research.\nSantiago Barreda and Noah Silbert’s Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R is an entire course on Bayesian stats geared towards linguists.\nMorgan Sonderegger’s book Regression modeling for linguistic data is a working draft of intermediate book on statistical analysis for language scientists.\nHave you used Varbrul or at least read a paper that has? You’ll know that there’s some terminology that is unique to that method. Josef Fruehwald’s video helps translate Varbrul to more contemporary terms.\n\n\n\nMiscelleny\n\nThis workshop, Dimension reduction with R, by Saskia Freytag shows different methods for dimension reduction, weighs their pros and cons, and includes examples and visuals of their applications. Pretty useful.\nIf you use statistical modeling in your research, the report package is a useful tool to convert your model into human-readable prose.\nHere’s an open source course on data science by Danielle Navarro.\nHere’s Michael Franke’s Introduction to Data Analysis.\nThis blog post by Alex Cookson does a cool job at explaining PCA while also including some super cool visuals.\nThis blog post by Joshua Loftus visualizes least squares as springs. Makes a lot of sense to me!\nIf you’ve come up with an outlier detection algorithm, try following Sevvandi Kandanaarachchi’s Testing an Outlier Detection Method to see if it works.\nEasystats “is a collection of R packages, which aims to provide a unifying and consistent framework to tame, discipline, and harness the scary R statistics and their pesky models.”"
  },
  {
    "objectID": "resources.html#praat-resources",
    "href": "resources.html#praat-resources",
    "title": "Resources",
    "section": "Praat Resources",
    "text": "Praat Resources\n\nWill Styler’s Praat tutorial is probably the most thorough I’ve seen. The PDF can be found here but don’t forget to look at the page it comes from which has more information about it.\nPhonetics on Speed: Praat Scripting Tutorial by Jörg Mayer is what I find myself coming back to again and again.\nSpeCT - The Speech Corpus Toolkit for Praat is a collection of well-documented Praat scripts written by Mietta Lennes. I often find my way to this page when I need help for a specific task in Praat and incorporate some of the code in these scripts into my own.\n\n\n\nMichelle Cohn has written and posted a bunch of very useful Praat scripts that you can download and use.\nA YouTube channel called ListenLab by Matt Winn that has a bunch of video tutorials on how to do stuff in Praat.\nAnother YouTube channel called Intro to Speech Acoustics that may be useful to students of acoustics, phonetics, etc.\nAnd I’ve written a tutorial on writing a script for basic automatic formant extraction."
  },
  {
    "objectID": "resources.html#working-with-audio",
    "href": "resources.html#working-with-audio",
    "title": "Resources",
    "section": "Working with audio",
    "text": "Working with audio\nThere are three main steps for processing audio: transcription, forced alignment, and formant extraction.\n\nAutomatic Transcription\nThere is software available that you can use to transcribe in like Praat, Transcriber, and ELAN. But here are some tools I’ve seen that do automatic transcription.\n\nCLOx is a new automatic transcriber available from the University of Washington. It’s a web-based service that uses Microsoft Bing’s Speech Recognition system to transcribe your audio. It’s estimated that a sociolinguistic interview can be transcribed in a fifth the time as a manual transcription. The great news is that it’s available for several languages!\nDARLA is actually a whole collection of tools available through a web interface from Dartmouth University. It can transcribe, align, and extract formants from your (English) audio files all in one go. For automatic transcription, you can use their own in-house system by using the “Completely Automated” method. They admit the transcriptions won’t be perfect, but they provide a handy tool for manual correcting.\nOH-Portal is by the Institute of Phonetics and Speech Processing. It works on several languages, and on clean lab data, it’s a little faster to run this and correct the transcription than it is to do a transcription from scratch. Runs entirely through the web browser, so you don’t have to download anything.\n\n\n\nForced Aligners\nI’ve got a lot of audio that I need to process, so a crucial part of all that is force aligning the text to the audio. Smart people have come up with free software to do this. Here’s a list of the ones I’ve seen.\n\nDARLA, avilable from Dartmouth University, is the one I’ve used the most. It can transcribe, align, and extract formants from your (English) audio files all in one go. Previously, its forced aligner is built using Prosody-Lab but now uses the Montreal Forced Aligner (see below).\nThe Montreal Forced Aligner is a relatively new one that I heard about for the first time at the 2017 LSA conference. It is fundamentally different than other ones in that it uses a software called Kaldi. It’s easy to set up and install and I’ve used it on my own data. The benefit of this over DARLA is that it’s on your own computer so you don’t have to wait for files to upload. And you can process files in bulk. Be sure to check out Michael McAuliffe’s blog on updates.\nFAVE is probably the most well-known forced aligner. It’s open source and you can download it on your own computer from Joe Fruehwald’s Github page. Or if you’d prefer, you can UPenn’s their web interface instead.\nProsodylab-Aligner is, according to their website, “a set of Python and shell scripts for performing automated alignment of text to audio of speech using Hidden Markov Models.” This is a software available through McGill University that actually allows you to train your own acoustic model (e.g. on a non-English audio corpus). I haven’t used it yet, but if I ever need to process non-English audio, this’ll be my go-to.\nSPPAS is a software package with several functions including forced alignment in several languages. Of the aligners you can download to your computer, this might be one of the easier ones to use.\nWebMAUS is another web interface with multiple functions including a forced aligner for several languages.\nGentle advertises itself as a “robust yet lenient forced aligner built on Kaldi.” It’s easy to download and use and produces what appear to be very good word-level alignments of a provided transcript. It even ignored the interviewer’s voice in the file I tried. The output is a .csv file, so I’m not sure how to turn that into a TextGrid, and if you need phoneme-level acoustic measurements, a word-level transcription isn’t going to work.\n\n\n\nFormant Extractors\n\nSantiago Barreda’s Fast Track is my current go-to tool for automated formant extraction. It’s a Praat plug-in, but it works really well with the accompanying R package, FastTrackR. Give them both a try!\nFAVE-Extract is the standard that tons of people use.\nPolyglotDB works well with large, force-aligned corpora.\nIf you want to do write a script yourself, I’ve written a tutorial on writing a script for basic automatic formant extraction."
  },
  {
    "objectID": "resources.html#phonetics-resources",
    "href": "resources.html#phonetics-resources",
    "title": "Resources",
    "section": "Phonetics Resources",
    "text": "Phonetics Resources\n\nThe rtMRI IPA chart has MRI videos of all the sounds on the IPA chart.\nJonathan Dowse’s IPA Charts with Audio includes basically any possible combination of co-articulatations, regardless of whether they’re actually attested in human language.\n\n\n\nPink Trombone is an interesting site that has a interactive simulator of the vocal tract. You can click around and make different vowels and consonants. Pretty fun resource for teaching how speech works."
  },
  {
    "objectID": "resources.html#typography-web-design-and-css",
    "href": "resources.html#typography-web-design-and-css",
    "title": "Resources",
    "section": "Typography, Web Design, and CSS",
    "text": "Typography, Web Design, and CSS\nI enjoy reading and attempting to implement good typography into my website. Here are some resources that I have found helpful for that.\n\nBeautiful Websites\nI designed this website more or less from scratch, so I can appreciate the work others put into their own academic sites. Here are some examples of beautiful websites that I have found that I really like.\n\nKieran Healy has one of the beautiful academic websites I’ve ever seen. I created this category on this page just so I could include his page on here. Wow.\nPractical Typography by Matthew Butterick is was my gateway into typography. My font selection and many other little details on my site (slides, posters, CV, etc.) were influenced by this book.\n\n\n\nCSS\n\nIf you enjoy the work of Edward Tufte and would like to incorporate some of his design principles into your website, you’ll be interested in Tufte CSS by Dave Liepmann. If you’re interested in your RMarkdown files rendering in a Tufte-style (like this), there are ways to do that too, which you can read in chapter 3 of bookdown by Yihui Xie or chapter 6 of R Markdown, by Yihui Xie, J. J. Allaire, and Garrett Grolemund."
  },
  {
    "objectID": "resources.html#academic-life",
    "href": "resources.html#academic-life",
    "title": "Resources",
    "section": "Academic Life",
    "text": "Academic Life\nOccasionally, I’ll see posts with really good and insightful tips on how to be an academic. For the ones I saw I Twitter, I’ve put the first post here: click on them to go directly to that tweet where you can read the rest.\n\nHow to make effective slides by Kieran Healy.\nAdvice to a young scholar by Kensy Kooperrider.\n\n\n\nTwitter for Scientists by Daniel S. Quintana has all insider tips and recommendations for how to use Twitter as an academic.\nA list of self-explanatory tweets:\n\n\n\nHey academics-coming-up! Congratulations on sending out that article! However, that probably also means, a few months later, you got your article rejected. Not even a Revise and Resubmit. Worry not. It happens to all of us, most of the time. Here's a thread on what I do.\n\n— Jeff Guhin (@jeffguhin) November 12, 2019\n\n\n\n\nI finally went through all my bookmarked tweets to compile a list of resources I want my grad students to have and wanted to (1) thank everyone who posted these resources, and (2) pay it forward and share the compiled list with all of you!\n\n— Kaitlin Fogg (@kaitlin_fogg) November 8, 2019\n\n\n\n\nAfter reading approximately 30 applications over the past few days that explicitly requests a diversity statement. I got some notes on what to do and what not to do. The \"DON'T\" list is long but please bear with me. But first, lets define a diversity statement (1/x) pic.twitter.com/qx1e8EyIGJ\n\n— Dr. Samniqueka Halsey (@Samniqueka_H) December 30, 2019\n\n\n\n\nUse less text. One of the most important tips for creating engaging scientific presentations is reducing text as much as possible. The audience is not there to read but to listen to you 1/7@AcademicTwitter #AcademicChatter pic.twitter.com/ybR7cSRor2\n\n— Timo Roettger (@TimoRoettger) March 1, 2020\n\n\n\n\nHow to revise:As an editor and author I have seen many revised papers return to journals. Given effort, most go well (ie step toward acceptance). Some go pear-shaped. I’ve slowly improved and have an approach known by my group as the ’Breakspear method”. Here is its essence\n\n— Michael Breakspear (@DrBreaky) June 19, 2020\n\n\n\n\nHere’s what you’ll need to prepare if you want to pitch yr academic book project to a publisher this year:1. A working title for the book. Don’t worry, you can change it later.2. A project description or overview. Summarize your main argument, how you prove it, why it matters\n\n— Laura Portwood-Stacer, Jeopardy Champ (she/her) (@lportwoodstacer) January 2, 2021\n\n\n\n\nA review of 2020 reviews & a 🧵of jumbled thoughts:Ad-hoc Review requests received: 109Requests accepted: 37Action Editor ms for J1: 35Action Editor ms for J2: 86Thoughts on the current state of review:1/\n\n— Koraly Pérez-Edgar 🇵🇷 (@Dr_Koraly) January 3, 2021\n\n\n\n\nHere's some of the best advice I got when I became a manager last year! It's simple, but considering most people receive no management training whatsoever these days, it's better than nothing. Thread!\n\n— ella dawson (@brosandprose) December 6, 2019\n\n\n\n\nIt is that time of the year where many aspirants will be applying for grad school and tenure track positions. I just wanted to share some advice that I wish I had known when I was going through these things. [continued below]\n\n— 𝙷𝚒𝚖𝚊 𝙻𝚊𝚔𝚔𝚊𝚛𝚊𝚓𝚞 (@hima_lakkaraju) November 24, 2019"
  },
  {
    "objectID": "resources.html#miscellaneous",
    "href": "resources.html#miscellaneous",
    "title": "Resources",
    "section": "Miscellaneous",
    "text": "Miscellaneous\nJust random stuff that doesn’t fit elsewhere.\n\nThe great American word mapper is an interactive tool put together by Diansheng Guo, Jack Grieve, and Andrea Nini that lets you see regional trends in how words are used on Twitter.\nCollecting, organizing, and citing scientific literature: an intro to Zotero is a great tutorial on how to use Zotero by Mark Dingemanse. Zotero is a fantastic tool for, well, collecting, organizing, and citing scientific literature and I’m not exaggerating when I say that I could not be in academics without it.\nVulgar: A Language Generator is a site that automatically creates a new conlang, based on parameters that you specify. The free web version allows you to add whatever vowels and consonants you’d like to include, and it’ll create a full language: a language name; IPA chart for vowels and consonants; phonotactics; phonological rules; and paradigms for nominal morphology, definite and indefinite articles, personal pronouns, and verb conjugations; derivational morphology; and a lexicon of over 200 words. For $19 you can download the software and get a lexicon of 2000 words, derivational words, random semantic overlaps with natural languages, and the ability to customize orthography, syllable structure, and phonological rules. In addition to just being kinda fun, this is a super useful resource for creating homework assignments for students.\nThe EMU-webApp “is a fully fledged browser-based labeling and correction tool that offers a multitude of labeling and visualization features.” I haven’t given this enough time to learn to use it properly, but it seems very helpful.\nJonhannes Haushofer’s CV of Failures. Other people have written this more elegantly than I could, but sometimes it’s nice to see that other academics fail too. You’re not going to get into all the conferences you apply for, your papers are sometimes going to be rejected, and you’re definitely not getting all the funding you apply for. I find it therapeutic to put together a CV of failures like his researcher did and to keep it updated and formatted just as would a regular CV. Don’t let impostor syndrome get in the way by thinking others haven’t failed too.\nKieran Healey’s The Plain Person’s Guide to Plain Text Social Science is an entire book on an aspect of productivity that I’ve only thought about occasionally: what kind of software should you do your work? Before you get too entrenched in your workflow, it’s good to consider what your options are.\nThisWordDoesNotExist.com is a fun site created by Thomas Dimson.\nNiche for fellow Mormons, but this post by “Ziff” called “Church President Probabilities, Changes with the Death of One Q15 Member” is a really in-depth analysis that predicts who the next president of the church will be.\nXKCD’s color survey is always fascinating to me. He displayed a random color and asked people to name it. People could retake the survey as much as they wanted. Hundreds of thousands of responses later, and he came up with a really cool crowd-sourced visualization of how English speakers categorize colors.\nFiveThirtyEight’s “The Ultimate Halloween Candy Power Ranking”. They took a couple dozen Halloween candys, displayed images of two of them at random, and asked people which they’d rather have. Many, many responses later, and they have a nice ranking of people’s favorite candy."
  },
  {
    "objectID": "pages/bones_workshop.html",
    "href": "pages/bones_workshop.html",
    "title": "Bones Workshops",
    "section": "",
    "text": "This page contains the handouts for the workshop for the “Research Experience for Undergraduates” site program. I am told the students spent three weeks collecting osteological data from skeletons at the 7–5th c. BCE Greek colony of Himera. The workshop took take place June 21 and 25, 2018 in the DigiLab."
  },
  {
    "objectID": "pages/bones_workshop.html#part-1-intro-to-r",
    "href": "pages/bones_workshop.html#part-1-intro-to-r",
    "title": "Bones Workshops",
    "section": "Part 1: Intro to R",
    "text": "Part 1: Intro to R\n9:00–9:50\nIn this first part, we’ll do an introduction to R. We’ll talk about what R is, look at the differences between R and RStudio, and then get our hands dirty with actual R code. We’ll see how to do some of the basics, get your data into R, and how to work with it. Most importantly, we’ll see how to get help."
  },
  {
    "objectID": "pages/bones_workshop.html#part-2-data-visualization",
    "href": "pages/bones_workshop.html#part-2-data-visualization",
    "title": "Bones Workshops",
    "section": "Part 2: Data Visualization",
    "text": "Part 2: Data Visualization\n10:00–10:50\nIn this second portion of the workshop, we’ll look at the ggplot package and see how to make some simple visualizations in R. We’ll see how to make scatter plots, bar plots, and boxplots, as well as some variations on each."
  },
  {
    "objectID": "pages/bones_workshop.html#part-3-statistical-tests",
    "href": "pages/bones_workshop.html#part-3-statistical-tests",
    "title": "Bones Workshops",
    "section": "Part 3: Statistical Tests",
    "text": "Part 3: Statistical Tests\n11:00–11:50\nThis last part of the workshop is specifically devoted to how to run some statistics on your data. We’ll look at samples of the kind of data you have and see how to run tests like the t-test, the Mann-Whitney test, and the Kruskall-Wallis test. Moving over to categorical data, we’ll see how to do chi-squared tests and Fisher’s exact test. Finally, we’ll see how to do Principal Components Analysis. At every step of the way, there will be little interjections on best practices and how to make useful visualizations to help you with these."
  },
  {
    "objectID": "pages/bones_workshop.html#bonus-part-4-regression",
    "href": "pages/bones_workshop.html#bonus-part-4-regression",
    "title": "Bones Workshops",
    "section": "Bonus! Part 4: Regression",
    "text": "Bonus! Part 4: Regression\nJune 25: 9:00–10:00\nThere were a few concepts we didn’t quite have time to cover on Thursday, so in this bonus workshop we’ll take a closer look at some additional statistical concepts. This handout is entirely devoted to linear regression, which is a way to analyze make predictions about your data. Specifically, we look at regression with continuous data, categorical data, and multiple regression, with tangents covering analysis of variance and stepwise variable selection."
  },
  {
    "objectID": "pages/latex.html",
    "href": "pages/latex.html",
    "title": "LaTeX Workshops",
    "section": "",
    "text": "Caleb Crumley, Jonathan Crum, and I hosted a series of three workshops on LaTeX during Spring 2020 as a way to introduce our new UGA Grad School–approved LaTeX dissertation template. This page will house some of the materials for those workshops, with links to GitHub repositories where you can find out more.\nNote: All workshops will be held at 3:35pm at the DigiLab (300 Main Library)"
  },
  {
    "objectID": "pages/latex.html#intro-to-latex",
    "href": "pages/latex.html#intro-to-latex",
    "title": "LaTeX Workshops",
    "section": "Intro to LaTeX",
    "text": "Intro to LaTeX\nJanuary 31, 2020—In this workshop, I introduce some essential LaTeX skills, including basic typesetting, special characters, text formatting, document structure, internal references, lists, alignment, and white space. I also get into more typographical topics like font choice, line spacing, hyphenation and justification, and a few other things. This workshop was my most well-attended so far, with 30 registered attendees and another 55 or so on the waitlist. It was a full house!"
  },
  {
    "objectID": "pages/latex.html#the-uga-latex-template",
    "href": "pages/latex.html#the-uga-latex-template",
    "title": "LaTeX Workshops",
    "section": "The UGA LaTeX Template",
    "text": "The UGA LaTeX Template\nFebruary 7, 2020—Caleb Crumley will lead this workshop and will introduce the UGA LaTeX template. This has been approved by the Graduate School. The purpose of the template is to take care of the nit-picky aspects of your document to help ensure it passes the format check so that you’re free to spend your time actually writing it. Caleb will show how to download it, toggle some of the options you may want, and show additional LaTeX skills like inserting tables and images."
  },
  {
    "objectID": "pages/latex.html#advanced-topics-in-latex",
    "href": "pages/latex.html#advanced-topics-in-latex",
    "title": "LaTeX Workshops",
    "section": "Advanced topics in LaTeX",
    "text": "Advanced topics in LaTeX\nFebruary 14, 2020—Jonathan Crum will lead this workshop and will illustrate some of the more technical aspects of LaTeX for more advanced users. Some topics include custom commands, advanced font encoding, more document structure, document splitting and compiling, environments, compilers, and everyday workflow."
  },
  {
    "objectID": "pages/idiolect/index.html",
    "href": "pages/idiolect/index.html",
    "title": "My Idiolect",
    "section": "",
    "text": "Here is a never-complete but growing description of my idiolect. I’ll add to it whenever I think of or discover new things about the way I speak English."
  },
  {
    "objectID": "pages/idiolect/index.html#overview",
    "href": "pages/idiolect/index.html#overview",
    "title": "My Idiolect",
    "section": "Overview",
    "text": "Overview\nI don’t like it when people say they don’t have an accent, however, my pronunciation is pretty close to what I’d call standard American English. I grew up in St. Charles County, Missouri, which is a suburb of St. Louis. You can read Matt Gordon and Chris Strelluf’s work for an in-depth analysis of Missouri English. See also Dan Duncan’s research which is focuses specifically on St. Charles County for an even closer match to my speech. My mom grew up in Minnesota and my dad grew up in Upstate New York and Minnesota."
  },
  {
    "objectID": "pages/idiolect/index.html#vowels",
    "href": "pages/idiolect/index.html#vowels",
    "title": "My Idiolect",
    "section": "Vowels",
    "text": "Vowels\nHere is a general look at my monophthongs. These come from a recording of me reading a bunch of real and nonce words where the vowel is flanked by coronals. (You can access this dataset with my joeysvowels package.)\n\nAs far as I can tell, I don’t have really any indication of any of the chain shifts that people are studying. Even though I grew up in the St. Louis Corridor, I don’t have the Northern Cities Shift. I also don’t have the Low-Back-Merger or the Low-Back-Merger Shift.\n\nMy low back vowels\nMy lot and thought vowels are rather close phonetically but are definitely not merged. Lot is slightly fronter and unrounded while thought is slightly backer and rounded. They are very close to what Jonathan Dowse transcribes as [ɐ̞] and [ɒ̈] here.\nYou can read a fairly comprehensive list of words that I classify as lot and thought in Appendix C (pg. 209) of my dissertation, reproduced in a blog post here. On page 161, I explain that I have a clear intuition about which words are classified into which lexical set, though admittedly a few like probably, prom, mom, bronco, and pond could go either way.\nLike a lot of Americans even with the distinction, dog is thought, as are many other pre-/ɡ/ words. However, there are a few exceptions, like cog, which has an especially fronted vowel quality. Also, the name Og, as in the author Og Mandino, which is short for Augustine, bothers me because Og is clearly lot while Augustine is thought.\nPrelaterally, I have a conditioned merger, which is best described in Aaron Dinkin’s (2016) JEngL paper. (Aaron’s paper is based in Upstate New York, close to where my dad is from, so perhaps this is something I’ve gotten from him.) I have lot if the vowel is in an open syllable (and the following lateral is the onset of the following syllable), as in collar, dollar, anthropology, ollie, and trolley. In fact, I have a rather fronted vowel, fronter than my lot normally is. However, I have thought if the lateral is tautosyllabic, as in golf, dolphin, volume, and doll. Thus, words like doll & hall rhyme, while hollar & hauler and collar & caller do not. You can hear me pronounce these words in the video in this tweet.\nAs is typical of American English, the cloth lexical set is indistinguishable from my thought. Similarly, the palm set (and any “Foreign a” words; Boberg 2009) mostly fits in with my lot. Except, ironically, palm itself (as well as psalm, qualm, and alms), because of that /l/, which makes it part of thought.\nOne important phonological distinction between my lot and thought is that, even though Hayes (2009:82) says tenseness is not specified for low vowels, lot appears to be a lax vowel. Like other lax vowels, it doesn’t appear word-finally, except in marginal cases like ma and pa, onomatopoeia (fa-la-la-la-la, haha, blah), and interjections (aha!, hurrah!). This means that some palm words like bra are reclassifed as thought. There are some exceptions though, like spa and schwa. So yes, for me, spa /spɑ/ and bra /bɹɔ/ don’t rhyme.\n\n\nMy CURE lexical set\nLike probably most Americans, my cure is all over the place. Here is how I pronounce the words listed here and here:\n\n[ɔɹ] (so merged with force/north): amour, bourgeois, Bourne, gourd, gourmet, moor, mourn (-ing, -er, -ful, -ed), paramour, parkour, poor, your (when stressed)\n[uɹ]: allure, boorish, contour, detour, endure (citation form), Fleur, lure, manure, McClure, pleura, tour (-ism, -ing)\n[ɚ] (so, merged with nurse): adjure, assure, bourbon, caricature, centurion, cesura, courier, during, embouchure, endure (non-citation form), ensure, futurity, Honduras, insure, jury, jurisdiction, luxurious, mature, Missouri, neural, neuron, plural, rural, spurious, sure, tournament, tourniquet, Ventura\n[jɚ] (nurse with a jod): bureau, cure, curious, curate, demure, Euro, Europe, fury (-ious), Huron, injure, injurious, manicure, mural, Muriel, obscure (-ity), pedicure, procure, pure (-ify, -ity), Puritan, secure, security, sulfuric, Ural, Uri, urine, Uruguay\n\nAnd here are words that I saw going through lists of cure words that I’m completely unfamiliar with or that I have never said outloud, so I don’t really know what I do: boor, Bourdain, bourrée, bourse, Boursin, bravura, burette, Chambourcy, coiffure, commissure, Courbet, Courvoisier, craquelure, cynosure, dasyure, dour, dourine, Douro, epicure, gourmand, houri, immure, inure, Jourdain, lurid, Muir, ouroˈboroi, penurious, photogravure, pourparler, rotogravure, sourdine, spoor, tambour, tandoori, tellurium, thurible, tourmaline, tournedos, Truro\n\n\n/æ/ and /ɛ/ before nasals\nI raise /æ/ before nasals. Before /n/ and /m/, it’s raised, fronted, and nasalized, such that ban is definitely not the same as bat or even bad. Before /ŋ/, it’s raised even higher to the point that naive me would classify the vowel in bang as /e/. In fact, I’m not actually 100% convinced that it even is /æ/ underlyingly; I may have rephonologized it as being truly /e/.\nAs I point out on page 74 of my dissertation, there are very few words with /ɛɡ/. A nearly complete list, as far as I know, is length, lengthen, strength, strengthen, penguin, dengue, and Bengal (tiger). For what it’s worth, those vowels are the same as /æŋ/, so that bang and the first syllable of Bengal are the same for me.\n\n\nbeg-raising\nAs I explain on page 400 of my 2022 American Speech paper on prevelar raising, I raise historic /ɛɡ/ to something like [eɪɡ] such that beg, egg, leg, and Greg all rhyme with vague. It happens in open syllables like in legacy, negative, and megaphone. It occurs in some infrequent words like renege. I’ve also got it when the vowel has secondary stress, like in Winnipeg, stegosaurus, and nutmeg.\nHowever, there are a handful of exceptions, which was a major part of the reason why I did that paper in the first place. For an unclear reason to me, integrity, segregate, interregnum, and segment all have [ɛ]. I noticed that the /ɡ/ in those words are all followed by sonorants, but that’s not a guarantee blocker of raising since regulate, pregnant, and segue are raised. Interestingly, negligible is raised but negligent is not. Also, peg is raised but JPEG is not. Finally, any word with &lt;x&gt; pronounced as [ɡz] (yes, it’s voiced for me) like exit, exile, excerpt, and exigence are firmly [ɛ] and not [e].\n\n\nRosa’s Roses\nI don’t know the technical term for this, but I have two, possibly three, unstressed vowels, such that Rosa’s and roses aren’t homophonous to me. The first has [ə] while the second is what I’d transcribe as [ɨ].\nMy [ɨ] category of words is rather large and I have it in a handful of environments. Some of the distribution is predictable. I have [ɨ] in plurals (classes, offices), 3rd person singular (loses, pushes), and past tense allomorphs (waited, decided). Word-finally, it’s always [ə], as in extra, area, and data.\nIn a lot of words, I think I’m influenced by spelling. Word-initially, if it’s spelled with an &lt;a&gt;, &lt;o&gt;, &lt;u&gt; I have [ə], as in again, among, & ago, occur, opinion & obtain, and upon, unless, & until. (This spelling preference might explain why I always have [ə] word-finally because as far as I can tell &lt;a&gt; is the only letter used for unstressed word-final vowels.) However, if it’s an &lt;e&gt;, then I have [ɨ], as in expect, edition, effect, emotion, event, and exactly.\nWord-internally though, I haven’t done enough digging to see if there are any patterns and I’m not familiar with the literature so I don’t know what to look out for. I have a suspicion that if it’s next to a coronal sound, the vowel is [ɨ] and [ə] otherwise. So I have [ɨ] in student, woman, & happen but [ə] in problem, system, & item. I have [ɨ] in minute, private, & unit, but [ə] in product, democrat, develop, proposal. Interestingly, I have both vowels in advocate [ˈædvəkɨt]. But there are exceptions to these generalizations, like stomach, perfect, galaxy, miracle, and obstacle all have [ɨ]. I have [ɨ] in regime but [ə] in machine, which makes me think spelling is a stronger factor than phonological context. Without being too exhaustive, I’m inclined to think that [ɨ] is the elsewhere allophone and that [ə] is the exception.\nAs for a possible third one, I have some [ʊ]-like vowel in words like success, support, and suggest. It seems like word-initial &lt;su&gt; might be the environment, but I also get it in to. I’ll have to dig a little deeper to think of other examples.\n\n\nPrelaterals\nI have lost the ability to intuit what’s going on with my back vowels before laterals, but I’ll try to explain what I think I have. I know I merge /ʊl/ with /ol/, so that pull and pole are homophonous. However, it’s the /ʌl/ class that is really tricky for me. When it’s in a closed syllable, like in hull, dull, cull, and mulch, I’m pretty sure I at least had it merged with /ol/. However, I’ve looked at the list of words so much and I’ve thought about this enough that I pretty much know all the words that fall into this category without thinking (at least the one-syllable words) and I apparently want to unmerge them, so now you’ll be hard pressed to find me saying hull the same as hole, even in casual situations. Words like culture, result, vulnerable, multuple, and ultimately, I have no idea what I do.\nIn fact, it was this homophone that got me interested in prelaterals in the first place! There is a small town near the University of Georgia named Hull, and I had to go there for something. I thought to myself over and over as a I drove there, “Wait, is this pronounced like Hole?” I never did really figure out what I did.\nHowever, when it’s in an open syllable, like color, gullet, and sculley. The word adult fits in this category as being firmly [ʌ] rather than [o]. Though not all open syllable words are [ʌ] because like gully and mulligan I think I said as [o] when I was younger. (Not sure what I do now.) A word like sullen could go either way, even now."
  },
  {
    "objectID": "pages/idiolect/index.html#some-of-my-favorite-quirks",
    "href": "pages/idiolect/index.html#some-of-my-favorite-quirks",
    "title": "My Idiolect",
    "section": "Some of my favorite quirks",
    "text": "Some of my favorite quirks\nHere are a list of some of my favorite things I have in my idiolect.\n\nI epenthesize a [k] in ancient, [ẽɪ̃ŋkʃɨnt]. I think what’s going on is I have [ŋ] instead of [n] in the first syllable, possibly analogous to anxious, and the [k] slips in there as I transition from the velar nasal to the post-alveolar fricative.\nbig has a bit of raising towards the end of the vowel. I’d transcribe it as [bɪi̯ɡ]. I don’t have it in any other /ɪɡ/ word, as far as I know, not even pig.\nwant is [wʌnt]. So, wants is homophonous with once.\nI 100% say camouflague as “camel-flague”. So I have an extra /l/ in there.\nThe last syllable of kindergarten has a /d/ underlyingly rather than /t/.\nThe default way I say grandma is [ɡɹæ̃mə]."
  },
  {
    "objectID": "pages/idiolect/index.html#other-things",
    "href": "pages/idiolect/index.html#other-things",
    "title": "My Idiolect",
    "section": "Other things",
    "text": "Other things\n\nPhonological\n\n/t/ and /d/ before /ɹ/ (as in try, train, dry, and drain) are affricated to [tʃɹ] and [dʒɹ]. In other words, little kid me would spell them as “chry” and “jrain”.\nI raise /æ/ before nasals but not in other environments.\n\n\n\nLexical\n\nI pronounce the &lt;l&gt; in words like psalm, alm, palm, qualm. I also pronounce it in wolf, yolk, and folk. I know I used to insert an [ɫ] in both and local, but I don’t think I do that anymore. I do do it in only though.\nThe second syllable of caterpiller doesn’t have an /ɹ/ underlyingly: /kætəpɪlɚ/\nlair is homophonous with layer and does not rhyme with hair.\nI don’t pronounce the &lt;t&gt; in often.\nThough both my parents grew up north of the on line and therefore have lot in on, I don’t, so on is firmly thought.\nI don’t have the pin-pen merger, but I have /ɪ/ in parentheses and /ɛ/ in symmetry.\nI consistently say settler with three syllables ([sɛ.ɾl̩.ɚ]) and not two (*[sɛʔ.lɚ]), even when saying the name of the game Settlers of Catan.\nI think I say violet with two syllables, meaning it’s [ˈvɑɪ.lɨt] instead of [ˈvɑɪ.ə.lət]. However, alveolar does have a very short schwa, so it’s [æɫˈvi.ə.lɚ], which does not rhyme with velar [ˈvi.lɚ]."
  },
  {
    "objectID": "pages/idiolect/index.html#my-kids-speech",
    "href": "pages/idiolect/index.html#my-kids-speech",
    "title": "My Idiolect",
    "section": "My kids’ speech",
    "text": "My kids’ speech\nSince my kids are growing up in an area different from where I grew up, they will likely acquire a different variety of English from my own. Here’s a list of things I’ve heard my 6-year-old say that is different from my own speech.\n\nkindergarten has a clear [t], i.e [kʰɨndɚɡɑɹtʰɨn] while I definitely have an underlying /d/ there.\nOccasional use of [ʔɨn] in words like Martin"
  },
  {
    "objectID": "pages/praat-workshops.html",
    "href": "pages/praat-workshops.html",
    "title": "Praat Workshops",
    "section": "",
    "text": "Lisa Lipani and I will be hosting a series of four workshops on Praat and Praat scripting during Fall 2019. This page will house the handouts and all materials for those workshops.\nNote: All workshops will be held at 3:35pm at the DigiLab (300 Main Library)"
  },
  {
    "objectID": "pages/praat-workshops.html#praat-basics-introduction-to-the-software",
    "href": "pages/praat-workshops.html#praat-basics-introduction-to-the-software",
    "title": "Praat Workshops",
    "section": "Praat Basics: Introduction to the software",
    "text": "Praat Basics: Introduction to the software\nSeptember 11, 2019—In this workshop, I’ll introduce the software Praat, and will how how to install the software, record audio, save audio, extract some acoustic measurements by hand, and do a manual transcription. It will also briefly discuss the important steps you need to take to prep your data before using Praat. (For former students of mine, this workshop will overlap with this homework assignment.)"
  },
  {
    "objectID": "pages/praat-workshops.html#praat-scripting-basics-loops-io-and-textgrids",
    "href": "pages/praat-workshops.html#praat-scripting-basics-loops-io-and-textgrids",
    "title": "Praat Workshops",
    "section": "Praat Scripting Basics: Loops, I/O, and TextGrids",
    "text": "Praat Scripting Basics: Loops, I/O, and TextGrids\nSeptember 18, 2019—With some basic understanding of the software, Lisa will lead the workshop on the fundamentals of Praat scripting: object management, looping through a TextGrid, and how to get your data in and out of a Praat script."
  },
  {
    "objectID": "pages/praat-workshops.html#automatic-formant-extraction-in-praat-and-supplement",
    "href": "pages/praat-workshops.html#automatic-formant-extraction-in-praat-and-supplement",
    "title": "Praat Workshops",
    "section": "Automatic Formant Extraction in Praat (and supplement)",
    "text": "Automatic Formant Extraction in Praat (and supplement)\nOctober 2, 2019—In this workshop, you’ll learn, from start to finish, how to write a Praat script that will automatically extract formant measurements and a other acoustic measurements from your audio and transcription. There will be quite a bit of overlap with this blog post, only you will get more detail with an in-person guide."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Ph.D. Linguistics, University of Georgia, 2020\n\nDissertation: Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington  \n\n\nCommittee: L. Chad Howe (chair), Margaret E. L. Renwick, William A. Kretzschmar, Jr.\n\nB.A. Linguistics, Brigham Young University, 2013\n\nMinor: Linguistics Computing"
  },
  {
    "objectID": "cv/index.html#education",
    "href": "cv/index.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Ph.D. Linguistics, University of Georgia, 2020\n\nDissertation: Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington  \n\n\nCommittee: L. Chad Howe (chair), Margaret E. L. Renwick, William A. Kretzschmar, Jr.\n\nB.A. Linguistics, Brigham Young University, 2013\n\nMinor: Linguistics Computing"
  },
  {
    "objectID": "cv/index.html#academic-employment",
    "href": "cv/index.html#academic-employment",
    "title": "Curriculum Vitae",
    "section": "Academic Employment",
    "text": "Academic Employment\nAssistant Professor, Department of Linguistics, Brigham Young University. 2020–\nInstructional Designer, Brigham Young University. March–May, 2020."
  },
  {
    "objectID": "cv/index.html#publications",
    "href": "cv/index.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\nEdited Volumes\nJoseph A. Stanley, Julia Steele Josephs, Jonathan Crum, & Frithjof Timo Wöhrmann (2022). Proceedings of the 6th Annual Linguistics Conference at UGA. Linguistics Society at UGA, Athens, GA. \nJoseph A. Stanley and Conni Covington, co-editors (2019). UGA Working Papers in Linguistics, Volume 4. Linguistics Society at UGA, Athens, GA. \n\n\nRefereed Articles\nMargaret E. L. Renwick, Joseph A. Stanley, Jon Forrest, & Lelia Glass (in press). “Boomer Peak or Gen X Cliff? from SVS to LBMS in Georgia English.” Language Variation and Change. DOI: 10.1017/S095439452300011X. \n\nStefano Coretta, Joseph V. Casillas, [128 other authors], Joseph A. Stanley, [23 other authors], & Timo B. Roettger (2023). “Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses.” Advances in Methods and Practices in Psychological Sciences 6(3). DOI: 10.1177/25152459231162567. \nJoseph A. Stanley & Betsy Sneller. 2023. Sample size matters in calculating Pillai scores. Journal of the Acoustical Society of America 153(1). 54–67. DOI: 10.1121/10.0016757. \nJoseph A. Stanley (2022). “Interpreting the Order of Operations in Sociophonetic Analysis.” Linguistics Vanguard 8(1). 279–289. DOI 10.1515/lingvan-2022-0065. \nJoseph A. Stanley (2022). “Regional patterns in prevelar raising.” American Speech. 97(3): 374–411. 97(3): 374–411. DOI: 10.1215/00031283-9308384. \nJoseph A. Stanley (2022). “A comparison of turn-of-the-century and turn-of-the-millennium speech in Georgia.” Proceedings of the 6th Annual Linguistics Conference at UGA. Linguistics Society at UGA, Athens, GA.  \nJoseph A. Stanley, Margaret E. L. Renwick, Katie Ireland Kuiper, & Rachel Miller Olsen (2021). “Back vowel dynamics and distinctions in Southern American English.” Journal of English Linguistics 49(4): 389–418. DOI: 10.1177/00754242211043163.\nMargaret E. L. Renwick & Joseph A. Stanley (2020). “Modeling dynamic trajectories of tense vs. lax vowels in the American South.” Journal of the Acoustical Society of America 147(1): 579–595. DOI: 10.1121/10.0000549. \nJoseph A. Stanley (2019). “Phonological Patterns in beg-Raising.” UGA Working Papers in Linguistics, 4, 69–91. \nJanis B. Nuckolls, Joseph A. Stanley, Elizabeth Nielson, & Roseanna Hopper (2016). “The Systematic Stretching and Contracting of Ideophonic Phonology in Pastaza Quichua”. International Journal of American Linguistics, 82(1). 95–116. DOI: 10.1086/684425 \n\n\nBook Chapters\nDominic Watt, Margaret E. L. Renwick, & Joseph A. Stanley (2023). “Sociophonetics and dialectology.” In Christopher Strelluf (ed.) The Routledge Handbook of Sociophonetics, 263–284. Routledge, London. 263–284. \nJoseph A. Stanley (2020). “The Absence of a Religiolect among Latter-day Saints in Southwest Washington.” In Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall (eds.) Speech in the Western States Volume III: Understudied Dialects. (Publication of the American Dialect Society 105), 95–122. Durham, NC: Duke University Press. DOI: 10.1215/00031283-8820642. \n\n\nOther Publications\n\n\n\n\n\n\nNote\n\n\n\nStudent collaborators highlighted in green.\n\n\nJohnson, Heather J., Wendy Baker-Smemoe, Joseph A. Stanley, & Alessandro Rosborough (2023). “Ducks in the Pond: Elementary-School-Age Children’s Perceptions of Standard American English, African American English, and Spanish-Accented English on Scales of Status and Solidarity.” In Paris Gappmayr & Jackson Kellogg (eds.), Proceedings of the 47th annual Boston University Conference on Language Development, 394–407. Somerville, MA: Cascadilla Press.  \nJoseph A. Stanley (2022). “Order of Operations in Sociophonetic Analysis,” University of Pennsylvania Working Papers in Linguistics: Vol. 28: Iss. 2, Article 17. Available at: https://repository.upenn.edu/pwpl/vol28/iss2/17.\nJoseph A. Stanley (2022). “Perceptual Dialectology of Utah.” Schwa: Language and Linguistics: 26. 1–10.  \nJoseph A. Stanley (2019). “(thr)-Flapping in American English: Social factors and articulatory motivations.” Proceedings of the 5th Annual Linguistics Conference at UGA, 49–63.  \nJoseph A. Stanley (2018). “Changes in the Timber Industry as a Catastrophic Event: bag-Raising in Cowlitz County, Washington” Penn Working Papers in Linguistics: Vol. 24: Iss. 2, Article 16. Available at: https://repository.upenn.edu/pwpl/vol24/iss2/16.\nJoseph A. Stanley & Kyle Vanderniet (2018). “Consonantal Variation in Utah English.” Proceedings of the 4th Annual Linguistics Conference at UGA, 50–65. \nMargaret E. L. Renwick & Joseph A. Stanley (2017). “Static and dynamic approaches to vowel shifting in the Digital Archive of Southern Speech.” Proceedings of Meetings on Acoustics 30, 060003; doi: 10.1121/2.0000582\nRachel M. Olsen, Michael L. Olsen, Joseph A. Stanley, Margaret E. L. Renwick, & William A. Kretzschmar, Jr. (2017). “Methods for transcription and forced alignment of a legacy speech corpus.” Proceedings of Meetings on Acoustics 30, 060001. doi: 10.1121/2.0000559.\nStanley, Joseph A. (2016). “Pacific Northwest English: Historical Overview and Current Directions”. UGA Working Papers in Linguistics, 3.  \nStanley, Joseph A. (2016). “When do Mormons Call Each Other by First Name?” Penn Working Papers in Linguistics: Vol. 22: Iss. 1, Article 31. Available at: https://repository.upenn.edu/pwpl/vol22/iss1/31. \nStanley, Joseph A. (2015). “Merging Phonemes in Real Time”. Proceedings of LSUGA’s Second Interdisciplinary Conference in Linguistics (LSUGA2), 1."
  },
  {
    "objectID": "cv/index.html#conference-presentations",
    "href": "cv/index.html#conference-presentations",
    "title": "Curriculum Vitae",
    "section": "Conference Presentations",
    "text": "Conference Presentations\n\nOral Presentations\n\n\n\n\n\n\nNote\n\n\n\nStudent collaborators highlighted in green.\n\n\nMonica Nesbitt, Joseph A. Stanley, & Margaret E. L. Renwick. “Movement, Economy, Orientation: 20th Century Shifts in North American Language.” Panel organized at the American Dialect Society Annual Meeting. New York City. January 5, 2024.\nZoe Eldredge & Joseph A. Stanley. “Exploring the Effects of Cross-Cultural Variation and Tourism in Utah English.” Linguistic Society of America Annual Meeting. New York City. January 4, 2024.\nJoshua Stevenson, Joseph A. Stanley, & Wendy Baker-Smemoe. “The Missionary Voice: Perceptions of an Emerging Register.” Linguistic Society of America Annual Meeting. New York City. January 4, 2024.\nHeather Johnson, Wendy Baker-Smemoe, Joseph A. Stanley, & Alessandro Rosborough. “Children’s Perceptions of Ethnic Varieties of English.” Florida Psycholinguistics Meeting. Gainesville, Florida. October 21, 2023.\nZoe Eldredge & Joseph A. Stanley. “Exploring the Effects of Cross-Cultural Variation and Tourism in Utah English.” New Ways of Analyzing Variation 51. New York City. October 13–15, 2023.\nKateryna Kravchenko & Joseph A. Stanley. “An analysis of Ukrainians’ language attitudes and ideology: A conflict-catalyzed identity shift.” New Ways of Analyzing Variation 51. New York City. October 13–15, 2023.\nChad Huckvale & Joseph A. Stanley. “Perceptions of ‘Southern’ Utah English”. New Ways of Analyzing Variation 51. New York City. October 13–15, 2023.\nKateryna Kravchenko & Joseph A. Stanley. “Surzhyk: Attitudes and Usage among Ukrainian People.” 5th annual Sociolinguistics Symposium. Champaign, Illinois. March 2–3, 2023. \nJoseph A. Stanley & KaTrina Jackson. “Is Idaho English really ‘the Epitome of Average English’?”. The American Dialect Society Annual Meeting. Denver, CO. January 6, 2023. \nJoseph A. Stanley. “Utahns sound Utahn when they avoid sounding Utahn.” The 97th Annual Meeting of the Linguistic Society of America. Denver, CO. January 6. 2023. \nHeather Johnson, Wendy Baker-Smemoe, Joseph A. Stanley, & Alessandro Rosborough. “Ducks in the pond: Elementary-school-age children’s perception of Standard American English, African American English, and Spanish-accented English on scales of status and solidarity.” The 47th Boston University Conference on Language Development (BUCLD). Boston, MA. November 4, 2022.\nJoseph A. Stanley & Betsy Sneller. “How Sample Size Impacts Pillai Scores – and What Sociophoneticians Should Do About It.” New Ways of Analyzing Variation 50. San Jose, CA. October 14, 2022. \nMargaret E. L. Renwick, Joseph A. Stanley, Jon Forrest, & Lelia Glass. “A Mid-Century Peak for the Southern Vowel Shift: Evidence from Georgia.” LabPhon 18. Online. June 23–25, 2022.\nJoseph A. Stanley. “Generational Change in Formant Trajectories: The Low-Back-Merger Shift in Longview, Washington.” The 4th Cascadia Workshop in Sociolinguistics. Online, May, 2022. \nDot-Eum Kim & Joseph A. Stanley. “The Participation in Non-Local Changes and the Rejection of Southern Speech by Korean Americans in Georgia.” Southeastern Conference on Linguistics 89. Online, March 31–April 2, 2022.\nJon Forrest, Margaret E. L. Renwick, Joseph A. Stanley, & Lelia Glass. “Consistent Variability: African-American Vowel Systems in Georgia.” Southeastern Conference on Linguistics 89. Online, March 31–April 2, 2022.\nJoseph A. Stanley, Jon Forrest, Lelia Glass, & Margaret E. L. Renwick. “Perspectives on Georgia vowels: From legacy to syncrhony.” The American Dialect Society Annual Meeting. Washington, D.C., January 6, 2022. \nJoseph A. Stanley & Lisa Morgan Johnson. “Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English.” The 96th Annual Meeting of the Linguistic Society of America. Washington, D.C. January 6–9, 2022. \nJoseph A. Stanley. “Order of Operations in Sociophonetic Data Analysis.” New Ways of Analyzing Variation 49. Online. October 19–24, 2021.  \nJoseph A. Stanley & Margaret E. L. Renwick. “100 Years of Georgia English.” New Ways of Analyzing Variation 49. Online. October 19–24, 2021.  \nJoseph A. Stanley. “Methodological considerations in the study of infrequent phonological variables: The case of English /eɡ/ and /ɛɡ/.” Word-specific phenomena in the realization of vowel categories: Methodological and theoretical perspectives (LabPhon 17 Satellite Workshop). Vancouver, British Columbia[Online]. September, 2020.\nJoseph A. Stanley. “Beyond midpoints: Vowel dynamics of the Low-Back-Merger Shift.” Cascadia Workshop in Sociolinguistics. Vancouver, British Columbia. April, 2021. (Cancelled due to COVID-19.)\nJoseph A. Stanley & Margaret E. L. Renwick. “Back vowel distinctions and dynamics in Southern US English.” The 94th Annual Meeting of the Linguistic Society of America. New Orleans, LA. January 2–5, 2020. \nJoseph A. Stanley. “Real Time Vowel Shifts in Georgia English.” The 6th Annual Linguistics Conference at UGA (LCUGA6). Athens, GA. October 4–5, 2019. \nWilliam A. Kretzschmar, Jr. & Joseph A. Stanley. “Visualization of Big Data Phonetics.” Digital Humanities Conference 2019. Utrecht, the Netherlands. July 9–12, 2019.\nJoseph A. Stanley. “Are beg and bag-raising distinct? Regional patterns in prevelar raising in North American English.” American Dialect Society Annual Meeting. New York City, NY. January 3–6, 2019. \nRachel M. Olsen, Joseph A. Stanley, Michael Olsen, Lisa Lipani, & Margaret E. L. Renwick. “Reconciling perception with production in Southern speech.” American Dialect Society Annual Meeting. New York City, NY. January 3–6, 2019. \nJoseph A. Stanley & Margaret E. L. Renwick. “Finding pockets of social variation in the Digital Archive of Southern Speech.” The 5th Annual Linguistics conference at UGA (LCUGA5). Athens, GA. October 12–13, 2018. \nJoseph A. Stanley. “(thr)-tapping in American English: Articulatory motivations and social factors.” The 5th Annual Linguistics conference at UGA (LCUGA5). Athens, GA. October 12–13, 2018. \nJoseph A. Stanley & Kyle Vanderniet. “What el[t]se is happening[k] with Utah English consonants?” American Dialect Society (ADS) Annual Meeting. Salt Lake City, UT. January 5–8, 2018. \nJoseph A. Stanley, Margaret E. L. Renwick, William A. Kretzschmar Jr., Rachel M. Olsen, & Michael Olsen. “The Gazetteer of Southern Vowels.” American Dialect Society (ADS) Annual Meeting. Salt Lake City, UT. January 5–8, 2018.   \nJoseph A. Stanley & Kyle Vanderniet. “Consonantal variation in Utah English: What el[t]se is happening[k]?” The 4th Annual Linguistics Conference at UGA (LSUGA4). Athens, GA. October 6–8, 2017.  \nJoseph A. Stanley. “The linguistic effects of a changing timber industry: Language change in Cowlitz County, WA.” The 4th Annual Linguistics Conference at UGA (LSUGA4). Athens, GA. October 6–8, 2017. \nRachel Olsen, Michael Olsen, Katherine Kuiper, Joseph A. Stanley, Margaret E. L. Renwick, & William A. Kretzschmar, Jr. “New Perspectives on Historical Southern Speech.” Panel presented at the 2017 Integrative Research and Ideas Symposium (IRIS). Athens, GA. March 20, 2017.\nRachel Olsen, Michael Olsen, Joseph A. Stanley & Margaret E. L. Renwick. “Transcribing the Digital Archive of Southern Speech: Methods and Preliminary Analysis.” 84th Meeting of the SouthEastern Conference on Linguistics (SECOL84). Charleston, SC. March 8–11, 2017. \nWilliam A. Kretzschmar, Joseph Stanley, & Katherine Kuiper. “Automated Large-Scale Phonetic Analysis: DASS.” 84th Meeting of the SouthEastern Conference on Linguistics (SECOL84). Charleston, SC. March 8–11, 2017. \nJoseph A. Stanley. “V[ɛ]ry v[e]ried vowel mergers in the Pacific Northwest.” Diversity and Variation in Language (DiVar 1). Atlanta, GA. February 10–11, 2017. \nJoseph A. Stanley. “The perception and production of two vowel mergers in Cowlitz County, Washington.” The American Dialect Society Annual Meeting. Austin, TX. January 5–8, 2017.  \nJoseph A. Stanley. “An EWP model of Quechua agreement: Further evidence against DM”. The Third Annual Linguistics Conference at the University of Georgia (LCUGA3). Athens, GA. October 7–9, 2016.  \nJoseph A. Stanley. “Separate Phonemes /ɔr/ Merging? The Cord-Card Merger in Real-Time”. The Second Interdisciplinary Linguistics Conference at UGA (LCUGA2). Athens, GA. October 9–11, 2015.   \nJoseph A. Stanley. “Brother Bell’s Audience Types: Forms of Address among Latter-day Saint Young Adults”. 82nd Meeting of the Southeastern Conference on Linguistics (SECOL82). Raleigh, NC. April 9–11, 2015.  \nJoseph A. Stanley. “Brother Bell’s Audience Design: Forms of Address among Latter-day Saint Young Adults”. 39th Annual Penn Linguistics Conference (PLC39). Philadelphia, PA. March 19–21, 2015.  \nJanis Nuckolls, Joseph Stanley, Elizabeth Nielsen, and Roseanna Hopper. “The systematic stretching and adjusting of ideophonic phonology in Pastaza Quichua”. Annual Meeting of the Society for the Study of Indigenous Languages of America (SSILA 2013). Boston, MA. January 3–6, 2013. \n\n\nPoster Presentations\n\n\n\n\n\n\nNote\n\n\n\nStudent collaborators highlighted in green.\n\n\nAmmon Hunt, Mark Tanner, Joseph A. Stanley, and Jeff Parker. “Using Corpus Data to Empirically Investigate Native English Speakers’ Pausing Patterns.” Poster Presentation at PSLLT2023 (Pronunciation in Second Language Learning and Teaching). West Lafayette, IN. September 6–8, 2023.\nMargaret E. L. Renwick, Jon Forrest, Lelia Glass, & Joseph A. Stanley. “Vowel trajectories of African Americans in Georgia, USA.” Poster presentation at the 183rd Meeting of the Acoustical Society of America (ASA). Nashville, TN. December 9, 2022.\nJoseph A. Stanley, Jessica Shepherd, & Auna Nygaard. “Homogeneity and Heterogeneity in Western American English.” Poster presentation at the American Dialect Society Annual Meeting. Washington, D.C. January 7, 2022. \nJoseph A. Stanley & Betsy Sneller. “Sample size matters when calculating Pillai scores.” Poster presentation at the 181st Meeting of the Acoustical Society of America (ASA). Seattle, WA. November 29, 2021. \nJoseph A. Stanley. “Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift.” Poster presentation at the 181st Meeting of the Acoustical Society of America (ASA). Seattle, WA. November 29, 2021. \nWilliam A. Kretzschmar, Jr., Margaret E. L. Renwick, Joseph A. Stanley, Katie Kuiper, Lisa Lipani, Michael Olsen, & Rachel Olsen. “The View of Southern Vowels from Large-Scale Data.” Poster presentation at the American Dialect Society Annual Meeting. New Orleans, LA. January 2–5, 2020. \nJoseph A. Stanley & Margaret E. L. Renwick. “Social factors in Southern US speech: Acoustic analysis of a large-scale legacy corpus.” Poster presentation at the 93rd Annual Meeting of the Linguistic Society of America. New York City, NY. January 3–6, 2019. \nJoseph A. Stanley. “The differences between and within beg and bag: Phonological, morphological, and lexical effects in prevelar raising.” Poster presentation New Ways of Analyzing Variation 47. New York City, New York. October 18–21, 2018. \nShawn Foster, Joseph A. Stanley, & Margaret E. L. Renwick. “Vowel Mergers in the American South.” Poster presentation at the 174th Meeting of the Acoustical Society of America (ASA). New Orleans, LA. December 4–8, 2017. \nJoseph A. Stanley. “Changes in the Timber Industry as a Catalyst for Linguistic Change.” Poster presentation at New Ways of Analyzing Variation 46. Madison, WI. November 2–5, 2017.  \nMargaret E. L. Renwick & Joseph A. Stanley. “An acoustic perspective on vowel shifting: Acoustic analysis of the Digital Archive of Southern Speech” Poster presentation at the 173rd Meeting of the Acoustical Society of America (ASA). Boston, MA. June 25–29, 2017. \nMargaret E. L. Renwick, Michael Olsen, Rachel M. Olsen, & Joseph A. Stanley. “Transcription and forced alignment of the Digital Archive of Southern Speech.” Poster presentation at the 173rd Meeting of the Acoustical Society of America (ASA). Boston, MA. June 25–29, 2017. \nJoseph A. Stanley & Margaret E. L. Renwick. “Phonetic Shift /ɔr/ Phonemic Change? American English mergers over 40 years”. Poster presentation at the 15th Conference on Laboratory Phonology (LabPhon15). Ithaca, NY. July 13–16, 2016."
  },
  {
    "objectID": "cv/index.html#workshops",
    "href": "cv/index.html#workshops",
    "title": "Curriculum Vitae",
    "section": "Workshops",
    "text": "Workshops\n2020 LaTeX Workshop Series with Caleb Crumley and Jonathan Crum. University of Georgia DigiLab. Athens, GA\n\n\nIntroduction to LaTeX. January 31 & February 21. \n\n\nThe UGA LaTeX Template. February 7 & 28. \n\n\nAdvanced Topics in LaTeX February 14. \n\n\n2019 Praat Scripting Workshop Series with Lisa Lipani. University of Georgia DigiLab. Athens, GA.\n\n\nPraat Basics: Introduction to the software. September 11. \n\n\nPraat Scripting Basics: Loops, I/O, and TextGrids. September 18. \n\n\nAutomatic Formant Extraction (and other acoustic measures) in Praat. October 2. \n\n\n2019 Data Visualization Workshop Series. University of Georgia DigiLab. Athens, GA.\n\n\nAn intro to data visualization in R using ggplot2. August 21. \n\n\nCustomizing your plots to make that perfect visual. August 28. \n\n\nGeneralize your aesthetics using custom themes in ggplot2. September 4. \n\n\nFidelity, integrity, and sophistication: Edward Tufte’s principles of data visualization. October 16. \n\n\nSend the right message: The dos and don’ts of color in data visualization. October 23. (with Meagen Duever) \n\n\n2018 R Workshop Series. University of Georgia DigiLab. Athens, GA.\n\n\nIntro to R (Part 1). January 19.  \n\n\nIntro to R (Part 2). January 26.  \n\n\nBuilding Interactive Webpages in R: Introduction to Shiny (Part 1). February 2. \n\n\nBuilding Interactive Webpages in R: Introduction to Shiny (Part 2). February 9. \n\n\nVisualizations I: Introduction to ggplot2. February 16.  \n\n\nVisualizations II: Customizing plots in ggplot2. February 23.  \n\n\nClean and tidy data: Tidyverse Part 1. March 2.  \n\n\nTransform, reshape, and modify your data: Tidyverse Part 2. March 23.  \n\n\nCommunicate to your audience with R Markdown. March 9.  \n\n\n2017 R Workshop Series. University of Georgia DigiLab. Athens, GA.\n\n\nAn Introduction to R: Learn the Basics. September 13.  \n\n\nAn Introduction to ggplot2. October 12.  \n\n\nAn Introduction to the Tidyverse. November 10.  \n\n\nProfessionalization Workshops\n\n\nHow to make an Academic Poster: An Opinionated Tutorial. Workshop given to members of the BYU Linguistics Department. Provo, UT. March 10, 2022.\n\n\nHow to make an Academic Poster: An Opinionated Tutorial. Workshop given to members of the BYU Linguistics Department. Provo, UT. October 20, 2021. \n\n\nA workshop on preparing conference abstracts. Given by invitation by the Linguistics Club at the University of Georgia. Athens, GA. March 5, 2020. \n\n\nBrand Yourself: Boosting Your Online Presence (for linguistics grad students). Workshop given by invitation by the University of Georgia Department of Linguistics. Athens, GA. March 3, 2020. \n\n\nBrand Yourself: Boosting Your Online Presence. Workshop given at the University of Georgia as a part of the Graduate Research Workshop Series sponsored by UGA Libraries. Athens, GA. September 20 and 26, 2019. \n\n\nHow to make an Academic Poster: An Opinionated Tutorial. Workshop given to members of the Linguistics Society at UGA and the Linguistics Club. Athens, GA. September 11, 2019. \n\n\nBrand Yourself: Creating a Digital, Professional Presence. Invited workshop at the DigiLab, Main Library, University of Georgia, Athens, GA. September 27, 2018.  \n\n\nBrand Yourself: A professionalization workshop for grad students. Guest lecturer in ANTH 8755: Topics in (Anthropology) Research. Athens, GA. April 13, 2017. \n\n\nBrand Yourself: A professionalization workshop for grad students. Workshop given at the University of Georgia DigiLab. Athens, GA. November 11, 2016. (with Emily McGinn)    \n\n\n\nExcel Workshops\n\n\nBe a Data Magician: An Excel Workshop for Humanists. Workshop given at the University of Georgia DigiLab. Athens, GA. January 27, 2017."
  },
  {
    "objectID": "cv/index.html#other-presentations",
    "href": "cv/index.html#other-presentations",
    "title": "Curriculum Vitae",
    "section": "Other Presentations",
    "text": "Other Presentations\n\nInvited Presentations\n“Enriching our understanding of language change with vowel formant trajectories.” Macquarie University Centre for Language Sciences (CLaS) Colloquium series. Sydney, Australia (delivered remotely). August 31, 2023. \n“Modeling Change in American English Accents.” BYU Statistics Department Seminar. Brigham Young University, Provo, Utah. September 8, 2022.\n“A Brief Intro to the study of Varieties of English.” BYU’s Summer of Academic Refinement program (SOAR). Brigham Young University, Provo, Utah. June 22, 27, and July 11.\n“Is Editing Racist, Elitist, or Discriminatory?” Panel member in “Situating Editing in a Linguistics Department: A Seminar Series.” Brigham Young University, Provo, Utah. March 1, 2022.\n“Explaining Office Hours.” Panel member in a seminar called “Make a Good First Impression with Your Syllabus.” Center for Teaching and Learning, Brigham Young University, Provo, Utah. February 18, 2022.  \n“What can vowel formant trajectories tell us about language change?” Sociolinguistics Group, University of Washington, Seattle, Washington. November 30, 2021. \n“What can vowel formant trajectories tell us about language change?” Linguistics Colloquium Talks, University of Utah, Salt Lake City, Utah. October 28, 2021. \n“Prepping for the future with a degree in linguistics.” The Linguistics Club at the University of Georgia. Athens, GA (delivered remotely). November 12, 2020.\nMargaret E. L. Renwick & Joseph A. Stanley. “100 years of speech in Georgia.” Workshop on Language, Technology, and Society series. Georgia Institute of Technology, Atlanta, GA (delivered remotely). November 11, 2020.\n“Data Visualization and Basic Statistical Modeling in R.” Invited workshop for an NSF-funded “Research Experience for Undergraduates” program, helping bioanthropology students analyze osteological data from skeletons at the 7–5th c. BCE Greek colony of Himera. June 21 and 25, 2018. \n“Build a better project: Starting a DH project from primary sources.” Presented at the First DigiLab Colloquium. Athens, GA. October 6, 2016.   \n\n\nGuest Lectures\n“Cool sociophonetics stuff I did with the statistics and coding as a linguistics major.” Guest lecturer in LING 198: Career Explorations in Linguistics. Brigham Young University, Provo, UT. October 26, 2023.\n“Why I’m glad I learned statistics and coding as a linguistics major.” Guest lecturer in LING 198: Career Explorations in Linguistics. Brigham Young University, Provo, UT. April 3, 2023.\n“Generalized Additive Mixed Effects Models.” Guest lecturer in LING 604: Research Methods. Brigham Young University, Provo, UT. March 28, 2023.\nGuest discussant in Advanced Methods in Sociophonetics. Georgetown University, Washington DC (delivered remotely). January 23, 2023.\n4-day lecture series on Praat: Introduction to the Software, Analyzing Consonants, Analyzing Vowels, and Audio Manipulation & Visualization. LING 240: Linguistics Tools. Brigham Young University, Provo, UT. October 16–23, 2020.\n“Modeling non-linear changes in time using generalized additive models.” LING 4886/6886: Text and Corpus Analysis. University of Georgia, Athens, GA. November 12, 2019.\n“Basics, Review, Summary, Help.” LING 4400/6400: Quantitative Methods in Linguistics. University of Georgia, Athens, GA. April 20.\n“Phonology of Tone.” LING 3060: Phonetics and Phonology. University of Georgia, Athens, GA. April 20.\n\n\nMiscellaneous Presentations\n“The Boomer–Gen X Transition and its effect on American English.” Linguistics Discussion Group. Provo, UT. April 11, 2023.\n“Why I’m glad I learned statistics and coding as a linguistics major.” Professor’s Story night. Linguistics Student Society. Provo, UT. January 24, 2023.\n“What can vowel formant trajectories tell us about language change?” Linguistics Discussion Group. Provo, UT. October 12, 2021.\n“Order of Operations in Sociophonetic Data Analysis.” Linguistics Discussion Group. Provo, UT. January 26, 2021.\nProfessor’s Story night. Linguistics Student Society. Provo, UT. November 19, 2020.\n“Vowel Dynamics of the Low Vowels in Cowlitz County, Washington”. University of Georgia Linguistics Colloquium. Athens, GA. January 10, 2020. \n“Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington.” Dissertation defense. Athens, GA. December 19, 2019. \n“Linguistics Papers Collection in Athenaeum” with Mariann Burright and Mary Willoughby. Libraries Spring Forum. Athens, GA. February 26, 2019.\n“/ɛɡ/-raising is straightforward? I beg to differ!” The Linguistic Society of the University of Georgia (LSUGA) Tiny Talks. Athens, GA. April 13, 2018. \n“Hey, Siri. Can you understand me?” Three Minute Thesis (3MT™) Competition at the University of Georgia. March 22, 2018.\n“New methods in outlier detection and formant measurement using a modified Mahalanobis distance and ‘mistplots.’” University of Georgia Linguistics Colloquium. Athens, GA. February 9, 2018.\n“Near-mergers in Cowlitz County, Washington.” Second Qualifying Paper defense. Program in Linguistics, Univeristy of Georgia, Athens, GA. May 4, 2017.\n“Volcanic Vocalic Changes”. University of Georgia Linguistics Colloquium. Athens, GA. April 7, 2017. \n“Linguistic Identity in Longview, Washington.” Three Minute Thesis (3MT™) Competition at the University of Georgia. March 23, 2017.\n“An EWP model of Quechua agreement: Further evidence against DM.” Presented at the Linguistic Society of the University of Georgia (LSUGA) Tiny Talks. Athens, GA. September 15, 2016. \n“Southeastern Washington English: What We Know So Far”. The Linguistic Society of the University of Georgia (LSUGA) Talks. Athens, GA. February 18, 2016. \n“When do Mormons Call Each Other by First Name?” First Qualifying Paper defense. Program in Linguistics, Univeristy of Georgia, Athens, GA. January 29, 2016.\n“Brother Bell’s Audience Design: Forms of Address among Latter-day Saint Young Adults”. University of Georgia Linguistics Colloquium. Athens, GA. February 27, 2015. \n“The systematic stretching and adjusting of ideophonic phonology in Pastaza Quichua” with Janis Nuckolls, Elizabeth Nielsen, and Roseanna Hopper. Presentation at the Brigham Young University Linguistics Department Brown Bag Meeting. Provo, UT. December 6, 2012."
  },
  {
    "objectID": "cv/index.html#digital-output",
    "href": "cv/index.html#digital-output",
    "title": "Curriculum Vitae",
    "section": "Digital Output",
    "text": "Digital Output\n\nWebsite\nJoseph A. Stanley, William A. Kretzschmar Jr., Margaret E. L. Renwick, Michael L. Olsen, and Rachel M. Olsen (2017) Gazetteer of Southern Vowels. Linguistic Atlas Project, University of Georgia. lap3.libs.uga.edu/u/jstanley/vowelcharts/.\n\n\nR Packages\njoeysvowels: An R package of datasets, based on my own speech, for use when demonstrating R code. R packgage version 0.1.0. http://joeystanley.github.io/joeysvowels/\nbarktools: Functions to help when working with Barks. R package version 0.2.0. http://joeystanley.github.io/barktools\nfuturevisions: Color Palettes based on Visions of the Future Poster Series. R package version 0.1.1. http://github.com/JoeyStanley/futurevisions\njoeyr: Functions for Vowel Data. R package version 0.3.5. https://joeystanley.github.io/joeyr/\n\n\nMedia\nKatie Cowart. “Classic Georgia accent fading fast.” UGA Today. September 7, 2023. Based on Renwick et al. (2023). \nJustin Higginbottom. KZMU, based in Moab, Utah. June 28, 2022. A 3.5-minute radio clip about my research on Utah English and “Mormonese.” \n\nLauren Paterson. “Language Of The Rockies: Linguist Seeks Speaking Patterns In Idaho And Utah.” Northwest Public Broadcasting, based in Coeur d’Alene, Idaho. April 13, 2022. A 1-minute radio clip about a project on Idaho English. \n“PhD. Candidate Seeks to Interview Multigenerational Wasatch County Families.” The Wasatch Wave. January 3, 2018. A local paper heard about my fieldwork and ran an article on the front page to help me find research participants. (Print only.)\nEhrenberg, Rachel. “The southern drawl gets deconstructed.” ScienceNews. June 30, 2017. Based on Peggy Renwick’s and my poster presentation, “A historical perspective on vowel shifting: Acoustic analysis of the Digital Archive of Southern Speech” at the June 2017 ASA conference. \nGuest host on Faith Promoting Rumors podcast. “Brother Joseph,” April 10, 2017, wherein I discuss my 2016 paper, “When do Mormons Call Each Other by First Name” published in the Penn Working Papers in Linguistics."
  },
  {
    "objectID": "cv/index.html#teaching",
    "href": "cv/index.html#teaching",
    "title": "Curriculum Vitae",
    "section": "Teaching",
    "text": "Teaching\n\nAt Brigham Young University (as faculty)\nAfrican American English (LING 495R). Spring 2023.\nEnglish Phonetics & Phonology (ELING 327). Winter 2022, Winter 2023.\nIntro to Sociolinguistics (LING 452). Winter 2021 (×2).\nIntroduction to Varieties of English (ELING 468). Summer 2020 (focus on the British Isles), Fall 2020, Fall 2021.\nLinguistic Data Analysis (LING 580R). Fall 2022 (with Earl Brown).\nLinguistic Tools 1 (LING 240). Fall 2020, Fall 2021.\nResearch Design in Linguistics (LING 604). Winter 2021 (with Dan Dewey), Winter 2022.\nSociolinguistic Fieldwork (LING 580R). Winter 2023 (with Lisa Johnson).\nSociolinguistics (LING 550). Winter 2021, Fall 2021, Fall 2022.\n\n\nAt the University of Georgia\nPhonetics and Phonology (LING 3060). Fall 2017, Spring 2019.\nQuantitative Methods in Linguistics (LING 4400/6400). Teaching Assistant for Peggy Renwick. Spring 2017.\n\n\nAt Brigham Young University (as a student)\nLinguistic Computing and Programming 1 (LINGC 220). Teaching Assistant for Jason Dzubak. Winter 2013.\nBasic Humanities Computing Skills (LINGC 200). Teaching Assistant for Monte Shelley. Fall 2011, Winter 2012, Fall 2012."
  },
  {
    "objectID": "cv/index.html#advising",
    "href": "cv/index.html#advising",
    "title": "Curriculum Vitae",
    "section": "Advising",
    "text": "Advising\n\nMA Thesis Chair\nKateryna Kravchenko (MA, Linguistics, current).\nChad Huckvale (MA, Linguistics, 2023). Thesis title: Utah English: A Perceptual Dialectology Study. \n\n\nMA Thesis Committee Member\nMax Jensen (MA, Linguistics, current)\nRebecca Brenkman (MA, Linguistics, 2023): Afrikaans Taboo Words: Offensiveness Ranking and Reflections of Usage.\nAmmon Hunt (MA, TESOL, 2023). Thesis title: Pausing Patterns in American English.\nYing Suet Michelle Lung (MA, TESOL, 2022). Thesis title: The Impact of Rubric Training on Students’ Self-Efficacy and Self-Regulated Learning. \nHeather Johnson (MA, Linguistics, 2022). Thesis title: Ducks in the pond: Elementary-school-age children’s perceptions of Standard American English, African American English, and Spanish-accented English on scales of status and solidarity. \n\n\nUndergraduate Honors Thesis Chair\nJoshua Stevenson (BA, Linguistics, 2023). Thesis title: The “Missionary Voice”: Bona Fide Sociolect or Figment of the Mormon Linguistic Imagination?\n\n\nOther Positions\nMentor for Sarah Beecroft, 2022 Recipient of a Jack & Mary Lois Wheatley Endowed Leadership Scholarship. Project Title: Foreign Language Return Missionaries and Accent Opinions. 2023.\nGraduate Thesis Reader for Shawn C. Foster, UGA Center for Undergraduate Research Opportunities Assistantship. Thesis title: Conditioned Front Vowel Mergers in the American South. 2018."
  },
  {
    "objectID": "cv/index.html#funding-and-awards",
    "href": "cv/index.html#funding-and-awards",
    "title": "Curriculum Vitae",
    "section": "Funding and Awards",
    "text": "Funding and Awards\n\nGrants\nBYU College of Humanities Research Funding ($2,500). “Idaho English.”\nJohn Topham and Susan Redd Butler BYU Faculty Research Award ($2,000), awarded by the Charles Redd Center for Western Studies. “The Development of Utah English in Heber City.”\nBYU College of Humanities Research Funding ($3,500). “The Low-Back-Merger Shift in the Western United States.”\nSummer Doctoral Research Fellowship ($3,500), awarded by the University of Georgia Graduate School. Project title: A New Method for Extracting Acoustic Measurements from Speech Audio. May–June, 2018\nAmerican Dialect Society student travel grant ($500). ADS. Salt Lake City, UT. January 2018.\nGraduate Research Award ($1000), awarded by the University of Georgia Willson Center for Humanities and Arts. Project title: “Intra-Family Language Variation in Utah County, Utah.” October 12, 2017.\nInnovative and Interdisciplinary Research Grant ($2,500), awarded by the University of Georgia Graduate School. Project title: “A Survey of Western American English using Amazon Mechanical Turk.” April 20, 2017.\nUniversity of Georgia Graduate School Dean’s Award ($1,250). Project title: “Linguistic Identity and the Founder Effect in Longview, Washington.” January 5, 2016.\n\n\nResearch Assistantships\nComplex Systems and the Humanities Research Assistantship awarded by the Graduate School at the University of Georgia ($17,664 per year). Split time between the Linguistic Atlas Project and the DigiLab, under the direction of William A. Kretzschmar, Jr. and Emily McGinn. 2016–2020.\nUniversity of Georgia stipend enhancement awarded by the Franklin College of Arts and Sciences. 2014–2015.\nUniversity of Georgia Graduate Research Assistantship. Included work for Chad Howe, Peggy Renwick, Mi-Ran Kim, Vera Lee-Schoenfeld, and Pilar Chamorro. 2014–2016.\nResearch Assistant for Monte Shelley, Maxwell Institute for Religious Scholarship, Brigham Young University. 2012–2013.\nResearch Assistant for Janis Nuckolls. Department of Linguistics and English Language, Brigham Young University. 2012–2013.\n\n\nTravel Awards\nLinguistic Atlas Project travel award. DH2019. Utrecht, the Netherlands. July 2019.\nLinguistic Atlas Project travel award. ADS and LSA. Salt Lake City, UT. January 2019.\nUniversity of Georgia Graduate School travel award. NWAV47. New York City, NY. October, 2018.\nLinguistic Atlas Project travel award. ADS. Salt Lake City, UT. January 2018.\nUniversity of Georgia Department of Linguistics travel award. NWAV46. Madison, WI. November, 2017.\nUniversity of Georgia Graduate School travel award. NWAV46. Madison, WI. November, 2017.\nLinguistics Society of the University of Georgia travel award. DiVar1. Atlanta, GA. February, 2017.\nUniversity of Georgia Graduate School travel award. ADS. Austin, TX. January, 2017.\nUniversity of Georgia Linguistics Program travel award. ADS. Austin, TX. January, 2016.\nBrigham Young University Department of Linguistics and English Language conference travel grant. SSILA. January, 2013.\n\n\nOther Employment\nUS English Linguistics Expert for DefinedCrowd. Summer 2019.\nAssistant for Dr. Peggy Renwick in developing materials and establishing an online repository for “Quantitative Methods in Linguistics” at the University of Georgia. 2016–2017.\nProgrammer for the Maxwell Institute for Religious Scholarship. 2013."
  },
  {
    "objectID": "cv/index.html#professional-service",
    "href": "cv/index.html#professional-service",
    "title": "Curriculum Vitae",
    "section": "Professional Service",
    "text": "Professional Service\n\nEditorial\nLinguistics Vanguard. Area Editor: Sociolinguistics. (2024–2027)\n\n\nManuscript reviewer\nAmerican Speech (2021, 2022, 2023)\nFrontiers in Artificial Intelligence. Handling Editor. (2021)\nFrontiers in Communication. Reviewer. (2021)\nJournal of English Linguistics (2021)\nJournal of the Acoustical Society of America (2022)\nLaboratory Phonology (2021)\nLanguage and Linguistic Compass (2022)\nLanguage and Speech (2021, 2023)\nLanguage Variation and Change (2020, 2022)\nLinguistics Vanguard (2021, 2022 (×3))\nStudia Linguistica Universitatis Iagellonicae Cracoviensis (2021)\nUniversity of Georgia Working Papers in Linguistics (2023)\n\n\nAbstract Reviewer\nLanguage Variety in the South V (2023)\nNew Ways of Analyzing Variation 50–51 (2022–2023)\n96th–98th Annual Meetings of the Linguistic Society of America (2021–2023)\n4th–6th Linguistics Conferences at UGA (2017–2019)\n\n\nConference Committees\n4th–6th Linguistics Conferences at UGA. Social media subcommittee, webmaster, organizer of the Undergraduate Poster Session, typesetter, and miscellaneous other duties. 2017–2019.\n\n\nOther Service\n\nLinguistic Atlas Project Advisory Board. 2022–\nFaculty liason to BYU Linguistics Department’s Graduate Student Society. 2022–.\nThree Minute Thesis (3MT™) Competition Judge (2021, 2022) and Emcee (2023) for the BYU Linguistics Department MA students.\nMember of the American Dialect Society New Words Committee. 2018–.\nUGA Linguistics Graduate Student Representative. 2018–2019.\nWeb developer for the Linguistics Society at UGA. 2018–2020.\nOrganizer of the UGA Sociolinguistics reading group. 2017.\nOrganizer of the UGA Perl Study group. 2016.\nOrganizer of the UGA Typology reading group. 2016.\nEnglish-to-Portuguese simultaneous interpreter, Missionary Training Center, Provo, Utah. 2011–2013.\n\n\nProfessional Affiliations\nThe Linguistic Society of America, 2012–lifetime\nThe American Dialect Society, 2016–lifetime\nThe Linguistcs Society at the University of Georgia, 2017–2020\nSouthEastern Conference on Linguistics, 2015\nSociety for the Study of the Indigenous Languages of the Americas, 2012"
  },
  {
    "objectID": "cv/index.html#skills-and-experience",
    "href": "cv/index.html#skills-and-experience",
    "title": "Curriculum Vitae",
    "section": "Skills and Experience",
    "text": "Skills and Experience\n\nComputer Skills & Tools\nExpert (=I could teach a course on these): R(Studio), tidyverse, ggplot2, Praat, LaTeX, Perl, Word, Excel, DARLA\nProficient (=I’ve used these regularly and could help with some things): COCA (and related corpora), ELAN, FastTrack, FAVE, Jamovi JMP, Markdown, Shiny, WordCruncher, Zotero, and enough CSS, HTML, and web design to make this site.\nFamiliar (=I still need guidance): Access, AntConc, C#, DreamWeaver, HTK, LaBB-CAT, MALLET, PCT, Photoshop, Prosody-Lab Aligner, Python, SAS, SoX, SPPAS, Transcriber, UCINET, VBA, WebMAUS, Wiki markup\n\n\n\nFieldwork Experience\nOngoing sociolinguistic fieldwork in Utah County, Utah. 2023–.\nSociolinguistic fieldwork in Utah and Wasatch Counties, Utah. January, 2018.\nSociolinguistic fieldwork in Cowlitz County, Washington. July, 2016.\nStudy abroad at the Andes and Amazon Field School near Tena, Ecuador, documenting Tena and Pastaza Kichwa phonology. June–July, 2011.\nBrazil (Marília, SP; Campo Grande, MS; Cáceres, Cuiabá, and Várzea Grande, MG). 2008–2010. (Not linguistics related.)\n\n\n\nLanguages (more details here)\nNative: English\nFluent: Brazilian Portuguese\nConversational: Spanish, Quechua\nAcademic Knoweldge: Guaraní, Mandarin, Tz’utujil, Tshiluba, Korean\n\n  \n Icon credits:     were made by Smashicons and   by Freepik of www.flaticon.com and are licensed by CC 3.0 BY."
  },
  {
    "objectID": "blog/I-got-a-job/index.html",
    "href": "blog/I-got-a-job/index.html",
    "title": "I got a job at BYU!",
    "section": "",
    "text": "I can’t believe I get to say this, but in June I’ll start a new job as an Assistant Professor in the Department of Linguistics at Brigham Young University!\nFirst off, I can’t believe my odds. There are so many amazingly talented grad students out there. Seriously. Several times during the past few years, I’ve seen and met some super accomplished folks, and I’m shocked they’re only grad students. Unfortunately, the academic job market is fierce and not all of them go on to get academic jobs. Especially this year. I’m convinced that once you get to a certain level, it mostly comes down to luck after that. I was dealt good cards this year and that’s really the only reason I get to write this right now and other people can’t."
  },
  {
    "objectID": "blog/I-got-a-job/index.html#byu-is-going-to-be-awesome",
    "href": "blog/I-got-a-job/index.html#byu-is-going-to-be-awesome",
    "title": "I got a job at BYU!",
    "section": "BYU is going to be awesome!",
    "text": "BYU is going to be awesome!\nI’m really excited for BYU for several reasons. First, it’s where I got my undergrad. I’ll be teaching in the same rooms where I myself learned linguistics. In fact, during my campus visit, my teaching demonstration was in the very room that my Intro to Linguistics course was. I’ll also be colleagues with my former professors. (Calling them by first name will take some getting used to!) The campus is beautiful and right in the foothills of an enormous mountain. The Humanities building is big and beautiful. It’ll be great to be there again.\nWhat I didn’t know as a student, but what I can really appreciate now as a future teacher, is the flexibility in teaching that BYU Linguistics offers. I’m slated to teach sociolinguistics, dialectology, phonetics & phonology, and quantitative methods this year. I don’t know if too many schools could have offered me such a perfect assortment of courses. Plus, they’ve got several courses on the books that are specifically designed for faculty interests. I probably won’t be able to teach those for a few years, but if there’s some cool topic I want to try out, I won’t need to go through too many hoops in trying it out once.Update: By the end of my third year, I already been able to teach African American English, and team-taught Linguistic Data Analysis and Sociolinguistic Fieldwork!\nIn fact, I’ve already started some work for BYU Linguistics. I’m helping to prepare some course material to be used online during a study abroad next year. I’ll also get started a little earlier than normal (end of June) so I can teach a course this summer. It’s crazy to think that my family and I will be moving in less than three months!\nI’m also excited to begin research in Utah! I’ve always been interested in dialectology, especially in the American West. How many dialectologists land jobs in the heart of the geographic region that most interests them?? I have so many research questions that have to do with Utah English and Mormon English, not to mention surrounding areas like Wyoming and Idaho. To think that a few years ago I had to get a grant to collect fieldwork in Utah—now all I’ll need to do is go get groceries! I’m looking forward to collecting some data as soon as I get there."
  },
  {
    "objectID": "blog/I-got-a-job/index.html#the-job-hunt",
    "href": "blog/I-got-a-job/index.html#the-job-hunt",
    "title": "I got a job at BYU!",
    "section": "The Job Hunt",
    "text": "The Job Hunt\nI wanted to post this chart summarizing my job hunt over the past two (or three-ish) academic years. I applied to basically everything I could possibly see myself doing. I’ve heard stories of people applying to over 100 jobs. Well, for a sociolinguist, there really aren’t 100 jobs out there—this was pretty much all of them.\n\nOf the 41 jobs I applied for, 26 were tenure-track, six were visiting professor positions, six were post-docs, two were lecturer positions, and one was Alt-Ac. I got 10 Skype interviews, one campus visit, and one offer. It has never crossed my mind to apply for an industry job (heck, I don’t even know how to!), so there aren’t any there. If things hadn’t worked out this year, I would have started down that route. I think finally did get the hang of applying for jobs towards the end, but I’m just so glad I don’t need to apply for jobs anymore. (Grants though, that’s a different story…)\nAgain, I’m aware that I basically won the lottery this year. So many grad students won’t be able to get academic jobs—especially with COVID-19 destroying the economy right now. I can’t imagine how bummed I would have been if nothing had panned out. And it’s depressing to think that many of my grad student colleagues will have to open that crushing final rejection letter. I express my deepest sympathy for them."
  },
  {
    "objectID": "blog/I-got-a-job/index.html#next-steps",
    "href": "blog/I-got-a-job/index.html#next-steps",
    "title": "I got a job at BYU!",
    "section": "Next steps",
    "text": "Next steps\nToday I submitted the revisions for my dissertation to my advisor, and I anticipate submitting the final version to UGA next week. I’ve still got some loose ends to tie up with my assistantship with the Linguistic Atlas Project that I’ll work on this month. After that, it’ll be course prep, house-hunting, and (gulp!) a 28 hour drive to Provo, UT. At least the weather will be nicer there!"
  },
  {
    "objectID": "blog/lcuga6/index.html",
    "href": "blog/lcuga6/index.html",
    "title": "LCUGA6",
    "section": "",
    "text": "I presented at the 6th annual Linguistics Conference at UGA today! My presentation, which was called “Real Time Vowel Shifts in Georgia English” compared Georgians born around the 1890s to those born in the 1990s—100 years of change! The gist: nearly every vowel has changed, and it seems like the trajectory of that change is in the direction of the Elsewhere Shift, rather than just a simple recession of Southern features.\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nThe data came from two very different sources. The older speakers, were born between 1887 and 1903, were recorded doing Linguistic Atlas interviews and are part of the Digital Archive of Southern Speech. The younger speakers, who were born between 1994 and 1997) were UGA undergraduates that I recorded as they read sentences in a lab. Two very different datasets, but they’re all Georgian natives, so it starts to give us a glimpse into how things have changed around here.\nHere’s a summary of the changes I found.\n\nprice was monophthongal for older speakers and definitely a diphthong in the younger speakers.\nThe Euclidean distance between fleece and kit and between face and dress was grew over 100 years, suggesting the younger speakers’ vowels are not as “Southern” sounding.\nWhile the older speakers’ lot and thought were close, the younger speakers’ were almost entirely merged.\nThe front lax vowels, trap, dress, and kit were front and monophthongal in the older speakers while the younger speakers realized them more centralized and more diphthongal. trap underwent the most change, then dress, and then kit.\ngoose was centralized even in the older speakers, but the younger speakers took it further by using an even more fronted onset.\n\nThis animation basically sums up the entire talk. Plus I just had fun making animations! Here’s the change for the women.\n\n\n\n\n\nAnd here’s the change for the men.\n\n\n\n\n\nOverall, it appears that it’s not just that the younger speakers lack features associated with the South, it’s that they’re also using realizations characteristic of other regions like California and Canada. It makes sense, since Atlanta is a large metropolitan area and is kind of known as a non-Southern city in the middle of the South. But this might be an indication that the Elsewhere Shift has made its way into the South."
  },
  {
    "objectID": "blog/using-mturk/index.html",
    "href": "blog/using-mturk/index.html",
    "title": "Using MTurk",
    "section": "",
    "text": "A few weeks ago, I wroteabout a grant I was awarded where I’ll use Amazon Mechanical Turk (“MTurk”) to collect data from people all across the West. Today, I did a soft launch of the request and already got recordings from five people!\nAfter weeks of carefully wrangling my MTurk request, my Qualtrics survey, and my IRB forms, I finally got it all set up. I’ve had a handful of projects get approved by the IRB, but this one was a little different since it was through MTurk, so I was a little unsure how to go about some things. Luckily, our IRB office was having open houses all through the semester, which were very helpful.\nI decided to do a soft release first. $2500 is a lot of money to just throw into a task all at once and I wanted to make sure things were working out right. So I put in enough for five people do to the task. Within the hour I was getting data sent to me! It was crazy!\nI got all five in one day with no problem. I’m glad I did the soft release though because there were a couple small snafus that I had to fix. For example, I underestimated how much time it would take people to finish the task, so I’ll raise the amount they’re compensated: I can afford fewer workers that way, but at least I pay them an honest amount.\nI’ll spend the next few days making absolutely certain that the task I want them to do is what’s right for this project. But at some point, I’ll pull the trigger and let’er rip. From that point on, all I need to do is approve people’s work (to make sure they get paid) and then just enjoy the hours and hours of recordings showing up in my inbox. What a way to collect data!"
  },
  {
    "objectID": "blog/using-mturk/index.html#may-22",
    "href": "blog/using-mturk/index.html#may-22",
    "title": "Using MTurk",
    "section": "May 22",
    "text": "May 22\nSo this happened:\n\n\nThank you, MTurker, for pointing out that my consent form says the software I ask you download \"will be harmful to your computer.\" #Typo\n\n— Joey Stanley (@joey_stan) May 22, 2017"
  },
  {
    "objectID": "blog/using-mturk/index.html#june-9",
    "href": "blog/using-mturk/index.html#june-9",
    "title": "Using MTurk",
    "section": "June 9",
    "text": "June 9\nOkay, so several weeks have passed, and the data collection phase is drawing to a close. In just a couple of weeks, I was able to get data from almost 200 people. I had some major time constraints on how I could use my money, so I had to find ways to use it quicker. I ended up creating an entirely new task, similar to the first one, with a whole new batch of sentences and words for people to read. A large portion of my participants returned to do the second part, meaning I have around 30 minutes of audio from almost 100 people.\nThis is an incredible dataset I’ve collected. I don’t know how much audio I have total yet, but it’s well over 50 hours. That’s pretty good for just three weeks.\nHowever, I will be the first to say that it was a rough three weeks. It seems like every hour I was getting data emailed to me, and several times a day I had to sit and catalogue the recordings and speaker metadata, while managing the MTurk tasks. Most of the time, it was relatively straightforward, but some participants needed a little extra attention because of technical difficulties, glitches in the system, or complaints here and there. Luckily, I did this when I wasn’t in classes, because otherwise it would have been impossible."
  },
  {
    "objectID": "blog/using-mturk/index.html#june-20",
    "href": "blog/using-mturk/index.html#june-20",
    "title": "Using MTurk",
    "section": "June 20",
    "text": "June 20\nAt last, my data collection has drawn to a close. I ended up with about 212 speakers and 84 hours of data. Not bad. Now comes the daunting task of processing all of this. For every person, if I just want to do a small task that only takes a minute, it’ll take over 3 hours to do it for all speakers! This will take a very long time for me to get through, but from the 2% that I’ve looked at so far, it’s going to be very fruitful corpus."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html",
    "href": "blog/ten-years-of-linguistics/index.html",
    "title": "10 Years of Linguistics",
    "section": "",
    "text": "On this day, ten years ago, I decided to major in linguistics. Today, I’m an assistant professor. To celebrate this decade of linguistics, I thought I’d write a little bit about where I came from and how I came to the decision to go into linguistics."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#music",
    "href": "blog/ten-years-of-linguistics/index.html#music",
    "title": "10 Years of Linguistics",
    "section": "Music",
    "text": "Music\n\nGrowing up, I was a total band geek. I’ll spare you the details, but I took piano lessons when I was eight, started saxophone in 6th grade band, hopped around to pretty much all the instruments I could for a few years, and finally settled on bass trombone junior year of high school. I wasn’t bad either: I made the district band most years (on trombone plus a couple other instruments) and even the Missouri All-State band my senior year.If you peek into the code of this webpage on Github, there’s a 1200-word summary of my band geekiness and musical background.\nWhen I started my freshman year at BYU, I auditioned to get into the Brass Performance major, but I didn’t get accepted. So I was officially a “pre-music major”, or rather, the “I-haven’t-realized-I’m-not-cut-out-for-this-yet major.” That gave me a year to knock out most of my general education courses though, which was nice.\nI had a fun music hobby though. I’d find movie scores or other songs from the library and I’d arrange them for piano ensembles (like a six-person, two-piano arrangement of a piece from Star Wars). I had been doing that since high school, and BYU seems to have a disproportionate number of people who can play piano, so it was a cinch to find a handful of good sight-readers to play with me. So at the recommendation of a music faculty member I met with one time, I auditioned a second time for the music school, but this time it was to be a Media Music Studies major. And I was accepted!\nBut, I was 19 years old, and the important thing to do was to go on a full-time, two-year mission. So I took time off from school, knowing my spot in the music school would be waiting for me. I’m so grateful for this break though because without it I would have just barreled through my music major. But the time off gave me the chance to stop and figure out what I was doing with my life."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#early-signs-that-i-wanted-to-be-a-linguist",
    "href": "blog/ten-years-of-linguistics/index.html#early-signs-that-i-wanted-to-be-a-linguist",
    "title": "10 Years of Linguistics",
    "section": "Early signs that I wanted to be a linguist",
    "text": "Early signs that I wanted to be a linguist\nUntil May 2008, linguistics was not on my radar at all. Like, I didn’t even know what it was. I didn’t even take any foreign language courses in high school.\nThere were a few signs though. The one I remember most was from when I was in a play my freshman year of high school. I didn’t have a big role, so I had to kill a lot of time in the drama room while the others rehearsed. I was flipping through one of the textbooks and I saw this chart with what’s called the International Phonetic Alphabet. My brother, who had taken a couple theater classes, had mentioned the IPA to me a few years prior. He described it as basically if you can transcribe it well and read it well, you can use it to, in theory, speak in any accent. I remember thinking it was so cool so I copied down all the symbols from that book.\n\n\n\nI copied this IPA guide from a theater book in 2003!\n\n\nAnyway, it was in May of 2008 that I got the call that I’d be a missionary in Brazil. Meaning I’d have to learn Portuguese. Even though I would be spending the first two months of my mission in an intensive language school, I went ahead and tried learning as much as I could beforehand. And it was then that I realized that learning languages was pretty cool! I finally saw the IPA in action, and was able to use it to learn the sounds. But I remember it blew my mind that, for hundreds of millions of people, “house blue” sounds totally normal and “blue house” sounds totally wrong. Blew. My. Mind.\nSo even before I had left for my mission, I had already been thinking about linguistics. I had even added to my Facebook profile was that I was considering a minor in linguistics. So, getting called to a foreign mission is what put linguistics on my radar for sure."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#my-time-in-brazil",
    "href": "blog/ten-years-of-linguistics/index.html#my-time-in-brazil",
    "title": "10 Years of Linguistics",
    "section": "My time in Brazil",
    "text": "My time in Brazil\nSo I went to Brazil and served my mission. While I was down there, I really enjoyed learning Portuguese. I practiced vocabulary like crazy and studied as much grammar as I could. Some of the other missionaries joked that if anyone needed to talk to a lawyer about the gospel that they should call Elder Stanley because he’s the only one that could understand him. After a year and a half or so, I could convince people that I was Brazilian (my darker complexion helped there)—not a local Brazilian, mind you, but I’d tell them I was from another part of the country. I would get phone calls from other missionaries—native Portuguese-speaking Brazilians!—asking about some nuanced aspect of grammar. It was fun.\nYou may know that Mormon missionaries have pretty strict rules about what they can and can’t do. At the time, the internet was completely off-limits except to email our parents once a week. Well, I would occasionally sneak on to Wikipedia and look up linguistics pages and print them out and stuff. Of all the ways to rebel, I think looking up IPA symbols was a pretty tame way to do so.\nAt one point, I was in a city relatively close to Paraguay and would occasionally run into Guaraní speakers. I wrote to my parents about the language, and my dad sent me a Guaraní translation of the Book of Mormon and encouraged me to learn as much of the language as I could. I also met someone who had what was basically a “Teach Yourself Guaraní” textbook (written in Spanish, so I had to quickly learn to read some basic Spanish) so I used that to learn some of the morphology. So in the little free time I had, I spent it trying to learn Guaraní.In retrospect, I’m not sure if they spoke Paraguayan Guaraní because they were Brazilians, but I wonder if they spoke some other Tupí language in that part of the country.\nTowards the end of my mission, I served in a college town and my companion and I went to the university’s bookstore. I bought a Portuguese Phonetics and Phonology textbook and really had fun reading that. I learned about minimal pairs and basic phonological distributions, and I especially enjoyed reading the dialectal variation that it mentioned. During my last month or so, I bought a comprehensive grammar book, one that was written entirely in Portuguese and was meant for Brazilian university students. In fact, my mission president saw me reading it one time and he’s like, “Elder Stanley, aren’t you going home in like three weeks? Why are you reading that?” Why not? It was fascinating!It was Thaïs Cristóforo Silva’s 2007 textbook, Fonetica e Fonologia do Portugues."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#realizing-music-wasnt-for-me",
    "href": "blog/ten-years-of-linguistics/index.html#realizing-music-wasnt-for-me",
    "title": "10 Years of Linguistics",
    "section": "Realizing music wasn’t for me",
    "text": "Realizing music wasn’t for me\n\nOctober–December 2010\nI got home from my mission in October so I had a few months before the next semester of school started. My plans hadn’t changed yet: I wanted to be the next John Williams so my mind was set on Media Music Studies. But I knew I also wanted to at least minor in linguistics, if not double-major.\nBecause I was starting in January, I couldn’t start all the the theory and other core classes with the other first-year students because those were only offered in the fall. So I had to sort of fill my semester with fluff. I was able to sign up for the Songwriting class though, which was kinda like the intro to the Media Music Studies major. And since I had a time slot available, I went ahead and signed up for Intro to Linguistics. I also signed up for Acoustics for Music and Speech, band, a non-audition choir, and private bass trombone lessons. So about as much music as I could do without those core classes.\nMy family visited Utah soon after I got back so I took the opportunity to meet with a linguistics advisor, just to see what classes they’d recommend. It was Alan Melby that I met with, and he recommended I minor in Linguistics Computing. I thought that was a pretty good idea, so I went ahead and signed up for an Intro to Linguistic Computing class as well.Now that I’m on the faculty side of the department, I can see why he pushed the minor: enrollment was low and they were struggling to keep the minor!\nIn the meantime, I worked for my dad and spent my free time getting the right equipment and software for my music studies. But I also spent a lot of time studying linguistics. Mostly looking at Wikipedia and other resources online, including lectures that I could listen to while driving. So even though I didn’t know linguistics would eventually be my major (and career), I was already investing a lot of time into learning it and had a decent grasp of a lot of basic topics.\n\n\nWednesday, January 5th, 2011\nFirst week of classes comes and I walk into my songwriting class full of confidence. This was going to be the first day of the rest of my life[Although, as my brothers point out, this is technically true every day :)]{.aside} That class turned out to be a pivotal moment like I had anticipated, it just wasn’t quite pivotal moment I was expecting.\nAfter going over the syllabus, we learn that the final project was going to be to write and record a pop song. Uh-oh. I don’t listen to pop music. We got a homework assignment that day too: submit the names of three pop artists you think most closely resemble your own style of music. Ummm… what? I was there to learn to write movie music, not learn about pop singers. I had just barely gotten back to the country, so I literally couldn’t even name three artists on the radio[I’d struggle with that today, to be fair.]{.aside} Pop music wasn’t my thing. But the assignment had to be pop music.\nFunnily enough, my Intro to Linguistics class was right after that. And I freaking loved it. I wrote in my journal that night that I was already considering changing majors. I think I had known for a long time—really deep down—that I wasn’t cut out for music. This experience in my Songwriting class was just what I needed to come to that realization.\n\n\nThursday, January 6th, 2011\nThe next day, I wrote to my songwriting professor expressing my concern. He said something along the lines of this: “Too bad. I accepted you into this major as a favor to a friend. The odds of you making it as a film score composer are basically zero. Either you expand your horizons or you’re not getting anywhere in the music world. Pop music is where the jobs are so if you can’t keep up, you’re in the wrong major.”\nOuch.\nSince middle school, I’d thought of nothing but music and for two years in Brazil, I eagerly anticipated the day when I’d finally start my music classes. And literally in the first hour of the first one, I get a slap in the face, a reality check, and a rude awakening to the fact that I was not going to have a career in music. What was my life for then? Was all that music a waste?"
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#switching-to-linguistics",
    "href": "blog/ten-years-of-linguistics/index.html#switching-to-linguistics",
    "title": "10 Years of Linguistics",
    "section": "Switching to linguistics",
    "text": "Switching to linguistics\n\nFriday, January 7th, 2011\nI spent several hours that evening looking through classes and figuring out what I was going to do. I considered switching to just a general Music major, but now that the rose-colored glasses were off, it occurred to me that the classes that looked the most fun (like orchestration and score analysis) were only possible after three or four long years of coursework that was not very fun-sounding. I’d have to slog through years of alone time in the practice room and classes I didn’t want to take just to finally get to those fun ones at the very end.\nMeanwhile, after taking a closer look at the courses in the linguistics major, I realized that they all sounded really fun! Phonetics? Phonology? Morphology? Sociolinguistics? Language documentation? Sign me up!\n\n\nSaturday, January 8th, 2011\nAt this point, I was learning towards a double major in (general) music and linguistics, but I was weighing my options. To help me out, I made some charts to see how my semesters would be spent, credit-hour wise. According to my journal, 65% of my time would be spent in music classes. I had already decided my career wouldn’t be in music at that point, so that’d be a lot of time spent doing something that wasn’t going to lead me anywhere.…and this offers a peek into how my mind works and was a early sign I’d do a lot of quantitative work\nSo at that point, the decision was clear. If all these music classes sounded lame and all the linguistics classes sounded fun, what was stopping me from switching to linguistics?\n\n\nSunday, January 9th, 2011\nMy parents have always been extremely supportive of everything I do. They were in the loop on all the developments up to that point, but that afternoon, I Skyped with them to hash a few things out. As expected, they were just as shocked as I was that I was considering switching, but still extremely supportive.\nI don’t recall exactly how it all went down, but I know that by the end of that conversation with my parents, my mind was made up: I was going to major in linguistics and minor in linguistics computing.\nIt’s pretty interesting what my future plans were at that time. I had these grand plans of learning lots of languages (Mandarin, Arabic, and potentially Hebrew), minoring in TESOL, and teaching abroad somewhere. I was already considering a Master’s program (probably based on conversations with my Intro to Linguistics professor). None of that really panned out, but at that point, a PhD and academia were not in the picture.I did end up taking two semesters of Mandarin the next school year. Read more about the languages I’ve learned here.\nI thought it was interesting that I wrote in my journal how much I was looking forward to the Varieties of English class. When I did finally take it a couple semesters later, I was stoked (and it did not disappoint)! Little did I know I’d grow up to be a dialectologist and that I’d be teaching that very course in 10 years’ time. In fact, I taught it in the very same classroom where my Intro to Linguistics class was!\nSo, at 11:00 on Wednesday the 5th, I was still confident that I’d be a music major. By the evening of Sunday the 9th, I had made up my mind to major in linguistics and was emotionally ready to abandon music studies. It was a tough five days, but that’s all it took. I don’t regret doing music or being in band or learning trombone at all. I still get opportunities to play piano (I’m playing church this Sunday) and I’m still doing arrangements of movie music (I’m working on Jurassic Park right now). But I am soooo glad I didn’t major in it."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#starting-linguistics",
    "href": "blog/ten-years-of-linguistics/index.html#starting-linguistics",
    "title": "10 Years of Linguistics",
    "section": "Starting linguistics",
    "text": "Starting linguistics\nI met with a humanities advisor a few days later and showed her my pie charts and she basically said she had nothing to say because it’s clear my mind was made up already. I don’t know when I officially made the switch according to the university systems, but it must not have been much later because soon after that I was calling myself a linguistics major in my journal.\nThe advisor also recommended I do a study abroad. So the next day, I was looking through potential study abroad programs and found one that went to Ecuador to study Pastaza Kichwa with Dr. Janis Nuckolls. I’ve told Janis this sense then, but that study abroad was what set me into motion to get a PhD and ultimately go into academia. Being there in the field doing rigorous linguistic documentation was a total blast.\nLater, she invited me to join her research team, which ultimately led me to a presentation at SSILA and attending my first LSA conference in Boston in 2013. I attended as many sessions as I could, including many in the American Dialect Society meetings. And that’s when I caught the bug. I knew then that I had to do my own research go to more conferences. I had already applied to grad schools at that point, but it was that conference that gave me the determination to present at conferences my first year of grad school (and many times since then).\nI’m a little fuzzy on the details about deciding to do a PhD and figuring out what my research focus would be. I know I mentioned to my mission president during my closing interview with him that I was considering a PhD, but it didn’t seem like it was on my radar when I switched to linguistics. I did apply to PhD programs though so it must have been that study abroad that sent me that direction. As far as my research focus, I started off wanted to do language documentation, but at some point I decided on sociolinguistics and, more specifically, dialectology. I’ll have to continue reading my journal and seeing if I can pinpoint exact dates for those too.I do have specifics about when I decided on my dissertation topic. Maybe I’ll do a blog post about that."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#conclusion",
    "href": "blog/ten-years-of-linguistics/index.html#conclusion",
    "title": "10 Years of Linguistics",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s it. Ten years ago today is when I decided to major in linguistics. After one difficult homework assignment (that I never finished by the way) and a strongly worded reality check from a professor, it took just five days to abandon the previous decade’s worth of plans to major in music. I look forward to another decade of linguistics and to see where this career will take me!"
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "",
    "text": "In English sociolinguistics, you’ll often see vowel phonemes represented by a single word in small caps. For example, trap represents /æ/. However, in a lot of American dialectology papers, you’ll see authors use the label bat instead. In this post, I explain why I think these competing labels are used… and why I prefer trap over bat.\nSee also: “Thoughts on allophonic extensions to Wells’ lexical sets.”"
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html#wells-lexical-sets",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html#wells-lexical-sets",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "Wells Lexical Sets",
    "text": "Wells Lexical Sets\nAs it turns out the trap label came first. In fact, trap is just one in a set of 24 labels, one for each English vowel. The creator of this lexical set is John C. Wells, who established them in his 1982 three-volume series, Accents of English. In the preface of each volume, Wells explains a notation system that has since been called the “Wells Lexical Sets.” Because it is brief, I’ll quote it in its entirety (bold and small caps in original):\n\nWords written in capitals\n\n\nThroughout the work, use is made of the concept of standard lexical sets. These enable one to refer concisely to large groups of words which tend to share the same vowel, and to the vowel which they share. They are based on the vowel correspondences which apply between British Received Pronunciation and (a variety of) General American, and make use of keywords intended to be unmistakable no matter what accent one says them in. Thus ‘the kit words’ refers to ‘ship, bridge, milk…’; ‘the kit vowel’ refers to the vowel these words have (in most accents, /ɪ/); both may just be referred to as kit.\n\nWells then provides this table:\n\nTable 1: Wells’ original lexical sets. From Wells (1982:xviii–xix).\n\n\n\n\n\n\n\n\n\nRP\nGenAm\n\n\n\n\n\n\n\nɪ\nɪ\n1.\nkit\nship, sick, bridge, milk, myth, busy…\n\n\ne\nɛ\n2.\ndress\nstep, neck, edge, shelf, friend, ready…\n\n\næ\næ\n3.\ntrap\ntap, back, badge, scalp, hand, cancel…\n\n\nɒ\nɑ\n4.\nlot\nstop, sock, dodge, romp, possible, quality…\n\n\nʌ\nʌ\n5.\nstrut\ncup, suck, budge, pulse, trunk, blood…\n\n\nʊ\nʊ\n6.\nfoot\nput, bush, full, good, look, wolf…\n\n\nɑː\næ\n7.\nbath\nstaff, brass, ask, dance, sample, calf…\n\n\nɒ\nɔ\n8.\ncloth\ncough, broth, cross, long, Boston…\n\n\nɜː\nɜr\n9.\nnurse\nhurt, lurk, urge, burst, jerk, term…\n\n\niː\nu\n10.\nfleece\ncreep, speak, leave, feel, key, people…\n\n\neɪ\neɪ\n11.\nface\ntape, cake, raid, veil, steak, day…\n\n\nɑː\nɑ\n12.\npalm\npsalm, father, bra, spa, lager…\n\n\nɔː\nɔ\n13.\nthought\ntaught, sauce, hawk, jaw, broad…\n\n\nəʊ\no\n14.\ngoat\nsoap, joke, home, know, so, roll…\n\n\nuː\nu\n15.\ngoose\nloop, shoot, tomb, mute, huge, view...\n\n\naɪ\naɪ\n16.\nprice\nripe, write, arrive, high, try, buy…\n\n\nɔɪ\nɔɪ\n17.\nchoice\nadroit, noise, join, toy, royal…\n\n\naʊ\naʊ\n18.\nmouth\nout, house, loud, count, crowd, cow…\n\n\nɪə\nɪ(r\n19.\nnear\nbeer, sincere, fear, beard, serum…\n\n\nɛə\nɛ(r\n20\nsquare\ncare, fair, pear, where, scarce, vary…\n\n\nɑː\nɑ(r\n21\nstart\nfar, sharp, bark, carve, farm, heart…\n\n\nɔː\nɔ(r\n22\nnorth\nfor, war, short, scorch, born warm…\n\n\nɔː\no(r\n23\nforce\nfour, wore, sport, porch, borne, story…\n\n\nʊə\nʊ(r\n24.\ncure\npoor, tourist, pure, plural, jury…\n\n\n\nLater on in the book (p. 122–124), Wells compares Received Pronunciation and General American English and goes into more detail about the principle behind the lexical sets:\n\nWhen we compare the pronunciation of particular words in the two accents, we find that in many respects there is a good match: for example, almost all words that have /iː/ in RP have the corresponding /i/ in GenAm, and vice versa: thus creep, sleeve, key, people and hundreds of other words. Likewise /aɪ/, transcribed identically for the two accents, and used in both cases for ripe, arrive, high, try and many other words…\n\n\nInvestigation shows that… we can successfully match the vowels in RP and GenAm forms of particular words for the vast bulk of the vocabulary…\n\n\nThis matching furnishes us with the framework of standard lexical sets which we use not only for comparing RP and GenAm but also for describing the lexical incidence of vowels in all the many accents we consider in this work. It turns out that for vowels in strong (stressed or stressable) syllables there are twenty-four matching pairs of RP and GenAm vowels. We identify each pair, and each standard lexical set of words whose stressed syllable exhibits the correspondence in question, by a keyword, which we shall always write in small capitals. Thus the correspondence between RP /iː/ and GenAm /i/ is the basis for the standard lexical set fleece…\n\n\nIn the rest of this work standard lexical set keywords will also be used to refer to (i) any or all of the words belonging to the standard lexical set in question; and (ii) the vowel sound used for the standard lexical set in question in the accent under consideration. Rather than using expressions such as ’short i/ for example, we shall speak of the kit vowel or simply of kit.\n\nIf you’re unfamiliar with these labels, I encourage you to look at Wells’ book. He explains each of these lexical sets in greater detail on pages 122–168 of Volume I."
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html#an-alternative-lexical-set-the-b_t-frame",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html#an-alternative-lexical-set-the-b_t-frame",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "An Alternative Lexical Set: the b_t frame",
    "text": "An Alternative Lexical Set: the b_t frame\nThe Wells sets are very useful, but for some reason, they have not become adopted universally. Several researchers have opted to use an alternative set of labels that take advantage of a large minimal set in English, the b_t frame.\nI did some digging in old American Speech and volumes of the Publications of the American Dialect Society to see when these labels were first used. The earliest instance I could find goes all the way back to  Sumner Ives’ 1954 study called The Phonology of the Uncle Remus Stories, which was the 22nd volume in the PADS series. On page 6, the author states that the following words are to refer to English vowels: beet, bit, bait, bet, bat, not, bought, boat, put, boot, but, curt, bite, bout, boy, and above. This set is remarkably close to what some researchers use today!Sumner Ives. 1954. The Phonology of the Uncle Remus Stories. Publication of the American Dialect Society 22(1): 3–59. https://doi.org/10.1215/-22-1-3\n In contemporary sociolinguistics, I believe the b_t frame was popularized by Erik Thomas & Malcah Yaeger-Dror’s 2009 edited volume, African American English Speakers and Their Participation in Local Sound Changes: A Comparative Study. In the introduction, starting on page 8 and spilling into page 9, they say that theyErik Thomas & Malcah Yaeger-Dror, eds. 2009. African American English Speakers and Their Participation in Local Sound Changes: A Comparative Study. Publication of the American Dialect Society 94. Available here.\n\n…found that it would be helpful to formulate a convention to unify the text and simplify the reader’s task; with that thought in mind, we have suggested that authors use neither a phonological / / nor a variable ( ) presentation, both of which differ in conventions from author to author. We have chosen instead to refer to a given vowel class using keywords, following the principle behind Wells (1982). To further simplify, we turned to Ladefoged’s (2005) choice of keyword paradigm, which uses words that are as untrammeled by their consonantal environment as possible. To obtain these keywords, he chose an h_d frame, to have his speakers “say heed again.\n\n\nTo minimize the need for varying the “carrier” environment, in each case, the vowel being focused on here will be a b_t paradigm.\n\n\nTable 2: The lexical set based on the b_t frame. This is a subset of the table by Thomas & Yaeger-Dror (2009:6).\n\n\nIPA\nKeyword\n\n\n\n\n/i/\nbeet\n\n\n/ɪ/\nbit\n\n\n/e/\nbait\n\n\n/ɛ/\nbet\n\n\n/æ/\nbat\n\n\n/ɑ/\nbot\n\n\n/ɔ/\nbought\n\n\n/o/\nboat\n\n\n/ʌ/\nbut\n\n\n/ʊ/\nbook\n\n\n/u/\nboot\n\n\n/aɪ/\nbite\n\n\n/aʊ/\nbout\n\n\n/ɔɪ/\nboy\n\n\n/ɚ/\nbird\n\n\n\n\nThey end with this statement:\n\nWe hope that this convention will permit the reader to follow all the authors without difficult transitioning between chapters.\n\n It appears that their goal for continuity has beyond their volume because the set was used in later volumes of the Publications of the American Dialect Society. For instance, here are the remarks by the editors of Speech in the Western States: Volume 1: The Coastal States (Fridland et al. 2016):Valerie Fridland, Tyler Kendall, Betsy E. Evans, & Alicia Beckford Wassink, eds. 2016. Speech in the Western States, Vol 1., The Coastal States. Publication of the American Dialect Society 101. Available here.\nThe description in Speech in the Speech in the Western States: Volume 2: The Mountain West (Fridland et al. 2017) is almost identical.\n\nFor the purpose of clarity and continuity, authors use the conventions of the International Phonetic Alphabet throughout the chapters, though, in many cases, keywords in the b_t frame are used to highlight particular word classes and subclasses, following other recent PADS volumes (Thomas & Yaeger-Dror 2009). These frames are built upon those made for comparative study of English dialects by Wells (1982) but have been adapted to allow representation of the particular vowel changes and conditioning environments of interest to the present study of the U.S. West.\n\n\nTable 3: The lexical set used in the Speech in the Western States volumes. From Fridland et al. 2016:3 and Fridland et al. 2016:5; Table 1.1 in both. The columns have been rearranged for consistency within this blog post.\n\n\nIPA\nWells Keyword\nb_t Keyword\n\n\n\n\nɪ\nkit\nbit\n\n\nɛ\ndress\nbet\n\n\næ\ntrap\nbat\n\n\nɑ ~ a\nlot\nbot\n\n\nɔ ~ a\ncloth/thought\nbought\n\n\nʌ\nstrut\nbut\n\n\nʊ\nfoot\nbook\n\n\nɚ\nnurse\nburt\n\n\ni\nfleece\nbeet\n\n\ne\nface\nbait\n\n\no\ngoat\nboat\n\n\nu\ngoose\nboot\n\n\naɪ\nprice\nbite\n\n\nɔɪ\nchoice\nboy\n\n\naʊ\nmouth\nbout\n\n\nɪɹ ~ iɹ\nnear\nbeer\n\n\nɚɹ\nsquare\nbare\n\n\nɑɹ\nstart\nbar\n\n\nɔr / or\nnorth/force\nbore\n\n\nʊɹ\ncure\nburr\n\n\nəɹ\nletter\n\n\n\nə\ncomma\n\n\n\n\nThe only difference is that bird was changed to burt.\nOutside of the PADS volumes, this frame was also used in McCarthy (2011), which explicitly states that these labels were used because of Thomas & Yaeger-Dror (2009).\nI don’t know the reason why the b_t frame was designed when the Wells lexical sets were already established. Perhaps the draw of the nearly complete minimal set to contrast all English vowels was useful. Maybe it’s because the words in the b_t frame are shorter, which makes for less cluttered visualizations and written prose. Thomas & Yaeger-Dror did say that the use of the consonants /b/ and /t/ in the keywords helped reduce the effects of surrounding consonants on the vowels themselves. Ultimately though, I’m not sure."
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html#my-thoughts-on-the-b_t-frame",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html#my-thoughts-on-the-b_t-frame",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "My thoughts on the b_t frame",
    "text": "My thoughts on the b_t frame\nHot take: I don’t think this the b_t is any more useful than IPA. Hear me out:\nImagine you’re at a conference talking about the low back merger and you yourself have the merger. Since you don’t naturally differentiate the words bot and bought, you struggle to explain the differences that may exist in your study. Even if you do distinguish the sounds, many people in your audience may not, so they’ll have a hard time understanding. This was one of the reasons Wells came up with his lexical sets: neither you nor your audience would have much difficulty understanding the words lot or thought since, as far as I know, /lɔt/ and /θɑt/ are not English words.\nEven if you’re not talking about a merger, if you are someone with particularly shifted vowels, when you say an isolated, ambiguous token like [bɛ̞t] or [bæ̙t], it may not be immediately clear to listeners of other dialects which vowel you’re talking about.\nThe words in the b_t frame may be “untrammeled by their consonantal environment,” but I don’t know if the lack of transition formants make for the most effective label in speech or writing. Keywords are labels to refer to large lexical sets, so while they may not make for ideal tokens when collecting phonetic data, they need to still serve the purpose of unambiguously identifying a vowel phoneme.\nIn fact, Wells specifically designed his original set so that it specifically would not use the b_t frame:\n\nThe keywords have been chosen in such a way that clarity is maximized: whatever accent of English they are spoken in, they can hardly be mistaken for other words. Although fleece is not the commonest of words, it cannot be mistaken for a word with some other vowel; whereas beat, say, if we had chosen it instead, would have been subject to the drawback that one man’s pronunciation of beat may sound like another’s pronunciation of bait or bit. (Wells 1982:123)\n\nI question the usefulness of the b_t frame. It’s convenient that a common enough English word can be created by filling in almost any vowel into the template, but I don’t know if this large minimal set makes for the most unambiguous lexical set. When he introduces his keywords, Wells says that they are “intended to be unmistakable no matter what accent one says them in.” This property is not retained in the b_t set of keywords. In fact, creating a set based on minimal pairs defeats the very purpose of a lexical set.\nTo put it another way, just because we call something the cot-caught merger doesn’t mean we should refer to the entire lexical sets as cot and caught. In fact, I think we should actively avoid referring to them as cot and caught for the very reason that they do form a minimal pair.\nIt’s my impression that researchers on non-American varieties of English use Wells’ original lexical sets without any problems. Using a different set is potentially confusing and may alienate ourselves from studies on other World Englishes.\nTo be clear, I am in no way criticizing the researchers who came up with or use the b_t frame. If you’re familiar with what I do you’ll know that their work is highly relevant to my own studies, and I cite a lot of studies that use the b_t frame. Their work is excellent and I model my own work after their theirs. I just think the labels could be clearer."
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html#conclusion",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html#conclusion",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "Conclusion",
    "text": "Conclusion\nThis reminds me of how some people use the Labovian transcription system instead of standard IPA. See Josef Fruehwald’s blog post on those. There are two competing systems of lexical sets being used in American dialectology: the Wells lexical set and the b_t frame. To answer my titular question of why people use bat instead of trap… I don’t really know. I think it may largely depend on what university the work is coming out of. But, I think Wells’ original set may be a little better.\nPS: Regardless of which system you use, I think we should make sure we use small caps instead of ALL CAPS or even Capitalized Small Caps. It’s truer to Wells’ original notation and I think they just look a lot better typographically.\nPPS: To my knowledge, Wells never intended for the lexical set labels (or even the example words in the original explanation) to be ideal tokens for eliciting the vowels they represent. So, while the b_t labels might be “untrammeled by their consonantal environment,” the Wells labels are not. So, there’s probably no need to eliciting the words fleece, kit, face, dress, etc. when getting tokens of these vowels.\n\nUpdate: Click here for further musings, ramblings, and recommendations for non-canonical extensions to Wells’ lexical sets when referring to allophones."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html",
    "title": "New publication in the latest PADS volume",
    "section": "",
    "text": "This week I finally got to lay my hands on a physical copy of my latest publication! It’s called “The Absence of a Religiolect among Latter-day Saints in Southwest Washington” and it’s in the latest Publication of the American Dialect Society, Speech in the Western States Volume III: Understudied Dialects by Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall. The physical copy was delivered to my office about two weeks ago, but my wife and daughter had just tested positive for covid-19 (they’re fine—very mild symptoms) so I was only just now able to see it now that my two-week quarantine is over."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html#a-brief-bit-of-background",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html#a-brief-bit-of-background",
    "title": "New publication in the latest PADS volume",
    "section": "A brief bit of background",
    "text": "A brief bit of background\nI’m so excited to make it into this PADS volume! Just to give a brief timeline, I decided that I wanted to study English in the West around January 2015, after attending the American Dialect Society annual meeting in Portland and seeing the great talks on language in the West there. I did as much background and reading as I could over the next year or so and put together a grant proposal to go do fieldwork in southwest Washington. So when the first volume came out in January 2016, I was thrilled to see the latest research and to know that my research idea was a hot topic in American dialectology. That first volume, which covered California, Oregon, and Washington was extremely important in helping me shape my own research.\nIn 2016, my research in the West began in earnest. I got my grant to go do fieldwork and spend June and July in Cowlitz County. I had processed enough of the wordlist data in time to submit a paper to the American Dialect Society annual meeting. I was excited because it was my first big conference and was my introduction to the field as a dialectologist. Around that time Speech in the Western States Volume II came out, which focused on the Mountain West and included chapters on Arizona, New Mexico, Nevada, Utah, Colorado, and Montana. I was bummed I wasn’t able to make it into that volume, but I really didn’t have anything relevant to that area at the time and my research was in the beginning stages still.Being excited about this meeting was the first thing I blogged about!\nSo at that point, I figured I had missed the train. I hopped on the West bandwagon just a year or so too late—soon enough to really benefit from the current research, but not soon enough to be a part of that first conversation. Nevertheless, my research continued.\nI had finished processing my Washington data and was working on my dissertation when I heard about a third volume of Speech in the Western States, this time focusing not on any geographic area within the West but rather on understudied communities. While I didn’t have any data from ethnic minorities from Washington, it did occur to me that I had a nice balance of members of the Church of Jesus Christ of Latter-day Saints to non-members. So I put together an abstract that compares the two groups, sent it to the editors, and the rest is history.\nSo I am just thrilled to be a part of this last volume on Speech in the Western States. The first two volumes were so important as my research was developing and so it just made it that much sweeter to be a part of the last volume."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html#a-brief-summary-of-the-chapter",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html#a-brief-summary-of-the-chapter",
    "title": "New publication in the latest PADS volume",
    "section": "A brief summary of the chapter",
    "text": "A brief summary of the chapter\nThe question I had was this: do Latter-day Saints in southwest Washington sound different than non–Latter-day Saints? In other words, do they have a religiolect? I thought they might since they do in Utah and in Southern Alberta. Specifically, they tend to have more (or more exaggerated) Utah English features and tend to lag behind regional language changes. I thought I’d test for that same thing in Washington since I had a nice, balanced sample of Latter-day Saints to non–Latter-day Saints.\nSo, using fancy statistics and looking at vowels known to be variable in Utah and Washington, I conclude that there was very little difference between the two groups. So, a null result. The question then is this, why not? Why don’t Washingtonian Latter-day Saints have a religiolect while Albertan and Utahn Latter-day Saints do? I got into a lot of detail on Latter-day Saint culture and history and eventually conclude that they’re just too small of a minority, aren’t locally salient enough, and aren’t as entrenched in Latter-day Saint culture for a religiolect to have developed."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html#lots-of-qualitative-analysis",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html#lots-of-qualitative-analysis",
    "title": "New publication in the latest PADS volume",
    "section": "Lots of qualitative analysis",
    "text": "Lots of qualitative analysis\nOverall the paper is dense. The whole chapter is 28 PDF pages, and a quarter of that is just references! I tend to be a little citation-heavy in all my writing to be honest, but over 100 references in this otherwise relatively short paper might have been a little much… But I had a lot of things to bring in: Utah English, Washington English, Latter-day Saints, religiolect, and GAMMs.\nYou may notice that the paper is and is a bit very top-heavy as well. If you just take the 6,962 words of actual body text, 2,967 of them (43%) is background and lit review. I go into a lot of detail on Utah English and Washington English, not to mention detail on Latter-day Saint culture and history. I then have 2,098 words (30%) of methods, which was also necessary since GAMMs can take a while to explain. I even had to leave out a lot of detail and refer readers to my dissertation! So you don’t even get to the linguistic analysis until three-quarters of the way into the paper!\nThe actual results are only 982 words (14%), but I think I made some good visualizations that explain things better than words do. Besides, it’s a null-result paper, so there’s not a lot to say other than, “nope and this isn’t significant either.” Finally, I finish with another 915 words (13%) of discussion and conclusion, which again go into detail about Latter-day Saint culture.In fact, a stylized version of one of my plots made its way to the cover of the book!\nAnd, on top of all this this is three pages of notes, some of which are pretty long, an appendix with more visuals, and online supplementary material. So, a lot of background for what turns out to be relatively little actual phonetic analysis, but I think all that background was important to really appreciate the overall message of the paper. I think it strikes a nice balance of qualitative and quantitative work."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html#my-favorite-parts",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html#my-favorite-parts",
    "title": "New publication in the latest PADS volume",
    "section": "My favorite parts",
    "text": "My favorite parts\nOne thing I’m particularly pleased with in that paper is the map on page 96. This one map shows a lot of information all at once, pulled together from a few different sources. First, it shows state and county boundaries, which come from publicly available sources. Those counties are then colored by number of Latter-day Saints per 1000 residents, which was data I had to pull from the US Religion Census. Finally, there are the outlines of the “Mormon Culture Region,” which was defined in Meinig (1965). For that, I had to scan in the original map and basically trace the original boundaries onto the digital map. I’m quite pleased with how it turned out, and the publishing editors did a fantastic job at sprucing it up so that it matches the look and feel of American Speech articles.\n\nAnother thing that this paper introduces is a method of outlier detection that I’ve been working on for a few years. I’ll give more detail on a future blog post, and perhaps a publication all on its own, but for now, you can read the details in page 102.\nFinally, I quite like deep dives that I took in some of the footnotes. In particular, Note 2 gives a brief history of the Church of Jesus Christ of Latter-day Saints in Cowlitz County. This was data that was also pulled from several sources. I had to ask my mother-in-law if she knew any old-timers who might be able to fill in some of the gaps, which was very helpful. But my friend Jonathan Hepworth, a history PhD student at the University of Georgia, was the most help. He was able to track down some primary sources that helped fill in some of the details (like the date of creation of the Longview Stake)—and he did so in like an hour! I’m amazed at the resources that historians have access to.\nAnyway, so that’s it for this paper."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html",
    "title": "The stories behind the languages I’ve studied",
    "section": "",
    "text": "There’s a trend on linguistics Twitter right now where people are sharing the ages that they started studying/learning whatever languages they know. As I thought about the ones I know, I realized that there’s a story behind each one, and like many linguists, I have a list of languages and a few-word summary of their fluency at the bottom of my CV. But the stories are much more than I’d like to explain on my CV or in a tweet. So, here are some of those stories behind the languages I’ve studied."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#english",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#english",
    "title": "The stories behind the languages I’ve studied",
    "section": "English",
    "text": "English\nMy first and, for a long time, only language. Read more about my idiolect here."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#modern-greek",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#modern-greek",
    "title": "The stories behind the languages I’ve studied",
    "section": "(Modern) Greek",
    "text": "(Modern) Greek\nIn fourth grade, I was in a “gifted” program or whatever and we had a unit on Greek. I remember nothing."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#swahili",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#swahili",
    "title": "The stories behind the languages I’ve studied",
    "section": "Swahili",
    "text": "Swahili\nIn fifth grade, that same gifted program had a Swahili unit. Literally the only thing I remember is the titular line from the song “Head, Shoulders, Knees, and Toes”, which is kichwa, mabega, magoti na migu. Google Translate has toes as vidole, so I don’t know what migu is.\nUnrelated to that, early in grad school, probably around summer 2015 or so, I thought I’d try auditing Swahili. I sat in on one class and decided I probably didn’t have the time to commit."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#portuguese",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#portuguese",
    "title": "The stories behind the languages I’ve studied",
    "section": "Portuguese",
    "text": "Portuguese\nWhen I was 19, I started my two-year stint as a Mormon missionary in Brazil. I took some private lessons for a couple months summer 2007 from a Brazilian woman from church. When I moved there in October, I was put into an intensive language training course, which was about 9 weeks, after which I was thrust into the streets pretty much and fully immersed in the language.\nIt took a while for me to attain fluency, but I made it there eventually. I made a ton of flashcards and learned lots of words. People would joke that if anyone needed to teach a lawyer, they’d have to call me because I’m the only one that has the vocabulary! My pronunciation wasn’t bad either: I could convince people that I was from Brazil—granted, I told them I was from Amapá, a remote, rural state way up in the Amazon. No one knew anyone from there, so they were often like, “Oh, so that’s what the accent sounds like!” As I explain here, I was in a college town for a while and inadvertently bought a phonetics and phonology textbook and learned some basic linguistics concepts from there. About a month before I was finished, I bought a big ol’ grammar book, one that was intended for Brazilian university students studying Portuguese, and read the whole thing. Perhaps because of that book and my general attitude towards language learning at the time, I still have somewhat prescriptivist attitudes in Portuguese, and I really wish I hadn’t adhered so strongly to the standard language ideology because I know I heard a lot of variation while living there.\nSince being back in the US, I’ve had relatively little opportunity to use my Portuguese. I took a grammar and a literature course as an undergrad. In grad school, I attended a conversation group. I also was briefly part of a Brazil-based research team that analyzed monkey vocalizations and I had one heck of a time trying to explain statistics in Portuguese. Mormon missionaries don’t exactly need that kind of vocabulary, so that was a lot of new words.\nHow is my Portuguese now? Pretty rusty. Probably not terrible and I could survive. I’ve forgotten a lot of the nuance on how to say certain things. I listen to podcasts occasionally and I don’t understand everything, which is a little disheartening. But, again, as a missionary there are a lot of semantic domains that you don’t ever need to talk about, so it makes sense. Maybe I just need to start busting out the vocab cards again."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#guarani",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#guarani",
    "title": "The stories behind the languages I’ve studied",
    "section": "Guarani",
    "text": "Guarani\nAfter living in Brazil for about nine months (this was June-ish of 2009, right after I turned 20), I was transferred to Campo Grande where I encountered Guarani for the first time. I thought it was interesting and I thought I’d try to learn it. A friend had what was basically a “teach yourself Guarani” book that was written in Spanish—so I had to quickly figure out a bit of basic Spanish to read it. My dad also sent me a copy of the Book of Mormon in Guarani and basically said, “Eh, since you’re learning another language anyway, might as well try this one too.” So over the next few months, here and there, I learned a bit of grammar and vocabulary. This was my first time looking at a non-Indo-European language and it was fascinating to me. By the end of my time in Brazil, I had somehow managed figured out quite a bit of grammar.\nWhen I got back to BYU, I found some old books on Guarani and used them to learn more of the grammar I hadn’t figured out via translation. In Fall 2011, I also sat in on a grammar course that was intended for missionaries who had served in Paraguay speaking Guarani. I didn’t do well because I really was out of my league and because I had also signed up for Mandarin that same semester. I’ve occasionally pecked at it and even have an interactive Guarani dictionary. I even submitted an abstract to LSA on templatic morphology, but it was (rightfully) rejected. I saw that they’ve recently added a Duolingo course in Guarani, taught in Spanish, so there’s more opportunity to practice. One day, I’ll get back to studying it."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#kichwa",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#kichwa",
    "title": "The stories behind the languages I’ve studied",
    "section": "Kichwa",
    "text": "Kichwa\nAfter my intro to linguistics course, I went on a language documentation study abroad to Amazonian Ecuador in June–July 2011. There, I started to learn Ecuadorian (Lowland, Pastaza, etc) Kichwa. I never really got to the point of being fluent, but I had two semesters’ worth of coursework done, so I had, at one point, a bit of vocabulary and a decent grasp of lots of basic grammar. Nowadays, I remember virtually nothing, but when I see my colleague Janis Nuckolls talk about her research, some things come back."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#spanish",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#spanish",
    "title": "The stories behind the languages I’ve studied",
    "section": "Spanish",
    "text": "Spanish\nAt some point while in Brazil, I started to study Spanish on my own. I had to know something because that Guarani book I had was written in Spanish. I’ve never taken formal classes though, so I don’t know much. Knowing Portuguese is helpful and I tell people that my Spanish is basically Portuguese with a Spanish accent. I also have a really hard time understanding spoken Spanish, though when I was in Ecuador, I noticed that Ecuadorian Spanish was much easier. My one success story was the one time I was in Sunday School while in Ecuador and the teacher called on me to answer a question, and not only did I understand the question, but I also produced a coherent answer."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#mandarin",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#mandarin",
    "title": "The stories behind the languages I’ve studied",
    "section": "Mandarin",
    "text": "Mandarin\nWhile in Ecuador, a fellow study abroader had studied Chinese. I had two roommates who spoke Mandarin at the time too. And I was interested in learning more about tone. So I signed up for Mandarin and ended up taking two semesters of it. I enjoyed it. Except for the writing. I don’t think we advanced very far at all because everything we learned to say we had to be able to read and write too, and learning characters was a slow process. I think by the end, I knew just shy of 500 characters. What little grammar I knew though I felt pretty comfortable. Literally the only thing I remember today is 我有兩個會說中文的室友, “I have two roommates who can speak Chinese.” Except I had to look up the characters on Google Translate. Knowing Pinyin at least has been helpful for reading Chinese names."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#kiche",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#kiche",
    "title": "The stories behind the languages I’ve studied",
    "section": "K’iche’",
    "text": "K’iche’\nDuring the Winter 2012 semester of BYU, they offered a introductory K’iche’ course. I audited it and attended for a few weeks, but ultimately couldn’t keep up with it and Mandarin in the same semester. (I wish had had continued, because it would have been helpful for studying Tz’utijil later on!)"
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#tshiluba",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#tshiluba",
    "title": "The stories behind the languages I’ve studied",
    "section": "Tshiluba",
    "text": "Tshiluba\nFall of 2012, I took a Field Methods course and our informant was a native speaker of Tshiluba (aka Luba-Kasai), a Bantu language spoken in the DRC. I really enjoyed that course and spent the majority of my time documenting the TAM system."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#bolivian-quechua",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#bolivian-quechua",
    "title": "The stories behind the languages I’ve studied",
    "section": "Bolivian Quechua",
    "text": "Bolivian Quechua\nWhen I started graduate school at the Univeristy of Georgia, they had just gotten a, in my advisor Chad Howe’s words, a “crap-ton” of money to help their Quechua program. I was in the second cohort of students to take Quechua there. I took two semesters from Chad Howe. I then continued to third- and fourth-semester courses, which were taught by a native speaker, remotely from Bolivia. (Keep in mind, this was 2015–2016, long before remote learning became mainstream!) The instructor didn’t speak any English though and, as you may recall, I don’t really speak Spanish that well, so much of the class was facilitated by a TA who was a Romance Languages PhD student and year ahead of me in school. (Hi, Bethany!) I was one of a couple students in Quechua 201 and the only student in Quechua 202. Towards the end, I was starting to achieve some semblance of fluency, and I could speak somewhat off the cuff and express most of what I wanted to, haltingly, and with lots of mistakes. I do have one line on my CV about Quechua morphology, a conference presentation from LCUGA in 2016."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#tzutujil",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#tzutujil",
    "title": "The stories behind the languages I’ve studied",
    "section": "Tz’utujil",
    "text": "Tz’utujil\nDuring my first year of grad school, I took field methods again, and our informant spoken Tz’utijil. Like I did with Tshiluba, I focused on verbal morphology, but this time it was more about valency. I had quite a lot of fun with it and wrote a “grammar” of something like 60 pages. Pretty fun."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#dutch",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#dutch",
    "title": "The stories behind the languages I’ve studied",
    "section": "Dutch",
    "text": "Dutch\nIn 2019, I started to study Dutch because I had been accepted to a conference in the Netherlands and I wanted to not be that kind of American that just expects everyone to speak English. I did what I could to study, but in a few months, while dissertating, I couldn’t learn that much. While in Utrecht and Amersfoort, I did learn enough to ask people if they spoke English—which they almost always replied, “Yes, of course.” However, I had really gotten into breadmaking around then and I ended up going to a corner bakery a little bit away from the touristy downtown area and when I asked the baker if she spoke English, she said, somewhat apologetically, “een beetje” (‘a little bit’). That was my one chance to actually use Dutch!\nSince then, I’ve continued studying Dutch. No real reason—I don’t have plans to go back to the Netherlands—but it’s fun to have something to work on. I’m using Duolingo, but I’m also reading grammar books, studying a ton of vocabulary, and listening to podcasts. I follow a few Dutch-speaking folks on Twitter and can often understand most of their tweets now. Right now, I’d say it’s the language I’ve learned the most, other than English and Portuguese."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#korean",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#korean",
    "title": "The stories behind the languages I’ve studied",
    "section": "Korean",
    "text": "Korean\nTowards the end of grad school, I needed a 8000-level course, so I took a Korean Linguistics course, which was under the auspices of a “Lesser-Taught Languages” course. Most of the students were Korean majors or minors, and the only other two grad students were themselves Korean. I didn’t do that well in the course because I had never studied Korean at all. And my lesser-than-ideal performance shows because I remember virtually nothing from it."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#honorable-mentions",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#honorable-mentions",
    "title": "The stories behind the languages I’ve studied",
    "section": "Honorable Mentions",
    "text": "Honorable Mentions\nLanguages I studied for a very brief time.\n\nHaitian Creole: For a brief week or so in about 2012, my dad thought he was going to be doing business in Haiti, so he asked me to try and learn some Haitian Creole to help him interpret. It didn’t pan out.\nGerman: In 2019, I applied for a job in Switzerland so I started to try and learn German. I as starting Dutch at the same time, and I soon found out I didn’t get the job, so I didn’t get very far.\nUkrainian: In 2022, I started to advise a student on the sociolinguistics of Ukrainian. I thought I’d try and learn some Ukrainian to help with that.\n\n Language I’d like to study in the future.\n\nAymara: Aymara books are near Guarani books in the library, and I have an uncle-in-law who learned it while serving as a missionary in Bolivia.\nKorean: My wife and I agreed that once we’re both done with our current Duolingo courses (Dutch for me, Russian for her), we’d learn Korean together. We’ve watched a few Korean shows on Netflix and Korean culture is becoming more and more prevalent in the US right now anyway, so it might be handy to know.\nNorwegian: My great-great-grandfather was the end of an at least 40-generation-long line of Norwegians, specifically from the southern tip of the country. I can trace my ancestry to folks born in the 700s AD, and even the ones born in the 1200s were born only about 30 miles from where my great-great-grandfather was born. Anyway, it’d be interesting to learn Norwegian and visit the area."
  },
  {
    "objectID": "blog/stories-behind-the-languages-ive-studied/index.html#end",
    "href": "blog/stories-behind-the-languages-ive-studied/index.html#end",
    "title": "The stories behind the languages I’ve studied",
    "section": "End",
    "text": "End\nAnyway, I’ve heard people say that the more languages you know, the harder it is to define fluent. That’s certainly the case for me."
  },
  {
    "objectID": "blog/generations/index.html",
    "href": "blog/generations/index.html",
    "title": "generations: Convert birth years to generation names",
    "section": "",
    "text": "I’m happy to announce the release of another R package, generations! I’ve apparently caught the creating-R-packages bug because this is my fourth one this year (futurevisions, barktools, joeysvowels, and now generations). This one provides some functions to easily convert years to generational cohorts (Boomer, Gen X, Millennial, Gen Z, etc.).\nI recently read Howe & Strauss’ book, Generations: The History of America’s Future, 1584 to 2069. While the generational theory they propose isn’t water-tight, it is intriguing. Relatedly, I’ve seen lots of linguistics studies that model age in generational cohorts. (Ideally, we’d model age as a continuous variable, of course, but sometimes there’s just not enough data to do so.) I used a categorical age variable in the models in my dissertation and in other recent studies and, while it’s not perfect, it seems to work well enough.\nWell, so now that I’m converting age into generational cohorts in lots of different projects, my code is starting to get a little repetitive. And in the R world, they say if you end up writing the same code a lot, might as well wrap it up into a package. This idea came to me about a week ago and this weekend I found some time to put this together.\nThe result is generations. And, I’ve made it so that it doesn’t depend on any other packages, so it was fun for me to figure out how to do some things in base R that I only knew how to do in tidyverse, which was fun for me! The rest of this post is the readme page for the package. You can find more about the package at joeystanley.github.io/generations."
  },
  {
    "objectID": "blog/generations/index.html#installation",
    "href": "blog/generations/index.html#installation",
    "title": "generations: Convert birth years to generation names",
    "section": "Installation",
    "text": "Installation\nThe package currently lives on GitHub, so you can install it like you would with any other package on GitHub:\n\nremotes::install_github(\"joeystanley/generations\")\n\nYou can then load it like you can with any library.\n\nlibrary(generations)\n\nFor the purposes of this tutorial, I’ll load ggplot and dplyr as well.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "blog/generations/index.html#converting-years-to-generations",
    "href": "blog/generations/index.html#converting-years-to-generations",
    "title": "generations: Convert birth years to generation names",
    "section": "Converting years to generations",
    "text": "Converting years to generations\nThe main function in this package is generations(). Given a vector of integers, it’ll return a factor of generation names. First, I’ll generate some random years of birth.\n\nyobs &lt;- floor(runif(10, 1900, 2020))\nyobs\n\n [1] 1972 1932 1987 2018 1974 1979 1952 1937 1994 1980\n\n\nI can now easy convert that into generations.\n\ngenerations(yobs)\n\n [1] Gen X      Silent     Millennial Gen Z      Gen X      Gen X     \n [7] Boomer     Silent     Millennial Gen X     \nLevels: Silent Boomer Gen X Millennial Gen Z\n\n\nThis function works on any year between 1435 and 2030. Numbers outside that range return NA.\nNote that by default, the function will return the vector as factor, with the levels ordered so that the oldest generation in the vector is first. To get a character vector instead, add the argument as_factor = FALSE.\n\nCustomizing output\nThere are some tweaks you can do to adjust the output of generations. First, you can return longer forms of the generational names by specifying full_names = TRUE.\n\ngenerations(yobs, full_names = TRUE)\n\n [1] Generation X          Silent Generation     Millennial Generation\n [4] Generation Z          Generation X          Generation X         \n [7] Boomer Generation     Silent Generation     Millennial Generation\n[10] Generation X         \n5 Levels: Silent Generation Boomer Generation ... Generation Z\n\n\nWhat this does is simply add \"Generation\" to the end of each one, unless it’s \"Gen X\" (or Y, or Z), in which case it’ll expand it out to simply \"Generation X\".\nYou can also show the years included in each generation by adding the years = TRUE argument. This will add a space and, inside a pair of parentheses, the start and end years of that generation, separated by an en dash.\n\ngenerations(yobs, years = TRUE)\n\n [1] Gen X (1964–1983)      Silent (1929–1945)     Millennial (1984–2007)\n [4] Gen Z (2008–2030)      Gen X (1964–1983)      Gen X (1964–1983)     \n [7] Boomer (1946–1963)     Silent (1929–1945)     Millennial (1984–2007)\n[10] Gen X (1964–1983)     \n5 Levels: Silent (1929–1945) Boomer (1946–1963) ... Gen Z (2008–2030)\n\n\nThe primary purpose of this is for visualizations, since not everyone is familiar with (or agrees with) the year ranges. For example, if you’ve got a bunch of people and want to visualize the distribution of when they were born, you could have very informative legends.\n\nmany_yobs &lt;- tibble(yob = floor(rnorm(1000, 1975, 15))) %&gt;%\n  mutate(gen = generations(yob, full_names = TRUE, years = TRUE))\nggplot(many_yobs, aes(yob, fill = gen)) + \n  geom_histogram(binwidth = 1) + \n  scale_fill_brewer(name = NULL, palette = \"Set1\")\n\n\n\n\nHow this additional portion is formatted can be adjusted. If rendering an en dash is troublesome for you, you can change it to something else with years_range_sep. You may also want to change the space between the generation name and the opening parenthesis into a newline character with years_sep, again for visualization purposes.\n\nmany_yobs &lt;- many_yobs %&gt;%\n  mutate(gen = generations(yob, full_names = TRUE, years = TRUE,\n                           years_sep = \"\\n\", years_range_sep = \" to \"))\n\nggplot(many_yobs, aes(yob, fill = gen)) + \n  geom_histogram(binwidth = 1) + \n  scale_fill_brewer(name = NULL, palette = \"Set1\") + \n  labs(x = NULL) + \n  theme(legend.key.height = unit(1, \"cm\"))\n\n\n\n\nIf you want to get really fancy, you can make the legend keys approximate the width they take up on the x-axis and put better tics marks.\n\nwidths &lt;- many_yobs %&gt;%\n  group_by(gen) %&gt;%\n  summarize(width = max(yob) - min(yob)) %&gt;%\n  ungroup() %&gt;%\n  mutate(width = width / max(width) * 1.4) # you may have to fudge this a little more\n\nggplot(many_yobs, aes(yob, fill = gen)) + \n  geom_histogram(binwidth = 1) + \n  scale_fill_brewer(name = NULL, palette = \"Set1\") + \n  scale_x_continuous(breaks = c(1929, 1946, 1964, 1984, 2008, 2030)) + \n  labs(x = NULL) + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 1, label.position = \"bottom\", \n                             keywidth = widths$width, default.unit = \"inches\"))"
  },
  {
    "objectID": "blog/generations/index.html#querying-generation-data",
    "href": "blog/generations/index.html#querying-generation-data",
    "title": "generations: Convert birth years to generation names",
    "section": "Querying generation data",
    "text": "Querying generation data\nTo see a list of the generational data, you can use show_generations(), which will return a data frame containing the names, start years, and end years.\n\nshow_generations()\n\n             name start  end\n1           Gen Z  2008 2030\n2      Millennial  1984 2007\n3           Gen X  1964 1983\n4          Boomer  1946 1963\n5          Silent  1929 1945\n6            G.I.  1908 1928\n7            Lost  1886 1907\n8      Missionary  1865 1885\n9     Progressive  1844 1864\n10         Gilded  1822 1843\n11 Transcendental  1794 1821\n12     Compromise  1773 1793\n13     Republican  1746 1772\n14        Liberty  1727 1745\n15      Awakening  1704 1726\n16  Enlightenment  1675 1703\n17       Glorious  1649 1674\n18       Cavalier  1621 1648\n19        Puritan  1594 1620\n20  Parliamentary  1569 1593\n21    Elizabethan  1542 1568\n22       Reprisal  1517 1541\n23    Reformation  1497 1516\n24       Humanist  1459 1496\n25     Aurthurian  1435 1458\n\n\nYou can also get simple information. For example, if you want to know when the start or end year of a particular generation is, you can use get_start() or get_end():\n\nget_start(\"Silent\")\n\n[1] 1929\n\nget_end(\"Millennial\")\n\n[1] 2007\n\n\nYou can also find the names of neighboring generations with get_prev_gen() and get_next_gen(), though these were mostly created for internal purposes only rather than for you to use.\n\nget_next_gen(\"Millennial\")\n\n[1] \"Gen Z\"\n\nget_prev_gen(\"Missionary\")\n\n[1] \"Progressive\"\n\n\nNote that if ask for something newer than Gen Z or older than Aurthurian it will return NA."
  },
  {
    "objectID": "blog/generations/index.html#customizing-generation-data",
    "href": "blog/generations/index.html#customizing-generation-data",
    "title": "generations: Convert birth years to generation names",
    "section": "Customizing generation data",
    "text": "Customizing generation data\nThe data that this package uses is loaded as a hidden object when you load the package. You may modify it with the functions described in this section. These changes will affect the dataset so long as the generations package is loaded. You’ll have to reset the data each time to reload it.\nThe labels and years for each generation are mostly borrowed from Howe & Strauss’ Generational Theory books. However, not everyone agrees on the names and year ranges for the various generations. For this reason, the generations package makes it easy to modify the generations data to your liking.\nTo rename a generation, use rename_generation(), with the old name first and the new name second. For example, if you want to use Zoomer instead of Gen Z, you can do so.\n\nrename_generation(\"Gen Z\", \"Zoomer\")\n\nGen Z has been renamed Zoomer\n\n\nYou’ll get a message informing you that the change has been made. If you now run show_generations() you’ll see that the change has been made and if you rerun generations(), you’ll get updated results.\n\nshow_generations()\n\n             name start  end\n1          Zoomer  2008 2030\n2      Millennial  1984 2007\n3           Gen X  1964 1983\n4          Boomer  1946 1963\n5          Silent  1929 1945\n6            G.I.  1908 1928\n7            Lost  1886 1907\n8      Missionary  1865 1885\n9     Progressive  1844 1864\n10         Gilded  1822 1843\n11 Transcendental  1794 1821\n12     Compromise  1773 1793\n13     Republican  1746 1772\n14        Liberty  1727 1745\n15      Awakening  1704 1726\n16  Enlightenment  1675 1703\n17       Glorious  1649 1674\n18       Cavalier  1621 1648\n19        Puritan  1594 1620\n20  Parliamentary  1569 1593\n21    Elizabethan  1542 1568\n22       Reprisal  1517 1541\n23    Reformation  1497 1516\n24       Humanist  1459 1496\n25     Aurthurian  1435 1458\n\n\n\ngenerations(yobs)\n\n [1] Gen X      Silent     Millennial Zoomer     Gen X      Gen X     \n [7] Boomer     Silent     Millennial Gen X     \nLevels: Silent Boomer Gen X Millennial Zoomer\n\n\nBecause people may want to use the term Zoomer instead of Gen Z, a shortcut function, use_zoomer(), which is just a wrapper around rename_generation(\"Gen Z\", \"Zoomer\"), is included in the package. The other shortcut functions are use_gen_y(), use_13th(), use_baby_boom() as well as their reciprocals use_gen_z(), use_millennial(), use_gen_x() and use_boomer().\nYou may also want to change the years. For example, many people consider 1997 as the end of the Millennial Generation. You can make this change with redefine_generation(). With this function, you must specify the new start and the new end year.\n\nredefine_generation(\"Millennial\", 1983, 1997)\n\nGen X is now from 1964 to 1982\n\n\nMillennial is now from 1983 to 1997\n\n\nZoomer is now from 1998 to 2030\n\n\nSince changing one generation impacts adjacent generations, you’ll get a message showing you what the new ranges are for this, the previous, and the next generations.\nYou can reset the data back to its original form with reset_generations()."
  },
  {
    "objectID": "blog/generations/index.html#conclusion",
    "href": "blog/generations/index.html#conclusion",
    "title": "generations: Convert birth years to generation names",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s the package so far! I plan on adding more things in the future, primarily to handle stability issues and to include some error catching. Hopefully, if you use generational cohorts in your data, this package is useful for you."
  },
  {
    "objectID": "blog/new-publication-in-linguistics-vanguard/index.html",
    "href": "blog/new-publication-in-linguistics-vanguard/index.html",
    "title": "New publication in Linguistics Vanguard",
    "section": "",
    "text": "I’m happy to announce that a paper of mine has been published in Linguistics Vanguard. It’s called “Interpreting the order of operations in sociophonetic analysis” and it’s a direct follow-up to a paper I wrote for the Penn Working Papers in Linguistics a couple months ago. While the PWPL paper showed that Order of Operations (OoO) matters, that we should be talking about it more, and that we should do our best to interpret others’ OoOs, it didn’t give any help as to how to interpret them. The main contribution for this follow-up paper then is to 1) arm researchers with knowledge of how to interpret order of operations and 2) justify the order I recommended in the PWPL paper."
  },
  {
    "objectID": "blog/new-publication-in-linguistics-vanguard/index.html#summary",
    "href": "blog/new-publication-in-linguistics-vanguard/index.html#summary",
    "title": "New publication in Linguistics Vanguard",
    "section": "Summary",
    "text": "Summary\nTo reiterate what I said when the PWPL paper came out, the idea for this paper got started when I was in the throes of data analysis and I noticed that reordering some of the processing steps resulted in different changes. I did a systematic study of OoO, presented it at NWAV49 in 2021, and published those results in PWPL. But it occurred to me that even if we all become diligent and report the OoO we used in our papers, we don’t really have the knowledge of how we’re supposed to interpret those orders. As I put it in the paper,\n\n[I]t does little good if researchers are not familiar with how the order affects the overall results. A detailed methods section may explain that normalization happened before outliers were removed, but it is not currently clear what effect that order had on the results. How should a reader evaluate the results of one study that normalized the data before removing outliers against another study that transposed those two steps? This paper addresses this gap and explores in more detail the effect that some orderings are likely to have on the results of a study.\n\nPerhaps phrase another way, what specifically is the effect of normalizing before removing outliers compared to removing outliers before normalizing? Do formants go up, down, or something more complicated? As I mention in footnote 1, this knowledge can be used as “cheat codes” to manipulate your data the way you want. Hopefully reviewers can spot this kind of hacking!\nAnyway, so when I analyzed the same data 5040 times but with different orders of operation, I ended up with 5040 hypothetical analyses of the same dataset. Obviously, it didn’t make sense to consider every single one. So instead, I concentrated my efforts on 1) finding orders that produced identical results and 2) seeing what happens when I swapped two steps in the recommended OoO I have in the PWPL paper.\nHere’s that recommended order (Figure 1):\n\n\n\nMy recommended Order of Operations. Sorry the figure is so tall. And yes, I use PowerPoint for all my non-data visualizations :)\n\n\nMany of the orders produced identical results. This was mostly the result of various subsetting functions removing different groups of unwanted data, such as unstressed vowels, presonorants, stopwords, and vowel trajectories. This makes sense because, as I say in the paper,\n\nThe set of observations that are excluded in these steps is fixed: regardless of the normalization procedure, whether outliers have been removed, or how the vowels are classified, the exact same set of observations will be excluded each time.\n\nThese functions are all similar in that they remove data that is not needed or wanted for the current analysis, but is otherwise good. This similarity is visualized by clumping them together within a single block. I argue that because this is good data, it should only be removed at the very end of the pipeline, even after normalization.\nSo then I go and explore each pair of steps in my recommended order. Allophones should be classified before removing outliers because it just makes sense to do so. Also, failing to do so will make Pillai scores go up because you artificially draw two vowel classes together. Outliers should be removed before normalization because you don’t want outliers to mess up the normalization.\nThe trickiest part of the paper to understand is section 6 and Figure 4. I use it to show the interaction between normalization and subsetting. I recommend normalizing first. If you subset first and then normalize, the data is altered in kind of a weird way, especially if you’re doing Lobanov normalization. In the end I conclude that it’s better to normalize first with this justification:\n\nOne may argue that subsetting should happen before normalization so that comparisons across studies are more meaningful. It is true that a study that collects interview data will exclude many more tokens than another study that only elicits wordlist data. An argument can be made that by subsetting before normalization, what remains from the interview study more closely matches what is even collected in the wordlist study, and therefore the effect of normalization is more comparable between the two. However, a follow-up study of the interview data that happens to focus on something that was previously excluded, like vowel trajectories, will end up with a different input into the normalization step than the first study. There will be a difference between the normalized data in the first study and the normalized data in the second study. In other words, a single token of [æ] will have different normalized F1–F2 measurements between the two studies. This makes no sense since the token has not changed whatsoever.\n\nHowever, I acknowledge that if speakers have markedly different sample sizes, normalization will affect them in different ways. I think this is more indicative of an imperfect normalization procedure than anything else. We just haven’t yet found a method that perfectly models the human ear. There is room to explore normalization procedures that perform better or worse when speakers’ datasets are not particularly comparable."
  },
  {
    "objectID": "blog/new-publication-in-linguistics-vanguard/index.html#conclusion",
    "href": "blog/new-publication-in-linguistics-vanguard/index.html#conclusion",
    "title": "New publication in Linguistics Vanguard",
    "section": "Conclusion",
    "text": "Conclusion\nAnyway, I encourage you to read the paper. I enjoyed writing it. And now that I’ve closed up that loose end, maybe I can now go back to doing actual sociolinguistic research!"
  },
  {
    "objectID": "blog/boustrophedonically/index.html",
    "href": "blog/boustrophedonically/index.html",
    "title": "“Boustrophedonically”",
    "section": "",
    "text": "Earlier this week, I tweeted about a data visualization that I made. I said:\nI then showed a plot that looked something like this:\nThe data, which comes from a paper I’m working on, is difficult to visualize because the vast majority of the responses is clustered around zero while the rest is spread out a bit. I continued:\nI got lots and lots of comments from people and people’s thoughts were all over the board. Some said it’s great; others said they didn’t like it. And there were a handful that had very strongly mixed feelings of loving it and hating it. It’s a new kind of plot, so interpretation isn’t super straightforward, but it’s funny, silly, surprising, interesting, and memorable, which is why I think it’s a good one."
  },
  {
    "objectID": "blog/boustrophedonically/index.html#boustrophewhat",
    "href": "blog/boustrophedonically/index.html#boustrophewhat",
    "title": "“Boustrophedonically”",
    "section": "Boustrophewhat??",
    "text": "Boustrophewhat??\nThe way the tallest plot sort of goes back along the top can be perfectly described in one fantastic word: boustrophedonically. It’s my favorite word ever. The word has its roots in describing how an ox plows a field and can also describe how Ancient Greek was written. Nowadays, older Millennials just think of playing Snake on a brick phone. You might think it’s so obscure, so long, and so specific that no one could ever find a use for it, but I did manage to find a way to use it twice in a previous blog post about Chutes and Ladders.\nI first saw plots like these when reading one of Edward Tufte’s books. I don’t have them on me, but I believe it was in The Visual Display of Quantitative Information. Guthrie McAfee Armstrong points out that that W. E. B. Du Bois used this technique in his visualizations from well over 100 years ago. I believe Tufte showed some of these plots. You can see some of those original visuals in this Medium article that Matthew Kay pointed me to.\nI got lots of great suggestions from people on Twitter, so I thought I’d try out their suggestions so you can judge for yourself which one is the best. I’ll try to credit everyone who made the suggestions: this was truly a group effort here!"
  },
  {
    "objectID": "blog/boustrophedonically/index.html#the-full-height",
    "href": "blog/boustrophedonically/index.html#the-full-height",
    "title": "“Boustrophedonically”",
    "section": "The full height",
    "text": "The full height\nJack Grieve and jordan t. thevenow-harrison suggested I just plot all the data on a mega y-axis. Well, because the first bar is so stinking huge, I’d have to make the plot suuuuuper tall.\n\nThis way of visualizing data is not always bad: on March 27, 2020, the New York Times made an epic plot showing unemployment numbers for that week. But for my data, I don’t know if it’s quite right. Though, see this relevant xkcd that Rodolpho Piskorski delightfully reminded me of!"
  },
  {
    "objectID": "blog/boustrophedonically/index.html#log-transformed",
    "href": "blog/boustrophedonically/index.html#log-transformed",
    "title": "“Boustrophedonically”",
    "section": "Log-transformed",
    "text": "Log-transformed\nWhen you’ve got wildly different heights like this, the first step is to do a transformation of some sort. Christian DiCanio recommended a log transformation like this:\n\nWhile it does show all the bars at once, I just can’t fully appreciate the magnitude of the biggest one."
  },
  {
    "objectID": "blog/boustrophedonically/index.html#split-the-y-axis",
    "href": "blog/boustrophedonically/index.html#split-the-y-axis",
    "title": "“Boustrophedonically”",
    "section": "Split the y-axis",
    "text": "Split the y-axis\nA common technique for something like this would be to split the yaxis so that there’s a discontinuity and several people recommended this route.\nAs Hadley Wickham mentions here, there’s no native way in ggplot2 to do a discontinuous y-axis, so I had to sort of fudge it with patchwork. Here’s what that plot might look like:\n\nThis method is generally frowned upon though since the amount of real estate devoted to the big plot is disproportionate to the amount of data it actually represents so it hides the magnitude of that big bar."
  },
  {
    "objectID": "blog/boustrophedonically/index.html#pointing-arrow",
    "href": "blog/boustrophedonically/index.html#pointing-arrow",
    "title": "“Boustrophedonically”",
    "section": "Pointing arrow",
    "text": "Pointing arrow\nOne workaround is to zoom in to the smaller bars, and just give an indicator of how tall the big one is. I just text with an arrow pointing up.\n\nThis was one that I’ve been considering for a while now, but again, the problem is the real estate issue and it’s difficult to fully appreciate the actual height of that plot."
  },
  {
    "objectID": "blog/boustrophedonically/index.html#chunky-first-bar",
    "href": "blog/boustrophedonically/index.html#chunky-first-bar",
    "title": "“Boustrophedonically”",
    "section": "Chunky first bar",
    "text": "Chunky first bar\nTJ Mahr’s funny recommendation was to, instead of retaining the bar’s original length, make it wider.\n\nLike the boustrophedon, it breaks the xy-coordinate system a histogram relies on, but the main strike against it is that humans aren’t good at judging areas as well as we think we are, so it only sort of does a good job at showing the size."
  },
  {
    "objectID": "blog/boustrophedonically/index.html#plot-within-a-plot",
    "href": "blog/boustrophedonically/index.html#plot-within-a-plot",
    "title": "“Boustrophedonically”",
    "section": "Plot within a plot",
    "text": "Plot within a plot\nHonestly, I think the best workaround would be to take Joseph Casillas, Márton Sóskuthy, Timo Roetger’s, and May Helena Plumb’s recommendations and split the plot into two, one showing the big bar relative to the rest and the other showing the detail of the smaller ones. One way to do this is with a plot within a plot, which I did with the patchwork package again.\n\nAgain, probably the best recommendation if I can get that smaller plot to look decent."
  },
  {
    "objectID": "blog/boustrophedonically/index.html#zoomed-in-plot",
    "href": "blog/boustrophedonically/index.html#zoomed-in-plot",
    "title": "“Boustrophedonically”",
    "section": "Zoomed in plot",
    "text": "Zoomed in plot\nAn alternative to doing the plot-within-a-plot is to make it clearer that there’s a zoom happening, so the relevant portion of the full-size plot is highlighted and linked to the zoomed in one. This is accomplished with ggforce::facet_zoom, as recommended by Justin Lo and Sandra Jansen:\n\nIn this case, I’m not a huge fan of the greyed portion in the upper plot, because it sort of gets in the ways of the bars in the y-axis."
  },
  {
    "objectID": "blog/boustrophedonically/index.html#conclusion",
    "href": "blog/boustrophedonically/index.html#conclusion",
    "title": "“Boustrophedonically”",
    "section": "Conclusion",
    "text": "Conclusion\nSo, there are lots of ways to do this. Honestly, I freaking love the boustrophedon one and I’m seriously considering including it in the paper. I ran a poll and the slight majority agreed with me:\n\n\nOverall impressions?\n\n— Joey Stanley (@joey_stan) December 17, 2020\n\n\nI think because it’s so controversial and because people have such mixed feelings, it made for a really good discussion about the purpose behind the plot, faithfulness to the data, and overall aesthetics. Who knows I’d have so much fun rallying people together over some silly visual?"
  },
  {
    "objectID": "blog/dissertation/index.html",
    "href": "blog/dissertation/index.html",
    "title": "Dissertation",
    "section": "",
    "text": "I’m happy to report that I successfully defended my dissertation today! The defense was held in the DigiLab (300 Main Library). The study itself is called “Vowel Dynamics of the Elsewhere Shift: A sociophonetic analysis of English in Cowlitz County, Washington.”\n Me with my committee: Chad Howe, Peggy Renwick, and Bill Kretzschmar (Skyping in).\n\n\n\n\n\n\nTip\n\n\n\nYou can download my dissertation here!\n\n\nThe version linked above is a revision that I’ve made after correcting some small typos. Click here to view the official, submitted version."
  },
  {
    "objectID": "blog/ads-and-lsa-2022/index.html",
    "href": "blog/ads-and-lsa-2022/index.html",
    "title": "ADS and LSA 2022",
    "section": "",
    "text": "I’m attending the Annual Meeting of the Linguistic Society of America and the American Dialect Society and I’ve got three presentations to tell you about! Please find links, summaries, and images from these presentations below!"
  },
  {
    "objectID": "blog/ads-and-lsa-2022/index.html#perspectives-on-georgia-vowels-from-legacy-to-synchrony",
    "href": "blog/ads-and-lsa-2022/index.html#perspectives-on-georgia-vowels-from-legacy-to-synchrony",
    "title": "ADS and LSA 2022",
    "section": "Perspectives on Georgia Vowels: From Legacy to Synchrony",
    "text": "Perspectives on Georgia Vowels: From Legacy to Synchrony\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nOn Thursday afternoon, Peggy Renwick, Jon Forrest, Lelia Glass, and I kicked off the ADS sessions with our presentation on English in Georgia. The four of us have been collaborating for about a year, pooling together datasets and sharing resources, on a project focusing on English in Georgia. This is our first talk showcasing some of our findings. Our results are largely descriptive at this point. Here are the main plots we used (in a fun dark mode!) split up by generation, gender, and ethnicity:For those of you that were there, this was the one that was horribly Zoombombed!\n\n\n\nTurns out pretty vowel changes if you give it 100 years. We’re just excited to see acoustic data from such a large span of time analyzed together."
  },
  {
    "objectID": "blog/ads-and-lsa-2022/index.html#homogeneity-and-heterogeneity-in-western-american-english",
    "href": "blog/ads-and-lsa-2022/index.html#homogeneity-and-heterogeneity-in-western-american-english",
    "title": "ADS and LSA 2022",
    "section": "Homogeneity and Heterogeneity in Western American English",
    "text": "Homogeneity and Heterogeneity in Western American English\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nAt the ADS poster session on Friday, I presented a poster with two students, Jessica Shepherd and Auna Nygaard. As a bit of background, in Speech in the Western States: Volume 2, Fridland et al (2012:172) point out that pretty much every study of the front lax vowels in the Western US has been based on independent, isolated studies. Because each research collects and processes data their own way, it’s difficult to disentangle differences that may be due to region and differences that may be due to methodological choices. They say that “clearly, collecting the same type of data from all sites would be optimal in allowing us the most reliable cross-region assessment.”\nThis project is a direct response to that call. When I was a grad student I recruited people via Amazon Mechanical Turk to a bunch of recordings of people reading sentences and wordlists. In total, 212 people completed the task, scattered all across the Western US. This poster describes the first results from this project. As it turns out, our findings match the West’s description as exhibiting both “homogeneity and heterogeneity” (Fridland et al. 2012:172). We find homogeneity in that most people have the LBMS to some degree and that education level and region weren’t statistically significant predictors. However, there’s a wide range of variation for the LBMS and ban-raising, with younger people and sometimes women appearing to lead both of these sound changes. Here are some sample plots from four representative speakers.\n\nAnd here’s an overall look at the vowel space in our dataset, with some additional allophones we don’t analyze here.\n\nWe look forward to digging into this dataset a little bit more in the future!"
  },
  {
    "objectID": "blog/ads-and-lsa-2022/index.html#vowels-can-merge-because-of-changes-in-trajectory-prelaterals-in-rural-utah-english",
    "href": "blog/ads-and-lsa-2022/index.html#vowels-can-merge-because-of-changes-in-trajectory-prelaterals-in-rural-utah-english",
    "title": "ADS and LSA 2022",
    "section": "Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English",
    "text": "Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nFinally, on Friday afternoon, Lisa Johnson and I talked about vowel trajectories and what they can tell us about vowel merger. We look at prelaterals in rural Utah and find that, on the surface, they look like mergers by approximation. However, when we looked at the trajectories (with the help of some pretty cool animations!), it seems like the lateral gradually increases its influence on the vowel so that the merger happens “leftward,” from the coda to the onset. In this example, we have zeal and guilt, representing /il/ (feel, deal, meal) and /ɪl/ (fill, dill, mill), respectively.\n\n\n\n\n\n\n\nThis was a pretty consistent pattern across all the pairs of prelateral vowels we looked at. We suspect that we might find this among other conditioned and vowel shifts, like prevelar raising, the mary-merry-marry merger, and post-coronal /u/-fronting. The point is, we think trajectories should be considered more when looking at vowel mergers because even among these supposed monophthongs, trajectories really illuminated how that merger happened."
  },
  {
    "objectID": "blog/updated_mvnorm.etest-function/index.html",
    "href": "blog/updated_mvnorm.etest-function/index.html",
    "title": "Updated mvnorm.etest() function",
    "section": "",
    "text": "In Levshina’s How to do Linguistics with R, the function mvnorm.etest() from the energy library is used. This runs what’s called the “E-statistic (Energy) Test of Multivariate Normality” which used to test whether multivariate data is normally distributed. This is important because it’s an assumption that should be met for several statistical tests like MANOVA and for testing statistical significance of a correlation. Well, the code from the book is broken.Levshina, Natalia. 2015. How to do Linguistics with R: Data exploration and statistical analysis. Amsterdam: John Benjamins Publishing Company.Maria L. Rizzo and Gabor J. Szekely (2016). energy: E-Statistics: Multivariate Inference via the Energy of Data. R package version 1.7-0. https://CRAN.R-project.org/package=energy\nI looked into it and it turns out that the book was based on an older version of the energy package (&lt;1.7). But if you’ve updated the package since August 2016 to version 1.7 or later, the code breaks. What happened? Here’s the old code:\nmvnorm.etest(cbind(x, y))\nWhile this worked with the old versions, in the newer versions this returns a p-value of “NA”. This function does some bootstrapping meaning it runs some function on the data over and over some number of times. In the old version of the package, the default was 999 replicates. In the new version there is no default, so you have to specify the number of replicates with the R=999 argument:\nmvnorm.etest(cbind(x, y), R=999)\nYou can of course change this number to whatever you want, but 999 was the default before so I figure it’s a good number to keep. Just thought you ought to know."
  },
  {
    "objectID": "blog/ads-and-lsa-2023/index.html",
    "href": "blog/ads-and-lsa-2023/index.html",
    "title": "LSA and ADS 2023",
    "section": "",
    "text": "This week I’m in Denver at annual meetings of the Linguistic Society of America and the American Dialect Society. I gave two talks, which you can download here:"
  },
  {
    "objectID": "blog/ads-and-lsa-2023/index.html#lsa-talk-on-mountain-in-utah",
    "href": "blog/ads-and-lsa-2023/index.html#lsa-talk-on-mountain-in-utah",
    "title": "LSA and ADS 2023",
    "section": "LSA talk on mountain in Utah",
    "text": "LSA talk on mountain in Utah\nMy first talk is called “Utahns sound Utahn when they avoid sounding Utahn.” Basically, I find that Utahns say moun[tʰɨn] more than other people do because they avoid the stigma assigned to the glottal stop in the local realization, moun[ʔɨn], and that has since spread to the nationwide standard moun[ʔn̩]. I’ve been very eager to talk about these ideas for a while so I hope you find them as interesting as I do.\n\nHere is the actual powerpoint file. In the notes for each slide you’ll see the actual script I read from. And since the audio is embedded into the file, if you enter presentation mode and mouse over a quote, you’ll hear the actual audio.\nIn case you don’t need all that and you just want a PDF, here’s that instead.\n\nIf you want to read a little more about this topic, please see my blog post from a few months ago with some preliminary results."
  },
  {
    "objectID": "blog/ads-and-lsa-2023/index.html#ads-talk-on-idaho-english",
    "href": "blog/ads-and-lsa-2023/index.html#ads-talk-on-idaho-english",
    "title": "LSA and ADS 2023",
    "section": "ADS talk on Idaho English",
    "text": "ADS talk on Idaho English\nMy other talk, “Is Idaho English really ‘the epitome of Average English’?”, was in collaboration with KaTrina Jackson, who unfortunately couldn’t be there to help present. The quote in the title comes from Dennis Preston’s 1989 book where he found that people didn’t think anything of Idaho English. We examine data on Idaho English and show that it lacks stigmatized linguistic features from Utah, but also innovative features like the LBMS. It does have features that are floating under the radar though. In the end we, conclude that Idaho English does appear to be the epitome of average English.\n\nHere’s a version that has the slides and the full script that I read from, in case you want to get all the nitty-gritty detail.\nHere is a PDF of the slides themselves, in case you just want that instead.\n\nPlease see my blog post from a few months ago which goes into a little more detail than what I had time to talk about here."
  },
  {
    "objectID": "blog/jealousy-list-2/index.html",
    "href": "blog/jealousy-list-2/index.html",
    "title": "Jealousy List 2",
    "section": "",
    "text": "This is the second post in my occasional series of Jealousy Lists. I’m subscribed to about 50 blogs, most of them Data Science–related, and I’ve see a lot of really cool stuff coming out recently. It makes me really want to take my R skills to the next level. Anyway, these are some cool posts that I read recently:\n\nMichael Höhle. “Judging Freehand Circle Drawing Competitions”.\nHave you ever noticed it’s really hard to draw a perfect circle? Apparently, there are people that are really good at it. This post shows how you might determine how perfect the a handdrawn circle is: “We took elements of computer vision, image analysis and total least squares to segment a chalk-drawn circle on a blackboard and provided measures of it’s circularness.” Spoiler alert: the guy’s circle was pretty dang near perfect.\nYihui Xie. “Impact: Depth or Breadth?”\nIn this brief post, Yihui discusses whether we should strive for breadth or depth in our research. He says, “I prefer a small number of people (could even be only one person) feeling extremely excited over a large number of people only slightly nodding.” The logic is that that one person may shout from the rooftops for you, and your impact will spread from there. Also, the one person may just be you. I’ve heard this about app development (make something that you want to use) and it was interesting to see it applied to research too.\nJulia Silge. “Training, Evaluating, and Interpreting Topic Models”.\nI’m a big fan of Julia Silge’s work, and though I’ve never needed to topic modeling in my own research, her blogs (and book) always make me want to start. In this post, she takes a bunch of texts from the Hacker News Corpus and shows how you might determine the best number of topics to choose when doing topic modeling. She then does the analysis itself and shows how the words fit into topics.\nThomas Lin Pedersen. “What Are We Plotting, What Are We Animating”\nAnimations are getting more and more popular, and Pedersen’s gganimiate package is a great tool for you to create them in R. This post looks at what happens when you try to animate things that are a big different from each other. One type of animation will do so clunkily (like if you want to use Powerpoint to animate transitions between slides…). This post takes a dive at what happens under the hood in ggplot2 and gganimate to help you understand your data and the functions you use.\nLaura Ellis. “Create stylish tables in R using formattable”.\nBy default, tables in R are nothing more than text. In this post, Laura Ellis demonstrates the formattable package in R and shows how you can make things look a lot prettier. You can add colors, alignment, font, and all sorts of other stuff.\n\nSo that’s my jealousy list for now. Image recognition, research, topic modeling, animations, and tables. I’m glad other people put so much interesting stuff online."
  },
  {
    "objectID": "blog/nwav46/index.html",
    "href": "blog/nwav46/index.html",
    "title": "NWAV46",
    "section": "",
    "text": "At the 46th New Way of Analyzing Variation conference in Madison, Wisconsin, I presented a poster called Changes in the Timber Industry as a Catalyst for Linguistic Change.\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nIn a nutshell, I found a couple interesting things going on in a small town in Washington:\nLinguistic changes—I focused on two variables: bag-raising and goat-diphthongization. Specifically, older people raised bag and younger people had a more diphthongal goat. The generational divide was around 1970 and the difference between older and younger people was sudden.\nCensus data—Based on topics of conversation in the interviews, I took a look at census data and found some correlation with these linguistic changes. The mills laid a bunch of people off, people were earning less money, a bunch of people left town, and more of those who stayed worked outside the community. Correlation does not equal causation, but it’s paints a pretty compelling picture.\nCatastrophic events—One of the interesting findings was using regression with breakpoint as a way to model catastrophic change. I’m refining this methodology right now, but it does seem to work for modeling language change in time.\n\n\n(Photo credits I believe go to Maciej Baranowski)"
  },
  {
    "objectID": "blog/ten-years-of-conferences/index.html",
    "href": "blog/ten-years-of-conferences/index.html",
    "title": "10 Years of wanting to be an academic",
    "section": "",
    "text": "Last week, I was at the LSA/ADS conferences in Denver, and it occurred to me that ten years ago this week is when I decided I wanted to be an academic.\nIn the summer of 2011, I went on a study abroad to Ecuador with my now-colleague Janis Nuckolls. It was a fantastic experience and really got me excited about linguistics. Afterwards, me and two other students started meeting with her as a research group. Over the next year we met weekly as we worked on the phonology of Kichwa ideophones, research that would eventually lead to our 2016 publication in IJAL.That paper may forever be my most cited publication, which is weird because it’s not at all what I do for research now!\nWe got the point where Janis was ready to submit the research to a conference. So, we wrote up the abstract and submitted it to the SSILA conference, which meets concurrently with LSA, ADS, and a few other sister societies. And we got in!\nAt first, I wasn’t planning on going. I was engaged and the conference was less than two weeks after my wedding. I decided last minute that I really should go though. My wife helped me pick out some conference clothes (I still wear the green shirt I got then as a homage to my first conference!), I managed to get some funds from the department, and after a red-eye flight that included layovers in Denver and Philadelphia, I made it to Boston.\nOur presentation was in the very first session on that Thursday, and I remember being pretty nervous as I read my section. I didn’t look up from my script once. But because the talk was over with soon after arriving to the conference, I was able to spend the the next two days wandering around attending whatever talks looked interesting.\nI’m pretty sure I ended up spending most of my time at the ADS sessions. I was still an undergrad, but I had already applied to some graduate schools with the intent of studying sociolinguistics, so I actually recognized some of the names there and I was able to put faces to names. According to my program book from that year, here are a few things I saw:\n\nJoseph Hill presented, in ASL, about the influence of African American English on African American Sign Language.\nNicole Rosen presented on the difference between Latter-day Saints and non-Latter-day Saints in Alberta.\nDan Villarreal (and my now-colleague, Grant Eckstein) presented on intonation patterns of Latter-day Saints compared to non-Latter-day Saints when reading scripture.\nBill Kretzschmar presented on computational approaches to dialect mapping. (Little did I know I’d be his RA for four years!)\nLisa Davidson and Daniel Erker presented on hiatus resolution in American English.\nLivia Oushiro and Ronald Mendes presented on nasal /e/ in Brazilian Portuguese\nPatricia Cukor-Avila on Texans’ perceptions of regional variation\nDavid Durian on what we now call the LBMS in Ohio.\n\nIt’s really interesting to look through the booklet and see what talks I didn’t consider but would definitely see today. It’s also interesting to see where people I now know were ten years ago.\nAnyway, by the end of the conference, I had caught the bug. I knew that that is where I wanted to be. Those were my people. In fact, it was at the next LSA/ADS I went to, which was two years later in Portland, that I got the idea for my dissertation topic. The story for that will have to wait for another blog post."
  },
  {
    "objectID": "blog/joeysvowels/index.html",
    "href": "blog/joeysvowels/index.html",
    "title": "joeysvowels: An R package of vowel data",
    "section": "",
    "text": "I’ve just released my third R package, joeysvowels. It provides a handful of datasets, some subsets of others, that contain formant measurements and other information about the vowels in my own speech. The purpose of the package is to make vowel data easily accessible for demonstrating code snippets when demonstrating how to work with sociophonetic data. There are no functions contained in joeysvowels; it’s a data-only package.\nThis is my third package. My first one, futurevisions, contains a collection of color palettes. The second is barktools, which helps you work with Barks in your sociophonetic data (and even has its own hex!). Technically, I have another package called joeyr that is sort of my sandbox package that I use it all the time and has useful functions for sociophonetic work but it’s not quite ready for distribution yet.\nWhy create a data-only package? A lot of it had to with my barktools and joeyr packages. As I was developing websites for them using pkgdown, I wanted to create some better help files and examples. To do that, I needed real vowel data to work with. So for a while I had a couple datasets built into each of those packages. Sometimes the same dataset was included with both packages, which is fine by themselves, but since I often had both loaded in the same script, it created a small clash. I figured I should just offload the data from the two packages onto a third one, which would then be a dependency of both. Thus joeysvowels was born!"
  },
  {
    "objectID": "blog/joeysvowels/index.html#installation",
    "href": "blog/joeysvowels/index.html#installation",
    "title": "joeysvowels: An R package of vowel data",
    "section": "Installation",
    "text": "Installation\nYou can install joeysvowels through GitHub:\n\nremotes::install_github(\"joeystanley/joeysvowels\")\n\nYou can then load the package like normally:\n\nlibrary(joeysvowels)\n\nI’ll load a few other packages for the purposes of this post.\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)"
  },
  {
    "objectID": "blog/joeysvowels/index.html#contents",
    "href": "blog/joeysvowels/index.html#contents",
    "title": "joeysvowels: An R package of vowel data",
    "section": "Contents",
    "text": "Contents\nCurrently, there are six datasets contained in joeysvowels. You can access them using data(). I’ll briefly visualize the datasets.\n\nA messy dataset: darla\ndarla is one that was prepared using pretty standard methods, using the DARLA web interface to automatically transcribe, force-align, and extract formants from the audio. The audio was me reading 300 prepared sentences. It’s a bit of a noisy dataset, so it’s a good example of working with real data and testing out various outlier detection functions. It is also very close to the format that FAVE exports its data, so any tutorials that use FAVE-produced spreadsheets can be followed along using darla.\n\ndata(darla)\nggplot(darla, aes(F2, F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse()\n\n\n\n\n\n\nCleaner datasets: coronals and its subsets\ncoronals is a much cleaner, more controlled dataset. You can read about the methods by viewing the documentation (?coronals). Essentially, I read a bunch of (C)CVC(C) nonce words where the consonants were (almost) all coronal. All my vowel phonemes are represented. I aligned and extracted formants from the data myself. Four formants were extracted every 5% of the vowels’ durations, so great for demonstrating functions and visuals involving vowel trajectories.\n\ndata(coronals)\navg_trajs &lt;- coronals %&gt;%\n  group_by(vowel, percent) %&gt;%\n  summarize(across(c(F1, F2), mean)) %&gt;%\n  print()\n\n# A tibble: 273 × 4\n# Groups:   vowel [13]\n   vowel percent    F1    F2\n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 LOT         0  454. 1616.\n 2 LOT         5  592. 1346.\n 3 LOT        10  648. 1283.\n 4 LOT        15  651. 1238.\n 5 LOT        20  661. 1178.\n 6 LOT        25  651. 1176.\n 7 LOT        30  636. 1152.\n 8 LOT        35  630. 1148.\n 9 LOT        40  633. 1153.\n10 LOT        45  625. 1158.\n# ℹ 263 more rows\n\nggplot(avg_trajs, aes(F2, F1, color = vowel)) + \n  geom_path(aes(group = vowel), \n            arrow = arrow(angle = 20, length = unit(0.15, \"in\"), type = \"closed\")) + \n  scale_x_reverse() + \n  scale_y_reverse()\n\n\n\n\nIf you don’t care about trajectories but would like something clearer than darla, then midpoints what you’ll want. It’s a subset of coronals and contains only the midpoints from F1 and F2.\n\ndata(midpoints)\nggplot(midpoints, aes(F2, F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse()\n\n\n\n\nIf you just need to demonstrate one vowel’s trajectory, check out mouth since it’s only the mouth (/aw/) vowel.\n\ndata(mouth)\nggplot(mouth, aes(percent, hz, color = formant)) + \n  geom_path(aes(group = traj_id))\n\n\n\n\nmouth_lite is a subset of mouth and trims away most of the columns and only contains 10 tokens.\n\ndata(mouth_lite)\nggplot(mouth_lite, aes(percent, hz, color = formant)) + \n  geom_path(aes(group = traj_id))\n\n\n\n\n\n\nMultiple speakers: idahoans\nidahoans contains formant measurements from 11 individuals from the state of Idaho in the US.These participants did consent to their data being used for teaching purposes and to be distributed to interested researchers. I needed something to test out some functions that do vowel normalization and relying on my own voice wasn’t going to cut it. It’s not a full dataset: for each of the 10 speakers, there are ten tokens per canonical monophthong, randomly selected from a larger dataset. But it should be enough for illustrative purposes. Plus, when was the last time you saw acoustic data from Idaho??\n\ndata(idahoans)\nggplot(idahoans, aes(F2, F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  facet_wrap(~speaker)"
  },
  {
    "objectID": "blog/joeysvowels/index.html#conclusion",
    "href": "blog/joeysvowels/index.html#conclusion",
    "title": "joeysvowels: An R package of vowel data",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s it so far! Feel free to use the datasets for teaching and demos. That’s what they’re there for."
  },
  {
    "objectID": "blog/dh2019/index.html",
    "href": "blog/dh2019/index.html",
    "title": "DH 2019",
    "section": "",
    "text": "At the Digital Humanities 2019 conference in Utrecht, the Netherlands, I presented with Bill Kretzschmar on ways to visualize a lot of phonetic data."
  },
  {
    "objectID": "blog/dh2019/index.html#the-gazetteer-of-southern-vowels",
    "href": "blog/dh2019/index.html#the-gazetteer-of-southern-vowels",
    "title": "DH 2019",
    "section": "The Gazetteer of Southern Vowels",
    "text": "The Gazetteer of Southern Vowels\nThe first half of the presentation was essentially me showcasing the Gazetteer of Southern Vowels (or GSV), a website I created in Shiny to help visualize 1.3 million acoustic measurements from the Digital Archive of Southern Speech.The full web address is http://lap3.libs.uga.edu/ u/jstanley/vowelcharts/, but I’ve got a redirect at joeystanley.com/gsv that’s easier to type. In the talk I spend most of the time in the “Vowel Plot Comparison” tab (below) and show how you can interact with the data.\n\nFirst, you can subset the data by demographic factors. The Speaker Selection tab has menu items for speakers’ sex, age, ethnicity, home state, social class, and a couple other variables. When you select one or more of these, the plot automatically updates to reflect that subset.\n\nYou can also subset the data by linguistic factors. In the Words tab, you’ll see that a list of stopwords is displayed and that those are excluded by default. You can add to or remove words from that stoplist, or switch it so display only those words (or some other set of words like numbers or colors).\n\nIn the Vowels tab, you’ve got a whole bunch of options. First, you can choose what vowel is being displayed, what kind of stress it can have, and what its phonetic environment is(based on following segment only). There are some methodological choices too, like ways of filtering and normalizing the data. You can also choose what transcription system is being used.\n\nThen, there are ways for you to customize the plot. The Plot Style tab as four main options (points, ellipses, means, and words), that act independently with their own controls for size and opacity. So if you want means and ellipses but no dots, you can do that. If you want to display the words themselves, but in a small font and transparent, be my guest.\n\nThe Plot Customization tab lets you change things like the zoom, axes, aspect ratio, and (some control over) colors. I’m hoping to add more options to this tab in the future. With these two tabs, I feel like you can make a lot of very different plots, all based on the same data, which is pretty cool.\n\nFinally, the Download Options is the newest tab. You could always take a screenshot, but you’re limited to how your web browser displays the image and your computer’s screensize. In this tab, you can set the height, width, quality, and format, so you can make publication-quality images. In fact, the plots in this blog post were all created using this download button, so you can recreate them yourself!\n\nSo that’s it! My goal in creating these options was to allow users to create any type of plot using any conceivable subset of DASS, and I think the GSV does a pretty good job at that. Here’s a quick gallery of six different plots:"
  },
  {
    "objectID": "blog/dh2019/index.html#point-pattern-analysis",
    "href": "blog/dh2019/index.html#point-pattern-analysis",
    "title": "DH 2019",
    "section": "Point Pattern Analysis",
    "text": "Point Pattern Analysis\nIn the second half of the presentation, Bill Kretzschmar took over and discussed using point pattern analysis in the visualization of vowel data. He has found that when you overlay a grid on the F1-F2 space (just as geographers do with geospatial data), you can see the central tendency of vowels by which “cells” in this new grid are the densest. They roughly follow the 80-20 rule, with a few cells being heavy concentrated, some having some tokens, and many with very few. Here’s just one image of a Georgia man’s fleece vowel.\nIncidentally, because the grid itself is new element, there are additional controls in the Plot Customization tab, like how many cells and the opacity or size of the labels. And since there’s only one color being used, I’ve got controls over whether the shading is discrete or continuous, if it’s discrete then how many levels, and you can even put a custom color in hex notation. Also, I’m still working on getting the ranges in the legend to display integers only, since 54.6 data points in a cell is somewhat nonsensical.\n\nBill finds that if you plot them in order of density, the resulting curve is an asymptotic hyperbolic curve, or just A-curve for short. And, as it turns out, this distribution is fractal in nature, so regardless of how much you subset the data, you’ll find the same distribution. The GSV makes it easy to see this distributions interactively.\nAt the very end, we hinted at some additional visualizations we’d like to develop to make it easier to view trajectory data, taking advantage of a third-dimension in the plot itself. Hopefully, we’ll have more to say about that in the future."
  },
  {
    "objectID": "blog/dh2019/index.html#conclusion",
    "href": "blog/dh2019/index.html#conclusion",
    "title": "DH 2019",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s our presentation! We’ve got a lot of data and we needed lots of plots to make sense of it all. Instead of saving plot after plot, we decided an interactive Shiny app might be a better option. Most importantly, I think we’ve learned a little more about how language works because of the size of the data and the interactivity of this tool."
  },
  {
    "objectID": "blog/mary-merry-marry/index.html",
    "href": "blog/mary-merry-marry/index.html",
    "title": "A big list of Mary-merry-marry words",
    "section": "",
    "text": "Most Americans, including me, have this thing called the Mary-merry-marry merger. We pronounce all three words—and the vowels in similarly patterning words—the same. However, some Americans retain at least a two-way distinction and most, if not all, varieties of English outside of North America distinguish between all three.\nAs is typical for people with a merger, it’s not easy for me to separate words into their historic distributions. But sometimes I need to for teaching or preparing wordlists. So, as I prepared to cover the merger (or rather, the lack thereof) in my Varieties of English course this semester, I wanted to show the students a list of words that group with Mary, merry, and marry. But I couldn’t find a decent list anywhere. So I asked Twitter and I was pleasantly surprised to get lots of help from my non-merging followers!\nThank you to all those who sent me lists of words that belong to each class! And to those who pointed me to searchable dictionaries that distinguish between the three! I won’t mention names here, but I hope they are okay with me turning their collective input into a blog post.\nSo, the purpose of this post is to provide the most comprehensive list I could come up with of Mary, merry, and marry words—the list I was hoping to find a few days ago—just in case any other American needs it.\nNote: I’m told that outside the US, there is little variation in which class each word belongs to. However, in areas of North America that do make some distinction, there can be some variability. So I guess take this list with a grain of salt."
  },
  {
    "objectID": "blog/mary-merry-marry/index.html#merry",
    "href": "blog/mary-merry-marry/index.html#merry",
    "title": "A big list of Mary-merry-marry words",
    "section": "merry",
    "text": "merry\nThe first set, merry, is actually the dress lexical set. John Wells points out that merry is typically spelled with &lt;e&gt;. It also seems like a lot of French words have this vowel.\nHere are some words that were verified by some folks who do not have the merger.\n\nberry, beryl, burial, bur{y|ied}, cherry, derring-do, Derry, error, derriere, ferret, ferrous, ferry, Gerry, inherit(ed), Jerry, Kerry, merit(s), Merovingian, Merriam, Merrion, Merrow, merry, Perrier, Perry, perry, seropositive, sherry, skerry, steril{e|ize}, terrier, terribl{e|y}, terror, terrif{y|ied}, Terry, verisimilitude, very, wherry \n\nHere’s a more complete list from the Britphone dictionary, based on the search pattern “ˈɛ ɹ” with a few additions from The Routledge Dictionary of Pronunciation for Current English, based on the search pattern “ɛr|”.\n\nAmerica{s|n|ns}, atmospheric, beret, burial, cerebral, ceremony, chemotherapy, cherish, Cheryl, clerical, Derek, deterren{t|ce}, equerry, Eri{c|k}, Ericsson, experiment(s), generic, Gerald, Herald, Hereford, Herefordshire, heritage, heroin, heron, inherent, inheritance, Jeremy, knobkerrie, merrily, necessarily, numeric(al), peril, perish, primarily, prosperity, referral(s), serif, severity, sheriff, stereo, stereotype, terrace, territor{y|ies}, terrorism, terrorist(s), therapist, therapy, verif{y|ied}"
  },
  {
    "objectID": "blog/mary-merry-marry/index.html#marry",
    "href": "blog/mary-merry-marry/index.html#marry",
    "title": "A big list of Mary-merry-marry words",
    "section": "marry",
    "text": "marry\nThe second of the three, marry, is actually the trap lexical set. John Wells points out that the when an a is followed by two r’s, it’s a decent indicator of marry.\nHere are some words that were verified by some folks who do not have the merger.\n\narable, arid, apparent, Aragon, Areopagus, arid, arrant, arrow, baritone, baron, barren, barricade, barrow, Barry, Caradon, caravan(s), caret, carob, carol, Carol{e|ine|yn}, carr{y|s|ied|ing|ier}, charabanc, chariot, charity, charitable, Clarence, clarify, clarion, clarity, comparison(s), circularity, Darrell, Darren, Darrow, Faraday, Farrell, farrier, farrow, Garamond, Gareth, Gary, guarantee, harakiri, harried, harrow, Harry, Iscariot, Jared, larrikin, Karen, Larry, larynx, marabou, Marazion, Marian, Marilyn, Marion, marital, maritime, marronage, marrow, marr{y|ied|s|ing}, narrative, narrow(ly), para(-bolic, -chute, -graph, -llel, -noid, -normal, -lyze* etc), *parr{y|ied}, parody, parrot, Raritan, saraband, Saracen, scarify, Sharon, taradiddle, tarry, varicose, yarrow, Zara\n\nAnd here’s a longer list from the Britphone dictionary, based on the search pattern “ˈæ ɹ” with a few additions from The Routledge Dictionary of Pronunciation for Current English, based on the search pattern “ar|”.\n\napparel, Arab, Arabic, barracks, barrel, barrier(s), caribou, carriage, Carrie, carrier(s), Carroll, carrot(s), character{s|ize}, charit{y|ies|able}, comparative(ly), Daryl, disparage, embarrass{ed|ing|ment}, garage(s), gharry, glengarry, harass(ment), Haringey, Harold, Harriet, Harris, Harrison, Harrogate, karri, Larry, marathon, marriage(s), Marriott, Maryland, Marylebone, miscarriage(s), paradigm, paradise, paradox, Paraguay, Paris, parish, popularity, similarit{y|ies}, solidarity, tariff, tarot, transparen{t|cy}"
  },
  {
    "objectID": "blog/mary-merry-marry/index.html#mary",
    "href": "blog/mary-merry-marry/index.html#mary",
    "title": "A big list of Mary-merry-marry words",
    "section": "Mary",
    "text": "Mary\nFinally, there’s mary, which is actually the square lexical set. John Wells points out that the when an a is followed by just one r, it’s a decent indicator of mary (though not always).\nHere are some words that were verified by some folks who do not have the merger.\n\nAaron, aerate, aerial, airing, area, baring, bear{ing|er}, Bering, carer(s), Carey, caring, chairing, Charing, Clary, dare{e|ing}, dairy, fairy, far{e|ing}, flare-up, flaring, glaring, (nom de) guerre, hairy, hare, haring, lair, lairy, mare, Mary, mayoral, Nair, nary, pair(ing), Pharaoh, precarious, rare, raring, Sara(h), scar{y|ing}, shar{er|ing}, sparing, squaring, staring, swearing, tear(ing), Vair, variable(s), varian{t|ce}, var{y|ying|ied|ies}, wary\n\nAnd here are some other ones based on the Britphone dictionary, based on the search pattern “ˈɛə ɹ”.\n\naerosol, aerospace, air{y|ing}, apparent(ly), aquarium, Averham, Barham, Bulgaria(n), canary, comparing, contrary, eire, Faeroes, hilarious, humanitarian, Hungarian, invariably, librarian, malaria, Ontario, parent{s|ing}, pharaoh(s), prairie, preparing, secretariat, sierra, variegated, various, vegetarian, wearing, whereabouts\n\n\nWord-final and pre-consonantal tokens\nIt occured to me that the search patterns that I used only included words that, in UK English, are pronounced with /ɹ/ (i.e. the /ɹ/ is intervocalic). So you’ll notice that caring is on the list of Mary words but care is not. So I searched for tokens in Britphone with “ˈɛə” that are not followed by an /ɹ/ (so, preconsonantal and word-final) to produce the following list.\nThis list includes words that I thought were from different lexical sets. For example, care and hair are on this list, which are presumably part of Mary since caring and hairy are too, but then the word square is also in this list, which is, by definition, part of merry, since merry is just square. But, when I asked for clarification on Twitter, I was told that these are all square—and therefore Mary.\n\n\nOkay, so what about \"pair\", \"pear\", and \"pare\"? Is that a minimal triplet for the non-MMM-mergers out there? Britphone doesn't differentiate the classes word-finally, so \"care\" and \"square\" are both ˈɛə even though I'm pretty sure they're respectively DRESS and SQAURE. https://t.co/3esqw2l4FO\n\n— Joey Stanley (@joey_stan) September 22, 2020\n\n\nAnyway, so here’s the fuller list:\n\naffair(s), air, aircraft, airfare, airline(s), airlines, airplane, airport(s), airways, aware, awareness, Ayrshire, bare(ly), bear(s), beware, Blair, blare, care{s|d}, careful(ly), chair(s), Clair(e), clare, compare(d), dare, declare(d), despair, downstairs, fair(ly), fare(s), flair, flare, glare, hair{s|ed}ed), haircut, hairdresser(s), hare, heir, heirloom, hilaire, impaired, lair, mare, mayor, pair(s), pare, pear, Pierre, prayer(s), prepare(d), questionnaire, rare(ly), repair(s), scarce(ly), scare(d), share{s|d}, shareholder(s), shareware, snare, spare, square{s|d}, stair(s), staircase, stare(d), swear, tear(s), their(s), there, therefore, unaware, unfair, upstairs, ware, warehouse, wear(s), where, wherewhithal\n\n\n\nSupplemental Mary words\nFinally, when I searched the The Routledge Dictionary of Pronunciation for Current English, I found these 695 words containing the search pattern “ɛ:r”. I didn’t bother incorporating them into the above list because many of them are very infrequent words. But I wanted to include them for the sake of completeness.\n\nAaron, abecedarian, Abertillery, actuarial, actuarially, adversarial, aerial, aerialist, aeriality, aerially, aeriated, aerie, aeriform, aero, aerobatic, aerobe, aerobiologist, aerobiology, aerodrome, aerodynamic, aerodynamically, aerodynamicist, aerodyne, aero-engine, Aeroflot, aerofoil, aerogram, aerogramme, aerolite, aerological, aeromagnetic, aeronaut, aeronautic, aeronautical, aeronautically, aeroplane, aerosol, aerospace, aerostat, aerostatic, aerostatically, aerotow, aerotrain, aery, affair, agrarian, Ahasuerus, airer, airily, airiness, airing, airy, Althusserean, Althusserian, Antares, antimalarial, antiquarian, antiquarianism, antisabbatarian, antitrinitarian, aorist, apiarian, Apollinaris, aquaria, Aquarian, aquarium, Aquarius, araucaria, area, areal, areaway, areometer, Ares, Arian, Arianism, ariel, Aries, Arius, armamentaria, armamentarium, armillaria, aroid, arum, Aryan, authoritarian, authoritarianism, Azeri, Balearic, ballbearing, Ballesteros, barbarian, baric, barite, barium, Bavaria, Bavarian, bearability, bearable, bearably, bearer, bearing, bearish, bearishness, Behrens, Behring, Belisarius, Berengaria, Bering, billionairess, bolero, Buenos Aires, Bulgaria, Bulgarian, burglarious, burglariously, bursarial, caballero, caesarean, caesarian, calcareous, calcareousness, calceolaria, caldaria, caldarium, caldera, Canaries, canary, Cancerian, carabiniere, carabinieri, carer, Carew, Carey, Caria, caries, caring, carious, Carpentaria, Carreras, Cary, cassowary, centenarian, cercaria, cercariae, certiorari, cesarean, cesarian, charily, chariness, Charing Cross, Charon, chary, cheeseparing, childbearing, ciguatera, cineraria, cinerarium, clairaudience, clairaudient, Clara, clary, columbaria, columbarium, commissarial, commissariat, communitarian, condottiere, condottieri, contrarily, contrariness, contrariwise, contrary, cordillera, costmary, covariance, cruzeiro, cupbearer, daguerreotype, daguerrotype, dairy, dairying, dairymaid, dairyman, dairymen, darer, Dari, Darien, daring, daringly, Darius, de-aerate, declarable, declarant, declaredly, declarer, Demerara, denarii, denarius, despairingly, de Valera, dinero, disciplinarian, doctrinairism, doctrinarian, dolphinarium, Dun Laoghaire, egalitarian, egalitarianism, √âire, equalitarian, equalitarianism, establishmentarian, establishmentarianism, Europarliamentarian, eyrie, eyry, faerie, Faeroe Islands, Faeroes, Faeroese, faery, fairing, fairish, fairy, fairyland, Fareham, faro, Faro, Faroe, Faroese, filaria, filariae, filarial, filariasis, Fleet Air Arm, forastero, forbearance, forbearingly, frigidaria, frigidarium, fruitarian, fusaria, fusarium, futilitarian, garish, garishly, garishness, gharial, Gibraltarian, glaireous, glairiness, glairy, glaringly, glaringness, glary, glossarial, godparent, grammarian, Gran Canaria, grandparent, gregarious, gregariously, gregariousness, Guarneri, Guarnerius, Guerrero, Guti√©rrez, habanera, hairily, hairiness, hairy, Halmahera, Hanoverian, hardwearing, harem, harum-scarum, heiress, herbaria, herbarium, Herero, Herrera, hilarious, hilariously, hilariousness, honoraria, honorarium, houseparent, humanitarian, humanitarianism, Hungarian, Indo-Aryan, inegalitarian, infralapsarian, insectaria, insectarium, invariability, invariable, invariableness, invariably, invariance, invariant, Inveraray, Karaite, lairage, lairy, lares, latitudinarian, latitudinarianism, Lehrer, leprosaria, leprosarium, libertarian, libertarianism, librarian, librarianship, Lilliburlero, llanero, Lothario, lumpenproletariat, lupus vulgaris, mace-bearer, Mainwaring, majoritarian, malaria, malarial, malarian, malarious, Marian, mariolatry, Mariology, Marist, Mary, Mary Celeste, Maryland, Mary Magdalene, Maryport, Maseru, mayoral, mayoralty, mayoress, miliaria, militaria, millenarian, millenarianism, millenarianist, millionairess, miserere, multifarious, multifariously, multifariousness, multivariate, nareal, nares, narial, nary, necessarian, necessarianism, necessitarian, necessitarianism, nectarean, nectareous, nefarious, nefariously, Nehru, nightmarish, nightmarishness, nonagenarian, notarial, notarially, Nyerere, obituarial, oceanaria, oceanarium, octogenarian, octonarian, octonarii, octonarius, Old Sarum, O’Meara, omnifarious, Ontario, ovarian, ovariectomy, ovariotomy, pairing, pallbearer, pampero, pareira, parent, parentage, parenthood, parentless, parer, Parian, paring, parliamentarian, Perak, pereira, Pharaoh, Pharaonic, Pharoah, pharos, Pierian, Pinero, planarian, planetaria, planetarium, platitudinarian, plein-airist, potrero, prairie, precarious, precariously, precariousness, predestinarian, prelapsarian, preparedness, preparer, primavera, proletarian, proletarianisation, proletarianise, proletarianism, proletarianization, proletarianize, proletariat, pulmonaria, quadragenarian, quinquagenarian, quodlibetarian, radiolaria, radiolarian, ranchero, rara avis, raree-show, rarefaction, rarefactive, rarefication, rarefy, rarify, raring, rarity, Rarotonga, Rarotongan, Rastafarian, Rastafarianism, repairable, repairer, retiarii, retiarius, riparian, Ripuarian, Rivera, Riviera, Romero, rosaria, rosarian, rosarium, Rotarian, sabbatarian, sabbatarianism, sacramentarian, sacraria, sacrarium, Sagittarian, Sagittarius, salariat, Salieri, Samaria, samarium, sanataria, sanatarium, sanitaria, sanitarian, sanitarium, Sara, Sarah, sarify, Saros, Sarum, Sauveterrian, scarer, scarification, scarificator, scarifier, scarify, scarily, scariness, scarious, scaroid, scarus, scary, scenario, seafarer, seafaring, secretarial, secretariat, sectarian, sectarianise, sectarianism, sectarianize, sederunt, seminarian, senarii, senarius, septenarii, septenarius, septuagenarian, sexagenarian, shareable, share-out, sharer, Sharon, sharon fruit, snarer, solaria, solarium, sombrero, sparer, sparerib, sparing, sparingly, sparingness, square-eyed, squarer, Squarial, squarish, starer, step-parent, Stradivarius, sublapsarian, sub-librarian, sudaria, sudarium, Sumerian, supralapsarian, swearer, swordbearer, talaria, talebearer, talebearing, tearable, tearaway, tearer, tear-off, temerarious, tepidaria, tepidarium, termitaria, termitarium, terraria, terrarium, therabouts, thereabout, thereafter, thereanent, thereat, therein, thereinafter, thereinbefore, thereinto, thereof, thereon, thereout, thereunder, thereunto, thereupon, time-sharing, Tipperary, Tocharian, Tokharian, topiarian, torch-bearer, torero, totalitarian, totalitarianism, tractarian, Tractarian, Tractarianism, transparence, transparency, transparent, transparently, transparentness, Trinitarian, Trinitarianism, Trocadero, turbellarian, ubiquitarian, ubiquitarianism, unbearable, unbearableness, unbearably, unbury, uncaring, uncaringly, uniformitarian, uniformitarianism, Unitarian, Unitarianism, unpreparedly, unpreparedness, unrepairable, unsectarian, unsparing, unsparingly, unsparingness, untearable, unvaried, unvarying, unvaryingly, unvaryingness, unwarily, unwariness, unwary, unwearable, urticaria, utilitarian, utilitarianism, vagarious, Valera, valerian, valetudinarian, valetudinarianism, vaquero, varia, variability, variable, variably, variance, variant, variate, variation, variational, variationally, variationist, varices, varicolored, varicoloured, varied, variedly, variegate, variegation, varifocal, variform, variolate, variole, variolite, variolitic, varioloid, variolous, variometer, variorum, various, variously, variousness, varix, varus, vary, varyingly, vegetarian, vegetarianism, veterinarian, vicarial, vicariate, vicarious, vicariously, vicariousness, vivaria, vivarium, vulgarian, Wareham, Wareing, warily, wariness, Waring, wary, wayfarer, wayfaring, wearability, wearable, wear-and-tear, wearer, wearing, wearingly, welfarism, welfarist, whereabouts, whereabouts, where’er, whereupon, worksharing"
  },
  {
    "objectID": "blog/mary-merry-marry/index.html#conclusion",
    "href": "blog/mary-merry-marry/index.html#conclusion",
    "title": "A big list of Mary-merry-marry words",
    "section": "Conclusion",
    "text": "Conclusion\nI hope this list is useful to other folks who have the merger but need to distinguish them for whatever reason. I’m starting to remember which class words belong to a little bit, though this is just learned knowledge rather than intuition. Also, if anyone has different intuitions than what this page shows, please let me know!"
  },
  {
    "objectID": "blog/brand-yourself/index.html",
    "href": "blog/brand-yourself/index.html",
    "title": "Brand Yourself",
    "section": "",
    "text": "Note\n\n\n\nGo here to see a more recent version of this talk.\nToday Emily McGinn of the Digital Humanities Lab at UGA and I did a workshop called “Brand Yourself: A professionalization workshop for grad students” [Edit: and again by invitation April 13, 2017]. We gave a presentation on different ways grad students can boost their online presence through building a personal webpage, utilizing social media, and finding your field’s conversation. We then let the attendees a chance to work on their own to create a new online profile, using what they learned."
  },
  {
    "objectID": "blog/brand-yourself/index.html#social-media",
    "href": "blog/brand-yourself/index.html#social-media",
    "title": "Brand Yourself",
    "section": "Social Media",
    "text": "Social Media\nAcademia.edu is a social networking site for academics. Users can create profiles, upload their papers, and follow particular research topics. They can also follow others that have done the same. It’s a great resource for finding papers that may be behind a paywall, although it has gotten a lot of criticism for this. Papers you upload can be found by Google Scholar, which is a nice perk. The website will keep track of your analytics, and there’s nothing more thrilling than getting an email saying someone has found your profile!\n\n\n\n\n\nThe site got some criticism for offering authors the chance to promote their work for a fee. There’s also a chance at any time the site could get shut down because publishers aren’t happy about it, but with 30 million users, I don’t know if that’s going to happen any time soon.\nI’m less familiar with ResearchGate, but in my cursory look, there’s a lot of overlap with academia.edu as far as its features. A big difference I noticed is that it seems like it’s more focused on creating networks based on people you cite and your co-authors while academia.edu is more focused on following your field and your interests. One thing I don’t like about ResearchGate is that the number of emails it sends you is borderline spam. It invites me to follow other grad students at my university, but, no offense to the sciences, I’m not particularly concerned with what a microbiologist on the other side of campus is doing.\nI would imagine most researchers use Google Scholar regularly, but did you know you can create a profile for others to see? You can tell a researcher has done that when you see their name underlined in a search:\n\n\n\n\n\nIn this screenshot (live link here), you can see that Walt Wolfram, Natalie Schilling, Sali Tagliamonte have created their profiles, but Shanna Poplack and Penny Eckert have not. I’d like to see what else the last two researchers have written, but I can’t simply click on their names like I can with the first three. When you do click on their links, you can see the full profile including what else they have written and how many times each has been cited.\n\n\n\n\n\nIt does take a bit of work to get a full profile going, because Google’s data can be a bit messy, so you’ll have to add stuff in by hand. But I think the payoff is worth the effort.\nThere are a handful of other websites out there that can help you build an online presence. Impact Story is one that can keep track of how much of an impact you have on people by keeping track of when people cite, mention, read you and your work. For $10 a month, it might not be worth it for a grad student, but for a professor applying for tenure this might be.\nLinkedIn is one I should mention, but I don’t find it terribly useful for academics. It might be worth it to set up a low-maintenance page that gives a good view of you in a nutshell, just in case people look."
  },
  {
    "objectID": "blog/brand-yourself/index.html#building-a-personal-webpage",
    "href": "blog/brand-yourself/index.html#building-a-personal-webpage",
    "title": "Brand Yourself",
    "section": "Building a Personal Webpage",
    "text": "Building a Personal Webpage\nKeeping track of all these profiles can be tedious. Do you need to update seven different profiles every time you present at a conference? Is it worth it to invest the time in these sites that don’t communicate with each other? One solution is to keep your top (or only!) three or four papers on the social media sites, but include links to a central page that has your full profile. For this reason, it’s nice to have a personal webpage.\nThe problem with personal webpages it that they come with a cost, either in money or skills (and sometimes both). You can set up a webpage through Word Press, Wix.com, or Square Space, which take little technical skill to get a professional page set up. These can be free, but you can get some extra features for $10 a month or more. To me, that’s a pretty penny to pay for a relatively simple webpage.\nAnother option, which is what I did for the previous version of this website, is to host the page on Github. It’s free, but it takes a bit of skill. I’ve had to learn to use Jekyll, Markdown, and CSS, but through some help on ProgrammingHistorian.com and Lynda.com, I was able to get this site up. The benefit of going this route is I have unlimited flexibility in how the site looks, and I really, really like that.\nEither way, it’s probably worth it to set up a personal domain name. For as little as $1 a month, you can buy your own domain name (like www.joeystanley.com), which looks much more professional than www.blogsplot.com/joeystanley or www.github.com/joeystanley."
  },
  {
    "objectID": "blog/brand-yourself/index.html#finding-your-conversation",
    "href": "blog/brand-yourself/index.html#finding-your-conversation",
    "title": "Brand Yourself",
    "section": "Finding Your Conversation",
    "text": "Finding Your Conversation\nThe last thing we talked about in our workshop is to find where the big names in your field are having their online conversations. This sounds a little weird at first, but every field has some secret space where people are collaborating and sharing ideas informally as well as posting calls for papers, invitations for publications, and job openings. The problem is that where is space is is different for every field.\nIn some fields, these are a listserv. As far as I know, network analysis and Slavic languages each have a well-known listserv where all the conversation happens. If you’re not on that listserv, you’re out of the loop. Digital Humanities has a space on Slack where over 800 researchers get together and talk. For some fields, it might just be at coffee breaks during certain conferences. You may have to ask around established academics in your field to find that space.\nOne thing I will mention is that a lot of action happens on Twitter. I’ve covered this in more depth in an earlier blog post, but basically a lot of good stuff can come out of following the right people and seeing just the right tweets."
  },
  {
    "objectID": "blog/brand-yourself/index.html#conclusion",
    "href": "blog/brand-yourself/index.html#conclusion",
    "title": "Brand Yourself",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, I thought the workshop went very well. Most of the attendees did end up setting up some sort of profile: some did an academia.edu profile, some google scholar, and a few were ambitious and set up a github page. At the very least, I got this very webpage set up as a result of preparing for this workshop, and I learned a lot about all these other pages. It was a great feeling to see a dozen students directly benefiting from our presentation.\n\nYou can download the old version slideshow we used for this presentation here.\nI am indebted to the Impact Challenge blog series, with the accompanying 200+ page pdf, from which I learned a lot about all this. I would highly recommend that you download it and take a look. Not only does it include much more than what I’ve mentioned here, including step-by-step how-to guides to getting these profiles set up, but also many more topics to get yourself more visible. Thanks, Impact Challenge."
  },
  {
    "objectID": "blog/lsa2017/index.html",
    "href": "blog/lsa2017/index.html",
    "title": "LSA2017",
    "section": "",
    "text": "Last weekend, I had the opportunity to present at the 2017 Annual Meeting of the American Dialect Society, as well as attend the other meetings of the Linguistic Society of America annual meeting in Austin, TX. There were a lot of really awesome things about the whole thing.\nFirst off, I feel like I had a great experience giving my presentation. I presented Thursday afternoon in the session called “Vowels, Vowels, Vowels” which was chaired by Erik Thomas, and saw other presentations by Charlie Farrington & Tyler Kendall, Matthew Gordon, and Michol Hoffman. There were about 30 people in the room, and I could name about half of them. In fact, while summarizing previous research, I realized that half the people I cited were sitting right there. Afterwards, I had a lot of discussion and great feedback. I couldn’t have asked for a better experience.As part of the presentation, I showed this video.\nDuring the course of the next several days, I made a point to introduce myself to people. Networking is an important part of going to conferences, and I haven’t really taken that opportunity in the past. So I was able to meet several of the greats and tell them how much I enjoyed some of the things they’ve written. I also met some grad students that have similar interests as me. I feel a lot more connected to other researchers than I did before.\nGoing into this conference, I knew I wanted to give it everything I had. I got funding from UGA for the first time, both through the Linguistics Program and the Graduate School, and I wanted to make sure the money I received went to a good cause. The conference was busy, but I attended as many presentations as I could. In fact, I hardly had time to eat, and ended up only eating one meal a day during the four days. I actually lost four pounds attending this conference! I also stayed at an Airbnb for the first time, and I didn’t want to take the bus all the way to the apartment when I knew there were things to do at the conference. I ended up attending 35 presentations and visited about a dozen posters. It made for four very long and busy days, but they were extremely productive.\nI also made an effort to be active on Twitter during the conference, but I have a separate blog post about that which you can read here.\nOverall, a fantastic experience. The best I could have hoped for, and the best conference I’ve been to."
  },
  {
    "objectID": "blog/lcuga5/index.html",
    "href": "blog/lcuga5/index.html",
    "title": "LCUGA5",
    "section": "",
    "text": "Today, I was fortunate to give two presentations on very different areas of my research at the 5th Annual Linguistics Conference at UGA, one on an obscure consonantal phonological pattern in the West using new recordings and another on well-studied vowel shifts in the South using very old recordings."
  },
  {
    "objectID": "blog/lcuga5/index.html#thr-flapping",
    "href": "blog/lcuga5/index.html#thr-flapping",
    "title": "LCUGA5",
    "section": "(thr)-flapping",
    "text": "(thr)-flapping\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nFirst, I presented on something I’ve noticed in a few speakers, something I call (thr)-flapping. Some people pronounce the /ɹ/ after /θ/ as a flap [ɾ]. In this presentation, I presented some data supporting this hunch.\nThe articulatory motivations are clear: as the tongue tip moves from between the teeth to a retroflexed position, it may make brief contact with the alveolar ridge. What may have started as an accidental gesture appears to have been phonologized by some speakers.\nLooking at phonological factors, in my data (thr)-flapping happened more when the following vowel was non-high and non-front. So it happened more in throb, throng, throne, thrust, and thrive than in thrash, threaten, thread, thrill, three and through. I offer some tentative explanations for this, but without articulatory data, I can’t know for sure.\nTo my surprise, the social factors I looked at were the opposite of what I expected. Age and sex were not significant, meaning there’s probably not a lot of change in time. But what state people came from (Washington verses Utah—my two fieldsites) was significant. Utah English has a lot of hyperarticulated consonants and at ADS this year, Di Paolo & Johnson (2018) hypothesize that this has to do with the high proportion of members of the Church of Jesus Christ of Latter-day Saints in Utah. Since public speaking is a common part of their worship services (even as early as the age of 3!), elements of this hyperarticulated register may have spread into other speech styles. (thr)-flapping may be just another manifestation of that. But without attitude or perceptual data, I can’t know for sure.\n(thr)-flapping is something I’ve noticed for a while now, and despite the shortcomings of this study, and I was glad to finally present solid evidence that it is a thing!"
  },
  {
    "objectID": "blog/lcuga5/index.html#vowel-shifts-in-southern-american-english",
    "href": "blog/lcuga5/index.html#vowel-shifts-in-southern-american-english",
    "title": "LCUGA5",
    "section": "Vowel Shifts in Southern American English",
    "text": "Vowel Shifts in Southern American English\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nIn the next session, I presented research I been doing with Peggy Renwick on the vowel shifts in the South. The Southern Vowel Shift has front lax vowels raising and front tense vowels lowering, resulting in vowel pairs swapping. Meanwhile, back vowels are fronting. The African American Vowel Shift has the front lax vowels raising, but tense vowels are not lowering, so there wouldn’t be any swapping. Furthermore, back vowels typically aren’t fronted.\nWe use the Digital Archive of Southern Speech, a corpus of interviews from the 1970s and 1980s. The people in these recordings were born while these shifts were going on, so we can see their development in a way that newer recordings wouldn’t be able to do.\nUsing Pillai scores and linear mixed-effects models, we find that younger, European American women are leading in the front vowel swapping and that African Americans are participating less in the back vowel fronting. These findings are exactly what we expect, showing that these older recordings confirm what newer ones suggest."
  },
  {
    "objectID": "blog/mount-st-helens-and-vowels/index.html",
    "href": "blog/mount-st-helens-and-vowels/index.html",
    "title": "Mount St. Helens and Vowels",
    "section": "",
    "text": "Today in our Linguistics Colloquium here at UGA, I got to present on some of my ongoing research on English in a smaller town in Washington. For the past few months I’ve mostly looked at vowel mergers and using lots of statistical tests to show some very subtle changes. Over the past week or so as I’ve prepared for this presentation, I’ve discovered something pretty awesome about my data. And it has to do with Mount St. Helens!\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nIn the presentation, I focus on a couple linguistic variables. The first is what linguists call /æg/-raising, which is where words like bag, flag, and dragon to sound more like bayg, flayg, or draygon. The other variable is what we call /o/-fronting and /o/-monophthongization, which is where vowel sounds in words like go, snow, or show sound kinda like they would in stereotypical “Minnesohhta”. These have been studied extensively by researchers in the West regarding Pacific Northwest English and surrounding regions. So, nothing new here.\nBut looking at the data in relation to speaker age, I noticed a striking pattern: there’s a clear difference between the speech of people born before 1970 and those born after. I mean really clear. In my sample, /æg/ raising virtually disappears after 1970, and /o/ is suddenly diphthongal. /o/ admittedly gradually fronts, so the 1970 date isn’t quite as drastic in that regard.\nSo what happened in 1970? Well, not much. But in 1980, Mount St. Helens erupted and seriously affected the logging-based economy of Longview. Up until then, it was easy to find work in the logging industry with only a high school degree, if that. And the salary was relatively good considering it was blue collar work. But when some of the mills started to close, there was a drastic change in the dynamics of the town. Now you need a college education to get a job and even then it’s not paying well.\nSo though nothing happened in 1970, those who were born around then were teenagers at the time of this change, and were the first affected by the lack of easy-to-get, high-paying jobs. This marks a paradigm shift in the culture of Longview, and I believe it had to do with the clear changes in the speech. In other words, I think Mount St. Helens played a role in linguistic change in Longview. (The title of the talk was “Volcanic Vocalic Changes”—a title I’m quite proud of!)\nThis is super exciting for me because up until now most of my work has been phonetic-based and focused on vowel mergers. This the first clearly sociolinguistic project I’ve done—something I’ve been meaning to do this whole time—and I think the results are cool. It’s uncharacteristically qualitative and the statistics don’t play a huge role, which is weird for me. I like this change and I hope I can do more with this research."
  },
  {
    "objectID": "blog/ar-raising/index.html",
    "href": "blog/ar-raising/index.html",
    "title": "/ɑr/-Raising",
    "section": "",
    "text": "I’ve noticed for a while in my own speech that the vowel in star is higher and longer than start. I have American Raising, which, simplifying a bit, is where /aɪ/ is raised before voiceless consonants. So I just expected this to be another manifestation of that. I had some time so I thought I’d test this empirically. So here’s a breif study on my own speech to figure out what’s going on.See Davis & Berkson 2021 for more detail on American Raising. I particularly liked Moreton’s chapter because it shows how stress, syllable structure, and morphologica structure all matter."
  },
  {
    "objectID": "blog/ar-raising/index.html#hypothesis",
    "href": "blog/ar-raising/index.html#hypothesis",
    "title": "/ɑr/-Raising",
    "section": "Hypothesis",
    "text": "Hypothesis\n/ɑr/ raises before voiceless segments."
  },
  {
    "objectID": "blog/ar-raising/index.html#methods",
    "href": "blog/ar-raising/index.html#methods",
    "title": "/ɑr/-Raising",
    "section": "Methods",
    "text": "Methods\nIn COCA’s list of the most frequent 5000 words, there are 88 that have the /ɑr/ sequence in stressed position. Here they are in order of frequency.\nlarge, car, art, party, heart, article, artist, argue, hard, card, bar, yard, garden, partner, sorry, argument, department, apartment, farm, tomorrow, start, army, farmer, largely, hardly, smart, sharp, regarding, dark, far, mark, marketing, parking, darkness, armed, chart, remarkable, market, garlic, partly, partnership, regardless, target, carbon, borrow, margin, architect, park, architecture, march, harsh, particle, guitar, guard, alarm, starter, carve, starting, departure, sharply, hardware, garbage, cart, barn, carpet, star, jar, shark, charter, charge, partially, harm, artifact, partial, marketplace, part, harmony, remark, regard, arm, apart, marble, charm, marker, spark, harvest, depart, cargo\nThe words were randomized and I recorded myself reading them using Praat in a quiet place with a good mic. I segmented out the /ar/ sequence by hand (incidentally, I sounded like a seal: “ar ar ar ar”). Using a Praat script, I extracted formants at 30% of the way into the /ar/ sequence (what seemed to be about the midpoint of the vowel). Filtered out a few bad measurements. Analyzed in R. For this analysis, word-final /ar/ is being treated as pre-voiced."
  },
  {
    "objectID": "blog/ar-raising/index.html#results",
    "href": "blog/ar-raising/index.html#results",
    "title": "/ɑr/-Raising",
    "section": "Results",
    "text": "Results\nI’ll split my analysis up into monosyllabic words and polysyllabic words because the results were cleaner.\n\nMonosyllabic words\nThe effect that place of articulation had on duration was clear. A simple boxplot shows the stark contrast.\n\nWith the exception of arm being short and mark being long, there’s a clear difference. In fact, the longest ones were mostly the word-final /ɑr/ words, but for this analysis I’ll group word-final and pre-voiced into one category.\nMoving on to vowel quality, it’s clear that the voicing of the following segment has an effect. In fact, there’s virtually no overlap between the two vowel classes.\n\nI ran a MANOVA test that took into account F1, F2, and duration, and the results show that the two groups are distinct.\nsummary(manova(cbind(F1, F2, dur) ~ fol_voice, data=ar))\n\n          Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nfol_voice  1 0.51552   28.375      3     80 1.348e-12 ***\nResiduals 82                                                                                        \n The p-value is small, which indicates the two are different, but I’ve noticed they tend to be on vowel data even for what seems like merged classes. More importantly, the Pillai score is pretty high: on a scale from 0 (=complete overlap) to 1 (completely separate), it’s about in the middle.Using the formula that me and Betsy Sneller came up with in our 2022 paper, we’d expect a Pillai score less than 0.0618 with this much data if the two classes were underlyingly merged. The fact that it’s so high with a sample size of 88 is pretty indicative of separation. In case the visual wasn’t clear enough.\nOut of curiosity, I did a k-means clustering analysis. I made it blind to phonetic environment so it finds the best two clusters based on F1 and F2 alone. As expected, the clusters essentially captured the voicing distinction, but there were a couple exceptions: march is voiced but clustered with the voiceless words, and both yard and guard are voiced but clustered with the voiceless words.\nConclusion so far: with the exception of a few words, it’s pretty clear that /ɑr/ is raised, fronted, and shorter before voiceless sounds.\n\n\nPolysyllabic words\nWhen there’s more than one syllable, things get complicated because of their distribution within the word. Most of the two-syllable words have word-initial stress, but there are a few misfits (alarm, depart, guitar, regard, remark). With three syllables stress is either on the first syllable (architect, argument, article, artifact, harmony, marketing, marketplace, partially, particle, partnership) or the second (apartment, department, departure, regarding, regardless, tomorrow). There were two four-syllable words, with the /ɑr/ segment on different syllables (architecture, remarkable). Most of these can be split up by the voicing of the following segment too, which spreads the data pretty thin.\n\nGenerally, the the raised variant occurs with voiceless segments, but this data is a little bit messier. What’s interesting is to look at the exceptions.\nThere were several pre-voiced segments that appeared to be raised: sorry, regardless, target (that one’s hard to see), regarding and maybe argue, argument, garden, and regard. I can’t help but notice that a lot of those words have the sequence /gɑr/. In fact, the only other /gɑr/ words were garlic and garbage, which were the most fronted voiced words even though they weren’t raised.\nIt’s interesting that guard was one of the few exceptions on the k-means clustering analysis on the monosyllabic words and was raised and fronted compared to the others. A part of me wants to think that /ɑr/ is raised after /g/, which would be cool.\nIt’s also worth noting that most of the voiceless words that were outside the ellipse were after bilabials: depart, market, remark, remarkable, etc. These are actually near the voiced segments after bilabials like borrow and tomorrow. This even explains why march was one of the exceptions detected by the k-means clustering analysis. Perhaps bilabials have something to do with backing."
  },
  {
    "objectID": "blog/ar-raising/index.html#discussion",
    "href": "blog/ar-raising/index.html#discussion",
    "title": "/ɑr/-Raising",
    "section": "Discussion",
    "text": "Discussion\nThere is a phonetic explanation for the behavior of both the velars and the bilabials. Let’s talk about transition formants for a second. To start, the low vowel /ɑ/ has a high F1 and a low F2. Velars cause F1 to lower and F2 to raise (the latter as a part of the velar pinch), which is more like a higher, fronter vowel. Meanwhile, bilabials cause all formants to lower (because of the slight lengthening of the vocal tract), meaning raising and backing.\nWhat’s happening is that some of these transition formants are raising the nucleus of the /gɑr/ sequence. The reason why we don’t see raising in /kɑr/ sequences such as car, carbon, cargo, carve, etc. is because of the extra padding of the aspiration. Transition formants do appear in aspiration and the fact that this VOT increases the time between the velar stop and the vowel gives my articulators time to transition to a full /ɑ/.\nI don’t know of a good formal way to test this right now, but we can look at measurements at other points along the vowels’ durations and see if they’re predicted by the place of articulation. Moving closer to the vowel onset, the /gɑr/ words are even higher and fronter. And in fact, at 10% into the /ɑr/ sequence, the distribution is more easily predicted by the previous place of articulation.\n\nHere, we see that the /gɑr/ sequences (highlighted in green now) are all very high and very front, together with /jɑr/ and /ʃɑr/ which have similar transition formant patterns. Similarly, the /ɑr/ words after bilabials (now in yellow) are somewhat backer, though this isn’t as stark. In fact the difference in voicing of the following stop is much smaller, and might not even be significant at all anymore.\nIf we measure later in time, the influence of the /r/ and its following consonant is increasing and the distinctions between the vowels also decreases.\n\nHere we see that at halfway through the /ɑr/ sequence, the /gɑr/ words are still among the higher and fronter within the blue cloud. But at 70%, they are pretty much randomly dispersed among the other pre-voiced segments. The bilabials make it clear that the following place of articulation predicts this vowel plot: words like mark, park, and spark are higher and fronter due to the velar pinch.\nGoing back to that 30% point, which is roughly the middle of the vowel itself, here it is again, but with the highlights.\n\nHere, the position of each word within the voiced/voiceless cloud is already being influenced by the surrounding consonants. Word that end in velars like mark, park, and spark are already fronting. Nearly all the /gɑr/ words are followed by an alveolar sound, which cause some raising and fronting but not as extreme as velars. So it makes sense that they would stay high but not as high as the pre-velars."
  },
  {
    "objectID": "blog/ar-raising/index.html#conclusion",
    "href": "blog/ar-raising/index.html#conclusion",
    "title": "/ɑr/-Raising",
    "section": "Conclusion",
    "text": "Conclusion\nIn my speech, I definitely have evidence to reject the null hypothesis and to conclude that /ar/ is raised before voiceless segments. I think that part is pretty clear.\nI’ll admit, I came into writing this blog in hopes of showing that my /gɑr/ sequence is also raised. But after doing more tests, visualizations, and especially after taking into account the pattern of the bilabials, I just don’t have enough evidence to suggest it’s anything more than influence of surrounding consonants.\nThis makes me think hard about how to interpret vowel plots in the future. Every combination of surrounding consonants has its own trajectory, and it’s crazy that we can find patterns anywhere. This means ideally we should take into account more consonantal influences when interpreting vowel data. But at the same time that often spreads out data out way too thin since there will be only so a few tokens for any combination.\nI’ve seen studies where they’ll put following place of articulation as a random effect in a mixed-effects model. In my opinion, that doesn’t seem methodologically sound because there’s a finite number of possible options, we generally know their effects, and any duplication study would mostly have the same places of articulation. Now, I’m reconsidering this, and I wonder if it might be a good idea to put a combination of previous and following consonant as a random factor (since they interact). While we probably know the effects of these interactions, any replication of the study might not have the same words and therefore might not have the exact same combinations of previous and following consonants. It’s quite common to go the extreme route and just put each word as a random effect since “every word has its own history.”\nIt’s also important to consider trajectory information more. As I showed above, just looking at one measurement can lead to erroneous claims, and we can get a more fuller picture by looking at the trajectory information. There are statistical methods out there that we can use for trajectories (SS-ANOVA, GAMMs, etc.) but I haven’t quite got the hang of them yet.\nWell, I learned a few things by doing this quick study: surrounding consonants affect vowels and trajectories are important. Seems like old news, but it’s nice to have learned this for myself."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Visualizing Jonathan Dowse’s Vowels\n\n\n\n\n\n\n\nphonetics\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nWebsite Version 3\n\n\n\n\n\n\n\nMeta\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV51\n\n\n\n\n\n\n\nConferences\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nThe stories behind the languages I’ve studied\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nMy cot-caught distribution\n\n\n\n\n\n\n\nDissertation\n\n\nLexical Sets\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nSoSy\n\n\n\n\n\n\n\nConferences\n\n\nPresentations\n\n\nResearch\n\n\nStudents\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n10 Years of wanting to be an academic\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLSA and ADS 2023\n\n\n\n\n\n\n\nConferences\n\n\nPresentations\n\n\nResearch\n\n\nStudents\n\n\nUtah\n\n\nWest\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNew publication in Linguistics Vanguard\n\n\n\n\n\n\n\nMethods\n\n\nResearch\n\n\nPublications\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nPrevelar raising paper published in American Speech\n\n\n\n\n\n\n\nPublications\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV50\n\n\n\n\n\n\n\nConferences\n\n\nMethods\n\n\nPhonetics\n\n\nPresentations\n\n\nResearch\n\n\nSimulations\n\n\nStatistics\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nIdaho, Montana, Wyoming, and Utah English Survey Results\n\n\n\n\n\n\n\nFor the public\n\n\nResearch\n\n\nUtah\n\n\nWest\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNew publication in the Penn Working Papers in Linguistics\n\n\n\n\n\n\n\nMethods\n\n\nResearch\n\n\nPublications\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nUsing Phonic for Collecting Sociophonetic Data\n\n\n\n\n\n\n\nHow-to Guides\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nAnimating Formant Trajectories\n\n\n\n\n\n\n\nAnimations\n\n\nData Viz\n\n\nHow-to Guides\n\n\nR\n\n\nSkills\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nADS and LSA 2022\n\n\n\n\n\n\n\nAnimations\n\n\nConferences\n\n\nData Viz\n\n\nMTurk\n\n\nPhonetics\n\n\nPresentations\n\n\nR\n\n\nResearch\n\n\nSouth\n\n\nStudents\n\n\nUtah\n\n\nVowel Overlap\n\n\nWest\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nCurved Text in ggplot2 with geomtextpath\n\n\n\n\n\n\n\nData Viz\n\n\nHow-to Guides\n\n\nSide Projects\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nASA181\n\n\n\n\n\n\n\nConferences\n\n\nDissertation\n\n\nMethods\n\n\nPacific Northwest\n\n\nPhonetics\n\n\nPresentations\n\n\nR\n\n\nResearch\n\n\nSimulations\n\n\nStatistics\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV49\n\n\n\n\n\n\n\nConferences\n\n\nMethods\n\n\nPresentations\n\n\nResearch\n\n\nSimulations\n\n\nSouth\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n365 Papers (Update)\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nKohler Tapes (Update)\n\n\n\n\n\n\n\nResearch\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nKohler Tapes\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nPillai scores don’t change after normalization\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nPhonetics\n\n\nR\n\n\nSkills\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n10 Years of Linguistics\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n“Boustrophedonically”\n\n\n\n\n\n\n\nData Viz\n\n\n\n\nEarlier this week, I tweeted about a data visualization that I made. It’s a new kind of plot, so interpretation isn’t super straightforward, but it’s funny, silly, surprising, intersting, and memorable, which is why I think it’s a good one.\n\n\n\n\n\n\nDec 18, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNew publication in the latest PADS volume\n\n\n\n\n\n\n\nPacific Northwest\n\n\nPublications\n\n\nResearch\n\n\nWest\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\ngenerations: Convert birth years to generation names\n\n\n\n\n\n\n\nGithub\n\n\nR\n\n\nR Packages\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\njoeysvowels: An R package of vowel data\n\n\n\n\n\n\n\nGithub\n\n\nMethods\n\n\nPhonetics\n\n\nR\n\n\nR Packages\n\n\nSide Projects\n\n\nTeaching\n\n\nWest\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nA big list of Mary-merry-marry words\n\n\n\n\n\n\n\nLexical Sets\n\n\nSide Projects\n\n\nTeaching\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nData Collection in Dialectology using Amazon Mechanical Turk\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMTurk\n\n\nMethods\n\n\nResearch\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nbarktools: Functions to help when working with Barks\n\n\n\n\n\n\n\nData Viz\n\n\nGithub\n\n\nHow-to Guides\n\n\nMethods\n\n\nPhonetics\n\n\nR\n\n\nR Packages\n\n\nSide Projects\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nI got a job at BYU!\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nBrand Yourself 3\n\n\n\n\n\n\n\nHow-to Guides\n\n\nPresentations\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nfuturevisions: My first R package!\n\n\n\n\n\n\n\nData Viz\n\n\nGithub\n\n\nR\n\n\nR Packages\n\n\nSide Projects\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nFull house at my first LaTeX workshop!\n\n\n\n\n\n\n\nGithub\n\n\nLaTeX\n\n\nPresentations\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nUGA Linguistics Colloquium 2020\n\n\n\n\n\n\n\nAnimations\n\n\nDissertation\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLSA and ADS 2020\n\n\n\n\n\n\n\nAnimations\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nResearch\n\n\nSouth\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nExtending Wells’ Lexical Set to Prelateral Vowels\n\n\n\n\n\n\n\nLexical Sets\n\n\nMethods\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nThoughts on Allophonic Extensions to Wells’ Lexical Sets\n\n\n\n\n\n\n\nLexical Sets\n\n\nMethods\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nDissertation\n\n\n\n\n\n\n\nDissertation\n\n\nPacific Northwest\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nReshaping Vowel Formant Data with tidyr 1.0\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nPhonetics\n\n\nR\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nAnimating Mergers\n\n\n\n\n\n\n\nAnimations\n\n\nData Viz\n\n\nGithub\n\n\nR\n\n\nSide Projects\n\n\nSimulations\n\n\nTeaching\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nWhy do people use BAT instead of TRAP?\n\n\n\n\n\n\n\nLexical Sets\n\n\nMethods\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLCUGA6\n\n\n\n\n\n\n\nAnimations\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nPresentations\n\n\nResearch\n\n\nSouth\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n3D Vowel Plots with Rayshader\n\n\n\n\n\n\n\nAnimations\n\n\nData Viz\n\n\nGithub\n\n\nHow-to Guides\n\n\nPhonetics\n\n\nR\n\n\nSide Projects\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nThank You\n\n\n\n\n\n\n\nData Viz\n\n\nMeta\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nJealousy List 3\n\n\n\n\n\n\n\nJealousy Lists\n\n\nData Viz\n\n\nR\n\n\nSkills\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nDH 2019\n\n\n\n\n\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nPresentations\n\n\nResearch\n\n\nSouth\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nYou’re a Statistician, Harry!\n\n\n\n\n\n\n\nGIS\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nSimulating Werewolf\n\n\n\n\n\n\n\nGithub\n\n\nSide Projects\n\n\nSimulations\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nSimulating Chutes and Ladders\n\n\n\n\n\n\n\nAnimations\n\n\nGithub\n\n\nSide Projects\n\n\nSimulations\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nAssigning Pseudonyms in R with the babynames package\n\n\n\n\n\n\n\nGithub\n\n\nHow-to Guides\n\n\nMethods\n\n\nR\n\n\nSide Projects\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nVowel overlap in R: More advanced topics\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nPhonetics\n\n\nR\n\n\nSkills\n\n\nData Viz\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nA tutorial in measuring vowel overlap in R\n\n\n\n\n\n\n\nData Viz\n\n\nHow-to Guides\n\n\nMethods\n\n\nPhonetics\n\n\nR\n\n\nSkills\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLSA and ADS 2019\n\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nPrevelar Raising Survey Results\n\n\n\n\n\n\n\nSide Projects\n\n\nFor the public\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV47\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nJealousy List 2\n\n\n\n\n\n\n\nJealousy Lists\n\n\nR\n\n\nSkills\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLCUGA5\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nPhonetics\n\n\nPresentations\n\n\nResearch\n\n\nSouth\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nBrand Yourself 2\n\n\n\n\n\n\n\nCSS\n\n\nGithub\n\n\nHow-to Guides\n\n\nMeta\n\n\nPresentations\n\n\nTwitter\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nJealousy List 1\n\n\n\n\n\n\n\nJealousy Lists\n\n\nR\n\n\nSkills\n\n\nStatistics\n\n\nGIS\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nTranscribing a Sociolinguistic Corpus\n\n\n\n\n\n\n\nDissertation\n\n\nMethods\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nMaking vowel plots in R (Part 2)\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nPhonetics\n\n\nR\n\n\nSkills\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nMaking vowel plots in R (Part 1)\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nPhonetics\n\n\nR\n\n\nSkills\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n365papers\n\n\n\n\n\n\n\nResearch\n\n\nSide Projects\n\n\nTwitter\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nRecording Equipment for Sociolinguistic Interviews\n\n\n\n\n\n\n\nMethods\n\n\nPhonetics\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nRandomizing a Wordlist\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nR\n\n\nResearch\n\n\nSkills\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nADS2018\n\n\n\n\n\n\n\nConferences\n\n\nDissertation\n\n\nLinguistic Atlas\n\n\nPhonetics\n\n\nPresentations\n\n\nResearch\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nA Tutorial on Extracting Formants in Praat\n\n\n\n\n\n\n\nHow-to Guides\n\n\nPhonetics\n\n\nPraat\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV46\n\n\n\n\n\n\n\nConferences\n\n\nDissertation\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLCUGA4\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n/ɑr/-Raising\n\n\n\n\n\n\n\nSide Projects\n\n\nPhonetics\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nTesting VOT Durations in A Course in Phonetics\n\n\n\n\n\n\n\nSide Projects\n\n\nTeaching\n\n\nStatistics\n\n\nPhonetics\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nGeneral Update\n\n\n\n\n\n\n\nWest\n\n\nUtah\n\n\nMTurk\n\n\nResearch\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nPacific Northwest\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nUsing MTurk\n\n\n\n\n\n\n\nResearch\n\n\nWest\n\n\nPacific Northwest\n\n\nMTurk\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLaboratory Research\n\n\n\n\n\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nAdmission to Candidacy\n\n\n\n\n\n\n\nResearch\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLots of Transcribing\n\n\n\n\n\n\n\nPacific Northwest\n\n\nResearch\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nA Survey of the Western American English using MTurk\n\n\n\n\n\n\n\nPacific Northwest\n\n\nResearch\n\n\nWest\n\n\nMTurk\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nBrother Joseph\n\n\n\n\n\n\n\nSide Projects\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nMount St. Helens and Vowels\n\n\n\n\n\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nSECOL 2017\n\n\n\n\n\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nPresentations\n\n\nResearch\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nUpdated mvnorm.etest() function\n\n\n\n\n\n\n\nStatistics\n\n\nR\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nWebsite Version 2\n\n\n\n\n\n\n\nCSS\n\n\nSkills\n\n\nMeta\n\n\nGithub\n\n\nHow-to Guides\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nExcel Workshop\n\n\n\n\n\n\n\nHow-to Guides\n\n\nPresentations\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nTweeting LSA2017\n\n\n\n\n\n\n\nConferences\n\n\nTwitter\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLSA2017\n\n\n\n\n\n\n\nPacific Northwest\n\n\nConferences\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nCustom Themes in ggplot2\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nSkills\n\n\nR\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Guarani Dictionary\n\n\n\n\n\n\n\nCSS\n\n\nGuarani\n\n\nSide Projects\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nBrand Yourself\n\n\n\n\n\n\n\nCSS\n\n\nGithub\n\n\nHow-to Guides\n\n\nMeta\n\n\nPresentations\n\n\nTwitter\n\n\n\n\nToday Emily McGinn of the Digital Humanities Lab at UGA and I did a professionalization workshop for grad students. We gave a presentation on different ways grad students can boost their online presence through building a personal webpage, utilizing social media, and finding your field’s conversation. We then let the attendees a chance to work on their own to create a new online profile, using what they learned.\n\n\n\n\n\n\nNov 11, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nDiVar\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nHow I Implemented the Links in this Site\n\n\n\n\n\n\n\nCSS\n\n\nHow-to Guides\n\n\nMeta\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nThe Importance of Twitter\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nResearch\n\n\nTwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nMaking a website is fun!\n\n\n\n\n\n\n\nCSS\n\n\nGithub\n\n\nHow-to Guides\n\n\nMeta\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nReviewer Feedback\n\n\n\n\n\n\n\nConferences\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nJMP\n\n\n\n\n\n\n\nData Viz\n\n\nPresentations\n\n\nSkills\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nThe Linguistic Atlas of the Pacific Northwest\n\n\n\n\n\n\n\nLinguistic Atlas\n\n\nPacific Northwest\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nADS Meeting!\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/colloquium-2020/index.html",
    "href": "blog/colloquium-2020/index.html",
    "title": "UGA Linguistics Colloquium 2020",
    "section": "",
    "text": "For the fourth time in six years, I presented some of my research at the UGA Linguistics Colloquium. I talked about some findings from my dissertation, though I focused on just the low vowels trap, lot, and thought."
  },
  {
    "objectID": "blog/colloquium-2020/index.html#these-are-relatively-old-changes",
    "href": "blog/colloquium-2020/index.html#these-are-relatively-old-changes",
    "title": "UGA Linguistics Colloquium 2020",
    "section": "These are relatively old changes",
    "text": "These are relatively old changes\nThe gist of the talk is that trap has been gradually lowering over the course of four generations in Cowlitz County Washington, with men consistently lagging behind the women and with the first half of the trajectory doing most of the lowering. Meanwhile, lot and thought have been in a near-merged state since at least the 1930s, with no apparent conditioning by sex or generation. Considering that Cowlitz County was settled by English speakers since only the 1850s, these are relatively old changes for this area.\nThe interesting part is that while the position of trap is consistently lower for the women, the shape of the trajectory is the same within a generation. That is, when it comes to the vowel’s dynamics, the men are keeping up with the women. It’s just that the global position of that vowel is less advanced.\nFor me, this opens up a lot of questions about vowel trajectories. I’m curious about what kinds of social conditioning can be found in the trajectory of a vowel, rather than its relative position in the F1-F2 space. In fact, I’ve got some experimental work in motion to answer just that…"
  },
  {
    "objectID": "blog/colloquium-2020/index.html#interpreting-difference-smooths",
    "href": "blog/colloquium-2020/index.html#interpreting-difference-smooths",
    "title": "UGA Linguistics Colloquium 2020",
    "section": "Interpreting Difference Smooths",
    "text": "Interpreting Difference Smooths\nOne thing I included in this presentation is an animation to help learn how to interpret difference smooths.I’ve been meaning to include this animation in a presentation for a bit now, and with the 40 minute afforded me in this presentation, I finally had the time to do so. Difference smooths are a type of plot that aid in the interpretation of GAMs and you can learn more about them in Mártin Sóskuthy’s tutorial on GAMs. Unfortunately for those of us that fit GAMs to vowel formant data, they look awfully like vowel formant curves, so they can be tricky to interpret. I’ll probably expand this into a full blog post later on, but for now, here’s a brief explanation of (my interpretation of) difference smooths.\nLet’s say we have some data, a blue curve and a red curve, sampled at 11 time points. Here, these 11 data points are plotted, with lines connecting the dots.\n\nWhen you fit a generalized additive model to this data, you can get two fit lines (left, below), which is basically a smoothed version of the jagged line above. It’s as if you had sampled continuously rather than at 11 discrete timepoints. When you plot a difference smooth, you get the plot on the right (below), which is essentially one curve “minus” the other curve.\n\n\n\n\n\n\n\n\n\n\nNow, it may not be completely transparent how the difference smooth relates to the two fit lines. So, to help out, the two plots below show the exact same curves, only several vertical lines have been added. On the fit lines, the vertical lines connect the two curves, with the height (and color) of the line representing the distance between them. On the right, the vertical lines connect the difference smooth and a horizontal line. The kicker: the height of the vertical lines in both plots is identical.\n\n\n\n\n\n\n\n\n\n\nIf you’re like me, it still might not be clear how they connect. The following animation may help. It starts with the two curves with the vertical lines between them. Since I’m getting the difference between the two, I’m “subtracting” the bottom from the top. This has the effect of flattening out the bottom one to a perfectly straight, horizontal line. In order to keep the vertical lines the same height, the amount of “bend” that has to happen to the bottom line has to apply equally to the top line. The result is a new curve called the difference smooth.\n\nOnce you’ve grasped that, you can then add some additional information to the plot. Typically, difference smooths come with confidence intervals, which I highlight in gray below. Wherever the confidence interval does not overlap with the horizontal (zero) line, the curves are interpreted as being statistically significantly different from each other.Exactly how these confidence intervals are calculated is something I’m still learning.\n\nIn this case, since the original red line is subtracted from the original blue one, with the confidence intervals on this difference smooth, I can say that the blue line is significantly higher than the red for some region.\n Finally, the plot below is the version of the difference smooths I use in this presentation, my dissertation, and anywhere else I’ve needed them so far.There are R packages that can create difference smooths (I think itsadug might be most well-known). I’m not a huge fan of the aesthetics of that plot, so I’ve created my own version using the same data, and in ggplot2. I’d like to release it in a package sometime soon so you can use them too if you’d like.\n\nI’ve just added some additional annotation to better highlight the region of statistical significance:\n\nThe center line is blue and slightly thicker in contrast with the gray, thinner lines.\nThe horizontal axis is blue in the region of statistical significance.\nAt the point where the confidence interval intersects 0 (the horizontal axis), I’ve added vertical dotted lines.\nI’ve also added the timepoint where that intersection happens. In this case, the time spans from 0 to 1, so these two lines are statistically significantly different from one another between 0.122s and 0.526s. NOTE: It’s important that you take these ranges with a grain of salt and perhaps interpret them very broadly rather than paying too much attention to the exact number.\n\nI just kinda like the look of this style of plot, so that’s the one I’ve been using."
  },
  {
    "objectID": "blog/colloquium-2020/index.html#conclusion",
    "href": "blog/colloquium-2020/index.html#conclusion",
    "title": "UGA Linguistics Colloquium 2020",
    "section": "Conclusion",
    "text": "Conclusion\nI’ve at least benefited from interpreting difference smooths in this way. Hopefully the attendees of my talk today (and now you!) will have slightly better understanding of them as well."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html",
    "href": "blog/interactive-guarani-dictionary/index.html",
    "title": "Interactive Guarani Dictionary",
    "section": "",
    "text": "The semester is finishing up, and as usual, the most productive week for me is during finals. Not necessarily productive regarding school work or current research projects, but I always rediscover side projects and hobbies. This week I rekindled my interest in Guarani."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html#brazil",
    "href": "blog/interactive-guarani-dictionary/index.html#brazil",
    "title": "Interactive Guarani Dictionary",
    "section": "Brazil",
    "text": "Brazil\nI’ve been working on Guarani off and on since 2009. I was living in Campo Grande, Mato Grosso do Sul, Brazil, as a Mormon missionary at the time. Fairly regularly I would meet people that spoke this language called Guarani, and I had friend (a fellow missionary), who had some pedagogical materials that taught Spanish speakers Guarani. So I had to work through the Spanish (I had only been speaking Portuguese for 9 months or at that point), but I was able to decipher some of the basic Guarani morphology and grammar. A while later my dad sent me a copy of the Book of Mormon in Guarani and said I ought to learn what I could. So I sat there with the Guarani, Portuguese, and English translations and would try to figure out new words and morphology.\nAgain, I was a Mormon missionary at the time, so I didn’t have a lot of time to spent learning this language. I hadn’t begun studying linguistics yet, so I had no idea what a non-Indo-European language could possibly be like and there were a few things that had me stumped. I also didn’t have access to a computer, so I couldn’t keep track of notes and vocabulary very well. So every couple of weeks I’d sit there with a dozen sheets of paper spread all over my desk, trying in vain to keep things alphabetized as I added vocabulary and translations. My Brazilian buddies all thought I was insane for trying to learn this language, but I found it to be a LOT of fun.\nOne of the more frustrating things was that I wanted to see how a single word was used in other contexts. If I was looking through a sentence and there were three Guarani words I didn’t know, I often had no way of knowing which word corresponded with the meaning in the English sentence. If only I could control+F the book and find the Guarani words in other contexts and figure out the meaning."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html#self-study",
    "href": "blog/interactive-guarani-dictionary/index.html#self-study",
    "title": "Interactive Guarani Dictionary",
    "section": "Self-study",
    "text": "Self-study\nAfter I came back to the United States and went back to college at BYU, I found that there were some books written about Guarani grammar, but they were mostly older ones. I didn’t know it at the time, but a former Department Chair in the Department of Linguistics and English Language at BYU was Robert W. Blair, who published some Guarani pedagogical material. I found his Guarani Basic Course at the library as well as his student, Charles Graham’s, Guarani Intermediate Course, and did what I could going through those. There were some other more descriptive grammars of the language written in the mid 20th Century, and I even sat in on a Guarani course for a semester.Yes BYU offers a course in Guarani! The class was taught only every once in a while and was intended for Mormon missionaries who had spent time in Paraguay. The class was taught in Spanish (again—not a language I’ve studied) by a native Guarani speaker, and was intended to add some formal instruction to people already familiar with the language. I was overwhelmed with other courses so I couldn’t keep up for more than a few weeks."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html#translation-program",
    "href": "blog/interactive-guarani-dictionary/index.html#translation-program",
    "title": "Interactive Guarani Dictionary",
    "section": "Translation Program",
    "text": "Translation Program\nI was in my last year at BYU. I was working as a programmer, creating eBooks for WordCruncher and had access to an HTML file of the Guarani Book of Mormon. I had taken a class in Perl already and had gotten pretty proficient through that job. I had also taken Mark Davies’ Corpus Linguistics course. So when I took an NLP course as the capstone to my minor in Linguistic Computing, I decided to write a Guarani translator.\nThe program worked pretty well and was exactly what I was dreaming of in Brazil. I had paired the Guarani and English text as a “parallel corpus”, meaning each line in one file corresponded to a translated line in the other. What the translator does is it takes an input string (say, mba’apo) and it displays all the Guarani sentences with that word with the English underneath it. Made it very handy to see how words (or parts of words) were used in other contexts.This corpus might actually be the largest Guarani-English parallel dictionary. It had 329K Guarani words when first wrote the translator, but it’s now up to 606K after adding some more translated church material. I’ve got another ≈250K to add to it, whenever I get the time. I could nearly double it even then if I get access to the Guarani Bible, though I don’t know if that’ll happen anytime soon. Granted, these are all translated texts from English, and are religious-based, obviously representing a very different style than naturally occurring, spoken Guarani.\nWhat it then does it is look at all the words in both the English and Guarani sentences with the word, keeps track of their frequencies, then looks at the frequencies for all words in the entire corpus and compares the two. Words that have nothing to do with the translation will occur with roughly the same frequency in the matched sentences as they do in the full corpus. But words that correspond to the same meaning will occur relatively much more often in the matched sentences compared the corpus as a whole. So say the word work appears once every 1000 words in the whole corpus. If it suddenly appears once every 25 words in the matched words, statistically that’s a big difference, and odds are pretty good that work is a translation for mba’apo (and it is).\n\n\n\nGuarani Search engine\n\n\nSo using this I could find out which English words correlated with which Guarani words. Not a perfect translator, especially since it didn’t use any fancy NLP processing, but not bad.\nMy interest in Guarani, which was mostly about its nasal harmony, verbal morphology, and trying to document the grammar as a whole, started to wane as I started grad school and focused more on sociolinguistics and dialectology. But my reading comprehension is still… okay let’s face it, not that great, but I’m surprised at how much I was able to learn through self-study and a custom computer program."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html#interactive-dictionary",
    "href": "blog/interactive-guarani-dictionary/index.html#interactive-dictionary",
    "title": "Interactive Guarani Dictionary",
    "section": "Interactive Dictionary",
    "text": "Interactive Dictionary\nI think what started this recent resurgence in Guarani was, strangely enough, making this website. I’ve acquired some more HTML and CSS skills and realized that I could make something useful with a web browser. So I dusted off my old files and started something fun.\nIn just a week I was able to make a pretty useful website (locally hosted only for now) with two main pages. The first is the entire corpus. Unlike what I had before, I could take advantage of the formatting to display useful information. All the words I know are in regular black text, but the words I don’t know stand out in blue. That makes it easy to figure out which ones I need to learn next. For the words I do know, the roots are underlined, so I can quickly see the base and what morphology is stemming off of it. The interactive part is that if I mouseover the root, a basic definition shows up in the form of a tooltip. So if I’ve forgotten a word, I can very quickly remind myself of what it means. Very handy.\n\n\n\nGuarani Corpus screenshot\n\n\nHow am I keeping track of what I know and don’t know? The other page on the site is a dictionary. I usually kept all this stuff in a spreadsheet somewhere, but here I can utilize the formatting to make it look like a real dictionary. I’ve got roots, possible word forms, derivatives, translations, parts of speech, etymology, other notes, and the infrastructure to include example sentences and other metadata. All this is stored on a file on my computer, and when I learn a new word, I just add it to the bottom of the file and a Perl script will take care of alphabetizing it and making sure it looks good for the CSS to take over.\n\n\n\nGuarani Dictionary screenshot\n\n\nThe result is a slick system where I can quickly see what words I need to learn and I can easily add them to the dictionary. I then run a lightning fast Perl script and refresh my browser, and I’ve got an updated corpus and dictionary.\nThe system is set up to handle as big of a corpus or dictionary as I’m willing to feed it. For now, I’m only a couple paragraphs in and I’ve got over 100 entries in the dictionary. It will take hundreds of hours to go through my entire corpus. But for the first time I’ll be creating a decent Guarani dictionary, which is kinda what I had in mind to do the whole time."
  },
  {
    "objectID": "blog/recording-equipment-for-sociolinguistic-interviews/index.html",
    "href": "blog/recording-equipment-for-sociolinguistic-interviews/index.html",
    "title": "Recording Equipment for Sociolinguistic Interviews",
    "section": "",
    "text": "Warning\n\n\n\nAs the years pass on this blog post, I’m starting to notice that it is aging. Some links are breaking as certain products are taken off the market and people take YouTube videos down. Also, the prices of most products have gone up. I’ll keep this page up though because, even if the specific advice is no longer helpful, it might be useful to see my thought process.\nToday a fellow grad student asked me if I had any recommendations for recording equipment for conducting fieldwork. I actually do have some suggestions. Since this is something I get asked fairly regularly, I thought I’d write a blog post about it.\nFor every project I do I keep a log/journal wherein I track my progress and write general thoughts about what’s happening. The following comes from a couple entries in mid 2016. At the time, I was about four weeks away from flying out to Washington State to conduct my first sociolinguistic interviews. I had received a small grant to fund the fieldwork, but I was hoping to borrow equipment from the school since the majority of the money would go towards flights and compensating participants. The rest is verbatim from my notes, with some minor edits for clarification.\nBy the way, a couple times I make reference to Natalie Schilling’s 2013 book Sociolinguistic Fieldwork. It’s fantastic. If you’re ever planning on doing fieldwork, I highly recommend this book!"
  },
  {
    "objectID": "blog/recording-equipment-for-sociolinguistic-interviews/index.html#notes-from-my-logjournal",
    "href": "blog/recording-equipment-for-sociolinguistic-interviews/index.html#notes-from-my-logjournal",
    "title": "Recording Equipment for Sociolinguistic Interviews",
    "section": "Notes from my log/journal",
    "text": "Notes from my log/journal\n\nMonday, May 30, 2016\nI’ve thought a lot about my equipment situation. I went to places on campus this week and it looks like checking out a microphone and recorder is not going to happen. All the ones from the phonetics lab are checked out, and anywhere else that has them on campus will only let me keep it for a few days. I don’t mind the thought of buying my own equipment, especially this certainly won’t be my only set of interviews. But I didn’t budget for them and I was counting on using really good ones for free.\n[Based on a recommendation from a UGA employee,] I wrote to a store called Sweetwater which is well-known for having quality equipment. They specialize in recording equipment for musicians and sound booths, and have a lot of professional equipment. The guy wrote to me and recommended a $260 recorder and a $650 necklace microphone. Yeah, I don’t have that kind of budget.\nI talked to [the UGA employee who recommended Sweetwater], who said I could probably hook up a lavaliere mic to my iPhone and it would turn out alright. I looked though, and it’s hard to find a decent lavaliere mic, especially one that would hook up to my phone. The three that they have at Sweetwater don’t fit the required specifications I need (frequency range for two, and self-noise for another). I looked on Amazon, and there are so many mics out there, and I can’t filter by specifications, so I stopped looking.\nI realized I do have a decent microphone already, my Blue Yeti. In Schilling (2013), she says using a computer isn’t bad acoustically, it’s just a lot of equipment and may change the nature of the interview. I gave the laptop/Yeti setup a few test runs and the quality seems fine. However, I was reading later that the Yeti works best if it’s about a foot away from the speaker’s mouth. I don’t know how I can manage that since it’s so big. Even if I could get it that close without it tipping over on the couch or something, it’s a little hard to ignore, especially with the retro design.\nAs a side note, I realized that the entire corpus I collect won’t take up that much disk space. Schilling (2013) says each minute at the right quality will be about 10MB of space. Well, 60 minutes times 50 speakers at 10MB a minute is 30GB of space. I certainly don’t need an entire TB of hard drive space. I budgeted $140 for a specific one I liked because it was rugged. But a look on Amazon and I found this one for less than half the cost. It seems rugged enough for me. So if I get that one, that leaves about $80 extra for other equipment. So I feel somewhat justified in spending some money on equipment.\nI’ve read that I can’t skimp on the sound quality. Get the best I can afford. Since this isn’t even my money (or rather, it’s free!), I can spend it how I want. And I would hate to do all this work to end up with poor sound quality. No, I need to get decent equipment, and if that means spending over $100 then that’s what it’ll take.\nI went back to the Sweetwater worker’s recommendations. He said the huge $260 recorder was the way to go. I looked at the specs, and there are things I simply don’t need, like multiple tracks, and a good microphone, and a guitar tuner, and different settings for different instruments. Nope. Okay, so the one I need is certainly less than $260. Well, I saw that is was a Zoom H5. I wonder if there’s a Zoom H1. Sure enough, it’s the top seller on Amazon. Turns out this is the go-to device, and everyone uses it. The specs are still more than what I need (like 96 kHz—who needs that?), so maybe I can go cheaper. Anything cheaper though was bad. Like not being able to record in WAV or not having the right frequency response. So I decided to look at the #2 or #3 [top-selling] recorders. Turns out the Tascam ones are also great.\nSo I wanted to do some comparisons, and it turns out YouTube was a resource I should have been using this whole time. I found this video which compares the Zoom H1 to the Tascam DR-22WL. It’s really long, but I jumped to the parts that seemed most relevant, like self-noise and how it does with speech, and they’re basically the same. The reviewer says he actually prefers the Tascam because it feels sturdier. Well, it turns out there’s also the Tascam DR-05, which has very similar specs. I googled a comparison between those two and found this, which basically concludes that the DR-05 is better. I don’t need the wifi/iPhone sync which can control the start and stop that the DR-22WL has. So, that saves me $10, and the DR-05 was cheaper, and the winner!\nSo, I noticed at the bottom of the YouTube video I saw, he listed the lavalier mic he used, and it wasn’t bad. Now that I had found my new resource, I googled comparison videos of lavaliere mics and found this one, which was exactly what I wanted. Basically, he said that this JK MIC-J 044 was the best, performing even better than an expensive one he had. Luckily it’s only $30!\nSo, I have a new equipment lineup. With other videos like this on how set one up, this which showed me why those that work with recorders won’t work with iPhones, this which showed how bad an iPhone set up is, and this which showed me where to hook it on, I feel like a pro.\n\n\nThursday, May 31, 2016\nI went ahead and bought the recorder, microphone, and hard drive. They’ll be here Thursday. I’m really excited.\n\n\nBoom. Just bought my recorder, microphone, and hard drive. Sociolinguistic interviews start in less than a month!\n\n— Joey Stanley (@joey_stan) May 31, 2016\n\n\n\n\nSaturday, July 2, 2016\n[I had already done several interviews at this point.]\nThe recording equipment is turning out to be pretty good. The lavaliere mic and recorder are producing some great quality recordings, though I think the first couple were a little too quiet, in an attempt to reduce background noise. Some of the places I’ve recorded in have been remarkably quiet, like in a church and houses way out in the mountains. I recorded in stereo, but I realized the mic doesn’t pick up stereo so it’s just in the left channel. I’ll have to fix that in Praat. I had some problems clearing the memory card so—good thing I brought my backup—I used the Yeti mic in the church building. Those two interviews did seem quite a bit more formal, and it could have been the set up (sitting across from each other at a table with the big microphone between us), or maybe it was because the people were younger. The files turn out to be just about a gigabyte per interview, and that’s with the higher-than-normal settings of 48kHz and 24-bit something or other, instead of 44kHz and 16-bit."
  },
  {
    "objectID": "blog/recording-equipment-for-sociolinguistic-interviews/index.html#additional-thoughts-on-technical-stuff",
    "href": "blog/recording-equipment-for-sociolinguistic-interviews/index.html#additional-thoughts-on-technical-stuff",
    "title": "Recording Equipment for Sociolinguistic Interviews",
    "section": "Additional thoughts on technical stuff",
    "text": "Additional thoughts on technical stuff\nOkay, 2018 Joey here…\nWhat I didn’t know before getting into all this is that there are two separate pieces of equipment: a microphone and a recorder. You need both to record sound and, crucially, you need both to have the technical specifications you want. If you plug in a $4 microphone into a $1000 recorder, you’re going to get a $4 sound. I considered for a while using my iPhone at least as a recorder and plugging in a good microphone to it, but I eventually rejected the idea. A lot of recorders come with their own microphone, but it’s recommended to not use it because of the self-noise. It’s also less conducive to sociolinguistic interviews.\nAs far as technical specifications, I wasn’t asking for anything crazy. First, I wanted a frequency range of at least 20Hz to 20,000Hz, which is about the range of normal human hearing. I also wanted at a least 16-bit rate, which has to do with the quality of sound (I’m still fuzzy on exactly what it means) and was a recommended spec. The recorder absolutely had to record in a WAV file, which is uncompressed and suitable for phonetic analysis, unlike mp3. I also wanted something that had low self-noise, but that was hard to find specs for.\nYou also need to pay attention to what kind of plug the equipment has. The small recorders I looked at used a plug the same size as what you would plug into your phone or computer. Big studio-quality microphones have an XLR connector which will only fit big, studio-quality recorders. As this video explains (just the first 30 seconds), even though the microphone will physically fit into your phone, it’s not going to work: you need specially designed cables to work with phones—which unfortunately won’t work with your other recording equipment. And, given that iPhones now are ditching the headphone jack anyway, it’s a good thing I didn’t invest in a phone-only microphone.\nYou also need to be aware that USB microphones like the Blue Yeti not only have a USB as a connector (so they won’t fit into your recorder), but they also require power. Your computer’s battery is fine, but that means you have to bring your computer around and set up this whole studio. It’s possible to use “phantom power” which is, as far as I can tell, a little power source that goes between your microphone and your recorder, that provides the power. But this has to be plugged in, it’s kind of expensive, and looked like a bit of a nuisance. So I think getting a battery-powered, handheld recorder and a simple lavaliere mic is the way to go at least for conducting fieldwork."
  },
  {
    "objectID": "blog/recording-equipment-for-sociolinguistic-interviews/index.html#tldr",
    "href": "blog/recording-equipment-for-sociolinguistic-interviews/index.html#tldr",
    "title": "Recording Equipment for Sociolinguistic Interviews",
    "section": "TL;DR",
    "text": "TL;DR\nI ended going with a Tascam DR-05 for my recorder.  I couldn’t really tell any difference between it and the Zoom H1 except that at the time the Tascam was about $10 cheaper. There’s a 2nd edition Tascam now, so I think the Zoom is cheaper now. It exceeds my needs for technical specs (I set it to 24-bit and 28,000Hz) and works great.Edit: At the time of writing, it was $90. The price has gone up since then.\nFor a microphone, I used a JK MIC-J 044 lavaliere mic. It was surprisingly only $30 and, according to YouTube videos, performed as well as the &gt;$100 ones. I wish it picked up less background noise, but otherwise I’ve been very impressed with it. It’s only 11 grams and people tend to forget pretty quickly that it’s there.\nI did end up with the external hard drive I mentioned in the post, a Transcend 25M3, which cost me about $60. I liked it so much I got a second one a year later.\nI would also recommend a 32GB micro-USB for the recorder. The one that came with mine was only 4GB and it got full after only 3–4 interviews. A 32GB lasts a day or two with no problem. Here’s the one I got for a later project.\nAnd don’t forget extra batteries!"
  },
  {
    "objectID": "blog/3d-vowel-plots-with-rayshader/index.html",
    "href": "blog/3d-vowel-plots-with-rayshader/index.html",
    "title": "3D Vowel Plots with Rayshader",
    "section": "",
    "text": "So Tyler Morgan-Wall has recently come out with the rayshader package and the R and data science Twitter community has been buzzing. I’ve seen people post some absolutely amazing 3D plots and animations. I haven’t seen any linguists using it though, so I’m hopping on that bandwagon—a little late in the game—to show what kinds of 3D visuals we can produce using vowel data."
  },
  {
    "objectID": "blog/3d-vowel-plots-with-rayshader/index.html#what-is-rayshader",
    "href": "blog/3d-vowel-plots-with-rayshader/index.html#what-is-rayshader",
    "title": "3D Vowel Plots with Rayshader",
    "section": "What is rayshader?",
    "text": "What is rayshader?\nRayshader is an R package that makes it easy to create stunning 3D plots. My impression is that it was designed primarily with geospatial data in mind, so that you can create really nice 3D models of terrain and stuff. But recently, rayshader has been updated to take any plot created in ggplot2 and make it 3-dimensional. Here are just a few examples I found on Twitter:\n\n\nFirst try with #rayshader #rstats pic.twitter.com/eEUq0eKSbq\n\n— David Solito (@dsolito) March 5, 2019\n\n\n\n\nFinally found the time to try out some of that sweet new #rayshader + #ggplot2, 3D functionality! Loving it 😍 and can't wait to play more! pic.twitter.com/qdcjLKZKRq\n\n— Laura Ellis (@LittleMissData) July 1, 2019\n\n\n\n\nFlying over the dunes of Mars using #rayshader + #rstats + @HiRISE data(working on a camera translation API so you can 🐦fly around your rayshader scenes) pic.twitter.com/8uPIxojNKg\n\n— Tyler Morgan-Wall (@tylermorganwall) August 15, 2019\n\n\n\n\nI just discovered the #rayshader package by @tylermorganwall. It turns a 2D ggplot2 object into a 3D object. #rstats pic.twitter.com/w0x3vy9qB3\n\n— We are R-Ladies (@WeAreRLadies) June 27, 2019\n\n\nIf you’re interested in learning how to use the package to make your ggplot2 plots stand out, I’d highly recommend looking through Morgan-Wall’s tutorial. What I’m doing in this blog is essentially following that post, but using some vowel data. Let’s get to it.\nlibrary(rayshader)\nlibrary(tidyverse)\nBy the way, the code and data for this blog post are all available as an Rmd file on my GitHub."
  },
  {
    "objectID": "blog/3d-vowel-plots-with-rayshader/index.html#an-f1-f2-vowel-plot",
    "href": "blog/3d-vowel-plots-with-rayshader/index.html#an-f1-f2-vowel-plot",
    "title": "3D Vowel Plots with Rayshader",
    "section": "An F1-F2 vowel plot",
    "text": "An F1-F2 vowel plot\nLet me start with sort of a basic vowel plot. I’ll first load in some of my own vowel data, and for simplicity, I’ll just isolate tokens with stressed /i/ before obstruents. I’ll use the Mahalanobis function to filter out bad measurements.\nSee my unannounced, buggy, but functional joeyr package for code for tidy_mahalanobis, which makes it easy to implement the Mahalanobis distance within a tidyverse pipeline.\nmy_iy &lt;- read_csv(\"http://joeystanley.com/data/joey.csv\") %&gt;%\n  \n  # filter it down\n  filter(vowel == \"IY\", \n         stress == 1, \n         plt_manner %in% c(\"stop\", \"fricative\", \"affricate\")) %&gt;%\n  \n  # Keep just the columns I need\n  select(word, F1, F2) %&gt;%\n  \n  # There were a few bad outliers.\n  mutate(mahal_dist = joeyr::tidy_mahalanobis(F2, F1)) %&gt;%\n  filter(mahal_dist &lt; 10) %&gt;%\n  select(-mahal_dist) %&gt;%\n  print()\nThat leaves me with 117 tokens of /i/.\n\n2D vowel plots\nLet’s look at that using a pretty traditional scatterplot.\nggplot(my_iy, aes(F2, F1, label = tolower(word))) + \n  geom_text() + \n  scale_x_reverse() + scale_y_reverse() + \n  coord_fixed(ratio = 2) + \n  theme_classic()\n\nSo, to get the 3D plot to work with rayshader, I need some variable that should act as the depth dimension. In Morgan-Wall’s example, he adopts a pretty standard technique when looking at coordinate data. You basically overlay some tessellating shape like a hexagon, count how many points are in each hexagon, and color that cell based on how may points there are in it. I haven’t seen this used too much in vowel data, other than Bill Kretzschmar’s recent work, but it’s a possibility. Fortunately, this is straightforward with geom_hex.Also, see my recent presentation at DH2019.\nhex_plot &lt;- ggplot(my_iy, aes(F2, F1)) + \n  geom_hex(bins = 10, size = 1, color = \"black\") + \n  scale_x_reverse() + scale_y_reverse() + \n  scale_fill_viridis_c(option = \"C\")\nhex_plot\n\nAlready kind of a cool way at looking at it. But we’re just getting started.\n\n\n3D vowel plots\nNow make it 3D. This is really quite straightforward with plot_gg. I’m mostly using the default parameters because I don’t know enough about 3D modeling to play with some of the other features, and yet it still looks fantastic.\nFYI, this took about a minute and half to render on my laptop.\nplot_gg(hex_plot, \n        height = 5, width = 7, \n        scale = 250, zoom = 0.6, \n        windowsize = c(1400, 800),\n        multicore = TRUE)\n\nWell that’s super cool.\nBy the way, here’s the code I used to export the plot to a file on my computer. I added a small bit of simulated focus/blur to it, which is always kind cool.\nrender_depth(focus = 0.5, focallength = 15, filename = \"hex_plot.png\")\nSo now let’s make this even better!\n\n\nA 3D animation\nNow a static plot is awesome. Don’t get me wrong. But, turning it into an animation is even cooler. Plus, Morgan-Wall says that viewing a 3D plot from different angles is one way to turn them from gimmicky visuals to legit tools for conveying information. He says here:\n\n“It’s difficult to interpret static 3D visualizations, as the display is an inherently 2D medium and the reader can’t accurately reconstruct the depth information of 3D data… [T]he continuous underlying substrate provides perceptual context for the missing depth information. The issue of ‘small box close, or big box far away?’ doesn’t occur with [rayshader plots], since those points can always be located in 3D space by referencing the surrounding data.”\n\nI’m paraphrasing slightly and you should go read the full post because I believe he makes a compelling argument in favor of 3D plots, which is something I had never seen before.\nSo here’s the code—and the amazing thing is that it’s really just this one line!—for turning this 3D vowel plot into an animation.\nThis took about 3.5 minutes to render.\nrender_movie(filename = \"hex_plot_orbit\", type = \"orbit\",\n             phi = 45, theta = 60)\n\nThat orbiting thing is cool. It’s interesting to see a vowel space from all sides.\nAs it turns out though, you can actually adjust where the “camera” is pretty freely. Here is some code I swiped from Morgan-Wall’s tutorial that does a really slick zoom in, pan, zoom out thing.\nThis took about 5 minutes to render.\n# Set up the camera position and angle\nphivechalf = 30 + 60 * 1/(1 + exp(seq(-7, 20, length.out = 180)/2))\nphivecfull = c(phivechalf, rev(phivechalf))\nthetavec = 0 + 60 * sin(seq(0,359,length.out = 360) * pi/180)\nzoomvec = 0.45 + 0.2 * 1/(1 + exp(seq(-5, 20, length.out = 180)))\nzoomvecfull = c(zoomvec, rev(zoomvec))\n\n# Actually render the video.\nrender_movie(filename = \"hex_plot_fancy\", type = \"custom\", \n            frames = 360,  phi = phivecfull, zoom = zoomvecfull, theta = thetavec)\n\nSo that is definitely a new way at visualizing vowel data."
  },
  {
    "objectID": "blog/3d-vowel-plots-with-rayshader/index.html#a-spectrogram",
    "href": "blog/3d-vowel-plots-with-rayshader/index.html#a-spectrogram",
    "title": "3D Vowel Plots with Rayshader",
    "section": "A Spectrogram",
    "text": "A Spectrogram\nAfter getting all this to work, the next question I had was this: What other kind of linguistics data can be represented in 3D? I immediately thought about a spectrogram. In Praat, you’ve got a spectrogram with three continuous variables: time along the x-axis, frequency along the y-axis, and amplitude being represented by color. What if, in addition to color, we could represent amplitude with the z-axis?\nAs it turns out, the tricker part with this plot was trying to get the data in a format I could plot with ggplot. The 3D rendering was a snap once that was done (again, hooray for rayshader!), so if you just want to skip to the good stuff, scroll to the bottom. If you’re curious about how I hacked this, read on.\n\nGetting a ggplot2 spectrogram\nNow, I’m sure there are lots of smart people that work with audio in R and could get spectrogram data in a format suitable for ggplot in a snap. But since I process my audio almost entirely in Praat, getting this to work in R was new and a bit tricky for me. But I found a way. It might not be the best way, but it is a way.\nFirst, I’ll use the tuneR package to process the raw .wav file. In this case, it’s a short recording of me saying the word boy.  I don’t exactly know how sound is digitized, but if it is just a bunch of acoustic samples, then I presume it’s nothing more than a bunch of numbers organized in some way. And numbers I can work with.I thought something nice and dynamic like /ɔɪ/ would make for a better visual.\nI know it’s sloppy style to load packages midway through an R script, but this is a pretty tangential point, and it better reflects my actual process of accomplishing this task.\nlibrary(tuneR)\nboy_wav &lt;- readWave(\"joey_boy.wav\")\nWhen I use the readWave function, it loads in the audio and stores it into an R object of class Wave. Unfortunately, it’s not immediately useful to me.\nboy_wav\n\n##Wave Object\n##  Number of Samples:      36874\n##  Duration (seconds):     0.84\n##  Samplingrate (Hertz):   44100\n##  Channels (Mono/Stereo): Mono\n##  PCM (integer format):   TRUE\n##  Bit (8/16/24/32/64):    16 \nSo the task is to extract the data I want into a plain ol’ dataframe. Again, I’m sure there’s a better way, but I used the inputw function from the seewave package even though it explicitly says in the documentation that it is “not to be called by the user.” But using these functions, I was able to access that list.\nlibrary(seewave)\ninput &lt;- inputw(wave = boy_wav)\nwave &lt;- input$w\nhead(wave)\n\n##      [,1]\n## [1,]  127\n## [2,]  125\n## [3,]  123\n## [4,]  131\n## [5,]  134\n## [6,]  131\nSo now, the wave object is very long and has one row for each sample. Okay, that means I’m getting close. I’ll take what I’ve got, convert it into a tibble, add the sample number, and then add a time variable to it, based on the sampling rate in the original audio.\nwav_df &lt;- wave %&gt;%\n    as_tibble() %&gt;%\n    rename(hz = V1) %&gt;%\n    rowid_to_column(\"sample\") %&gt;%\n    mutate(t = sample / boy_wav@samp.rate) %&gt;%\n    print()\n\n## # A tibble: 36,874 x 3\n##    sample    hz         t\n##     &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n##  1      1   127 0.0000227\n##  2      2   125 0.0000454\n##  3      3   123 0.0000680\n##  4      4   131 0.0000907\n##  5      5   134 0.000113 \n##  6      6   131 0.000136 \n##  7      7   135 0.000159 \n##  8      8   134 0.000181 \n##  9      9   142 0.000204 \n## 10     10   140 0.000227 \n## # … with 36,864 more rows\nThe next question then is whether I can plot this to make it look like a spectrogram. The way it’s structured now, it’s a cinch to plot the wave form:\nggplot(wav_df, aes(t, hz)) + \n  geom_line()\n\nBut I would kinda like to see a spectrogram. Fortunately, the phonTools package by Santiago Barreda does exactly what I want! This package can do a heck of a lot more than plot a single spectrogram, and I encourage you to explore the package more. But for now, I’ll do a little more hacking to get the visual I want.\nlibrary(phonTools)\nSo first off, phonTools already has a spectrogram function that can plot the data as is.\nspectrogram(wav_df$hz)\n\nThis is an excellent plot and I appreciate the work that goes into creating it. But, being the nit-picky person I am, I wanted to have some more control over the plot and I wanted to use ggplot2 so that I could then incorporate rayshader. The data that’s being plotted had to have been processed in some way because it’s not a waveform anymore. I wanted to find how it transformed it from a time-vs.-amplitude format to the time-vs.-frequency formant you see above.\nI did some digging and I just could not find a way to extract the data that gets eventually plotted in spectrogram. I mean, it has to have gone through some Fourier analysis or something first to be able to extract frequencies. So, I figure if the spectrogram function could do it, the key was in the function itself. Thanks to R’s naturally open-source nature, I popped the hood of the code behind spectrogram and extracted the portion I needed. So, basically what you see here is a version of the spectrogram function from phonTools written by Barreda, except the final plotting portion is commented out. I’ll admit, I don’t fully understand what all this code is doing, but after lots of trial and error, it works, so I’ll leave it at that.\njoey_spec &lt;- function (sound, fs = 22050, windowlength = 5, timestep = -1000, \n                       padding = 10, preemphasisf = 50, maxfreq = 5000, colors = TRUE, \n                       dynamicrange = 50, nlevels = dynamicrange, maintitle = \"\", \n                       show = TRUE, window = \"kaiser\", windowparameter = 3, quality = FALSE) \n{\n    if (class(sound) == \"ts\") \n        fs = frequency(sound)\n    if (class(sound) == \"sound\") {\n        fs = sound$fs\n        sound = sound$sound\n    }\n    n = ceiling((fs/1000) * windowlength)\n    if (n%%2) \n        n = n + 1\n    if (timestep &gt; 0) \n        timestep = floor(timestep/1000 * fs)\n    if (timestep &lt;= 0) \n        timestep = floor(length(sound)/-timestep)\n    if (preemphasisf &gt; 0) \n        sound = preemphasis(sound, preemphasisf, fs)\n    spots = seq(floor(n/2), length(sound) - n, timestep)\n    padding = n * padding\n    if ((n + padding)%%2) \n        padding = padding + 1\n    N = n + padding\n    spect = sapply(spots, function(x) {\n        tmp = sound[x:(x + n - 1)] * windowfunc(sound[x:(x + n - 1)], window, windowparameter)\n        tmp = c(tmp, rep(0, padding))\n        tmp = tmp - mean(tmp)\n        tmp = fft(tmp)[1:(N/2 + 1)]\n        tmp = abs(tmp)^2\n        tmp = log(tmp, 10) * 10\n    })\n    spect = t(spect)\n    for (i in 1:nrow(spect)) spect[i, 1] = min(spect[i, -1])\n    hz = (0:(N/2)) * (fs/N)\n    times = spots * (1000/fs)\n    rownames(spect) = as.numeric(round(times, 2))\n    colnames(spect) = as.numeric(round(hz, 2))\n    if (colors == \"alternate\") \n        colors = c(\"black\", \"red\", \"orange\", \"yellow\", \"white\")\n    if (maxfreq &gt; (fs/2)) \n        maxfreq = fs/2\n    spect = spect - max(spect)\n    # specobject = list(spectrogram = spect, fs = fs, windowlength = windowlength,\n    #                   timestep = timestep, dynamicrange = dynamicrange, colors = colors,\n    #                   maxfreq = maxfreq)\n    # class(specobject) = \"spectrogram\"\n    # if (show == TRUE) \n    #     plot(specobject, ylim = c(0, maxfreq), quality = quality)\n    # invisible(specobject)\n    return(spect)\n}\nWith this modification of the function, I can now convert my data from waveform format to spectrogram format. What you se here is a giant spreadsheet where each row is a time point, each column is a frequency, and the cells contain amplitude (or something) at that time for that frequency.\nspec &lt;- joey_spec(wav_df$hz) %&gt;%\n    as.tibble() %&gt;%\n    rowid_to_column(\"sample\") %&gt;%\n    print()\n\n## # A tibble: 1,020 x 618\n##     sample   `0` `17.9` `35.8` `53.69` `71.59` `89.49` `107.39` `125.28`\n##      &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n##  1       1 -80.3  -53.7  -53.5   -53.1   -52.6   -52.1    -51.7    -51.3\n##  2       2 -77.9  -42.7  -42.8   -43.0   -43.2   -43.5    -43.9    -44.3\n##  3       3 -82.5  -33.3  -33.4   -33.5   -33.6   -33.8    -34.1    -34.4\n##  4       4 -71.0  -41.9  -39.6   -37.4   -35.6   -34.1    -32.9    -31.8\n##  5       5 -73.0  -27.7  -27.7   -27.6   -27.5   -27.4    -27.3    -27.2\n##  6       6 -74.6  -39.4  -39.2   -38.9   -38.5   -38.1    -37.6    -37.0\n##  7       7 -88.0  -39.5  -39.5   -39.6   -39.6   -39.7    -39.8    -39.9\n##  8       8 -93.7  -33.5  -33.5   -33.6   -33.7   -33.9    -34.0    -34.2\n##  9       9 -69.6  -34.5  -34.6   -34.8   -35.0   -35.3    -35.7    -36.0\n## 10      10 -83.2  -33.6  -33.6   -33.7   -33.7   -33.7    -33.8    -33.9\n## # … with 1,010 more rows, and 609 more variables: [truncated for space]\nThis isn’t the most useful format, so I’ll do some reshaping with gather and turn it into a very tall spreadsheet with 629,340 rows.\nspec_tall &lt;- spec %&gt;%\n    gather(hz, value, -sample) %&gt;%\n    mutate(hz = as.double(hz)) %&gt;%\n    print()\n\n## # A tibble: 629,340 x 3\n##     sample    hz value\n##      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1       1     0 -80.3\n##  2       2     0 -77.9\n##  3       3     0 -82.5\n##  4       4     0 -71.0\n##  5       5     0 -73.0\n##  6       6     0 -74.6\n##  7       7     0 -88.0\n##  8       8     0 -93.7\n##  9       9     0 -69.6\n## 10      10     0 -83.2\n## # … with 629,330 more rows\n And now, finally, I’ve got my acoustic data in the format I want! Now we’re in business.Also, let’s take a second to appreciate that there are 629,340 data points for one 0.83 second recording. I can now understand why long audio files are so large!\n\n\nA 2D spectrogram\nOkay, so using this data, I can already do a basic plot in ggplot2 to make sure it looks good.\nggplot(spec_tall, aes(sample, hz, color = value)) + \n    geom_point(size = 0.5, alpha = 0.03) + \n    scale_color_viridis_c(option = \"A\") + \n    scale_y_continuous(limits = c(0, 5000))\n\nIt may not look like it, but that plot is made up of several hundred thousand dots. With that much data, a PDF would get huge since each dot has to be rendered individually. Since they all visually sort of blur together anyway, might as well simplify things and turn it into a 2D-density plot with geom_raster.\nspec &lt;- ggplot(spec_tall, aes(sample/max(sample) * 0.83, hz)) + \n    geom_raster(aes(fill = value), interpolate = TRUE) + \n    scale_y_continuous(limits = c(0, 4500)) + \n    scale_fill_viridis_c(option = \"A\") + \n    labs(x = \"time (s)\")+ \n    theme_classic()\nspec\n\nThe difference visually is relatively small, other than the colors being bolder. But rather than plotting individual points, this is a mosaic of tiles all arranged neatly to fill the space. So the underlying structure is quite different, and in this case, more appropriate for transforming into a 3D plot.\n\n\nA 3D spectrogram\nAnyway, I’ve belabored the point for too long. Let’s now turn this into a 3D spectrogram!\nTook about a minute to render and save.\nplot_gg(spec, \n        width = 6, height = 3, scale = 300, \n        windowsize = c(1000, 800),\n        fov = 70, zoom = 0.6, \n        theta = 330, phi = 40,\n        multicore = TRUE)\nrender_depth(focus = 0.68, focallength = 1, filename = \"spec_3D\")\n\nI love this! There’s so much detail. I mean, you can really see the individual glottal pulses. Super cool.\nAnd now I’ll use the same code from before to make a super awesome video only this time I’ll make a slight change to the viewing angle.\nTook about 2.5 minutes.\nphivechalf = 45 + 45 * 1/(1 + exp(seq(-7, 20, length.out = 180)/2))\nphivecfull = c(phivechalf, rev(phivechalf))\n\nrender_movie(filename = \"spec_plot_fancy\", type = \"custom\", \n            frames = 360,  phi = phivecfull, zoom = zoomvecfull, theta = thetavec)\n\nOkay that is slick. I’m extremely impressed with the quality of this animation. Especially considering I know nothing about animation."
  },
  {
    "objectID": "blog/3d-vowel-plots-with-rayshader/index.html#conclusion",
    "href": "blog/3d-vowel-plots-with-rayshader/index.html#conclusion",
    "title": "3D Vowel Plots with Rayshader",
    "section": "Conclusion",
    "text": "Conclusion\nThe rayshader package really is an impressive one and does so much work behind the scenes to make it easy for people to become amateur animators, at least with quantitative data. For right now, I’ll admit that I’m not exactly gleaning anything new from the visuals that I didn’t already know, but that doesn’t mean looking a wider variety of linguistic data using 3D animations like this won’t uncover something new. I can’t wait until my next conference presentation so I can see if I can weasel one of these in there somehow."
  },
  {
    "objectID": "blog/thoughts-on-allophonic-extensions-to-wells-lexical-sets/index.html",
    "href": "blog/thoughts-on-allophonic-extensions-to-wells-lexical-sets/index.html",
    "title": "Thoughts on Allophonic Extensions to Wells’ Lexical Sets",
    "section": "",
    "text": "In a previous post called “Why do people use bat instead of trap”, I wrote a little bit about the Wells Lexical sets (fleece, trap, thought, etc.), a competing set (beet, bat, bought, etc.), and why I think Wells’ original labels are better. In this post, I continue my musings on Wells’ inspired labels for lexical sets, only this time I focus on those used for specific allophones of vowels (ban, pin, toot, etc.). I point out several issues that have arisen over the years and offer some solutions that may make future papers more consistent and less confusing."
  },
  {
    "objectID": "blog/thoughts-on-allophonic-extensions-to-wells-lexical-sets/index.html#the-need-for-labels-for-allophones",
    "href": "blog/thoughts-on-allophonic-extensions-to-wells-lexical-sets/index.html#the-need-for-labels-for-allophones",
    "title": "Thoughts on Allophonic Extensions to Wells’ Lexical Sets",
    "section": "The need for labels for allophones",
    "text": "The need for labels for allophones\nAs explained previously, John C. Wells came up with some labels as a shorthand to refer to words “which tend to share the same vowel, and to the vowel which they share.” These labels, traditionally notated in small capitals, are carefully selected words that don’t form minimal pairs with another words so that there is no confusion as to which vowel is being referred to when spoken.John C. Wells. 1982. Accents of English: Vol 1. p. xviii.\n\n\n\n\n\nRP \n\n\nGenAm\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\nɪ\n\n\nɪ\n\n\n1.\n\n\nkit\n\n\nship, sick, bridge, milk, myth, busy…\n\n\n\n\ne\n\n\nɛ\n\n\n2.\n\n\ndress\n\n\nstep, neck, edge, shelf, friend, ready…\n\n\n\n\næ\n\n\næ\n\n\n3.\n\n\ntrap\n\n\ntap, back, badge, scalp, hand, cancel…\n\n\n\n\nɒ\n\n\nɑ\n\n\n4.\n\n\nlot\n\n\nstop, sock, dodge, rmp, possible, quality…\n\n\n\n\nʌ\n\n\nʌ\n\n\n5.\n\n\nstrut\n\n\ncup, suck, budge, pulse, trunk, blood…\n\n\n\n\nʊ\n\n\nʊ\n\n\n6.\n\n\nfoot\n\n\nput, bush, full, good, look, wolf…\n\n\n\n\nɑː\n\n\næ\n\n\n7.\n\n\nbath\n\n\nstaff, brass, ask, dance, sample, calf…\n\n\n\n\nɒ\n\n\nɔ\n\n\n8.\n\n\ncloth\n\n\ncough, broth, cross, long, Boston…\n\n\n\n\nɜː\n\n\nɜr\n\n\n9.\n\n\nnurse\n\n\nhurt, lurk, urge, burst, jerk, term…\n\n\n\n\niː\n\n\nu\n\n\n10.\n\n\nfleece\n\n\ncreep, speak, leave, feel, key, people…\n\n\n\n\neɪ\n\n\neɪ\n\n\n11.\n\n\nface\n\n\ntape, cake, raid, veil, steak, day…\n\n\n\n\nɑː\n\n\nɑ\n\n\n12.\n\n\npalm\n\n\npsalm, father, bra, spa, lager…\n\n\n\n\nɔː\n\n\nɔ\n\n\n13.\n\n\nthought\n\n\ntaught, sauce, hawk, jaw, broad…\n\n\n\n\nəʊ\n\n\no\n\n\n14.\n\n\ngoat\n\n\nsoap, joke, home, know, so, roll…\n\n\n\n\nuː\n\n\nu\n\n\n15.\n\n\ngoose\n\n\nloop, shoot, tomb, mute, huge, view…\n\n\n\n\naɪ\n\n\naɪ\n\n\n16.\n\n\nprice\n\n\nripe, write, arrive, high, try, buy…\n\n\n\n\nɔɪ\n\n\nɔɪ\n\n\n17.\n\n\nchoice\n\n\nadroit, noise, join, toy, royal…\n\n\n\n\naʊ\n\n\naʊ\n\n\n18.\n\n\nmouth\n\n\nout, house, loud, count, crowd, cow…\n\n\n\n\nɪə\n\n\nɪ(r\n\n\n19.\n\n\nnear\n\n\nbeer, sincere, fear, beard, serum…\n\n\n\n\nɛə\n\n\nɛ(r\n\n\n20\n\n\nsquare\n\n\ncare, fair, pear, where, scarce, vary…\n\n\n\n\nɑː\n\n\nɑ(r\n\n\n21\n\n\nstart\n\n\nfar, sharp, bark, carve, farm, heart…\n\n\n\n\nɔː\n\n\nɔ(r\n\n\n22\n\n\nnorth\n\n\nfor, war, short, scorch, born warm…\n\n\n\n\nɔː\n\n\no(r\n\n\n23\n\n\nforce\n\n\nfour, wore, sport, porch, borne, story…\n\n\n\n\nʊə\n\n\nʊ(r\n\n\n24.\n\n\ncure\n\n\npoor, tourist, pure, plural, jury…\n\n\n\n\n\n\nWells’ original lexical sets. From Wells (1982:xviii–xix).\n\n\nAs research on American English continues though, we find ourselves needing to propose extensions to these lexical sets refer to specific allophones of vowels. For example, a common phenomenon is that goose is fronted even more if it’s following a coronal sound, so that toot has a fronter vowel than boot.In fact, boot itself is probably fronter than pool. I won’t get into prelateral allophones too much here though because I’m going to devote an entire post to prelateral allophones in the near future. A lot of this variability is simply phonetic conditioning, it’s probably not necessary to propose a label for every allophone known to exist in English.\nHowever, since some of these allophones exhibit socially conditioned variation, it becomes important to talk about these subsets of words. And, to fit in with the existing labels that Wells proposed, these relevant environments are often referred to using a new label. That’s where the trouble begins."
  },
  {
    "objectID": "blog/thoughts-on-allophonic-extensions-to-wells-lexical-sets/index.html#problems-that-arise-with-labels-for-allophones",
    "href": "blog/thoughts-on-allophonic-extensions-to-wells-lexical-sets/index.html#problems-that-arise-with-labels-for-allophones",
    "title": "Thoughts on Allophonic Extensions to Wells’ Lexical Sets",
    "section": "Problems that arise with labels for allophones",
    "text": "Problems that arise with labels for allophones\nProposing a lexical set for the vowel classes in English (and select allophones) may be somewhat straightforward (Wells said later he created his over the course of a weekend). However, when trying to do the same for English allophones, there are a number of issues that have come up: competing labels, variety-specific labels, and keeping track of modifications to Wells’ original labels. Let’s consider each of these problems.\n\nCompeting standards\nWe see a recurring problem when scientists study some new thing: we don’t know what to call it. And when independent research is happening in parallel on that new thing, multiple terms are introduced, each with perfectly good and justifiable reasoning. This is what has happened with labels for allophones in the study of English sociolinguistics.Another example is the California Vowel Shift a.k.a. Canadian Vowel Shift a.k.a. Elsewhere Shift a.k.a. Western Vowel Pattern a.k.a. Low-Back-Merger Shift a.k.a. Short Front Vowel Shift a.k.a. Third Dialect Shift etc.\nAs an example, let’s look at the kit and dress vowels before nasals. I have not studied the pin-pen merger with very much depth, but three labels I have seen are pin & pen, bin & ben, and kin & den. Without careful reading, it wouldn’t be immediately clear that each of these pairs refers to the exact same allophones. So why so many different labels?\nWell, each one is perfectly justified. First, because the phenomena is most often called the pin-pen merger, it would make sense to just refer to the two classes of words as pin and pen. Alternatively, some people use bin and ben because they most closely resemble the b_t frame, so that bit, bet, bin, and ben can be used alongside each other. Similarly, kin and den were used because they have the same onset as Wells’ kit and dress labels. All are perfectly reasonable explanations—which is unfortunate because I think that’s why these competing standards are perpetuated.\n\nThese inconsistencies have lead some papers to use the lexical set and an IPA symbol, just for clarity. Boberg (2019:22) consistently refers to vowels by their ANAE transcription followed by the Wells keyword in parentheses. For example: “the raising of /aw/ (mouth) and /ay/ (price) is clearly evident in the vowel means produced by the present sample of Canadians….” Similarly, Wassink (2016) uses IPA followed by keyword in the b_t frame: “It may be seen that for these speakers, /ɑ/ bot is clearly merged with /ɔ/ bought.” I think a labeling system fails if it needs backup from another transcription system.\nAs long as authors are clear about what their labels refer to, it shouldn’t pose much of a problem when reading a particular work (since it hasn’t really so far). However, it does create inevitable confusion when comparing different labels across studies.\n\n\nNot all varieties need all labels\nAnother issue that arises when creating extensions to the Wells’ Lexical sets is that they’re often specific to some varieties. The original labels were designed to be used for studying both American and British varieties of English. This mostly works since they more or less have the same vowel inventory. The problem is not all allophones are relevant to all varieties of English.\nTake, for example, Wells’ force and north labels for pre-rhotic allophones. If you’re like me, you may not have previously realized that words like four, porch, and sport are not in the same lexical set as short, scorch, and born. The first set, force, historically contained /oɹ/ and the second, north, historically had /ɔɹ/. Most varieties of North American English merge them today, though in a few places like St. Louis, Utah, and Texas you might hear north with a lower vowel, often merged with start. The point is, for most varieties of North American English, it doesn’t make sense to distinguish force and north because they’ve long since merged.Growing up near St. Louis, I heard that you’re a true St. Louisian if you call Interstate 44 “highway farty-far”. But this is not actually how they’d say it: four is part of force but forty has north, so a true St. Louisian would actually say it like farty-four. Since I grew up in the suburbs, I don’t have this in my speech, but if you want to hear an authentic St. Louis pronunciation of that number, listen to Phyllis Smith in season 8 episode 21 of The Office.\nAs a more extreme example, an increasing number of varieties of English have collapsed lot, thought, cloth, and palm down to just a single low back vowel. When studying such varieties, there is clearly no need for all four labels.\nOn the other hand, phonemic splits have necessitated more specific labels. At least one has been encoded into Wells’ original labels: the trap-bath split. However, in Philadelphia and other areas, the short-a split divides trap another way into “tense” and “lax” realizations, sometimes labeled bad and bat, respectively. Other varieties have other splits: Rebecca Starr has recently reported on the apparently arbitrary lexical split of dress in Singapore English, a phenomena she calls the next-text split. Not all varieties need to worry about these splits, so these labels are only relevant to certain varieties of English.\nIf you look through studies on other World Englishes, particularly the Varieties of English volumes,  you’ll see lots and lots of ad hoc, variety-specific labels. One that I read recently was in relation to Southern American English is the dance class, which contains “words in which RP shows [ɑː] before a nasal/obstruent cluster.” Some varieties of Southern American English realize this vowel as /æ̱ɛ/ which is distinct from the hand (/æ/ before nasals) class that is realized as a triphthongal /æɛæ/. To my knowledge, not all varieties of English will need to distinguish dance from hand (or ban or tan or whatever you want to call it), but for this variety it’s important.Edgar Schneider, ed. 2005. Varieties of English (4 volumes) and Bernd Kortmann, et al., eds. 2008. A Handbook of Varieties of English (2 volumes).Erik R. Thomas. 2005. “Rural Southern White Accents.”\nOverall, it becomes difficult to standardize these labels. In future varieties of English, we will most certainly see allophonic/morphological/lexical splits that we could not have anticipated. So even if someone were to canonize a large set of extensions to Wells’ lexical sets, they’d become outdated as language change continues its merry way. Furthermore, not all varieties and not all speakers classify words into the same lexical sets, so a strict definition wouldn’t be uniform across all dialects anyway.\nIn the end, I think defining ad hoc labels is fine. However, I feel that they should be more rigorously defined to facilitate comparison across studies.\n\n\nNarrowing of the original Wells label\nOne final issue that I’ve run into is that the original Wells or b_t label is sometimes narrowed to refer to a specific allophone (often the elsewhere allophone) of the original vowel.\nFor example, in the South, glide reduction in price is partially conditioned by phonological environment. Therefore, researchers have proposed the labels prize and pry to refer to /aɪ/ before voiced sounds and /aɪ/ word-finally, while the original price is narrowed to just /aɪ/ before voiceless sounds. It makes for a tidy set within that study (price, prize, and pry) but is there a more cross-dialect-friendly way?\nThe problem is that we now have two different meanings of Wells’ original price label. In its original sense, price referred to /aɪ/ in all environments; in this new sense, price only refers to prevoiceless /aɪ/. In other words, some allophones of /aɪ/ have been carved out of the full price lexical set, leaving a smaller, modified price behind. The result is that when a researcher refers to price, it is not clear whether they are referring to the phoneme /aɪ/ or the pre-voiceless allophone of /aɪ/.\nThis failure to create a new label for the elsewhere allophone can lead to confusing labels. One study I’ve read used the labels bag for /æɡ/, bang for /æŋ/, and ban for /æn/ or /æm/. Those are mostly fine. However, confusingly, the label bat2 was used to refer to the elsewhere allophone of /æ/ (i.e. before all obstruents except /ɡ/). So in a single paper, the label bat was used to refer to the entire vowel category, and bat2 was used for preobstruent /æ/. There was nothing wrong with the study itself, but I think the transcription system could have been better.\nMy recommendation is that, if allophones are to be referred to by a non-canonical label, the elsewhere allophone should also get a new one. The canonical label can then be reserved for the entire vowel phoneme. In my dissertation, I used a set of labels for allophones of /æ/ (bag, bang, ban, and bat), but collectively I refer to the entire vowel class as trap. In an upcoming LSA presentation, I’ll refer to post-coronal /u/ as toot and non-post-coronal /u/ as boot, with goose being the umbrella term for both allophones. It is not a perfect solution, but I think using the Wells set for entire phoneme and using standardized sets for allophones makes intuitive sense. At the very least, bat2 (as well as bet2 and bit2) should not be used.I acknolwedge that this now creates confusion for those who are used boot as synonymous with goose, because my boot refers to a more restricted set of words than others’ boot. But I think it strikes a nice balance between the Wells and b_t labels."
  },
  {
    "objectID": "blog/thoughts-on-allophonic-extensions-to-wells-lexical-sets/index.html#recommendations",
    "href": "blog/thoughts-on-allophonic-extensions-to-wells-lexical-sets/index.html#recommendations",
    "title": "Thoughts on Allophonic Extensions to Wells’ Lexical Sets",
    "section": "Recommendations",
    "text": "Recommendations\nTo sum up my ramblings, here are some suggestions that I have come up with:\n\nIf you are considering proposing a new label, check to see whether one has already been created for that set of words. We don’t need more labels to refer to the same thing. However, if the label(s) violates some of the suggestions below, perhaps a new standard is needed.\nNon-canonical, Wells-inspired labels should be more rigorously defined, ideally accompanied with wordlists in an appendix. It should not be difficult to provide at least the 20 most common words. In fact, for reasonably-sized corpora, I don’t think it’s asking too much to put an exhaustive list of words that are attested in your data, especially if online appendices are an option. After all, how did you code your data? You’ve got that list on some spreadsheet somewhere already.\nThe set of words that existing Wells labels refer to should not be modified; if the lexical space of some set is divided into subsets, be sure to also propose a new label for the elsewhere allophone. This avoids the confusion with price-as-/aɪ/ vs. price-as-prevoiceless-/aɪ/ and the coexistence of bat and bat2. Reserve the original Wells label to refer to the phoneme rather than to any specific allophone. Consider using the b_t frame for these elsewhere allophones.\nMore going back to my previous post, labels should not be based on minimal pairs or minimal sets. They may be fine in writing, but they get confusing when spoken. If a new label is proposed, carefully consider potential mergers with nearby vowels and choose a word that satisfies the ideal property of being unambiguous regardless of the speaker’s variety."
  },
  {
    "objectID": "blog/jmp/index.html",
    "href": "blog/jmp/index.html",
    "title": "JMP",
    "section": "",
    "text": "As a part of my assistantship this year, I get to work with the DigiLab in the Main Library at UGA. It’s a fun little gig where I get to do presentations, workshops, and seminars on digital humanities, in addition to helping researchers one-on-one on their own projects.\nI’ve always been fairly tech savvy in my research. I minored in Linguistic Computing and had a job as an undergrad creating eBooks. I’ve always been fairly quantitative about things too, and I went over to the stats department last year and took classes on linear regression and multivariate analysis.\nThis week is my first presentation where I’ll show how to extract data from primary sources, introduce people to spreadsheets, and showcase JMP If you’ve never heard of it, JMP (pronounced “jump”) is a pretty sophisticated piece of statistical software that can do all sorts of statistical analyses and visualizations. The great part is it’s drag-and-drop interface. No coding means it’s great for beginners.\nI’ll be the first to admit that JMPs visualizations (at least as well as I can make them) are not as good as what I can do with ggplot2 in R. But I do like how easy it is to make quick and dirty visualizations in JMP. For this reason, it’s usually my go-to when exploring a new dataset. With just a few clicks, it makes it really easy to get to know your data visually as well as what’s under the hood.\nAs far as I know, JMP is not widely used in Linguistics or even in the Humanities, so showcasing it will be a lot of fun to people this week. I still think that everyone should learn R for their research, but since coding can be intimidating, JMP is a great tool for researchers wanting to get more quantitative about their projects.\nUpdate (October 7): I just finished the presentation. I had about a dozen people in attendance, and I was the youngest person in the room. And besides the one other grad student there, I was probably the only one without a Ph.D. But, I feel like it went well. Not to mention, it was the first time I was introduced before a presentation, and there was food!"
  },
  {
    "objectID": "blog/a-survey-of-western-american-english-using-mturk/index.html",
    "href": "blog/a-survey-of-western-american-english-using-mturk/index.html",
    "title": "A Survey of the Western American English using MTurk",
    "section": "",
    "text": "I’m so happy to announce I’ve been selected as a recipient of the UGA Graduate School Innovative and Interdisciplinary Research Grant! This $2,500 grant is part of the Graduate School’s strategic initiative to support innovation and interdisciplinary in the research being conducted by doctoral students. My project is entitled “A Survey of Western American English using Amazon Mechanic Turk.”\nAmazon Mechanical Turk (“MTurk”) is a crowdsourcing marketplace through Amazon.com where people and business can post tasks for others to perform for a small amount of payment. They are usually menial tasks like completing surveys or data entry. At NWAV last year, Kim, Reddy, Wyschogrod, and Stanford did a presentation on how they cleverly used MTurk to gather recordings of people across the Northeast. After some discussion with some of the authors, I decided to apply for a grant that would pay for the same kind of data collection but targeting the West.Kim, Chaeyoon, Sravana Reddy, Ezra Wyschogrod & James Stanford. 2016. A large-scale online study of dialect variation in the US Northeast: Crowdsourcing with Amazon Mechanical Turk. Paper presented at the New Ways of Analyzing Variation 45, Victoria, BC.\nCompared to other parts of the country, the West has relatively little linguistic research. Some of that is changing, thanks to some of the folks in Stanford, University of Washington, and other universities. But there are large portions of the country like Montana, Wyoming, and other places that have very little written about them. The population isn’t huge, but there are people there, and those people do speak. So what do they sound like?\nThis project will hopefully get recordings from possibly up to 500 people in specific western states. This will result in a corpus of over 100 hours of audio and roughly 200,000 words—a sizable linguistic corpus which I can analyze for many years to come. This will allow a slightly deeper look into some of these places, acting as a launchpad for further research in specific cities.\nI’m really excited to have gotten this grant and to get started. I will post updates as they come. Stay tuned."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html",
    "href": "blog/brand-yourself-2/index.html",
    "title": "Brand Yourself 2",
    "section": "",
    "text": "Today, I was asked to do a professionalization workshop on different ways grad students can boost their online presence through building a personal webpage, utilizing social media, and finding their field’s conversation—basically, how to make yourself more googleable. At the end, I challenged people to not leave the room until they had built some sort of new online profile they didn’t have when they walked in."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html#social-media",
    "href": "blog/brand-yourself-2/index.html#social-media",
    "title": "Brand Yourself 2",
    "section": "Social Media",
    "text": "Social Media\n\nAcademia.edu\nIf you have zero online presence, Academia.edu is a great way to get something up and running quickly. Basically, it’s a freemium social networking site for academics. Users can create profiles, upload their papers, and follow particular research topics. They can also follow others that have done the same. It’s a great resource for finding papers that may be behind a paywall, although it has gotten a lot of criticism for this. Papers you upload can be found by Google Scholar, which is a nice perk. The website will keep track of your analytics, and there’s nothing more thrilling than getting an email saying someone has found your profile!\n\nThere are some negative aspects of Academia.edu. The site got some criticism a few years ago for offering authors the chance to promote their work for a fee. As of about two years ago, they launched a Premium version ($8.25 a month) with extra features and they constantly bug you about it. There’s also a chance at any time the site could get shut down because publishers aren’t happy about it, but with 30 million users, I don’t know if that’s going to happen any time soon. Full disclosure, I deleted my account about a year ago, now that I have this site up and running.\n\n\nResearchGate\nI’m less familiar with ResearchGate, but in my cursory look, there’s a lot of overlap with academia.edu as far as its features. A big difference I noticed is that it seems like it’s more focused on creating networks based on people (folks you you cite and co-author with) while academia.edu is more focused on following topics. Two thing I don’t like about ResearchGate is that the number of emails it sends you is borderline spam and that it’ll create a profile for you based on information that other people put up without you knowing. I think in some fields ResearchGate is more popular than Academia.edu, so it might be worth it to have a profile on both.\n\n\nGoogle Scholar\nI would imagine most researchers use Google Scholar regularly, but did you know you can create a profile for others to see? You can tell a researcher has done that when you see their name underlined in a search:\n\nIn this screenshot (live link here), you can see that Walt Wolfram, Natalie Schilling, Sali Tagliamonte have created their profiles, but Shanna Poplack and Penny Eckert have not. I’d like to see what else the last two researchers have written, but I can’t simply click on their names like I can with the first three. When you do click on their links, you can see the full profile including what else they have written and how many times each has been cited.\n\nIt does take a bit of work to get a full profile going, because Google’s data can be a bit messy, so you’ll have to add stuff in by hand. But I think the payoff is worth the effort.\n\n\nOther Academic Social Media\n There are a handful of other websites out there that can help you build an online presence. Impact Story is one that can keep track of how much of an impact you have on people by keeping track of when people cite, mention, read you and your work. For $10 a month, it might not be worth it for a grad student, but for a professor applying for tenure this might be worth it.I am indebted to the Impact Challenge blog series, with the accompanying 200+ page pdf, from which I learned a lot about all this. I would highly recommend that you download it and take a look. Not only does it include much more than what I’ve mentioned here, including step-by-step how-to guides to getting these profiles set up, but also many more topics to get yourself more visible. Thanks, Impact Challenge.\n\n\nWhat about LinkedIn?\nI haven’t found LinkedIn terribly useful for people looking for academic jobs (like me). It might be worth it to set up a low-maintenance page that gives a good view of you in a nutshell, just in case people look."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html#building-a-personal-webpage",
    "href": "blog/brand-yourself-2/index.html#building-a-personal-webpage",
    "title": "Brand Yourself 2",
    "section": "Building a Personal Webpage",
    "text": "Building a Personal Webpage\nKeeping track of all these profiles can be tedious. Do you need to update seven different profiles every time you present at a conference? Is it worth it to invest the time in these sites that don’t communicate with each other? One solution is to choose one site to be your main one and build a full profile there. Then, create a “highlights” version on the other social media sites and redirect people to your main page. What should be your main site though? For this reason, it’s nice to have a personal webpage.\nThe problem with personal webpages it that they come with a cost, either in money or skills (and sometimes both). You can set up a webpage through Word Press, Wix.com, or Square Space, which take little technical skill to get a professional page set up. These can be free, but you can get some extra features for $10 a month or more. To me, that’s a pretty penny to pay for a relatively simple webpage.\nAnother option, which is what I’m doing for [Edit: did for the old version of] this website], is to host the page on Github. It’s free, but it takes a bit of skill. I’ve had to learn to use Jekyll, Markdown, and CSS, but through some help on ProgrammingHistorian.com and Lynda.com, I was able to get this site up. The benefit of going this route is I have unlimited flexibility in how the site looks, and I really, really like that.\nEither way, it’s probably worth it to set up a personal domain name. For as little as $1 a month, you can buy your own domain name (like www.joeystanley.com), which looks much more professional than www.blogsplot.com/joeystanley or www.github.com/joeystanley."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html#finding-your-conversation",
    "href": "blog/brand-yourself-2/index.html#finding-your-conversation",
    "title": "Brand Yourself 2",
    "section": "Finding Your Conversation",
    "text": "Finding Your Conversation\nThe last thing we talked about in our workshop is to find where the big names in your field are having their online conversations. This sounds a little weird at first, but every field has some secret space where people are collaborating and sharing ideas informally as well as posting calls for papers, invitations for publications, and job openings. The problem is that where is space is is different for every field.\nIn some fields, these are a listserv. As far as I know, network analysis and Slavic languages each have a well-known listserv where all the conversation happens. If you’re not on that listserv, you’re out of the loop. Digital Humanities has a space on Slack where over 800 researchers get together and talk. For some fields, it might just be at coffee breaks during certain conferences. You may have to ask around established academics in your field to find that space.\nOne thing I will mention is that a lot of action happens on Twitter, like livetweeting conferences. I’ve covered this in more depth in an earlier blog post, but basically a lot of good stuff can come out of following the right people and seeing just the right tweets.\n\nI’ve given this presentation multiple times now. You can see the (very similar) slides from April 13, 2017 and the ones from November 11, 2016."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html#conclusion",
    "href": "blog/brand-yourself-2/index.html#conclusion",
    "title": "Brand Yourself 2",
    "section": "Conclusion",
    "text": "Conclusion\nIn today’s job market, there’s no surefire way to ensure you’ll get hired. But increasing your online presence and making yourself more googleable probably helps."
  },
  {
    "objectID": "blog/brand-yourself-3/index.html",
    "href": "blog/brand-yourself-3/index.html",
    "title": "Brand Yourself 3",
    "section": "",
    "text": "Tip\n\n\n\nDownload the slides here\n\n\nToday, I led a professionalization workshop as a part of the Graduate Research Workshop Series, sponsored by UGA Libraries. In it, I talked about different ways grad students can boost their online presence through building a personal webpage, utilizing social media, and finding their field’s conversation—basically, how to make yourself more googleable. In today’s job market, there’s no surefire way to ensure you’ll get hired. But increasing your online presence and making yourself more googleable probably helps. I spent the workshop discussing academic social media, building a personal webpage, and using Twitter to join the conversation. At the end, I challenged people to not leave the room until they had built some sort of new online profile they didn’t have when they walked in.\nAdditional Materials: I’ve given this presentation several times, each one with some updated information. You can see the (very similar) slides from September 2019, September 2018, from April 2017 and from November 2016. My views on a few things have changed since then (I value personal webpages more and academic social media less) and there are a few topics that were cut in the latest version."
  },
  {
    "objectID": "blog/excel-workshop/index.html",
    "href": "blog/excel-workshop/index.html",
    "title": "Excel Workshop",
    "section": "",
    "text": "Today I had the opportunity to give a workshop in the DigiLab in UGA’s main library. It was a packed with librarians and grad students from across campus. In just over an hour, I started with the absolute basics and showed more and more tricks that I think would help people with their research projects.\nThis was the first time I’ve ever given a presentation without powerpoint slides. As I was preparing though, it seemed silly to include detailed descriptions and screenshots when I could just switch over the Excel and show it live. I ended up putting together a handout instead, which had all of the information on it instead. The presentation (and handout) went through the following topics."
  },
  {
    "objectID": "blog/excel-workshop/index.html#the-basics",
    "href": "blog/excel-workshop/index.html#the-basics",
    "title": "Excel Workshop",
    "section": "The Basics",
    "text": "The Basics\nI started off by explaining what the differences were between Microsoft Office 2016, 365, and Online. Essentially, Office 2016 is stand-alone software that you buy once and keep forever but doesn’t upgrade. Office 365 is a subscription service where you have the software as long as you subscribe to it, but it upgrades all the time. Office Online is a free cloud-based version. Through UGA, we can get Office 365 for free, and a heavily reduced priced Office 2016.\nI then opened up Excel and covered the absolute basics. Data entry. Moving around the spreadsheet. Cell formatting. Borders. Text formats. I’m pretty sure everyone there knew this basic stuff, but I thought I’d cover it anyway because, you never know, there might be someone who’s never seen it before.\nAfter that, into topics that make it easier to play around with your data. Search and replace, with some extras like matching the entire cell. I also covered sorting and filtering and showed that you can filter multiple columns to get a really specific subsets of your data."
  },
  {
    "objectID": "blog/excel-workshop/index.html#pivot-tables",
    "href": "blog/excel-workshop/index.html#pivot-tables",
    "title": "Excel Workshop",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nThis is where I wanted to spend the most time. Pivot tables are things that a lot of people had heard of (and from the show of hands, about half the people in the room), but for some reason—and I don’t say this to be all high and mighty—I have never met anyone who knows how to use them. I learned them in my Intro to Linguistic Computing class with Monte Shelley at BYU and have used them a ton since then, but I guess people just haven’t had the chance to learn about them.\nPivot tables are dang useful. They can summarize your data in tons of fancy ways. At the very basic level, you can at least see all the unique values in a particular column in your dataset, which is good for checking typos or for copying and pasting into lookup tables (see below). But when you add columns, you can then see how many row of your data frame there are that match the row and column. In the presentation we looked at come census data and saw how many people were from each city within that county. By adding columns, we could see how many men and women there were in each city. We could then add additional columns (like fabricated favorite color data) so that we could see how many men and women liked each color within that city.\nThere are different ways of viewing the data as well. Instead of the raw count, we looked at how to view the proportion of men to women there were in each town. We switched to a numerical data type (fabricated weight and height data), and were able to see the average weight and height for men and women in each city, as well as the tallest, shortest, heaviest, and lightest man and women in each town. I heard some audible whoa’s from people as I showed some of this stuff, which was great to hear."
  },
  {
    "objectID": "blog/excel-workshop/index.html#functions",
    "href": "blog/excel-workshop/index.html#functions",
    "title": "Excel Workshop",
    "section": "Functions",
    "text": "Functions\nI then looked at some Excel functions. We started with some basic math, but quickly went into some functions. I showed how some can just stand on their own, like pi(), today(), and now(). We looked at how to reference other cells in functions and how they update automatically. I showed how to create a sequential list of numbers using functions that reference each other. There are some functions like year(), month(), weekday(), and datedif() that work on dates and others like concat(), upper(), lower(), left(), and right() are for manipulating strings. Then there are some that make reference to ranges, like sum() and average(). I showed how the concat() function can be useful to string together last name and first name to create a “Last, First” column. I know there are much more complicated and useful functions than the ones I covered, but I didn’t want to intimidate anyone.\nWe then looked at some conditionals and how they work. As an example, I created a new column in the census data that essentially collapsed the birth state down to two: Washington or not Washington."
  },
  {
    "objectID": "blog/excel-workshop/index.html#lookup-tables",
    "href": "blog/excel-workshop/index.html#lookup-tables",
    "title": "Excel Workshop",
    "section": "Lookup Tables",
    "text": "Lookup Tables\nThe lookup() function is one that I use all the time, and I wanted to make sure I covered it in the workshop. When preparing for the workshop, I looked at some other site and they all mention that lookup() is essential. What this function can do is basically link together multiple spreadsheets so it starts to act like a database. You set up a table that acts like a dictionary: alphabetic, unique values in one column, with paired information in another. You can then “lookup” some value in this table, and the function will return the information associated with it.\nWhy is this useful? I use it for two main purposes: a converter, and a collapser. I use it as a converter for things like turning ARPABET representations of vowels into IPA. In one column is the ARPABET pair of letters, and the other column are the IPA symbols. It’s perfect for that. I use it also to collapse data down to fewer categories. We did this in the workshop by collapsing the 50 states down to 4–5 regions. We used this lookup table to add a “region” column to the census data, and then made a pivot table with it. Pretty cool.\nFor the last part of this section, I showed how to handle the places where lookup() fails, like blank cells or cells not in the dictionary. For blanks, it returns an error, and you can overcome that by wrapping the lookup() function in if(isblank()). But for those pesky typos, lookup() returns the closest value, which I only discovered recently and was not happy with it. I didn’t have to demonstrate, but in the handout I show that if you do something like =IF(COUNTIF(A1:A5,C2)&gt;0, LOOKUP(C2, A1:A5, B1:B5), \"ERROR\") it’ll work great."
  },
  {
    "objectID": "blog/excel-workshop/index.html#visualizations",
    "href": "blog/excel-workshop/index.html#visualizations",
    "title": "Excel Workshop",
    "section": "Visualizations",
    "text": "Visualizations\nNext was how to do quick and dirty visualizations in Excel. I explained briefly (probably too briefly) that not all visualizations work for all kind of data, which I feel is important for people to know. I then showed how to make a bar chart, pie chart, line graph, and scatterplot. I of course used pivot tables to help summarize the data for visualization. I did say though that I don’t use Excel’s visualizations for anything because I find them ugly, not customizable enough, and not robust enough to handle what I want to do."
  },
  {
    "objectID": "blog/excel-workshop/index.html#bonus-tips-and-tricks",
    "href": "blog/excel-workshop/index.html#bonus-tips-and-tricks",
    "title": "Excel Workshop",
    "section": "Bonus Tips and Tricks",
    "text": "Bonus Tips and Tricks\nIn the last four minutes, I tried to cover some bonus little tips and tricks that I’ve picked up along the way. There are little things like anchoring and freezing/splitting the table. I did show conditional formatting because I use that all the time (my tables look a little psychedelic actually). In the handout I cover how to convert text-to-columns, which can be useful when importing data from somewhere else or for just splitting things up like “Last, First” into two columns. I covered paste special and how you can transpose, paste multiply, and overwriting functions.\nIt was a real whirlwind of a presentation but I think some people got a lot out of it. I don’t know if too many people walked away with any new skills per se, but at least people were exposed to what kinds of things Excel can do, and were given the resources (i.e. this handout) to learn how to do it themselves. I enjoyed giving the presentation, and even though I use R for most of my work nowadays, knowing the ins and outs of Excel sure is useful."
  },
  {
    "objectID": "blog/excel-workshop/index.html#downloads",
    "href": "blog/excel-workshop/index.html#downloads",
    "title": "Excel Workshop",
    "section": "Downloads",
    "text": "Downloads\nAgain, you can download this handout here. Feel free to also download the three datasets I used: the vowels for one speaker and the vowels subset of larger dataset, which both come from the Linguistic Atlas of the Gulf States, and the Cowlitz County 1930 census data, which I gathered myself. Please also visit the accompanying blog post on the DigiLab website."
  },
  {
    "objectID": "blog/animating_mergers/index.html",
    "href": "blog/animating_mergers/index.html",
    "title": "Animating Mergers",
    "section": "",
    "text": "I’ve dabbled with creating animations in R, but since the newest version of gganimate came out, I’ve been trying to find a useful way to use it. (I don’t know if visualizing simulations of Chutes and Ladders counts as “useful”…) But as I was putting together a lecture on mergers last semester, it occured to me that the best way to illustrate them would be with animations! So I took the opportunity and created some fun visuals."
  },
  {
    "objectID": "blog/animating_mergers/index.html#using-gganimate",
    "href": "blog/animating_mergers/index.html#using-gganimate",
    "title": "Animating Mergers",
    "section": "Using gganimate",
    "text": "Using gganimate\nAs it turns out, gganimate is awesome! I mean all you need to do is add literally one line of code to your plot and it animates it! I find it easiest to think of the data in movie frames. When I group things by frame, gganimate will take care of the rest for me.\nThe tricky part is creating the underlying dataset. Let’s say I want to illustrate a merger with 50 hypothetical datapoints from vowel A and 50 from vowel B—that’s 100 data points total. If I’ve got an animation of in mind that has 15 different frames, I’m going to need a dataframe with 1500 rows in it. And when the data is all artificial, it gets tricky to create all those carefully.\nSo, this is not a tutorial because my code was awful, hacky, clunky, etc. It’s on my Github for reproducibility and stuff but I won’t talk about it much here because detracts from what I really wanted to show: the animations."
  },
  {
    "objectID": "blog/animating_mergers/index.html#merger-by-approximation",
    "href": "blog/animating_mergers/index.html#merger-by-approximation",
    "title": "Animating Mergers",
    "section": "Merger by Approximation",
    "text": "Merger by Approximation\nI’ll start with a Merger by Approximation because it’s the most straightforward. The first to describe this was Trudgill & Foxcroft, who looked at the merger of Middle English ō [ɔː] (road, go) with the diphthong [ɔu] (flow, know). My American ears can’t even imagine these being pronounced differently, but apparently some speakers in East Anglia retain the distinction, though they were in the process of becoming merged.\nI’ll let you dig into the original study , but the main point they make is that the merger happens “gradually.” Their description of this merger tactic is that speakersTrudgill, Peter, and Tina Foxcroft. 1978. On the Sociolinguistics of Vocalic Mergers: Transfer and Approximation in East Anglia. In Sociolinguistic Patterns in British English, edited by Peter Trudgill, 69–79. London: Edward Arnold.\n\n“could gradually approximate the two vowels by bringing them closer together phonetically until, finally, they become identical.” (Trudgill & Foxcroft 1978:73)\n\nHere’s what I have in mind for what a merger by approximation is like:\n\nAfter several additional cases of this merger had been documented, Labov (1994:321) further clarifies, saying that the final vowel can end up somewhere between the two historic vowel classes. In other cases, like what I’ve illustrated above, one vowel stays put while the other vowel shifts to join it. In either case, the overall distribution of the final vowel is approximately the same size as one the original ones, leaving a gap in the F1-F2 space.Labov, William. 1994. Principles of Linguistic Change. Vol. 1: Internal Features. Language in Society. Oxford: Wiley-Blackwell."
  },
  {
    "objectID": "blog/animating_mergers/index.html#merger-by-transfer",
    "href": "blog/animating_mergers/index.html#merger-by-transfer",
    "title": "Animating Mergers",
    "section": "Merger by Transfer",
    "text": "Merger by Transfer\nIn the very same paper, with the very same phenomenon, and among the same people, Trudgill and Foxcroft describe another mechanism of merger, which has since been called a Merger by Transfer. With this method, speakers\n\n“could variably, one by one, transfer lexical items from one lexical set to another.” (Trudgill & Foxcroft 1978:72)\n\nHere’s how I visualize this type of merger. I’ve animated so words shift slowly at first, then a whole bunch very quickly, and then tapering off. I don’t know if that S-shaped rate of change is officially part of a merger by transfer, but it’s kind of how I picture it.\n\nLabov (1994:321) points out that you don’t really find any words phonetically intermediate between the two historic classes, which I think is the most distinguishing property of this merger. They’re either vowel A or vowel B. Labov also finds that this type of merger is more often found when there’s prestige involved and that the merger is a change from above.\nIt’s a haphazard shift, and during the change each person could have their own personal distribution of which words belong to which vowel class. Which probably makes it really juicy to study."
  },
  {
    "objectID": "blog/animating_mergers/index.html#merger-by-phonological-transfer",
    "href": "blog/animating_mergers/index.html#merger-by-phonological-transfer",
    "title": "Animating Mergers",
    "section": "Merger by Phonological Transfer",
    "text": "Merger by Phonological Transfer\n Nearly 40 years later, Aaron Dinkin published a paper describing what he calls a Merger by Phonological Transfer. The phenomenon in question is the merger of lot with thought. In upstate New York, he finds that many speakers do not have the merger—except in before coda laterals. Thus, cot and caught are distinct, and collar and caller are distinct, but doll rhymes with hall. In Dinkin’s words, what’s happening here isDinkin, Aaron J. 2016. Phonological Transfer as a Forerunner of Merger in Upstate New York. Journal of English Linguistics 44(2): 162–88. doi:10.1177/0075424216634795\n\n“the discrete replacement of one phoneme in a given phonological environment with a different phoneme.” (Dinkin 2016:183)\n\nIt’s a type of merger by transfer, only it happens phonologically rather than lexically. Instead of individual words defecting to another category, entire groups of words, defined by some phonological property, jump ship en bloc to the new phoneme.\nTrudgill and Foxcroft also hint at this in their original 1978 paper, saying that\n\n“In some cases, that is, diffusion of the innovation may not be genuinely lexical but rather influenced by phonological conditioning. In other words, the change may also be linguistically gradual, in that it affects some environments before others.” (Trudgill & Foxcroft 1978:73)\n\nIn Dinkin’s data, it appears that it’s not quite so abrupt. Before one vowel is replaced by another, words in that phonological category phonetically approximate the new vowel first, before switching over. So in the animation, I’ve included that detail:\n\nIncidentally, I’ve totally got this merger in my own speech. The vowels in collar and caller are distinct but in doll and hall they are not. I grew up in a suburb of St. Louis, but my dad is from Upstate New York. I’ll have to ask him what his vowels are like in those words."
  },
  {
    "objectID": "blog/animating_mergers/index.html#merger-by-expansion",
    "href": "blog/animating_mergers/index.html#merger-by-expansion",
    "title": "Animating Mergers",
    "section": "Merger by Expansion",
    "text": "Merger by Expansion\n The final merger I’ll show is was described by Ruth Herold in her 1990 dissertation. She too looks at the merger of lot and thought, this time in Western Pennsylvania. She describes a merger by expansion as followsHerold, Ruth. 1990. Mechanisms of Merger: The Implementation and Distribution of the Low Back Merger in Eastern Pennsylvania. Dissertation, University of Pennsylvania, 1990.\n\n“The lexical constraints on the distribution of two former phonemes are lifted. As a result, the entire phonetic range formally divided between the two phonemes becomes available for the realization of either. (Herold 1990:91–92)\n\nShe provides some really excellent plots, showing a father-son pair where the father has a distinction and the son does not. The crucial part though is that “the phonetic range of the new phoneme is roughly equivalent to the union of the range of the two phonemes that merged” (Labov 1994:322). Here’s my interpretation of this merger.\n\nThis is a very abrupt change, happening quite suddenly, and Herold uses the word explode several times to describe what happens, so I wanted to convey that in my animation. Dan Johnson (2010) finds that even within the same family, younger children may have the merger and older kids do not."
  },
  {
    "objectID": "blog/animating_mergers/index.html#conclusion",
    "href": "blog/animating_mergers/index.html#conclusion",
    "title": "Animating Mergers",
    "section": "Conclusion",
    "text": "Conclusion\nI’ve learned that I’m a visual learner, so when the gganimate package came out, I thought I’d use it to illustrate different mechanisms of merger. These are my interpretations of how they progress, but I feel like these may be useful to other people as well."
  },
  {
    "objectID": "blog/website-version-3/index.html",
    "href": "blog/website-version-3/index.html",
    "title": "Website Version 3",
    "section": "",
    "text": "After exactly seven years with my old website, I’ve decided to change it to what you are seeing now. This page serves as a bit of a summary of what I did and why. It’s not particuarlly well-organized since I added to it here and there over four months. But it might be helpful to you if you’re considering such a move."
  },
  {
    "objectID": "blog/website-version-3/index.html#what-was-wrong-with-the-old-one",
    "href": "blog/website-version-3/index.html#what-was-wrong-with-the-old-one",
    "title": "Website Version 3",
    "section": "What was wrong with the old one?",
    "text": "What was wrong with the old one?\nI built my old website in September 2016. I had a research assistantship at the DigiLab at UGA, and Emily McGinn, the supervisor, suggested I find ways to increase my online presence. I learned some web design and CSS skills skills and eventually made Version 1 of my website. That version was essentially the same as what I built in the tutorial I followed, so a few months later I rewrote everything from scratch and made Version 2. (Let’s be honest though, it’s still obviously heavily based on the tutorial.) Other than very minor tweaks to a few things, that’s how my website has been since then.\nHowever, it got a bit unwieldy. The blog was organized just fine, but I also added pages here and there to go along with workshops and other presentations I gave. It got more confusing when I gave the same workshop a second time and had multiple similar pages floating around. Since I didn’t foresee some of these additions, its growth was reminiscent of unplanned suburban sprawl. For examples, sometimes images were just dumped into a folder, others were better organized. Non-blog pages were hidden and were sometimes a top-level page and other times within a dedicated subdirectory. Each individual addition wasn’t a big deal, but once I stepped back and looked at it all, it was a mess.\nThe format of my tutorials wasn’t consistent either. I have lots of handouts on my website, tucked away here and there. If they were associated with a workshop, they were separate R Markdown files that didn’t fit in with the rest of the site. Some of my earliest ones are PDFs of Word files! If they weren’t associated with a workshop, they’re regular blog posts. But because the site wasn’t connected to R, I had to do a lot of copying and pasting R Markdown code and careful insertion of images to get those tutorials to look right. In some cases, the extra work made it possible to do things like syntax highlighting in Praat and highlighting specific lines of code. But that was all done by manually inserting HTML tags and updating my CSS.\nAlso, as careful as I was about my CSS, it wasn’t perfect. I think there were some issues if like a list had only one element, and there were things with hyperlinks. Some one-off portions of blogs or tutorials sometimes didn’t look right. I had a disclaimer at the bottom of every page, saying something like, “This website is built from scratch. Pardon the flaws; I am not a web designer.” Which was a humble brag if anything. But as the site grew I didn’t want to change the CSS because it might change some blog post from years ago in unexpected ways.\nUltimately, I didn’t mind the mess because it’s what made my site unique. But, what made me finally decide to migrate to Quarto was the underlying architecture. It was built using Jekyll, which involves a programming language called Ruby in some way. After seven years I still have no idea what either of those are. I did this because it’s what the tutorial I followed used. When the site worked, it was great. But sometimes, the Ruby dependencies (called “gems”) would update or break or whatever and I had to google around trying to find a fix. I had no idea what I was doing and it led to a lot of frustrated late nights trying to get my website up and running again.\nThen Quarto comes along, which makes it easy to make a blog entirely within R Studio. I have been very familiar with the R world for a while. In 2017, I was an early adopter of Shiny (at least in linguistics, I think), so I was able to integrate all my html, CSS, and R skills into the Gazetteer of Southern Vowels. In 2020, I also started dabbling with creating my own R Packages and using the amazing pkgdown to make dedicated websites for them (see joeyr, futurevisions, barktools, and joeysvowels). Finally, I have a side project that involves collecting and analyzing data about what hymns are sung in LDS congregations, and in 2023 I decided to build the site entirely in Quarto.\nSo, I’ve gradually built up to web development in R over the years and Quarto seems like the logical place to migrate to. Plus, it has some features that I’ve always wanted, like scrolling table of contents and a search feature. After some encouragement from folks on Twitter, I decided it’s time to bite the bullet and go for it."
  },
  {
    "objectID": "blog/website-version-3/index.html#what-does-it-take-to-migrate",
    "href": "blog/website-version-3/index.html#what-does-it-take-to-migrate",
    "title": "Website Version 3",
    "section": "What does it take to migrate?",
    "text": "What does it take to migrate?\nI’m doing this page by page. Here’s the order I took:\n\nMy homepage and any links on it. I didn’t clean up the linked pages, but at least there weren’t any dead links.\nMy blog posts\n\nI started with some of my earliest ones because they were the simplest.\nThen moved on to any conference or paper announcement. As part of that, I also transferred the associated slides, papers, and other files.\nI then finished any non-tutorial posts.\nI saved the tutorials for last because they would take the most time. Fortunately, it wasn’t too bad changing the static code blocks to ones that are run when the page is rendered. However, because dependencies like packages and dataset have been updated, some of the details and specific results have changed. Nothing major though.\n\nOther pages that are top-level or close to top-level, but not right on my home page. Things like splashscreens to workshops I’ve done. Thse in particular were tricky because many of them had links to other parts of the website, including many redirects. I hope they all work!\nFinally, I updated my Resources and CV pages. Those were mostly checking to make sure links worked (and removing dead links), and making sure all the files in my downloads folder transfered over.\n\nThis took several months to make the full transfer. Little did I know that I had over 100 different webpages (mostly blog posts) all contained within my site. This was a surprise to me because I had not idea I had created so much content. I very slowly made progress by creating a reminder on my phone to migrate one webpage a day. Some were pretty straightforward. Others took more time, like tutorials and other code-based ones.\nIt was kinda fun to read through some of my old posts. In some cases, I added marginalia that basically pointed out how silly I was several years ago. I don’t think many people visit those early blog posts, so no one’s going to see them anyway."
  },
  {
    "objectID": "blog/website-version-3/index.html#appearance",
    "href": "blog/website-version-3/index.html#appearance",
    "title": "Website Version 3",
    "section": "Appearance",
    "text": "Appearance\nThe biggest change is obviously the look of the site. One of the main reasons I didn’t want to switch to Quarto initially is because I didn’t want it to look like all the othe Quarto sites. But, once I started digging into the HTML, I saw how I could modify the CSS to resemble my previous site.\nThe navigation bar at the top was the trickiest, but I’m very pleased with the results. I tried for a long time to get my name and title to across the top, but I could not figure it out. It bothered me a lot because it made the buttons not centered over the body of the site. But, my wife recommended I make it a different color. Basically, just lean into the navbar. And it worked. I really like how this turned out.\n\n\nYou may have noticed too that I removed the “Resources” link and replaced it with “Idiolect.” I haven’t been updating my “Resources” page for a few years, so it was getting a little stale, and honestly a bit unwieldy. It still exists, but it’s now joined the several other pages on my site that you kinda have to know about to get to because there are no obvious links to it. (Still trying to find a place for those links.) Plus, I like my idiolect page.\nI’ve also moved all my socials to the top right corner. Since Quarto has a spot for those, I figured I might as well use that, rather than having them under my picture. That way they’re accessible from every webpage within the site.\nI also updated my profile picture since my old one was from 2019.\nFinally, I changed the hyperlink style back to something more standard. I used to have hyperlinks displayed in the same color as regular text, and then have a little red circle after it. I saw it on Butterick’s Practical Typography and liked it. I don’t know how I feel about it anymore. It’s a simple change with CSS, and I even blogged about it when I first did it. But, I guess my preferences have changed.\nWhat I could not figure out was a link and previous on my homepage to my latest blog post. Not a big deal I suppose."
  },
  {
    "objectID": "blog/website-version-3/index.html#changes",
    "href": "blog/website-version-3/index.html#changes",
    "title": "Website Version 3",
    "section": "Changes",
    "text": "Changes\nI’ve tried to keep as much of the original structure of the site the same as I could. However, as I migrated, I realized that I could make things overall a bit more contained by changing the structure of associated files. However, some things did change, but I tried my best to make sure any external links to the site did not break. Here’s a list of the changes I’ve made.\nThe main change is that each blog is now in its own self-contained folder. The previous structure had all posts in a single folder and all images in another folder. This time, the images associated with a blog post are contained within that folder. So, instead of this:By the way, I’m stealing this way of visualizing file structure directly from TJ Mahr’s Migrating-to-Quarto page..\n├──📁blog\n|  ├──📄blog post 1.md\n|  ├──📄blog post 2.md\n├──📁images\n|  ├──🌅image1.png\n|  ├──🌅image2.png\nIt’s now this:\n├──📁blog\n|  ├──📁blog post 1\n|  |  ├──  📄index.qmd\n|  |  ├──  🌅image1.png\n|  ├──📁blog post 2\n|  |  ├──  📄index.qmd\n|  |  ├──  🌅image2.png\nIt shouldn’t affect any urls to existing blog posts because the url blog/blog post 1 in the old format would go to the blog post 1.md file and in the new one it’ll go to the blog post 1 directory, which’ll display index.qmd by default. I was concerned about changing the url because I know some people have cited my turorials in published work and I didn’t want those urls to break. I think this’ll work and it’ll keep the site better organized."
  },
  {
    "objectID": "blog/website-version-3/index.html#the-process",
    "href": "blog/website-version-3/index.html#the-process",
    "title": "Website Version 3",
    "section": "The Process",
    "text": "The Process\nIt took from September 10 to November 27 to transfer all my entire website. I set a reminder to do at least one page every day, and since there are over 100 webpages on this website, it took a while.\nAs far as specific things I did to transfer each page over, here’s what I did\n\nI created the file structure shown above and moved the file over to its new folder. I then renamed it from “2023-12-02-title-of-blog-post.md” to “index.qmd”, making sure to do the file extension too.\nWithin each post, I needed to update the header. I change from “tags” to “categories”, from “redirect_from” to “aliases.” I add a date-modified if needed.\nI then had to go through and make sure the whole post looked good. For the most part, there were very few changes I needed. The only things that needed manual work was when I added HTML code directly into the markdown file to get some specific CSS. So like, side notes, or custom “buttons” for downloading files had to be updated.\nI haven’t figured out if I can do redirects, which is a bummer because I used those a lot.\nAs I went through, I added sidenotes to clarify, update, or just comment on what I said a few years ago.\n\nAs mentioned above, for some of my code-based blog posts, like tutorials, I ended up rerunning the code. This resulted in small changes here and there, but nothing that should hurt anyone.\nThere are a few changes I’d like to do still, like rename blog post folders to something shorter and more consistent, which would mean I’ll have to carefully add aliases to the old names and make sure they all work."
  },
  {
    "objectID": "blog/website-version-3/index.html#conclusion",
    "href": "blog/website-version-3/index.html#conclusion",
    "title": "Website Version 3",
    "section": "Conclusion",
    "text": "Conclusion\nI really like the new look of the new site. I have a whole laundry list of things I’d like to do still, like cleaning up old files and whatnot. I also want to add retroactive blog posts about every conference I went to and every paper I presented, so I can always have a home for each one. I also have over a dozen ideas for future blog posts."
  },
  {
    "objectID": "blog/ads-and-lsa-2020/index.html",
    "href": "blog/ads-and-lsa-2020/index.html",
    "title": "LSA and ADS 2020",
    "section": "",
    "text": "For the first time in a few years, I did not attend the ADS/LSA annual meetings. It would have been nice to hang out in New Orleans this weekend. However, my research will still be represented: my colleagues presented some research focusing on Southern American English vowels that I’ve been a part of this year."
  },
  {
    "objectID": "blog/ads-and-lsa-2020/index.html#fridays-ads-poster-on-southern-american-english-vowels",
    "href": "blog/ads-and-lsa-2020/index.html#fridays-ads-poster-on-southern-american-english-vowels",
    "title": "LSA and ADS 2020",
    "section": "Friday’s ADS poster on Southern American English vowels",
    "text": "Friday’s ADS poster on Southern American English vowels\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nFriday morning at the American Dialect Society poster session, Bill Kretzschmar presented some research on behalf of the entire Linguistic Atlas research team at UGA (Peggy Renwick, Katie Kuiper, Lisa Lipani, Mike Olsen, Rachel Olsen, and me). In it he presents findings from our now-complete dataset from the Digital Archive of Southern Speech. With a dataset this size, we can make comparisons to other varieties of American English regarding the distribution of vowels in the F1-F2 space."
  },
  {
    "objectID": "blog/ads-and-lsa-2020/index.html#sundays-lsa-presentation-on-formant-trajectories",
    "href": "blog/ads-and-lsa-2020/index.html#sundays-lsa-presentation-on-formant-trajectories",
    "title": "LSA and ADS 2020",
    "section": "Sunday’s LSA presentation on formant trajectories",
    "text": "Sunday’s LSA presentation on formant trajectories\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nSunday, Peggy Renwick presented our research on formant dynamics of back vowels in Southern American English vowels. Focusing on just the White Americans in the Digital Archive of Southern Speech (DASS), we wanted to see how formant trajectory shapes and their relative positions vary across male and female speakers in different generations. Many of our findings can be summarized in this animation:\n\ngoose doesn’t change much, especially for the women, suggesting goose fronting is an old shift and was nearing completion by the time our speakers acquired language. Meanwhile goat lowers between the Lost and GI generations of women, and then a generation later for the men. foot doesn’t change much, suggesting it doesn’t pattern with goose-fronting. The low vowels lot and thought are near each other, but not merged because of their different positions in the F1-F2 space and their different formant shapes.\nSomething that stands out to me is that each vowel’s trajectory didn’t change all that much over time. This wasn’t a product of how our GAMM was specified: each combination of vowel, sex, and generation was interacted and therefore, fit independently of the other. So the fact that the trajectories show remarkable stability is really interesting to me. So, at least among these DASS speakers, Southern American English vowels appear to shift the nucleus and glide in tandem."
  },
  {
    "objectID": "blog/thank-you/index.html",
    "href": "blog/thank-you/index.html",
    "title": "Thank You",
    "section": "",
    "text": "Some of you may have noticed that at the bottom of a lot of the pages on my website, I’ve got this button:\nI’ve created an account on ko-fi.com, which is a platform that, in their words, provides “a friendly way for fans to support your work for the price of a coffee.” To put it more bluntly, I’ve created a way for people to give me money for my tutorials and stuff. At the risk of sounding arrogant, I’ll brag for just a little bit before geeking out about the new books I’ve gotten because of that little button."
  },
  {
    "objectID": "blog/thank-you/index.html#let-me-brag-for-just-a-sec",
    "href": "blog/thank-you/index.html#let-me-brag-for-just-a-sec",
    "title": "Thank You",
    "section": "Let me brag for just a sec",
    "text": "Let me brag for just a sec\nSo I’ve put together half a dozen tutorials and another dozen or so workshops. The most popular ones are the tutorials on how to make vowel plots, how do formant extraction, measuring vowel overlap, and my series of R workshops. You can find them all on my Resources page.\nThese were skills that aren’t typically taught in a linguistics course, but they’re expected of sociophoneticians, and I knew I had to learn how to do them. So, I hunkered down, did lots of googling, and figured out this stuff on my own by frankensteining what I could find from lots of other pages online. After a while, word spread among my peers that I had acquired some skills, and I ended up helping lots of people do these things on their own data.\nEventually, I realized that there was a niche to be filled: my peers could benefit from a clear tutorial on some basic scripting techniques in sociophonetics. So, I started to write the tutorials that I wish I had had when I was learning this stuff. When I finish them, I share them on Twitter, and they’ve consistently remained among my most visited pages on my website. People from about ten different countries on four continents have contacted me for various reasons, explaining how useful they’ve found the tutorials. And there are likely many more anonymous folks too.\nSince then, lots of other people have produced excellent tutorials that cover some of the same topics mine have. And that’s awesome! I’m just glad that people have so many resources available to them to learn these skills."
  },
  {
    "objectID": "blog/thank-you/index.html#so-whats-this-ko-fi-thing",
    "href": "blog/thank-you/index.html#so-whats-this-ko-fi-thing",
    "title": "Thank You",
    "section": "So what’s this Ko-fi thing?",
    "text": "So what’s this Ko-fi thing?\nSo someone (I forget who) offhandedly mentioned that I should charge people for my tutorials. There was no way I was going to do that. I got this information for free, so I give it for free.\nBut, I figured if people wanted to donate money, well, who was I to stop them? So I looked around and settled on Ko-fi as a way to give people that opportunity if they wanted to take it. It seems simple enough: the button above will take you to my Ko-fi page, and from there you can donate $3, or about the price of a coffee.  The money gets sent to my PayPal account and I can deposit into my bank account.Ironically, I don’t drink coffee, so you can think of it as a fancy glass bottle of Coca-Cola or something.\nSo I set it up, put and put it at the bottom of my tutorials, and didn’t think anything of it because I didn’t expect much to happen. But slowly, here and there—and much to my surprise—the donations came in. It’s not bumping me up to the next tax bracket or anything, but it does make me feel that the work I put into the tutorials is appreciated.\nUpdate: Now that I’m faculty, I have a real salary, and I feel back taking money from people, especially grad students. But, I’ve been told that sometimes people appreciate having a way to express their appreciation."
  },
  {
    "objectID": "blog/thank-you/index.html#what-am-i-doing-with-the-money",
    "href": "blog/thank-you/index.html#what-am-i-doing-with-the-money",
    "title": "Thank You",
    "section": "What am I doing with the money?",
    "text": "What am I doing with the money?\nInstead of just dumping it into my bank account, I figure I should do something special with the money. I decided to get a couple books. In particular, I wanted to get some books on data visualization, since that’s relevant to the tutorials themselves.\nMy first goal was Data Visualization by Kieren Healy. I had been following the book as it was approaching publication and was excited to see it’s positive reception. Yes, it’s pretty much all available online, but I wanted the physical copy for two reasons. For one, it’s a beautiful book and a paragon of excellence in typography. But also, I should return the favor of financially supporting someone who produces tutorials online. Thanks to one large donation that bumped me well past my goal, I’m now a proud owner of Healy’s book!\n\nSo I tweeted about it and did a small humble brag. And again, much to my surprise, some more donations came in! So in the same week, I met my second goal of purchasing Fundamentals of Data Visualization by Claus O. Wilke.\n I’m now realizing the front phone on my camera is quite fuzzy.\nWith these two books in hand, I hope my knowledge of data visualization will increase and that this percolates into future tutorials and workshops.\nSo I didn’t think this whole thing would work. Apparently it has. I’ve now started to compile a list of additional books I’d like to get, including Edward Tufte’s quartet of data visualization books, some linguistics books I’ve been meaning to get, and a couple statistics manuals."
  },
  {
    "objectID": "blog/thank-you/index.html#thank-you",
    "href": "blog/thank-you/index.html#thank-you",
    "title": "Thank You",
    "section": "Thank You",
    "text": "Thank You\nThe reason for all this is to say thank you. I’m so thrilled to hear that people seem to find some utility in my tutorials, and I’m humbled that some folks went out of their way to send me a little extra pocket money as a token of their appreciation. I’m honored to be helping the linguistics community in a small way. Thank you."
  },
  {
    "objectID": "blog/website-version-2/index.html",
    "href": "blog/website-version-2/index.html",
    "title": "Website Version 2",
    "section": "",
    "text": "Today I finally rolled out a new version of my website! The previous version was great and was an excellent stepping stone into web design, but it was mostly borrowed code. Unsatisfied with some of the way it was designed, I decided to go ahead and just write a new site completely from scratch. It has taken about a month or so to get it going, but I think it’s a lot better than before."
  },
  {
    "objectID": "blog/website-version-2/index.html#new-site",
    "href": "blog/website-version-2/index.html#new-site",
    "title": "Website Version 2",
    "section": "New Site?",
    "text": "New Site?\nAs I’ve mentioned previously, I went through a course on Lynda.com, called “Jekyll for Web Designers” that showed me the basics in how to get that website up. I was still in unfamiliar territory though as I navigated my way around CSS and HTML and any changes I wanted to make to the site were difficult to do. I liked the instructor for that course (James Willliamson) though, so when I looked up some of his other ones and saw that he did several others that would be relevant to me, I decided to go ahead an take them as well.\nI first took his “CSS Core Concepts” where I learned all about CSS and how it works. At a whopping 9 hours long, I knew I was going to get a thorough treatment of CSS. Essentially I learned how to make things look the way I want on a webpage. I learned how to do anything I want to text like change the font, size, and color as well as add things like bold, italic, and small caps. I also learned how to add space around text or between elements on a webpage.\n\n\nI learned about #Web on https://t.co/n13drggo1T. I completed CSS: Core Concepts by @jameswillweb https://t.co/at3VlMgyPD via @lynda\n\n— Joey Stanley (@joey_stan) February 5, 2017\n\n\nWhen I finished that, I continued on to the next course called “CSS Page Layouts” which I’m pretty close to finishing. This is more about web design and how to get from a sketch book drawing of a webpage to the screen. Most of the time was in CSS still (as opposed to the HTML) but I learned how to position things on the webpage.\nThe layout of site I had before was simply copied from the last tutorial in the first course. Since I didn’t write the HTML or the CSS, I didn’t know exactly what was going on, so if I wanted to make changes it was really hard to do. Now that I’ve written this new site from scratch, I know exactly what’s going on in all the webpages and in CSS.\nOf course, the sacrifice is that this new site isn’t quite as clean as the old one was. For example, I know it looks good on my Apple laptop in Safari, but I don’t know all the code I need to watch out for to make it fully compatible with other browsers, let alone other versions of other browsers. The site also doesn’t adapt to smaller screens like phones and tablets. I’m still working on that."
  },
  {
    "objectID": "blog/website-version-2/index.html#so-whats-the-difference",
    "href": "blog/website-version-2/index.html#so-whats-the-difference",
    "title": "Website Version 2",
    "section": "So what’s the difference?",
    "text": "So what’s the difference?\nI didn’t do any major changes to the overall structure or the general typographic details that I use in everything I do. The fonts are still Iowan Old Style and Avenir and the background is still “whitesmoke” (96% white, 4% black). The blue from before was, coincidentally, very similar to the blue I use in my power point slides, but not exactly, so I went ahead and changed it to my blue to match my slides.\nJust to give you an idea of what the site looked like before, here are some screenshots. This first one is my home page from my old site:\n\n\n\nOld site’s homepage\n\n\nAnd this is my new home page:\n\n\n\nNew site’s homepage\n\n\nA couple layout changes. First, I’ve widened it from something like 70% of the screen to a standard 960 pixels. That gave me the option of keeping my ideal width at roughly 66 characters still while giving me room for a sidebar.\nI changed the header as well. I felt like it was the weakest part of my page, and I didn’t like how it looked. I knew I wanted my social media links to be somewhere prominent, and of course the navigation links should be there too. But I wanted to include temporary links to recent presentations so people can download the slideshow if they visit my site, and those didn’t quite fit up there. And I didn’t have anywhere to put a photo.\nOne solution was to have the top just include my name and the navigation buttons, which are now more descriptive. A sidebar could then have my photo, social media, and temporary links to recent presentations. There was also room to put an excerpt of my most recent blog post too, which I figured out how to do dynamically, which is pretty cool."
  },
  {
    "objectID": "blog/website-version-2/index.html#a-new-research-tab",
    "href": "blog/website-version-2/index.html#a-new-research-tab",
    "title": "Website Version 2",
    "section": "A new Research tab!",
    "text": "A new Research tab!\nI also added a Research tab. I’ve seen this on lots of other people’s pages and I thought I’d include one in my own. I guess I figured everyone would clearly see what my research was by skimming through my CV, but putting it in prose like that makes a lot more sense.\nThe layout was a blatant copy from the UGA DigiLab projects tab. I think adding images, even if they’re simple like mine, contribute a lot to the page.\nEventually, I’d like to create a separate page for each of these. There I’d go into more detail on my findings, put a list of publications, and have links to any relevant blog posts. I’ll get there eventually. For now, I’ll just make the the excerpts a little longer."
  },
  {
    "objectID": "blog/website-version-2/index.html#blogi-mean-news",
    "href": "blog/website-version-2/index.html#blogi-mean-news",
    "title": "Website Version 2",
    "section": "Blog—I mean, “News”",
    "text": "Blog—I mean, “News”\nMy blog is now relabeled “News” just because it sounds less, well, blog-like. Here’s the old one:\n\n\n\nOld site’s blog\n\n\nAnd here’s the new one:\n\n\n\nNew site’s blog\n\n\nI also redesigned the blog page itself. First off, I didn’t like that you could only see 4 at a time and that you’d have to scroll back to see older entries. I don’t like it that way. I’ve still got the code still there, but it’ll only create a second page once I’ve got 50 posts. I’ve got like 15 for now so I don’t think it’ll slow anyone down by loading this page.\nThe rest of the page was completely revamped. To be quite honest, I googled around for great examples of blog layouts and I found this one, which I copied quite a bit of. I think it works well for my site.\nAnother big thing I didn’t like about the old layout was that the “Archive” tab didn’t make sense as its own page and the label didn’t really make sense.\n\n\n\nOld site’s “archive” page\n\n\nInstead, I moved it to a separate sidebar on the blog. That way it’s clear that the tags and the blog work together.\n\n\n\nNew site’s “tags” page\n\n\nIf I could change one thing on the main “news” page, it would be that sidebar though. Right now, if you click on a tag, it’ll take you to a page where it lists all the blog posts by category. I don’t like that separate page. I wanted to do something fancy like you click on a tab and a list of posts will expand down. Turns out I couldn’t do that without Java scripting and I don’t know how to do any of that. I also looked into just having pages for each category created on the spot, with blog excerpts instead of just the titles for that one category, but I couldn’t figure out how to do that."
  },
  {
    "objectID": "blog/website-version-2/index.html#anything-else",
    "href": "blog/website-version-2/index.html#anything-else",
    "title": "Website Version 2",
    "section": "Anything else?",
    "text": "Anything else?\nNope. That’s about it. My CV page has remained essentially the same. I added “Resources” and “Teaching” tabs, but those are blank for now. I’ll add content to them eventually. There are still a few layout things I need to work on, but I thought I’d launch the site anyway—imperfect as it is for now—because it’s a major improvement over the last one."
  },
  {
    "objectID": "blog/how-i-implemented-the-links-in-this-site/index.html",
    "href": "blog/how-i-implemented-the-links-in-this-site/index.html",
    "title": "How I Implemented the Links in this Site",
    "section": "",
    "text": "A while ago I stumbled across Butterick’s Practical Typography. I’ve never been that into typography, but I do make sure my documents look good. I’ve yet to implement good typography into this website, but I did find a neat trick that I was able to pull off.\nHyperlinks traditionally are in blue and underlined. It’s been the standard for so long now that we’ve gotten used to it and don’t notice how bad it looks. In Butterick’s book though he has linked text the same color and style as the rest of the page, but there’s a little red circle after it, as if it was a link to a footnote or something. It’s a pretty clever way to soften the look, but still get the point across that that particular text is a link.\nI like to write stuff with links everywhere. I figure it might save people a google search if I just do it for them. On my CV, I link readers to the PDF and other documents, my co-authors’ pages, and other pages that might be useful. The result though makes it look like Wikipedia, unless I can find a way to soften how links look.\nThanks to some CSS skills I’ve been learning from Lynda.com, I’ve found out that I can add those little red circles by adding this to my .css file:\na[href*=\"http\"] {\n    color: black;\n}\na[href*=\"http\"]:after {\n    content: 'º';\n    color: darkred;\n}\nWhat the first block does is it targets just the &lt;a&gt; tags that contain an href, meaning they’re an external link, and it changes the color to black, overwriting the default blue. From there, the :after selector in the next block puts specific text, the little circle, after the hyperlinked text in the tag every time. I may need to change some things later to make sure it does exactly what I want on all the pages, but it works for now.\nI don’t know if I’ll keep it the way it is, but it does look a lot better than tons of blue underlined text."
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "",
    "text": "Between April and July, I distributed a survey to people in Idaho, Montana, Wyoming, and Utah, asking them to record themselves reading a bunch of words and answering some open-ended questions about language. The results from this study will be published in more academic venues, but for now I wanted to explain in non-academic terms what the basic, preliminary results are from that study.\nIf you haven’t taken the survey yet and you want to, you can do so by clicking here."
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#overview-of-the-study",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#overview-of-the-study",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "Overview of the study",
    "text": "Overview of the study\nI set up a study to analyze pronunciation in Idaho, Montana, Wyoming, and Utah. Of the four states, Utah has been reseached the most by linguists. There have been very occasional studies on word choice and grammar in the other states over the last century, but the number of studies on actual pronunciation can be counted on one hand: two in Idaho, two in Montana, and zero in Wyoming. So, this study aims to give a basic first look at what’s going on.\nI decided to have people record audio from their own devices. Yes, this is going to make some things inconsistent across recordings because people will use different microphones. Again, this is a first pass at what’s going on. Besides, it’s a lot easier (and more covid-friendly) to collect data remotely than it is to travel to people in-person.\nWhat do I look for? I decided to focus my study around things known to be interesting in Utah. I know it’s not the best method since there’s no reason to expect Utah English pronunciation to be found in other states. But, I’m basically going in blind, and I had to start somewhere. I’ll list those linguistic variables below.\nAs a way to collect additional data, I decided to have people answer some open-ended questions as well. For example, I asked people whether they think their state has an accent. I could have asked them type the response, but I think the responses were more detailed and genuine when spoken rather than typed out. The side benefit of this is I get a chance to hear how people talk when they’re not reading a rigid list of words.\nTo distribute the survey, I went to Reddit. I found every subreddit dedicated to a city, region, or university based in the four states. For example, I posted in bigger ones like r/Wyoming, but also smaller ones like r/Butte, r/NorthIdaho, and r/UVU. I posted in something like 60 subreddits over the course of a couple months. 324 people completed the survey.\nFrom there, I analyzed the data using pretty typical techniques for current sociolinguistic work. For some things (like consonants), I just listened and transcribed what I heard.  For other things (like vowels), I’m in the processs of extracting acoustic measurements from the audio and some statistics and stuff to crunch the numbers.Down the road, I’d like to do a more sophisticated analysis of the consonants, but I’m not sure if the recording quality is good enough in this audio.\nHopefully that gives you an decent big-picture view of what’s going on with this study."
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#linguistic-variables",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#linguistic-variables",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "Linguistic Variables",
    "text": "Linguistic Variables\nIn the survey, I asked people to read aloud a list of 200 words. Each of those words was specifically targetting one more of the following linguistic variables. I’ll briefly mention them here as an overview, but I’ll give more detail in the sections below.\nThings having to do with consonants:\n\nThe word mountain and similar words (Latin, gluten, rotten, etc.). How is that t pronounced and how is the second syllable pronounced?\nInserting a t sound in words like else, Chelsea, Nelson, and also.\nHow are ng’s pronounced at the ends of words? There’s “g-dropping”, but there are others who add an extra sound at the end.\nHow do people pronounce thr as in three, through, or throw?\n\nUnfortunately, while I did collect a lot of data on vowels, I just haven’t had the time to analyze them yet."
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#what-results-do-i-have-so-far",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#what-results-do-i-have-so-far",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "What results do I have so far?",
    "text": "What results do I have so far?\nI collected far more data than I ever could have hoped for! That’s good news for me and my research agenda. The bad news is it’s going to take some time to get all the results. Here are some of the things I’ve looked at (briefly) so far.\n\nWhat are the ways that people pronounce the linguistic variables listed above?\nWhat differences are there across the four states?\n\nOf course there are many, many other questions I can of this data that I just haven’t gotten to yet. Here are a few:\n\nHow are the pronunciation differences listed above different across genders, ethnicities, ages, or other demographic info?\nAre there regional differences within states? Many participants said that they think so, and I think they’re probably right. But I don’t have quite enough data to say for sure, and I haven’t done that analysis yet.\nDo people from urban areas sound different from people in rural areas? Again, lots of people said there is. I just haven’t done the actual linguistic analysis to support this. I asked participants whether they wanted to live in an urban or rual place. I think that aspiration may have a bigger effect than the actual place you grew up. We’ll see.\nDo Mormons have an accent? Is that distinct from non-practicing Mormons, ex-Mormons, and non-members? I don’t know. I asked about it and I have data on people’s affiliation with the Church of Jesus Christ of Latter-day Saints. I suspect that I’ll find very slight differences, but we’ll see.\nIf people said they had some connection to the Church, I asked some additional questions about “Relief Society Voice,” “Missionary Voice,” and “General Authorty Voice.” I got some great comments. So the question is, are those real? I won’t be able to answer this question directly with this data, but I certainly have a lot of comments to draw from when designing my next study!\n\nSo, I apologize if the question you want me to answer hasn’t been addressed. Hopefully I can get to some of these soon!\nWithout further ado, let’s get into the results!"
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#mountain",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#mountain",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "Mountain",
    "text": "Mountain\nWhen asked whether their own state has an accent, by far the most common thing people mentioned was how people say the word mountain and similar words (fountain, gluten, satin, etc.). Over half the Utahns mentioned this, and a handful of people in other states did too.\nSo how do people say the word mountain?\nI’m going to color code each variant to visually help you keep track of the variants I discuss here.\n\nVariant 1: moun’n\nDespite what most people think, the most common way of saying this word (and words like it) across North American English is like this:I’ll refer to people using fake names that I’ve randomly assigned. I did not collect real names from anyone.\n\n\n\n threaten, fountain, mitten, gluten, kitten, satin, button, cotton, Latin  (Alexia, White, female, 2002, Kalispell, MT)\n\n\nThis has what’s called a “glottal stop”, which is that sort of catch in the back of your throat, instead of a typical t sound that you might get in a word like top. After that glottal stop, people go straight into the n sound. So it’s “moun’n”. There is no vowel sound in that second syllable. For the Utahns reading this, I know it sounds crazy to hear that that this is the most common way of saying this word, but it is standard American English pronunciation. You’ll hear it in the news, on the radio, in movies, and in songs.\n\n\nVariant 2: moun’in\nFor those with good ears though, you might have picked up on a slightly different way of saying mountain.\n\n\n\n Scranton, mountain, button, titan, sentence, threaten, kitten, Latin, certain, gluten, cotton, potent, Clinton, satin, fountain, mitten  (Makenzie, White, female, 1999, Tooele, UT)\n\n\nWhat we have here is that same glottal stop that we had in the standard variant (moun’n). But this time, instead of going straight to the n sound, there’s still a vowel there. If we wanted to spell this out, it might be something like “moun’in”.\nAs it turns out, this variant is somewhat common in Utah. Several studies have anaylzed it and have found that it occurs roughly 15% of the time. It’s not unique to Utah, but it is more common there than in most other areas.In Encanto, Luisa says this variant in the song “Under the Surface” in the line “I move mountains, I move churches…”. I don’t think I’ve ever heard it in the media anywhere else.\n\n\nTangent: Commentary on moun’n and moun’in\nThe interesting thing is that even though this pronunciation of moun’in is not particularly common, it’s very much on the mind of Utahns. Ask any random Utahn about Utah English and you’ve got decent odds that they’ll say something about mountain. As a grad student, I traveled to Utah to collect some audio and people said, “Oh, so you want me to say mountain, right?”\nIn the comments people left on the survey, particularly when people talked about whether their state has an accent, it’s clear that this pronunciation is stigmatized. People make fun of it and think it’s bad in some way. However, what’s interesting to me is that the stigma is associated with not just this moun’in variant, but it’s also associated with the General American standard, moun’n. In fact, most of the participants for this study, when discussing mountain, mentioned moun’n instead of moun’in. For example, “Catherine” here apparently feels like moun’n is a Utah thing and has consciously avoided using it.\n\n\n\n I know I’ve corrected some of my Utah accent since I’ve been gone for about twenty years. Uh, I no longer say moun’n.  (Catherine, White, female, 1978, Farmington, UT)\n\n\nSo, what seems to be happening is that the stigma associated with the glottal stop in moun’in has spread to the glottal stop in moun’n—even though that’s how most people say it. I don’t know if I’ve ever seen another case where the mainstream, General American pronunciation is stigmatized. For a linguistic nerd like me, this is super fascinating.\n\n\nVariant 3: mountain\nSo, for people like Catherine who don’t like the glottal stop, what do they do? They say the t in moutain very clearly. I call this the hyperarticulated variant, mountain. Here’s how Catherine said mountain and similar words in the wordlist:\n\n\n\n Scranton, mountain, button, titan, sentence, threaten, kitten, Latin, certain, gluten, cotton, potent, Clinton, satin, fountain, mitten  (Catherine, White, female, 1978, Farmington, UT)\n\n\nBased on people’s comments, it seems like many Utahns are under the impression that this is how most North Americans say mountain most of the time. I don’t know how else to say this… but that’s simply not accurate! Yes, it is true that most people say mountain with a nice strong t sometimes, particularly when trying to speak clearly or if they’re in a formal situation. But, most of the time, most North Americans say mountain as moun’n. Here’s a chart show the distribution of variants across the four states I collected data from:\n\nThe data shows that all three variants are used in all four states. However, you’ll notice that Idaho, Montana, and Wyoming have similar-looking patterns. Pay attention in particular to the height of the purple bars, representing moun’n. People from those three states typically pronounced mountain and similar words in the standard American English way, with a glottal stop and no vowel sound in the second syllable. You’ll also hear moun’in in those three states, though not very often. Research has shown that this variant is not limited to Utah and can be found across the United States, so this is to be expected.\nWhat strikes me though is that the hyperarticulated variant, mountain, is the most common in Utah. Visually, it’s the tallest bar in the Utah panel of the plot above. It seems like the stigma associated with the glottal stop in the other two variants has gotten to people. In Idaho, Montana, and Wyoming, you don’t get this variant nearly as much; it seems like saying mountain with a nice clear t is a Utah thing. So, in a bit of irony, in an effort to avoid sounding like a Utahn (by saying moun’in), Utahns actually sound like Utahns (by overusing the hyperarticulated mountain). Isn’t that wild?\n\n\nMountain in Idaho\nFocusing on Idaho for a moment, I did hear several people comment on mountain when they were asked whether Idaho has an accent. Perhaps this stigma has made its way from Utah northward into Idaho. To test this, I split the Idahoans into three groups: southeast (Pocatello, Blackfoot, Idaho Falls, Rigby, Rexburg, etc.), southwest (Boise metro area, Twin Falls, etc.), and north (Coeur d’Alene, Lewiston, Moscow, etc.). Here’s a plot showing the distribution of variants by region, compared to Utah:\n\nAs it turns out, the closer you get to Utah, the more hyperarticulated mountain you get. In fact, the plot just looks more and more like Utah’s plot. In other words, Utahns say mountain the most, then southeast Idahoans, then southwest Idahoans, and then northern Idahoans. So are there regional accents in Idaho? Here’s one small piece of evidence that there are!\nSo, next time you hear someone talking about mountain in Utah, you can tell them that most people say moun’n and it’s only in Utah (and areas close to Utah) that you get people saying that distinct t in mountain outside of formal contexts."
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#t-insertion",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#t-insertion",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "t-insertion",
    "text": "t-insertion\nSomething that I think has largely flown under the radar is the fact that, in some parts of the Mountain West, people put a t sound between l and s sounds, as in faltse, eltse, saltsa, Cheltsea, Oltsen, and Neltson. We’ll call this t-insertion. I have a collection of tapes from rural Utahns born in the 1910s and 1920s and it seems to be quite common among those people. Is it as common today?\nAs it turns out, no. Of the 2,476 words I’ve listened to so far, just 51 of them (2.1%) had t-insertion. Another 76 words (3.1%) sounded like there could have been a t but I wasn’t sure  so I decided to classify them as “maybe.” Still, that means that about 95% of the data didn’t have any indication of t-insertion at all.I’ll have to do a more detailed phonetic analysis later.\nThat’s not to say it didn’t happen at all. In fact, a couple people did it quite a bit. Here are a few examples (words with asterisks indicate t insertion):\n\n\n\n Olsen, pul(t)se, Wilson, Nel(t)son, compul(t)sive, holster  (Michael, White, male, 1961, Coleville, UT)\n\n\n\n\n\n Olsen, fal(t)se, pul(t)se, salsa, Wil(t)son, Nel(t)son, compul(t)sive, Chel(t)sea, holster  (Lorenzo, White, male, 1991, Idaho Falls, ID)\n\n\n\n\n\n Olsen, fal(t)se, pul(t)se, salsa, Wil(t)son, Nelson, compulsive, Chelsea, holster  (Pamela, White, female, 1971, Salt Lake City, UT)\n\n\nThis t-insertion appears to be the most common (or perhaps the least rare) in Utah, showing up in about 10% of the words there. In the other states, it’s marginal at best."
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#ing",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#ing",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "-ing",
    "text": "-ing\nThe most common way of pronouncing ing at the ends of words (as in bowling, talking, and sitting) is with that sort of ng sound said towards the back of the mouth.  It the result of an n and a g sound combining into one sound. Of 1,949 words I’ve listened to so far, I heard this way of pronouncing ing in 58.3% of them. Here’s a clip of one of the many people who used this variant 100% of the time.Linguists call this sound a “velar nasal” and use the symbol [ŋ] to represent it.\n\n\n\n polling, scaling, spelling, bowling, drilling, mulling, feeling, stealing, pulling, fooling, willing, sailing, chilling, kneeling, smelling, pooling, lulling, spooling, ceiling, wailing  (Neil, White, male, 1978, Helena, MT)\n\n\nMost people are aware of another way that ing is pronounced. People sometimes call it “g-dropping” The term “g-dropping” is a bit of a misnomer because you’re not actually dropping any sounds. You’re just changing the single sound that happens to be spelled with two letters, ng, into a different sound that happens to be spelled with one letter, n.]{.aside} and spell it with an apostrophe: huntin’, fishin’, swimmin’. As you might expect, this variant was used some of the time—I heard this one 123 times (6.2%) so far. That may seem a bit low, but keep in mind that people tend to speak pretty carefully in this kind of wordlist task and this variant is often perceived as being informal sounding. It was mostly heard in folks from Idaho and Wyoming and hardly ever used by the Utahns or Montanans I’ve listened to so far. Here are clips of two people who used this variant almost 100% of the time:\n\n\n\n polling, scaling, spelling, shelling, bowling, drilling, mulling, feeling, stealing, pulling, fooling, willing, sailing, chilling, kneeling, smelling, pooling, lulling, spooling, ceiling, wailing  (Lillian, White, female, 2003, St. Anthony, ID)\n\n\n\n\n\n polling, spelling, shelling, bowling, drilling, mulling, feeling, stealing, pulling, fooling, willing, sailing, chilling, kneeling, smelling, pooling, lulling, spooling, ceiling, wailing  (Brayden, White, male, 2002, Thermopolis, WY)\n\n\nYou may think that the standard ng and the g-dropped version are the only ways that ing can be pronounced. But you may have also noticed that I have yet to account for about a third of the data still. It may surprise you then that I heard at least five other pronunciations of ing. Intrigued? Read on.\n\nAdding a g or a k\nA handful of people pronounced ing with either a g or a k sound at the end. When people mock this accent, they sometimes spell it as talkingk. What’s going on is that the combined ng sound isn’t fully combined, and the g is still pronounced very lightly. This was actually the secondmost common variant and I heard it 284 (14.3%) times. Of the 201 people I’ve listened to so far, 93 of them (46.3%) used it at least once, so it’s not like it’s only said by a few people. It was heard about the same amount in each state, except for Idaho which had it slightly less.\nHere’s Matthew, who uses this pronunciation most of the time. You may notice a few words though where he uses a different variant. (Asterisks indicate words with the pronunciation we’re focusing on.)\n\n\n\n polling, scaling, spelling, shelling, bowling, drilling, mulling, feeling, stealing, pulling, fooling, willing, sailing, chilling, kneeling, smelling, pooling, lulling, spooling, ceiling, wailing*  (Matthew, White, male, 1996, Idaho Falls, ID)\n\n\nAnd here’s Parker who also uses this variant most of the time. His g is is a little bit lighter than Matthew’s.\n\n\n\n polling, scaling, spelling, shelling, bowling, drilling, mulling, feeling, stealing, pulling, fooling, willing, sailing, chilling, kneeling, smelling, pooling, lulling, spooling, ceiling, wailing  (Parker, White, male, 1999, Cheyenne, WY)\n\n\nAs you can see, overall, the g sound is pretty light.\nOther people pronounce that extra consonant at the end more strongly so it sounds like a full-on k. Overall, I heard this one 137 times (6.9%). But, if we split it up by state, we see that it’s definitely the most common in Utah: Utahns said it 12.8% of the time, Idahons 4.8%, Wyomingites 4.8%, and Montanans 2.2%. So it’s actually used more in Utah than in the other three states combined. Overall, 46 different people used this variant, or 22.9% of the people I’ve listened to so far. The difference between the g and the k is admittedly slight but the main difference is whether there’s a very slight h sound after the consonant (kʰ compared to g). Here’s Payton, for example, who uses a mixture of k, g, and other variants.\n\n\n\n tongue, sang, clothing, ceiling, thrashing, evening, hang, king, fang, sting, mourning, sing, morning, nothing, sung, song, bang, something, throbbing, sibling  (Payton, White, non-binary, 2002, Grantsville, UT)\n\n\nAnd here’s Hayley, who has this k sound a little more consistently and a little stronger than Payton.\n\n\n\n polling, spelling, shelling, bowling, drilling, mulling, feeling, stealing, pulling, fooling, willing, sailing, chilling, kneeling, smelling, pooling, lulling, spooling, ceiling, wailing  (Hayley, White, male, 1997, Nampa, ID)\n\n\nAs a side note, this pronunciation is not unique to the Rocky Mountains at all. It’s quite common in England, around cities like Manchester and Liverpool. It’s also heard in New York City and is sometimes made fun of in phrases like “Long Guy-land” (for “Long Island”). You occasionally hear it in other people scattered across the USA too, including Shumita Basu, who hosts the Apple News Today podcast.\n\n\nEEN\nAnother way that ing is pronounced is sort of a mix of the standard ng and “g-dropping.” What’s happening is you get the “ee” vowel in ing combined with the regular n in “g-dropping.” It’s as if the word sailing were pronounced as salene (that clear salty liquid used in hospitals). I call this one “een” because when people say it, it sounds like the word ends in een instead of ing. There’s actually not a lot of research on this variant.\nOverall, it’s not particularly common. I heard it in just 3.2% of the words. But, it does appeaar to be the most common in Montana, where it occured in about 7% of the words. Here’s Hope, who doesn’t do it 100% of the time, but is the person who I heard use it the most. In fact, you might even hear a few times when she adds an extra vowel at the end, sort of a “eenuh” sound. I suppose this is just an especially exaggerated form of this variant.\n\n\n\n polling, scaling, spelling, shelling, bowling, drilling, mulling, feeling, stealing, pulling, fooling, willing, sailing, chilling, kneeling, smelling, pooling, lulling, spooling, ceiling, wailing*  (Hope, White, female, 1997, Idaho Falls, ID)\n\n\nAnd here’s Glen, and older man, who uses een sporadically, mixed with other variants.\n\n\n\n clothing, ceiling, thrashing, evening, king, sting, mourning, sing, morning, nothing, something, throbbing, sibling*  (Glen, White, male, 1970, Salt Lake City, UT)\n\n\nMy Californian mother-in-law says this a lot and I heard it a lot in Washington too. So, is this something unique to the Rocky Mountains? It’s probably not exclusive to the area, but my general impression is that it’s more common than in other places.\n\n\nAdding other sounds at the end\nThe last two variants I heard were ones that added other sounds at the end of the ng. The first one is where it sounds like there’s an h sound. It’s a little bit like the person exhales after saying the word. This was not particularly common, but I heard it 144 times (7.3%). What’s interesting is that 52 different people said this at least once. A few people even used it more than they used the standard variant. Here’s one of those people saying it that way in all her ing words:\n\n\n\n polling, scaling, shelling, bowling, drilling, mulling, feeling, pulling, fooling, willing, sailing, chilling, smelling, pooling, lulling, spooling, ceiling, wailing  (Teresa, White, female, 1968, Pocatello, ID)\n\n\nThe other one I heard is that sometimes people put almost a vowel-like sound at the end. Again, it wasn’t very common, only showing up in 25 (2.5%) of the words I heard. But 10 people said it at least once, and one person used it more than the standard variant. Here’s a clip of someone who uses it a lot, together with other variants mixed in:\n\n\n\n polling, scaling, spelling, shelling, bowling, drilling, mulling, feeling, stealing, pulling, fooling, willing, sailing, chilling, kneeling, smelling, pooling, lulling, spooling, ceiling, wailing  (Albert, White, male, 1982, Idaho Falls, ID)\n\n\nFor these last two, it’s hard to tell whether their extra h or uh only happens after ing words, or if it happens more generally. Listening through the words, there are definitely people who add these sounds at the ends of other words too. I’ll have to do some more listening and digging to see if it’s just something that that person does after all/most words or if it really is something to do with ing words in particular.\n\n\nRegional variation on ING\nHere’s an overall view of the variants across the states:\n\nOverall, somewhat similar to each other. Some of the less common variants are more common in some states but not others, but at this point it’s hard to say whether it’s representative of the whole state or just the influence of one person there.\nAgain, focusing on Idaho, there did seem to be some differences within the state. I’ll again show it compared to Utah:\n\nIt’s a bit messy just because there are so many variants, but what stands out to me is that soutwestern Idaho (the purple bar) has the least amount of standard variants ing compared to northern and southwest Idaho and even Utah. Utah has more of the g and k variants than any region in Idaho, but where you hear the most of the other variants is all in southwest Idaho. Again, it’s hard to say whether this is indicative of something going on in the entire region or if it’s just the influence of a couple people, but I thought that pattern was interesting.\n\n\nFinal thoughts on ING\nWhat I’ve learned from this data is that there is massive variation in how ing words are pronounced. I did not anticipate hearing seven different variants. And even if there are that many, I didn’t expect to find at least one person favoring each variant. I want to end with just a couple more clips. These are people that go the opposite of what I’ve played so far in this section. These are people that pronounce ing in lots of different ways, and don’t just favor one variant.\nHere’s Catherine. (The same one that we heard from in the section on mountain above.) She uses at least five different variants: a lot of g and een, with some k, h, and ng.\n\n\n\n tongue(g), sang(g), clothing(ng), ceiling(een), thrashing(een), evening(een), hang(k), king(g), fang(g), sting(g), mourning(een), morning(een), nothing(ng), sung(k), song(k), bang(h), something(ng), throbbing(een), sibling(een)  (Catherine, White, female, 1978, Farmington, UT)\n\n\nMost people used more than one variant of ING. What I’d like to do is try to figure out if there are any patterns to when they use what."
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#thr",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#thr",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "THR",
    "text": "THR\nThis last one is one of my favorites because very few people have noticed it. Consider the sound you hear when an American says the t in words like water and latter. Most people describe it as a d sound, though it’s not exactly the same as the d you get in like dog. It’s pretty much the same sound that you use for a Spanish r like in pero. Linguists call this sound a “tap” or a “flap.” Despite what some people said on social media maybe a year ago, this is not a just California thing—it’s typical of any American or Canadian English speaker.You may notice the d in ladder or feeding is quicker and ligher than the d in Adobe.\nSo, what I’ve noticed is that people pronounce r’s as a flap when they occur after th, as in three, throw, and threat. If you don’t believe me, here is a recording of someone doing that. In the wordlist she read, she pronounces taps 100% of the time.\n\n\n\n thrust, thrive, threaten, thrash, three, through, thrill, throne, throb, thread, throng  (Bonnie, female, White, 1948, Wallsburg, UT)\n\n\nAnd in case you thought it was exclusive to older, rural people, here’s a clip from a younger person from suburban Utah County :You may notice she has other features in her speech like hyperarticulated mountain and adding a k after ing.\n\n\n\n thrill, thrashing, throat, throttle, threaten, thrive, thrust, throne, thrift, throw, thread, throbbing, through, three  (Deborah, White, female, 1989, Highland, UT)\n\n\nDespite these examples where people do this a lot, pronouncing these words in this way is not particularly common. I heard it about 8.7% of the time. Here’s the breakdown by state.\n\nSo, while not super common anywhere, the flap is heard in all four states. It looks like Utah and Montana are pretty even at around 10% of the time. Meanwhile, Idaho and Wyoming are a little less at roughly 6% of the time.The difference between Utah and Wyoming was not statistically significant according to a chi-squared test (χ² = 0.074, df = 1, p = 0.78), neither was difference between Idaho and Wyoming (χ² = 0.057, df = 1, p = 0.81). Collapsing those two pairs of states down, the difference between them was significant (χ² = 9.275, df = 1, p = 0.002).  Interestingly, I took some similar data I had from Washington and it turns out it patterned pretty close to Idaho and Wyoming."
  },
  {
    "objectID": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#conclusions",
    "href": "blog/idaho-montana-wyoming-and-utah-english-survey-results/index.html#conclusions",
    "title": "Idaho, Montana, Wyoming, and Utah English Survey Results",
    "section": "Conclusions",
    "text": "Conclusions\nThat’s it for now! I’m sorry I wasn’t able to explore more linguistic features. In particular, I’d like to start analyzing some vowels, but acoustic analysis of vowels takes more time and I wanted to get some results out to you in a reasonable amount of time. I will continue to update this page as I get more results, so be sure to check back every few months for new findings.\nIf you participated in this study, thank you! I gathered way more data than I anticipated, which is always a delight. If you did not participate in the survey and would like to, you can try taking it here. Note that it costs money to keep the survey active because of the third-party plugin I’m using to collect audio, so the survey won’t be live for too much longer. Regardless, thanks for your interest in the project!"
  },
  {
    "objectID": "blog/futurevisions/index.html",
    "href": "blog/futurevisions/index.html",
    "title": "futurevisions: My first R package!",
    "section": "",
    "text": "Today I released my first complete, functional, R package! It’s called futurevisions and it’s available on my github. It’s just a little one that contains about 20 different color palettes. I’ve had the idea to work on it for a few months and this week, I decided to go ahead and do it! The rest of this post is the README file for that package and explains the posters the palettes were based on, installation, usage, the list of palettes, and some background."
  },
  {
    "objectID": "blog/futurevisions/index.html#introduction",
    "href": "blog/futurevisions/index.html#introduction",
    "title": "futurevisions: My first R package!",
    "section": "Introduction",
    "text": "Introduction\nThe color palettes are based on NASA’s Visions of the Future poster series, which were produced through NASA’s Jet Propulsion Laboratory through the California Institute of Technology. They are retro-future-style posters depicting humans visiting other planets, moons, and exo-planets. In their words,\n\nImagination is our window into the future. At NASA/JPL we strive to be bold in advancing the edge of possibility so that someday, with the help of new generations of innovators and explorers, these visions of the future can become a reality. As you look through these images of imaginative travel destinations, remember that you can be an architect of the future.\n\nThe posters are available for download for free. You can get them as PDFs or as 20x30 inch TIFF files."
  },
  {
    "objectID": "blog/futurevisions/index.html#demo",
    "href": "blog/futurevisions/index.html#demo",
    "title": "futurevisions: My first R package!",
    "section": "Demo",
    "text": "Demo\nYou can install the package through my github. The library imports ggplot2, so if there are problems, make sure you have ggplot2 installed already. (I’m not good at testing things so if there is trouble, let me know.)\n\nremotes::install_github(\"JoeyStanley/futurevisions\")\n\nYou can then load it like a normal R package.\n\nlibrary(ggplot2)\nlibrary(futurevisions)\n\nThe main two functions are futurevisions, which returns a list of colors, and show_palette, which produces a simple image using ggplot2 to highlight the colors. For example, here is the palette called mars.\n\nshow_palette(\"mars\")\n\n\n\n\n\nfuturevisions(\"mars\")\n\n[1] \"#DB3A2F\" \"#EAB33A\" \"#275D8E\" \"#902A57\" \"#F7EBD3\" \"#0B0C0B\"\n\n\nThis can be easily used within ggplot2 using scale_color_manual:\n\nggplot(mpg, aes(cty, hwy, color = factor(cyl))) +\n  geom_jitter() +\n  scale_color_manual(values = futurevisions(\"mars\"))\n\n\n\n\n\nNote on color selection\nThis is not a rigorous sampling of colors. I picked a few colors from each poster that I felt were representative. They may not necessarily be colorblind-friendly. When using these palettes in data visualization, take care to ensure that your data is not misrepresented."
  },
  {
    "objectID": "blog/futurevisions/index.html#list-of-palettes",
    "href": "blog/futurevisions/index.html#list-of-palettes",
    "title": "futurevisions: My first R package!",
    "section": "List of palettes",
    "text": "List of palettes\n\nGradient\nThese are palettes that may lend themselves better to more gradient purposes.\n\nshow_palette(\"ceres\")\n\n\n\nshow_palette(\"europa\")\n\n\n\nshow_palette(\"titan\")\n\n\n\nshow_palette(\"cancri\")\n\n\n\nshow_palette(\"pso\")\n\n\n\n\n\n\nDiverging\nThese are palettes that may lend themselves more to highlighting deviations from a center point.\n\nshow_palette(\"earth\")\n\n\n\nshow_palette(\"enceladus\")\n\n\n\nshow_palette(\"kepler186\")\n\n\n\nshow_palette(\"atomic_clock\")\n\n\n\n\n\n\nCategorical\nThese are palettes that may lend themselves more to purposes where each color is a stand-alone entity with no meaningful order.\n\nshow_palette(\"venus\")\n\n\n\nshow_palette(\"mars\")\n\n\n\nshow_palette(\"jupiter\")\n\n\n\nshow_palette(\"hd\")\n\n\n\nshow_palette(\"kepler16b\")\n\n\n\nshow_palette(\"pegasi\")\n\n\n\nshow_palette(\"trappest\")\n\n\n\nshow_palette(\"grand_tour\")\n\n\n\nshow_palette(\"atomic_red\")\n\n\n\nshow_palette(\"atomic_blue\")\n\n\n\nshow_palette(\"atomic_orange\")"
  },
  {
    "objectID": "blog/futurevisions/index.html#background",
    "href": "blog/futurevisions/index.html#background",
    "title": "futurevisions: My first R package!",
    "section": "Background",
    "text": "Background\nA portion of the 3rd floor of the Main Library at the University of Georgia has been designed to be evocative of the 1950s when the library was first built. It has some retro-style furniture in a nice study room. It also has some of these Visions of the Future posters hanging up in the hallway. I walk down that hallway every day since the linguistics books, the DigiLab, the best study room on campus, and my personal carrel are all on that floor.\n\nIn fall 2019 I put together a series of workshops on data visualization. One of them was devoted to color, and in preparations for it, I saw that people have made color palettes based on all sorts of things: Wes Anderson movies, Skittles, Pokemon, you name it. I had the idea that the posters on that floor might make for some fun color palettes. I ended up making the package in February 2020."
  },
  {
    "objectID": "blog/vowel-overlap-in-r-advanced-topics/index.html",
    "href": "blog/vowel-overlap-in-r-advanced-topics/index.html",
    "title": "Vowel overlap in R: More advanced topics",
    "section": "",
    "text": "options(dplyr.summarise.inform = FALSE)\nThis is a continuation of my previous tutorial on how to calculate Pillai scores and Bhattacharyya’s Affinity in R for the purposes of measuring vowel overlap. It occurred to me as I was putting the previous one together though that I had a lot of things to say and the tutorial got really long and complicated. So I moved all the more advanced topics to this one to keep the main one a little lighter and more approachable.\nIn this post, I’ll cover some topics like what to do if you have multiple vowel pairs you want to measure in each speaker, errors you may encounter with the select function when you’re calculating Bhattacharyya’s Affinity, ways at making the functions less error-prone, and some visualizations you can do with your data after you’ve collected it."
  },
  {
    "objectID": "blog/vowel-overlap-in-r-advanced-topics/index.html#data-prep",
    "href": "blog/vowel-overlap-in-r-advanced-topics/index.html#data-prep",
    "title": "Vowel overlap in R: More advanced topics",
    "section": "Data prep",
    "text": "Data prep\nThe data prep for this post is covered in Part 1 already, so I’ll put the code here without further comment.\n\nlibrary(tidyverse)\n\n\nmy_vowels &lt;- read.csv(\"../../data/joey.csv\") %&gt;%\n    filter(stress == 1) %&gt;%\n    select(vowel, word, dur, F1.50., F2.50., fol_seg, plt_manner, plt_place, plt_voice) %&gt;%\n    rename(F1 = F1.50., F2 = F2.50.) %&gt;%\n    mutate(fake_speaker = sample(c(\"Joey\", \"Stanley\"), nrow(.), replace = TRUE))\n\n\n\n\n\nlow_back &lt;- my_vowels %&gt;%\n    filter(vowel %in% c(\"AA\", \"AO\"), \n           !fol_seg %in% c(\"L\", \"R\"),\n           word != \"ON\")\n\nSo where I left off at the last tutorial was a function that can calculate the Pillai score and another for the Bhattacharyya’s Affinity, each with the help of summarize:\n\npillai &lt;- function(...) {\n    summary(manova(...))$stats[\"vowel\",\"Pillai\"]\n}\nbhatt &lt;- function (F1, F2, vowel) {\n    vowel_data &lt;- droplevels(data.frame(vowel))\n    \n    sp_df &lt;- sp::SpatialPointsDataFrame(cbind(F1, F2), vowel_data)\n    adehabitatHR::kerneloverlap(sp_df, method='BA')[1,2]\n}\nlow_back %&gt;%\n    summarize(low_back_pillai = pillai(cbind(F1, F2) ~ vowel),\n              low_back_bhatt = bhatt(F1, F2, vowel))\n\n  low_back_pillai low_back_bhatt\n1       0.2250877      0.8296413\n\n\nOkay cool. Let’s see what else can be done with this."
  },
  {
    "objectID": "blog/vowel-overlap-in-r-advanced-topics/index.html#multiple-vowel-pairs",
    "href": "blog/vowel-overlap-in-r-advanced-topics/index.html#multiple-vowel-pairs",
    "title": "Vowel overlap in R: More advanced topics",
    "section": "Multiple vowel pairs",
    "text": "Multiple vowel pairs\nWhat I’ve shown so far is how to calculate the Pillai score for multiple speakers for a single pair of vowels. The next question is what how to get the Pillai score for multiple speakers for multiple pairs of vowels.\nUnfortunately, I don’t really know of a quick way to do that. The problem is a single vowel might be used in multiple pairs (like measuring trap-lot overlap or trap-fleece overlap). Plus, you might be only interested in certain vowels in certain phonetic environments, like before nasals or before tautosyllabic /l/, so defining all that on the fly would be tricky. So there’s no good way that I know of to loop through or group everything in a straightforward manner like we could with speaker.\nThe easiest solution I can think of is to define separate datasets like we’ve done with low_back and run the same code on them. That way, you have all the flexibility of defining specific allophones for each question. And, you probably won’t be getting the Pillai score on too many pairs of vowels, right?\nLet’s make two more subsets. First, I’ll look at the pin-pen merger which I don’t really have, but is common in the South where I live right now. (I’ll also remove some stop words from the dataset.) I’ll also look at the fail-fell merger, which I also don’t really have, but can be found in places like Utah and parts of Texas. I’ll create these subsets and plot them so you can see what they look like in my own speech. Maybe you can roughly estimate the Pillai score.\n\npin_pen &lt;- my_vowels %&gt;%\n    filter(vowel %in% c(\"IH\", \"EH\"), \n           fol_seg %in% c(\"M\", \"N\"), \n           !word %in% c(\"IN\", \"INTO\"))\nhead(pin_pen)\n\n  vowel        word  dur    F1     F2 fol_seg plt_manner plt_place plt_voice\n1    EH       MEANT 0.06 448.8 1631.9       N      nasal    apical    voiced\n2    EH      MENTAL 0.06 570.5 1678.6       N      nasal    apical    voiced\n3    EH       ENTER 0.12 482.7 1738.0       N      nasal    apical    voiced\n4    IH   BEGINNING 0.05 338.9 1968.6       N      nasal    apical    voiced\n5    EH TEMPERATURE 0.06 495.6 1674.6       M      nasal    labial    voiced\n6    EH TEMPERATURE 0.05 477.2 1612.0       M      nasal    labial    voiced\n  fake_speaker\n1      Stanley\n2         Joey\n3         Joey\n4         Joey\n5         Joey\n6      Stanley\n\n\n\nggplot(pin_pen, aes(F2, F1, color = vowel, label = word)) + \n    geom_text(size = 3) + \n    scale_x_reverse() + scale_y_reverse()\n\n\n\n\n\nfail_fell &lt;- my_vowels %&gt;%\n    filter(vowel %in% c(\"EY\", \"EH\"), fol_seg ==\"L\")\nhead(fail_fell)\n\n  vowel         word  dur    F1     F2 fol_seg plt_manner plt_place plt_voice\n1    EY     AILMENTS 0.06 387.3 2149.5       L    lateral    apical    voiced\n2    EY       SAILOR 0.08 382.5 1898.5       L    lateral    apical    voiced\n3    EH    CELLPHONE 0.09 509.4 1549.1       L    lateral    apical    voiced\n4    EY SURVEILLANCE 0.09 468.9 2104.8       L    lateral    apical    voiced\n5    EY       TAYLOR 0.05 311.7 2069.7       L    lateral    apical    voiced\n6    EH         ELSE 0.06 583.1 1343.6       L    lateral    apical    voiced\n  fake_speaker\n1         Joey\n2      Stanley\n3         Joey\n4      Stanley\n5      Stanley\n6         Joey\n\n\n\nggplot(fail_fell, aes(F2, F1, color = vowel, label = word)) + \n    geom_text(size = 3) + \n    scale_x_reverse() + scale_y_reverse()\n\n\n\n\nOkay, so now we’re ready to calculate Pillai scores to those new pairs. Because we’ve created the pillai function, we can relatively easily apply it to each of these datasets.\n\npin_pen %&gt;%\n    summarize(pin_pen_pillai = pillai(cbind(F1, F2) ~ vowel))\n\n  pin_pen_pillai\n1      0.6964555\n\n\n\nfail_fell %&gt;%\n    summarize(fail_fell_pillai = pillai(cbind(F1, F2) ~ vowel))\n\n  fail_fell_pillai\n1        0.8189749\n\n\nSo this is pretty cool. Here you can see that my fail-fell vowel classes are quite distinct with a Pillai score of 0.82. My pin-pen vowels are also quite distinct with a Pillai score of 0.70.\nSo we’ve run the same thing three times on three different datasets. But can we do this even more elegantly? Sure!\nOkay, so first, I’ll combine all three datasets (low_back, pin_pen and fail_fell) into one combined dataframe using bind_rows. But in order to “keep track” of which dataframe a particular row came from, I’ll create a new column called vowel_pair using the .id = \"vowel_pair\" argument. To get this to work then, I’ll “name” each of the dataframes as I combine it. So for example, the low_back dataframe is called \"low back\" and I show that by putting the new name between ticks (that little thing next to the number 1 key on my keyboard that looks like this: `). If you combine the dataframes in this way, you’ll have a new column saying which one it came from.\n\nall_pairs &lt;- bind_rows(`low back` = low_back,\n                       `pin pen`  = pin_pen,\n                       `fail fell` = fail_fell, \n                       .id = \"vowel_pair\")\nhead(all_pairs)\n\n  vowel_pair vowel   word  dur    F1     F2 fol_seg plt_manner   plt_place\n1   low back    AA   TODD 0.17 614.6 1065.2       D       stop      apical\n2   low back    AA    GOD 0.09 554.0 1250.3       D       stop      apical\n3   low back    AA FATHER 0.12 598.6 1000.2      DH  fricative interdental\n4   low back    AO  WATER 0.05 587.5  986.7       T       stop      apical\n5   low back    AO   LONG 0.09 577.6 1008.3      NG      nasal       velar\n6   low back    AA STOCKS 0.13 578.1 1073.7       K       stop       velar\n  plt_voice fake_speaker\n1    voiced         Joey\n2    voiced      Stanley\n3    voiced         Joey\n4 voiceless         Joey\n5    voiced      Stanley\n6 voiceless         Joey\n\ntail(all_pairs)\n\n    vowel_pair vowel     word  dur    F1     F2 fol_seg plt_manner plt_place\n172  fail fell    EY     GAIL 0.12 431.1 1908.6       L    lateral    apical\n173  fail fell    EY AILMENTS 0.12 438.0 2048.3       L    lateral    apical\n174  fail fell    EY     SAIL 0.06 423.7 1797.2       L    lateral    apical\n175  fail fell    EH     FELL 0.05 502.3 1539.2       L    lateral    apical\n176  fail fell    EY  WAILING 0.12 421.3 2086.7       L    lateral    apical\n177  fail fell    EY    STALE 0.14 437.9 1862.4       L    lateral    apical\n    plt_voice fake_speaker\n172    voiced         Joey\n173    voiced      Stanley\n174    voiced         Joey\n175    voiced      Stanley\n176    voiced         Joey\n177    voiced      Stanley\n\n\nNow that may seem like more work than it’s worth, but the payoff is that when you actually go to do the Pillai scores, it’s hardly any more complicated than before. In the group_by function, just put that you want to group the data by the vowel pair and by the speaker and it’ll magically do the rest for you.\n\nall_pairs %&gt;%\n    group_by(vowel_pair, fake_speaker) %&gt;%\n    summarize(pillai = pillai(cbind(F1, F2) ~ vowel))\n\n# A tibble: 6 × 3\n# Groups:   vowel_pair [3]\n  vowel_pair fake_speaker pillai\n  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;\n1 fail fell  Joey          0.825\n2 fail fell  Stanley       0.849\n3 low back   Joey          0.117\n4 low back   Stanley       0.301\n5 pin pen    Joey          0.731\n6 pin pen    Stanley       0.708\n\n\nIn my opinion, it’s worth it to do the extra bit of work beforehand prepping your data with the whole bind_rows business and creating the new function because the benefit is that when it actually comes time to calculate the Pillai score, you’ve got a tidy dataset to work with and a flexible function to use."
  },
  {
    "objectID": "blog/vowel-overlap-in-r-advanced-topics/index.html#the-select-clash-and-bhattacharyyas-affinity",
    "href": "blog/vowel-overlap-in-r-advanced-topics/index.html#the-select-clash-and-bhattacharyyas-affinity",
    "title": "Vowel overlap in R: More advanced topics",
    "section": "The select clash and Bhattacharyya’s Affinity",
    "text": "The select clash and Bhattacharyya’s Affinity\n Okay, in Part 1, I talked about the library needed to run Bhattacharyya’s Affinity, adehabitatHR. However, it always seemed to be the case that my tidyverse code, particularly the select function always broke in scripts that used this package. It was super annoying and for the longest time I couldn’t figure out why.In Dan Johnson’s NWAV presentation where he introduces Bhattacharyya’s Affinity, he installs and loads an additional package sp so that the SpatialPointsData-Frame function can be used. This is technically not necessary to do explicitly, because sp is a dependency of adehabitatHR, meaning when you install and load adehabitatHR you also are bringing sp along too.\nAs it turns out, when you load the adehabitatHR package, it also loads the MASS package. Unfortunately for us, there’s a function in MASS called select. Well, dplyr also has a select function. Having two select functions at the same time creates a clash. So, R chooses the one that was loaded most recently, which is MASS (via adehabitatHR).\nYou can actually see this in a warning when you load adehabitatHR if dplyr (via tidyverse) is already installed:\nTo replicate the messages and output in this section you’ll have to restart R and load the packages again from scratch in this order. To do that, go to Session→Restart R.\n\nlibrary(tidyverse)\nlibrary(adehabitatHR)\n\n## ...output truncated...\n\n## Loading required package: MASS\n\n## Attaching package: ‘MASS’\n\n## The following object is masked from ‘package:dplyr’:\n\n##     select\n\n## ...output truncated...\n\nAnd when you go to use select, R thinks it should run MASS::select instead of dplyr::select:\n\nlow_back %&gt;%\n    select(vowel)\n\n## Error in select(., vowel) : unused argument (vowel)\n\nSo, there are two solutions to this. First, you can simply take advantage of the fact that the most recently loaded package takes priority, and load adehabitatHR before you load tidyverse\n\nlibrary(adehabitatHR)\nlibrary(tidyverse)\n\n\nlow_back %&gt;%\n    select(vowel)\n\n##     vowel\n## 1      AA\n## 2      AA\n## 3      AA\n## 4      AA\n## 5      AO\n## 6      AA\n\nThat’ll ensure that dplyr::select takes precedence. However, I’ve never really liked this for a couple reasons. One, I’m stubborn, and I like to load my packages in a specific order (basically the order that I use them) and at the top of my code it seems weird to load a package before tidyverse. Also, it means that it I’m already going along in an R session, with tidyverse loaded already, if I want to then do some Bhattacharyya’s Affinity, I’ll need to either unload the library or restart R, which is never really all that convenient.\nThe other option is to not actually load the adehabitatHR package at all. You can still use functions when a package is not loaded (as long as you already have them installed to your computer) by prefacing the function name with the name of the package and a pair of colons. So when it comes time to use the one function I need from adehabitatHR, the kerneloverlap function, I’ll just type adehabitatHR::kerneloverlap. In my opinion, this is a better route just because I don’t like loading an entire package (and reworking lots of others things in my code) just to make sure of one function.\nFor the remainder of this tutorial, I’m going to implement this second option. And for consistency, I’ll also not load the sp package either and call SpatialPointsDataFrame using sp:: prefixed before it."
  },
  {
    "objectID": "blog/vowel-overlap-in-r-advanced-topics/index.html#making-the-functions-more-robust",
    "href": "blog/vowel-overlap-in-r-advanced-topics/index.html#making-the-functions-more-robust",
    "title": "Vowel overlap in R: More advanced topics",
    "section": "Making the functions more robust",
    "text": "Making the functions more robust\nIn this section, I’ll go into some more detail about how to make your functions less likely to crash. Some of the topics here get tedious and cover use some more advanced R skills. If you’ve been finding that your code breaks when you apply it to a bunch of speakers, this might help with these issues.\nI’ll start with the bhatt function because it’s a little more straightforward. Then we’ll get into some slightly more confusing stuff with pillai.\n\nMaking bhatt more robust\nI’ve noticed when running Bhattacharyya’s affinity on my data that it tends to crash if certain conditions aren’t met. For example, the calculation requires at least five observations from each vowel class to work. So, let’s say I wanted to look at the pull-pole merger (that is, the merger of foot and goat before laterals). We can create the dataset the same way as beforeI think I have this merger, but I’m really not sure. I thought studying it in my data would help my own intuitions, but now I’m always hyperaware of the relatively small group of relevant words. But I digress…\n\npull_pole &lt;- my_vowels %&gt;%\n    filter(vowel %in% c(\"UH\", \"OW\"), fol_seg %in% c(\"L\")) %&gt;%\n    droplevels()\nhead(pull_pole)\n\n  vowel       word  dur    F1    F2 fol_seg plt_manner plt_place plt_voice\n1    OW       GOAL 0.22 366.7 827.0       L    lateral    apical    voiced\n2    OW       POLO 0.19 418.2 893.9       L    lateral    apical    voiced\n3    OW      ROLLS 0.11 385.7 785.1       L    lateral    apical    voiced\n4    OW      MOULD 0.06 388.1 732.0       L    lateral    apical    voiced\n5    OW  PORTFOLIO 0.06 444.1 988.3       L    lateral    apical    voiced\n6    OW CONTROLLED 0.07 389.0 849.7       L    lateral    apical    voiced\n  fake_speaker\n1      Stanley\n2         Joey\n3      Stanley\n4         Joey\n5      Stanley\n6      Stanley\n\n\nBut when we run it…\n\npull_pole %&gt;%\n    summarize(pull_pole_bhatt = bhatt(F1, F2, vowel))\n\n## Error in kernelUD(xy, same4all = TRUE, ...): At least 5 relocations \n#  are required to fit an home range\n\nIt crashes. You can see in the error message that it says you need at least five “relocations” per “home range”. You can tell this package was intended for animal location data instead of vowel data! But we can see that there aren’t five words in the pull (= “UH”) class:\n\ntable(pull_pole$vowel)\n\n\nOW UH \n50  1 \n\n\nThere’s just one token! This is problematic because if I’m going to apply this function to like 30 speakers, even if one speaker has insufficient data I want it to work on everyone else without crashing.\nSo, how can I make the bhatt function a little more robust? Expert R users might know of fancier ways to do what I’m about to do, but I just put a little if statement in there: if the lowest value in that table is less than five, then return—meaning abort the function—with the value NA.\n\nbhatt &lt;- function (F1, F2, vowel) {\n    vowel_data &lt;- data.frame(vowel) %&gt;%\n        droplevels()\n    \n    if (min(table(vowel_data)) &lt; 5) return(NA) # there are at least 5 of each vowel\n\n    adehabitatHR::kerneloverlap(sp::SpatialPointsDataFrame(cbind(F1, F2), vowel_data), method='BA')[2,1]\n}\npull_pole %&gt;%\n    summarize(pull_pole_bhatt = bhatt(F1, F2, vowel))\n\n  pull_pole_bhatt\n1              NA\n\n\nNow, when I run it, it won’t crash anymore. It won’t return a number and instead it’ll just return NA, but at least it doesn’t crash. You can appreciate now how handy this might be when we combine all the data together and then then the function on all vowel pairs.\n\nall_pairs &lt;- bind_rows(`low back` = low_back,\n                       `pin pen`  = pin_pen,\n                       `fail fell` = fail_fell, \n                       `pull pole` = pull_pole,\n                       .id = \"vowel_pair\")\nall_pairs %&gt;%\n    group_by(vowel_pair) %&gt;%\n    summarize(bhatt = bhatt(F1, F2, vowel))\n\n# A tibble: 4 × 2\n  vowel_pair  bhatt\n  &lt;chr&gt;       &lt;dbl&gt;\n1 fail fell   0.122\n2 low back    0.830\n3 pin pen     0.393\n4 pull pole  NA    \n\n\nNow, the function will work just fine on the vowel pairs with enough data and will quietly return NA if there’s not enough data.\nAs it turns out, there were some other problems I ran into in my data, so I had to make the function even more robust. I’ve gone ahead and added a couple other warnings as well. So like one time I didn’t have any tokens of a particular vowel, but plenty of the other. So after applying droplevels, only one vowel remained, so then when I ran table, the lowest number in that table was indeed higher than 5, but it crashed because there was only one vowel there. So, I added a check to makes sure that the table contained exactly two vowel tokens.\nFinally, I found out that sometimes there will be a speaker or a vowel pair that happens to have no data. Zero tokens of either vowel. In theory, the other two checks should handle it, but it turns out the checks themselves crash if there’s zero data. So, I added that first line, if (nrow(vowel_data) &lt; 1) return(NA), before the other checks are done, to ensure that there’s at least something there.\nSo, the complete function looks like this:\n\nbhatt &lt;- function (F1, F2, vowel) {\n    vowel_data &lt;- data.frame(vowel) %&gt;%\n        droplevels()\n    \n    # Make sure there's enough data\n    if (nrow(vowel_data) &lt; 1) return(NA) # not zero\n    if (length(table(vowel_data)) &lt; 2) return(NA) # both vowels are represented\n    if (min(table(vowel_data)) &lt; 5) return(NA) # there are at least 5 of each vowel\n\n    adehabitatHR::kerneloverlap(sp::SpatialPointsDataFrame(cbind(F1, F2), vowel_data), method='BA')[2,1]\n}\n\nThis version of the function is the one that I use in my data. It’s a little more robust than the simpler version and it can safely iterate over whatever groups you want.\n\nlow_back %&gt;%\n    group_by(fake_speaker) %&gt;%\n    summarize(bhatt = bhatt(F1, F2, vowel))\n\n# A tibble: 2 × 2\n  fake_speaker bhatt\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Joey         0.892\n2 Stanley      0.728\n\n\n\nall_pairs %&gt;%\n    group_by(vowel_pair, fake_speaker) %&gt;%\n    summarize(bhatt = bhatt(F1, F2, vowel))\n\n# A tibble: 8 × 3\n# Groups:   vowel_pair [4]\n  vowel_pair fake_speaker   bhatt\n  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;\n1 fail fell  Joey          0.124 \n2 fail fell  Stanley       0.0830\n3 low back   Joey          0.892 \n4 low back   Stanley       0.728 \n5 pin pen    Joey          0.291 \n6 pin pen    Stanley       0.453 \n7 pull pole  Joey         NA     \n8 pull pole  Stanley      NA     \n\n\nSo that’s it for the Bhattacharyya’s Affinity (for now). Now let’s more on to pillai.\n\n\nTrying to make pillai more robust\nBecause of the way pillai is implemented right now, particularly with the ... syntax, it’s a little tricky to add the same data validation checks that we added in the bhatt function above. In fact, I spent a bit of time on it, but the main hurdle is that the manova function requires things to be in a formula (that is, something like y ~ x). So, for now, I can’t offer a perfect solution to the pillai function.\nInstead, I offer a less-than-ideal solution: two different functions, each with their strengths and weaknesses.\nThe first is pillai, which is exactly how we have it already (repeated here for clarity):\n\npillai &lt;- function(...) {\n    summary(manova(...))$stats[\"vowel\",\"Pillai\"]\n}\n\nThe good part about this function is that whatever formula you want to use, works perfectly fine. So if you want to add duration or F3 to the dependent variable matrix, or pack on all sorts of social or phonological factors, go for it and it’ll work as long as manova can handle it.\nThe bad part about this is that we can’t add any additional data validation procedures to account for potentially missing data. So like if I try to run it on the pull_pole dataset like I did with bhatt, it’ll crash.\n\nall_pairs %&gt;%\n    group_by(vowel_pair, fake_speaker) %&gt;%\n    summarize(pillai_score = pillai(cbind(F1, F2) ~ vowel))\n\n## Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): \n## contrasts can be applied only to factors with 2 or more levels\n\nNow, apparently manova can handle less data than kerneloverlap can, because the only reason that crashed is because one of my fake speakers had zero tokens of pull. I’ll try it again but pooling all my data together. If there’s even just one token, it’ll actually run and return a value for you.\n\nall_pairs %&gt;%\n    group_by(vowel_pair) %&gt;%\n    summarize(pillai_score = pillai(cbind(F1, F2) ~ vowel))\n\n# A tibble: 4 × 2\n  vowel_pair pillai_score\n  &lt;chr&gt;             &lt;dbl&gt;\n1 fail fell        0.819 \n2 low back         0.225 \n3 pin pen          0.696 \n4 pull pole        0.0687\n\n\nI’ll admit, I’m a little skeptical of a Pillai score where one of the vowels only has one token in it. And sure enough, if we look at the model output, the p-value is high, probably because there’s just not a lot of data.\n\nsummary(manova(cbind(F1, F2) ~ vowel, data = pull_pole))\n\n          Df   Pillai approx F num Df den Df Pr(&gt;F)\nvowel      1 0.068656   1.7692      2     48 0.1814\nResiduals 49                                       \n\n\nSo, if I could, I would still implement the data validation checks like I did with bhatt to ensure there’s enough data before running so that I can feel more confident about the values I’m getting, rather than getting values without any sort of warning message informing me of the potentially bad result.\nWell, one solution is to create a separate function that can do the checks for sufficient data. I’ll create one called pillai2 that actually mimics the syntax used in bhatt.\n\npillai2 &lt;- function(F1, F2, vowel) {\n    \n    vowel_data &lt;- data.frame(vowel) %&gt;%\n        droplevels()\n    \n    if (nrow(vowel_data) &lt; 1) return(NA) # not zero\n    if (length(table(vowel_data)) &lt; 2) return(NA) # both vowels are represented\n    if (min(table(vowel_data)) &lt; 5) return(NA) # there are at least 5 of each vowel\n\n    summary(manova(cbind(F1, F2) ~ vowel))$stats[\"vowel\",\"Pillai\"]\n}\n\nNow, when I call pillai2, I don’t need to use cbind or the formula syntax. Instead, I just give it the name of the columns corresponding to my F1, F2, and vowel columns.\n\nall_pairs %&gt;%\n    group_by(vowel_pair, fake_speaker) %&gt;%\n    summarize(pillai = pillai2(F1, F2, vowel))\n\n# A tibble: 8 × 3\n# Groups:   vowel_pair [4]\n  vowel_pair fake_speaker pillai\n  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;\n1 fail fell  Joey          0.825\n2 fail fell  Stanley       0.849\n3 low back   Joey          0.117\n4 low back   Stanley       0.301\n5 pin pen    Joey          0.731\n6 pin pen    Stanley       0.708\n7 pull pole  Joey         NA    \n8 pull pole  Stanley      NA    \n\n\nSo, yay, right? The problem with this version of the pillai2 function is that you can’t control the MANOVA formula. So if you want to run a MANOVA that controls for things like following place of articulation, this function, pillai2, won’t be able to do that, unlike the previous version, pillai.\nMy recommendation is that if you want to add additional variables, use the original pillai function (not this pillai2 that we just made) but do the data validation separately.\nSwitching gears one more time, there’s one more thing we can do to the pillai function to make it more robust. Right now, it assumes that the variable in your dataframe is called “vowel”. Sometimes, that might not be the case. I sometimes use “vowel_class” if I’m working with specific subsets of the data. If I run the pillai function as it is on a dataframe like that, it’ll crash.\n\nall_pairs %&gt;%\n    rename(vowel_class = vowel) %&gt;%\n    group_by(vowel_pair, fake_speaker) %&gt;%\n    summarize(pillai = pillai2(F1, F2, vowel))\n\n## Error in data.frame(vowel): object 'vowel' not found\n\nSo, we’ll have to add a little more robustness to the function. You could use the number 1 instead of \"vowel\" because the vowel argument is first in our formula.\n\npillai &lt;- function(...) {\n    summary(manova(...))$stats[1,\"Pillai\"]\n}\npillai(cbind(F1, F2, dur) ~ vowel + plt_place + plt_manner + plt_voice, data = low_back)\n\n[1] 0.2767855\n\n\nBut that assumes the vowel argument is first, which it might not always be. Here, I’ll put the plt_place variable first after the tilde, and you’ll notice that the result changes.\n\npillai &lt;- function(...) {\n    summary(manova(...))$stats[1,\"Pillai\"]\n}\npillai(cbind(F1, F2, dur) ~ plt_place + plt_manner + plt_voice + vowel, data = low_back)\n\n[1] 0.3752574\n\n\nThe solution here is to either make sure the vowel column is first in your function or to rename whatever it is that your column is called to “vowel.” I don’t like either of those solutions personally but, yet again, I don’t really have a good solution. Using the number 1 might be slightly better than changing the column name because it doesn’t force you to change the underlying structure of your dataframe.\nHowever, if you like the pillai2 function more—the one that has some robustness already built into it but is limited to just one independent variable—you’re in luck because it actually can handle different column names already. Regardless of what my vowel column is called, when it gets passed to the function, it is temporarily renamed \"vowel\". So, this will still work:\n\nall_pairs %&gt;%\n    rename(this_is_a_random_long_name = vowel) %&gt;%\n    group_by(vowel_pair) %&gt;%\n    summarize(pillai = pillai2(F1, F2, this_is_a_random_long_name))\n\n# A tibble: 4 × 2\n  vowel_pair pillai\n  &lt;chr&gt;       &lt;dbl&gt;\n1 fail fell   0.819\n2 low back    0.225\n3 pin pen     0.696\n4 pull pole  NA    \n\n\nSo again, there’s a toss up between pillai, which can handle any MANOVA function, and pillai2 which is less error prone. Perhaps in the future I’ll be able to find a way to combine both into one super robust pillai function. For now, you know as much as I do."
  },
  {
    "objectID": "blog/vowel-overlap-in-r-advanced-topics/index.html#miscellaneous-material",
    "href": "blog/vowel-overlap-in-r-advanced-topics/index.html#miscellaneous-material",
    "title": "Vowel overlap in R: More advanced topics",
    "section": "Miscellaneous material",
    "text": "Miscellaneous material\nAt this point, we’re done; I won’t be playing around with the functions anymore. Instead, in this last section, I’ll look at some fun tricks when using them and how to reshape the output to be more useful for you. Again, parts of this section get a little tedious and use some tricksy R stuff.\n\nCombining functions\n Sometimes, it’s a little bit silly to have both the pillai2 function and the bhatt function when they’re mostly similar. So, we can actually combine them into one function, if that makes sense to you. We’ll create a new function called overlap. The first part is the same as both of the other functions.This section only works if you want to use pillai2. If you want to be flexible in the MANOVA formula, you’re stuck with pillai and you can’t really combine them like this.\nThe difference is I create a new argument called method and by default it’s \"pillai\". I then do an if statement that basically does different things depending on what is in that method argument. If it’s \"pillai\", then it’ll return the Pillai score. If it’s \"BA\" it’ll do the Bhattacharyya’s Affinity. If it’s something else, it’ll give a little warning message saying that it was an improper method and will return NA.\n\noverlap &lt;- function(F1, F2, vowel, method = \"pillai\") {\n    \n    vowel_data &lt;- data.frame(vowel) %&gt;%\n        droplevels()\n    \n    if (nrow(vowel_data) &lt; 1) return(NA) # not zero\n    if (length(table(vowel_data)) &lt; 2) return(NA) # both vowels are represented\n    if (min(table(vowel_data)) &lt; 5) return(NA) # there are at least 5 of each vowel\n\n    if (method == \"pillai\") {\n        summary(manova(cbind(F1, F2) ~ vowel))$stats[\"vowel\", \"Pillai\"]\n    } else if (method == \"BA\") {\n        adehabitatHR::kerneloverlap(sp::SpatialPointsDataFrame(cbind(F1, F2), vowel_data), method='BA')[2,1]\n    } else {\n        warning(\"Improper method\")\n        return(NA)\n    }\n}\n\nThe result is that we just have one function, overlap, that takes care of both measurements.\n\nall_pairs %&gt;%\n    group_by(vowel_pair, fake_speaker) %&gt;%\n    summarize(pillai = overlap(F1, F2, vowel),\n              bhatt  = overlap(F1, F2, vowel, method = \"BA\"))\n\n# A tibble: 8 × 4\n# Groups:   vowel_pair [4]\n  vowel_pair fake_speaker pillai   bhatt\n  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1 fail fell  Joey          0.825  0.124 \n2 fail fell  Stanley       0.849  0.0830\n3 low back   Joey          0.117  0.892 \n4 low back   Stanley       0.301  0.728 \n5 pin pen    Joey          0.731  0.291 \n6 pin pen    Stanley       0.708  0.453 \n7 pull pole  Joey         NA     NA     \n8 pull pole  Stanley      NA     NA     \n\n\nThis is kinda cool I guess. If you like that better, go for it. But if you like the bhatt and pillai2 functions better, use those instead.\nNow, if you wanted to get really fancy, you could have the best of both worlds. You could retain the pillai2 and bhatt functions for the user, but under the hood they actually do the same thing.\n\npillai2 &lt;- function(...) {\n    overlap(..., method = \"pillai\")\n}\nbhatt &lt;- function(...) {\n    overlap(..., method = \"BA\")\n}\nall_pairs %&gt;%\n    group_by(vowel_pair, fake_speaker) %&gt;%\n    summarize(pillai = pillai2(F1, F2, vowel),\n              bhatt  = bhatt(F1, F2, vowel))\n\n# A tibble: 8 × 4\n# Groups:   vowel_pair [4]\n  vowel_pair fake_speaker pillai   bhatt\n  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1 fail fell  Joey          0.825  0.124 \n2 fail fell  Stanley       0.849  0.0830\n3 low back   Joey          0.117  0.892 \n4 low back   Stanley       0.301  0.728 \n5 pin pen    Joey          0.731  0.291 \n6 pin pen    Stanley       0.708  0.453 \n7 pull pole  Joey         NA     NA     \n8 pull pole  Stanley      NA     NA     \n\n\n If I were to put together an R package with these functions, I’d do something like that. It’s easiest to maintain while still being user-friendly.Incidentally, if you want to help me create an R package with all this stuff, let’s talk."
  },
  {
    "objectID": "blog/vowel-overlap-in-r-advanced-topics/index.html#reshape-and-visualize",
    "href": "blog/vowel-overlap-in-r-advanced-topics/index.html#reshape-and-visualize",
    "title": "Vowel overlap in R: More advanced topics",
    "section": "Reshape and visualize",
    "text": "Reshape and visualize\nThe last thing I’ll do in this lengthy tutorial is to look at how make a couple visualizations for pillai scores and how to transform your data along the way. But, this is best illustrated with more speakers, so I’m going to do take my data and duplicate it a bunch of times to simulate the effect of a larger sample. I’ll then randomly divide that data up into eight fake speakers.\n\nfake_data &lt;- rbind(all_pairs, all_pairs, all_pairs, all_pairs, all_pairs, all_pairs, all_pairs, all_pairs) %&gt;%\n    filter(vowel_pair != \"pull pole\") %&gt;%\n    mutate(fake_speaker = sample(c(\"Andy\", \"Betty\", \"Chuck\", \"Dolly\", \"Earl\", \"Flo\", \"Gary\", \"Helga\"), nrow(.), replace = TRUE))\nsample_n(fake_data, 10)\n\n   vowel_pair vowel        word  dur    F1     F2 fol_seg plt_manner\n1    low back    AA   DOCUMENTS 0.10 623.6 1200.6       K       stop\n2     pin pen    EH  IDENTIFIED 0.05 489.4 1707.0       N      nasal\n3    low back    AA         FOX 0.07 633.4 1102.7       K       stop\n4     pin pen    EH DEMONSTRATE 0.05 420.2 1549.5       M      nasal\n5     pin pen    IH     WINNING 0.05 377.2 1678.3       N      nasal\n6    low back    AA        BODY 0.11 682.6 1103.2       D       stop\n7     pin pen    EH      MENTAL 0.06 570.5 1678.6       N      nasal\n8    low back    AA  IMPOSSIBLE 0.09 576.5 1042.1       S  fricative\n9    low back    AO       AWFUL 0.13 624.2 1101.2       F  fricative\n10   low back    AA      SOCCER 0.11 651.3 1127.5       K       stop\n     plt_place plt_voice fake_speaker\n1        velar voiceless         Earl\n2       apical    voiced        Chuck\n3        velar voiceless         Andy\n4       labial    voiced         Earl\n5       apical    voiced         Earl\n6       apical    voiced         Earl\n7       apical    voiced        Helga\n8       apical voiceless         Andy\n9  labiodental voiceless         Andy\n10       velar voiceless         Earl\n\n\nWhen you run the pillai and bhatt functions, the results is a table such that each combination of speaker and vowel pair are on their own unique row.\n\npillai_table &lt;- fake_data %&gt;%\n    group_by(fake_speaker, vowel_pair) %&gt;%\n    summarize(pillai = pillai2(F1, F2, vowel))\npillai_table\n\n# A tibble: 24 × 3\n# Groups:   fake_speaker [8]\n   fake_speaker vowel_pair pillai\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;\n 1 Andy         fail fell   0.805\n 2 Andy         low back    0.265\n 3 Andy         pin pen     0.712\n 4 Betty        fail fell   0.883\n 5 Betty        low back    0.188\n 6 Betty        pin pen     0.622\n 7 Chuck        fail fell   0.828\n 8 Chuck        low back    0.236\n 9 Chuck        pin pen     0.645\n10 Dolly        fail fell   0.801\n# ℹ 14 more rows\n\n\nThis structure may be handy for some types of visualizations. For example, you can make a plot like this which shows the spread of Pillai scores across each vowel pair. I’ve underlaid a violin plot so you can see the distribution a little better.\n\nggplot(pillai_table, aes(vowel_pair, pillai)) + \n    geom_violin(color = \"gray75\") + \n    geom_text(aes(label = fake_speaker))\n\n\n\n\nBut you may need to rearrange your data in a different way. For example, you may want just one speaker per row and each vowel pair in its own column. Fortunately, the tidyr package has a really handy function called spread that’ll do this. There are two arguments to spread:I explain this spread function in a little more detail in my second tutorial on plotting formant data.\n\nThe name of the column whose values you want to spread out into their own columns. In our case, the vowel_pair column contains the four different pairs.\nThe name of the existing columns whose values you want to be contained in the new columns. In our case, the pillai column contains all those numbers and we want those numbers to fill the cells of the new columns being created.\n\nSo, if we put it together, this is what we get.\n\npillai_table_wide &lt;- pillai_table %&gt;%\n    spread(vowel_pair, pillai)\npillai_table_wide\n\n# A tibble: 8 × 4\n# Groups:   fake_speaker [8]\n  fake_speaker `fail fell` `low back` `pin pen`\n  &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Andy               0.805      0.265     0.712\n2 Betty              0.883      0.188     0.622\n3 Chuck              0.828      0.236     0.645\n4 Dolly              0.801      0.282     0.796\n5 Earl               0.828      0.284     0.769\n6 Flo                0.849      0.229     0.663\n7 Gary               0.827      0.194     0.668\n8 Helga              0.803      0.244     0.709\n\n\nNow, with our data reshaped in this way, we could do a scatterplot to see the correlation between two mergers:\nThere are ticks around low back and pin pen because those are the column names themselves and when you refer to column names that have spaces in them you have to surround them with ticks. These show up in the plot itself, but you can fix that with labs.\n\nggplot(pillai_table_wide, aes(`low back`, `pin pen`)) + \n    geom_text(aes(label = fake_speaker))\n\n\n\n\nBecause I’ve randomly split by data up, there’s really no pattern, but if you suspect a correlation between mergers in your dataset, this plot might be more informative for you.\nAnother plot you may want to see is the correlation between the Pillai score and the Bhattacharyya’s Affinity. We’ll calculate both of them at once and create an overlap_table object.\n\noverlap_table &lt;- fake_data %&gt;%\n    group_by(fake_speaker, vowel_pair) %&gt;%\n    summarize(pillai = pillai2(F1, F2, vowel),\n              bhatt  = bhatt(F1, F2, vowel))\noverlap_table\n\n# A tibble: 24 × 4\n# Groups:   fake_speaker [8]\n   fake_speaker vowel_pair pillai  bhatt\n   &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 Andy         fail fell   0.805 0.118 \n 2 Andy         low back    0.265 0.802 \n 3 Andy         pin pen     0.712 0.389 \n 4 Betty        fail fell   0.883 0.0817\n 5 Betty        low back    0.188 0.831 \n 6 Betty        pin pen     0.622 0.385 \n 7 Chuck        fail fell   0.828 0.119 \n 8 Chuck        low back    0.236 0.827 \n 9 Chuck        pin pen     0.645 0.424 \n10 Dolly        fail fell   0.801 0.128 \n# ℹ 14 more rows\n\n\nNow, with this table, we can do another scatterplot, but pitting the two measurements against each other. We’d expect a negative correlation since overlap in Pillai means a 0 and in Bhattacharyya’s Affinity it means 1. But can we find anything that stands out? Here, I’ll plot all the data, but color the names by vowel pair.\n\nggplot(overlap_table, aes(pillai, bhatt, color = vowel_pair)) + \n    geom_segment(x = 1, y = 0, xend = 0, yend = 1, color = \"gray50\") + \n    geom_text(aes(label = fake_speaker))\n\n\n\n\nSo this is kind of cool. For the low back merger, the distribution is tight and there’s not a lot going on. For the other two, they’re more spread out. We don’t see too many points straying too far from the line, so there’s not a lot of concern here. Maybe because it’s just a bunch of random samples of my own data it’s not particularly enlightening. Perhaps if you try this on your own data, you might find some interesting differences between the two measurements, which will require some digging."
  },
  {
    "objectID": "blog/vowel-overlap-in-r-advanced-topics/index.html#conclusion",
    "href": "blog/vowel-overlap-in-r-advanced-topics/index.html#conclusion",
    "title": "Vowel overlap in R: More advanced topics",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s it. In this post I covered more of the nitty-gritty detail on a couple topics relating to getting vowel overlap measurements in R. As you can tell, there are some residual problems, like coming up with one function that is flexible enough to allow for any MANOVA formula but still allow for data validation. But the once those formulas are written, you can do some pretty cool stuff in just a couple lines of code. Plus, the visuals are straightforward to implement. Hopefully, thanks to this tutorial, the coding itself is no longer a barrier for you if you’ve been meaning to get vowel overlap measurements for your data."
  },
  {
    "objectID": "blog/asa181/index.html",
    "href": "blog/asa181/index.html",
    "title": "ASA181",
    "section": "",
    "text": "I’m in Seattle at the 181st Meeting of the Acoustical Society of America right now! This is my first in-person conference since October 2019, so it’s great to be here. I presented two posters today, which you can read about and download below."
  },
  {
    "objectID": "blog/asa181/index.html#beyond-midpoints-vowel-dynamics-of-the-low-back-merger-shift",
    "href": "blog/asa181/index.html#beyond-midpoints-vowel-dynamics-of-the-low-back-merger-shift",
    "title": "ASA181",
    "section": "Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift",
    "text": "Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nFor some reason, I hadn’t yet presented any of my dissertation findings at a conference, not even while I was working on them or writing them up. Anyway, I’m happy to finally present some of my results at a conference. The purpose of this paper is to describe changes in vowel trajectory that accompany changes in midpoints. The Low-Back-Merger Shift is a now-widespread shift across much of North America. My data from Washington shows it pretty clearly across generations. But when I take a wide-angle lens at the vowel trajectories, I found that there was much more to the story than just a global lowering/centralizing of the front lax vowels.\n\n\n\n\n\nThere were perhaps three patterns I noticed when I modeled vowel trajectories. First is that the trajectory length was different between them. The low vowel /æ/ was much longer, then /ɛ/, and then /ɪ/. There’s also a general U-shaped pattern. Finally, the “angle” of this U-shaped was more towards the “left” for /æ/, more towards the right for /ɪ/, and in the middle for /ɛ/. These descriptions are consistent across generations and between the two geneders modeled, so it may say more about American English articulation than anything sociolinguistic.\nPerhaps more interestingly though was how these trajectories changed—within the parameters just described—across generations. Older people’s vowels traversed through much more of the F2 space than younger generations did. The result is that the older people’s vowels look more like a shallow U-shape while the younger people’s is more of V-shape or even a “bounce” straight up and down in the F1-F2 vowel space. The fact that this was consistent across all three front lax vowels and between the genders suggests some interesting sociolinguistic change.\nAt this point, this is largely descriptive work. I don’t know how perceptible these differences are and I’m not even sure if everything I just described is statistically significant. It’ll take additional work to confirm both of these. Trajectories are often ignored because they’re chalked up articulatory causes; are we comfortable saying that trajectories are 100% phonetic and 0% sociolinguistic? (Meanwhile, if I may be a bit snarky, are the arbitrary single-point measurements that are typically analyzed magically sociolinguistically important?) I think people can exploit trajectories for sociolinguistic purposes. I just don’t know how or to what extent yet."
  },
  {
    "objectID": "blog/asa181/index.html#sample-size-matters-when-calculating-pillai-scores",
    "href": "blog/asa181/index.html#sample-size-matters-when-calculating-pillai-scores",
    "title": "ASA181",
    "section": "Sample size matters when calculating Pillai scores",
    "text": "Sample size matters when calculating Pillai scores\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nI’m very excited about this Pillai scores paper with a new colleague, Betsy Sneller! The background for this papers is that a while ago I was analyzing some cot-caught merger data I had collected. I noticed that, without exception, I got higher pillai scores in wordlists than I did in conversational data. I thought I had stumbled upon some interesting style shifting! But it was too clean of a pattern, so I did some digging and found that it’s likely because of the sample size between the two subsets. I had less data from the wordlists than I did from the conversation. I hypothesized then that less data leads to higher Pillai scores.\n\nMethods and Experiements\nSo in this paper, we test this hypothesis specifically by running a bunch of simulations. We started with a single bivariate normal distribution. We then randomly drew 5 numbers from that distribution and called it “group 1.” We then drew another 5 numbers from the exact same distribution and called it “group 2.” The fact that they’re drawn from the same underlying distribution represents a true underlying vowel merger. We then calculated the Pillai score of those two groups. We repeated with these group sizes 1000 times. Then we drew 6 tokens from each group 100 times and calcualted Pillai scores. Then 7. And all the way up to 100.\nAs seen in the main figure in the poster (slightly modified below), the results were clear: the larger the sample sizes, the lower the Pillai scores were. In theory, the Pillai scores should all be around zero since they’re from the same distribution. But with small samples sizes (&lt;10) observations per group, we very often got pretty high Pillai scores—scores that some researchers have considered to be distinct. It took around 30 observations per group to reliably (meaning 95% of the time) get the Pillai score under the somewhat conservative threshold of 0.1. It took 60 observations per group to get Pillai scores reliably below 0.05. This was concerning to us because few sociophonetic studies have sample sizes that large!\n\n\n\n\n\nWe were also concerned about unequal sample sizes betwen groups. So we reran the experiment, except the group sizes weren’t the same size. Each group could be anywhere from 5 to 100 observations, and we ran all 9000 or so combinations. The results were surprising to us—unequal group sizes doesn’t matter at all. The only thing that mattered was the total sample size. You can see this in the figure in the top right of the poster (or below): as you go from bottom-left to top-right, the average log Pillai scoreWe used log(pillai) because it worked better for this visual and for the math. goes from high to low. But the fact that there is no pattern from the top-left to the bottom-right diagonal means that unequal sizes don’t matter.\n\n\n\n\n\nIn other words, if Group A has 25 observations and Group B has 5, the Pillai score will average around 0.07. That’s the same as if you had two groups of 15. If Group A has 25 observations and Group B has 100, the Pillai score will average around 0.01. That’s the same as if you had two groups of 62.\n\n\nImplications\nWe can think of a lot of implications for these findings. For one, mergers are probably underreported and splits/distinctions are probably overreported. This is because many sociophonetic studies run Pillai scores on somewhat smaller samples.\nBecause of sample size differences, comparison across studies is difficult. A study that collects lots of data per person will likely report lower Pillai scores than a study that is based on fewer observations per person.\nGoing back to the main impetus for this paper, comparison within studies is difficult. Since more careful speech styles typically elicit fewer observations, reading tasks will have higher Pillai scores than conversational data. To a naive researcher, this will be interpreted as style differences, when it is really just a reflection of the underlying math! This is such an important point and you can count on hearing more from me and Betsy about this in later venues.\nFinally, one way to overcome the sample size difference is to look at the p-value that comes out the MANOVA test that the Pillai score came from. These p-values do seem to be reported in more phonetics-oriented papers, but for some reason they’re not in sociophonetics papers. So rather than us coming up with arbitrary and ad hoc thresholds for what a merged Pillai score should be, let’s us the p-value instead. Not reporting this p-value, to me at least, is kinda like reporting a t-statistic or F-ratio but without the accompanying p-value.\nAs a final note, and this is more of an after-thought for us, I wonder if it would be more helpful to report log(pillai) rather than raw pillai scores. Since Pillai ranges from 1 (completely distinct) to 0 (complete overlap), log Pillai would range from 0 (completely distinct) to negative infinity (complete overlap). In practical terms, it would mostly be betwen 0 and around –4 (the latter corresponding to a raw Pillai score of about 0.01). We’ll probably talk more about this in other venues so stay tuned for that."
  },
  {
    "objectID": "blog/admission-to-candidacy/index.html",
    "href": "blog/admission-to-candidacy/index.html",
    "title": "Admission to Candidacy",
    "section": "",
    "text": "This morning I successfully defended my second qualifying paper, “Near-Mergers in Cowlitz County, Washington,” which means I’m officially a doctoral candidate! (Okay, actually, a couple forms need to be signed, but that’s no biggie.) What an important step for me!\n\n\n\"Doctoral candidate\" just sounds so much cooler than mere \"Ph.D. student.\" I like my new title :)\n\n— Joey Stanley (@joey_stan) May 4, 2017\n\n\nThe paper I defended was essentially what I presented at the ADS conference in January and at DiVar in February. I presented some of my data that I collected in Washington State and discussed the Mary-merry-marry merger (or lack thereof) and a collection of mergers involving higher back vowels before coda laterals (pool, pull, pole, and pulp). I got some great feedback from my committee.\nWhat are my plans for the rest of my schooling? I’m still deciding if I want to finish my dissertation and graduate next year or the year after. I’ll start things up and see how I’m feeling in six months, but I’m on the fence. On the one hand, while I will probably be able to secure funding for my fifth year, no matter what it is it’s probably going to be less pay than a real job, so I’ll want to graduate after my fourth year. On the other hand, giving myself two years to finish will allow me to really put a lot of time into the dissertation, as well as the couple other sizable side projects I have going on, thus beefing up my CV before graduating.Edit: I ended up graduating in 2020 after my sixth year.😳\nI’ll have to weigh my research goals against my personal, familial, and financial needs and make some decisions at some point. But for now I’m just happy to have made that leap to ABD."
  },
  {
    "objectID": "blog/pillai-scores-dont-change-after-normalization/index.html",
    "href": "blog/pillai-scores-dont-change-after-normalization/index.html",
    "title": "Pillai scores don’t change after normalization",
    "section": "",
    "text": "I was playing around with some data the other day and I discovered that if you calculate the pillai score on raw data you get the same result as if you calculated it on normalized data. This might be common knowledge among sociophoneticians who work with this kind of data, and now that I think about how normalization works, it makes sense. But it’s new to me so I thought I’d write about it and illustrate it.\nIncidentally, this post is also somewhat of a tutorial on how to do a few different vowel normalization procedures and how to calculate pillai scores. I hope to do a separate tutorial on normalization, but I do have a detailed tutorial on calculating vowel overlap already, which you can view here.\nlibrary(tidyverse)\nlibrary(joeyr)\nlibrary(joeysvowels)"
  },
  {
    "objectID": "blog/pillai-scores-dont-change-after-normalization/index.html#calculating-pillai-scores",
    "href": "blog/pillai-scores-dont-change-after-normalization/index.html#calculating-pillai-scores",
    "title": "Pillai scores don’t change after normalization",
    "section": "Calculating Pillai scores",
    "text": "Calculating Pillai scores\nI’ll start by loading a sample dataset. This is a simplified dataset of 10 English speakers from the state of Idaho, which comes from my joeysvowels package. It contains F1–F4 measurements from a random sample of 10 tokens from 11 monophthongs.\n\nidahoans &lt;- joeysvowels::idahoans %&gt;%\n  print()\n\n# A tibble: 1,100 × 7\n   speaker sex    vowel    F1    F2    F3    F4\n   &lt;fct&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 01      female AA     699. 1655. 2019. 3801.\n 2 01      female AA     685. 1360. 1914. 4257.\n 3 01      female AA     713. 1507. 2460. 3617.\n 4 01      female AA     801. 1143. 1868. 2908.\n 5 01      female AA     757. 1258. 1772. 2778.\n 6 01      female AA     804. 1403. 2339. 4299.\n 7 01      female AA     664. 1279. 1714. 2103.\n 8 01      female AA     757. 1325. 1929. 2660.\n 9 01      female AA     730. 1578. 2297. 2963.\n10 01      female AA     700. 1546. 2109. 3432.\n# ℹ 1,090 more rows\n\n\nI’ll start off by normalizing the data in three different ways.\n\nFirst, I’ll do the Lobanov transformation which is pretty common although Santiago Barreda’s forthcoming paper in LVC suggests that we really shouldn’t be using it. I can accomplish the normalization with one line of code using by applying the base R scale function to both the F1 and F2 columns dplyr::across. I’ll create new columns called F1_lob and F2_lob.\nThen, I’ll normalize using the method described in the Atlas of North American English. I won’t go into detail about how this normalization procedure works, but I’ve written it up as a function and made it available in my sandbox joeyr package. To get this to work, I’ll first specify which columns contain the formant measurements to normalize (F1 and F2). I also need to specify which column contains unique values for each vowel token; in this case, since it’s one row per token (i.e. this isn’t trajectory data), I can just supply the row names. I’ll then specify which column contains unique values per speaker.\nFinally, I’ll normalize the data using the ΔF technique described in Johnson (2020). Again, I won’t go into detail, but I wanted to try it out anyway because it’s less common and because it takes into account F3 and F4 as well. I’ve also wrapped that one up into a function in my joeyr package, norm_deltaF, and I just need to specify the formant columns to be used.\n\nSo I can easily incorporate all three of these normalization procedures in just three lines of a tidyverse pipeline. Pretty cool!\n\nidahoans_norm &lt;- idahoans %&gt;%\n  group_by(speaker) %&gt;%\n  mutate(across(c(F1, F2), scale, .names = \"{col}_lob\")) %&gt;%\n  norm_anae(hz_cols = c(F1, F2), token_id = row_names(.), speaker_id = speaker) %&gt;%\n  norm_deltaF(F1, F2, F3, F4) %&gt;%\n  print()\n\n# A tibble: 1,100 × 15\n# Groups:   speaker [10]\n   speaker sex    vowel    F1    F2 F1_anae F2_anae    F3    F4 F1_deltaF\n   &lt;fct&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 01      female AA     699. 1655.    714.   1690. 2019. 3801.     0.637\n 2 01      female AA     685. 1360.    700.   1388. 1914. 4257.     0.624\n 3 01      female AA     713. 1507.    728.   1539. 2460. 3617.     0.650\n 4 01      female AA     801. 1143.    818.   1167. 1868. 2908.     0.730\n 5 01      female AA     757. 1258.    772.   1284. 1772. 2778.     0.689\n 6 01      female AA     804. 1403.    821.   1432. 2339. 4299.     0.733\n 7 01      female AA     664. 1279.    678.   1306. 1714. 2103.     0.605\n 8 01      female AA     757. 1325.    773.   1353. 1929. 2660.     0.690\n 9 01      female AA     730. 1578.    746.   1611. 2297. 2963.     0.665\n10 01      female AA     700. 1546.    715.   1578. 2109. 3432.     0.638\n# ℹ 1,090 more rows\n# ℹ 5 more variables: F2_deltaF &lt;dbl&gt;, F3_deltaF &lt;dbl&gt;, F4_deltaF &lt;dbl&gt;,\n#   F1_lob &lt;dbl[,1]&gt;, F2_lob &lt;dbl[,1]&gt;\n\n\nI’ll focus on the cot-caught merger here. To calculate the pillai scores then, I can use another function in joeyr called pillai. To use it, I’ll first need to only include the vowels I want run the pillai score on. I then put in the formula that I’d normally use in manova(). I’ll do that four times, once for the raw data and then once for each of the normalizations.\n\nidahoans_pillai &lt;- idahoans_norm %&gt;%\n  filter(vowel %in% c(\"AA\", \"AO\")) %&gt;%\n  summarize(pillai_raw    = pillai(cbind(F1,        F2)        ~ vowel),\n            pillai_lob    = pillai(cbind(F1_lob,    F2_lob)    ~ vowel),\n            pillai_anae   = pillai(cbind(F1_anae,   F2_anae)   ~ vowel),\n            pillai_deltaF = pillai(cbind(F1_deltaF, F2_deltaF) ~ vowel)) %&gt;%\n  print()\n\n# A tibble: 10 × 5\n   speaker pillai_raw pillai_lob pillai_anae pillai_deltaF\n   &lt;fct&gt;        &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n 1 01           0.592      0.592       0.592         0.592\n 2 02           0.500      0.500       0.500         0.500\n 3 03           0.763      0.763       0.763         0.763\n 4 04           0.663      0.663       0.663         0.663\n 5 05           0.557      0.557       0.557         0.557\n 6 06           0.136      0.136       0.136         0.136\n 7 07           0.412      0.412       0.412         0.412\n 8 08           0.310      0.310       0.310         0.310\n 9 09           0.173      0.173       0.173         0.173\n10 10           0.267      0.267       0.267         0.267\n\n\nAt a glance, you can see that the scores are pretty much the same for all 10 speakers across all the versions of the data. We can test this for sure using the dplyr::near function. Technically, due to rounding errors, the numbers could be very slightly off from each other, so near checks for whether two number are—for all intents and purposes—identical.\n\nidahoans_pillai %&gt;%\n  ungroup() %&gt;%\n  mutate(sameAB = near(pillai_raw, pillai_lob),\n         sameAC = near(pillai_raw, pillai_anae),\n         sameAD = near(pillai_raw, pillai_deltaF)) %&gt;%\n  print()\n\n# A tibble: 10 × 8\n   speaker pillai_raw pillai_lob pillai_anae pillai_deltaF sameAB sameAC sameAD\n   &lt;fct&gt;        &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;lgl&gt;  &lt;lgl&gt;  &lt;lgl&gt; \n 1 01           0.592      0.592       0.592         0.592 TRUE   TRUE   TRUE  \n 2 02           0.500      0.500       0.500         0.500 TRUE   TRUE   TRUE  \n 3 03           0.763      0.763       0.763         0.763 TRUE   TRUE   TRUE  \n 4 04           0.663      0.663       0.663         0.663 TRUE   TRUE   TRUE  \n 5 05           0.557      0.557       0.557         0.557 TRUE   TRUE   TRUE  \n 6 06           0.136      0.136       0.136         0.136 TRUE   TRUE   TRUE  \n 7 07           0.412      0.412       0.412         0.412 TRUE   TRUE   TRUE  \n 8 08           0.310      0.310       0.310         0.310 TRUE   TRUE   TRUE  \n 9 09           0.173      0.173       0.173         0.173 TRUE   TRUE   TRUE  \n10 10           0.267      0.267       0.267         0.267 TRUE   TRUE   TRUE  \n\n\nYeah, so it looks like they’re equal. Regardless of whether you run the pillai score on raw data, data that’s been normalized where F1 and F2 are adjusted independently (Lobanov), data that’s been normalized where F1 and F2 are treated the same (ANAE normalization), or whether F3 and F4 are also included (ΔF), the results are going to be the same."
  },
  {
    "objectID": "blog/pillai-scores-dont-change-after-normalization/index.html#why-does-this-work",
    "href": "blog/pillai-scores-dont-change-after-normalization/index.html#why-does-this-work",
    "title": "Pillai scores don’t change after normalization",
    "section": "Why does this work?",
    "text": "Why does this work?\nOkay, so we’ve established that they’re the same then. But why?\nWell, you may know that normalization doesn’t change the relative positions of the F1 or F2 measurements to each other. It just sort of stretches them out and recenters them. You can think of normalization as taking image of vowel plots (one for each speaker), pasting them in PowerPoint or something, and then pulling the corner of them so that they’re all the same size. You can also drag them around so that they’re on top of each other. In the case of log-scale normalization procedures (like ANAE and ΔF) that apply to F1 and F2 at the same time, you can only move them along a diagonal line going from the bottom left to the top right. For Lobanov, you’re free to reposition them any way you’d like. The point is, normalization just stretches/contracts and recenters vowel spaces rather changing individual points’ relative positions.In the case of Lobanov, you actually can only adjust one side and then the other, rather than pulling on the corner, which results in some distortion.\nI can kinda illustrate this by visualizing the same speaker’s data four different ways: once for the raw data and once for each normalization type. First, I’ll have to reshape it a little bit so that all the F1 values are in the same column. I can do that with pivot_longer. Before I do that, I’ll change the raw F1 column to F1_raw so that the names all have the same template of {formant}_{procedure}.\n\nidahoans_reshaped &lt;- idahoans_norm %&gt;%\n  rename_with(~paste0(., \"_raw\"), c(F1, F2, F3, F4)) %&gt;%\n  rename_with(~str_replace(., \"norm\", \"delta_F\"), ends_with(\"norm\")) %&gt;%\n  pivot_longer(cols = starts_with(\"F\"),\n               names_to = c(\".value\", \"method\"),\n               names_pattern = c(\"(F\\\\d)_(\\\\w+)\"),\n               values_to = \"value\") %&gt;%\n  print()\n\n# A tibble: 4,400 × 8\n# Groups:   speaker [10]\n   speaker sex    vowel method  F1[,1]   F2[,1]      F3      F4\n   &lt;fct&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 01      female AA    raw    699.    1655.    2019.   3801.  \n 2 01      female AA    anae   714.    1690.      NA      NA   \n 3 01      female AA    deltaF   0.637    1.51     1.84    3.46\n 4 01      female AA    lob      1.15    -0.341   NA      NA   \n 5 01      female AA    raw    685.    1360.    1914.   4257.  \n 6 01      female AA    anae   700.    1388.      NA      NA   \n 7 01      female AA    deltaF   0.624    1.24     1.74    3.88\n 8 01      female AA    lob      1.04    -0.906   NA      NA   \n 9 01      female AA    raw    713.    1507.    2460.   3617.  \n10 01      female AA    anae   728.    1539.      NA      NA   \n# ℹ 4,390 more rows\n\n\nI’ll then pick a speaker, we’ll say speaker 02, and just get their two low back vowels. (There will be NAs in the F3 and F4 columns because they were not involved in the Lobanov or ANAE normalizations.)\n\ns02_reshaped &lt;- idahoans_reshaped %&gt;%\n  filter(speaker == \"02\", \n         vowel %in% c(\"AA\", \"AO\")) %&gt;%\n  print()\n\n# A tibble: 80 × 8\n# Groups:   speaker [1]\n   speaker sex   vowel method  F1[,1]   F2[,1]      F3      F4\n   &lt;fct&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 02      male  AA    raw    449.    1081.    2593.   3047.  \n 2 02      male  AA    anae   592.    1423.      NA      NA   \n 3 02      male  AA    deltaF   0.478    1.15     2.76    3.24\n 4 02      male  AA    lob      0.196   -0.833   NA      NA   \n 5 02      male  AA    raw    582.    1105.    2593.   3757.  \n 6 02      male  AA    anae   766.    1456.      NA      NA   \n 7 02      male  AA    deltaF   0.619    1.18     2.76    4.00\n 8 02      male  AA    lob      1.68    -0.765   NA      NA   \n 9 02      male  AA    raw    512.    1152.    2238.   3655.  \n10 02      male  AA    anae   675.    1517.      NA      NA   \n# ℹ 70 more rows\n\n\nI can then plot those four together.\n\nggplot(s02_reshaped, aes(F2, F1, color = vowel)) + \n  geom_point() + \n  stat_ellipse(level = 0.67) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  ggthemes::scale_color_ptol() + \n  facet_wrap(~method, scales = \"free\") + \n  theme_minimal() + \n  labs(title = \"Four versions of speaker 02's low back vowels\",\n       subtitle = \"To quote Pam Beesly, \\\"They're the same picture.\\\"\")\n\n\n\n\nSo, as you can see, the plots are virtually identical, other than the scales of the x- and y-axes. This might not be the best illustration of what’s going on because ggplot2 will automatically stretch and center the data, but maybe it works for you?"
  },
  {
    "objectID": "blog/pillai-scores-dont-change-after-normalization/index.html#conclusion",
    "href": "blog/pillai-scores-dont-change-after-normalization/index.html#conclusion",
    "title": "Pillai scores don’t change after normalization",
    "section": "Conclusion",
    "text": "Conclusion\nSo there you have it. It doesn’t matter if you calculate pillai scores before or after normalization—at least with the three procedures used here—the results are going to be the same regardless."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html",
    "href": "blog/the-importance-of-twitter/index.html",
    "title": "The Importance of Twitter",
    "section": "",
    "text": "I’m preparing a workshop right now for the DigiLab here at UGA on how to increase your web presence. I’ll give a more detailed explanation of that later on, but I just wanted to point how how cool Twitter has been for me.\nI don’t remember when or why I got a Twitter account, but I remember early on that I wanted to keep it professional. I don’t follow very many friends or family: just other random linguists I find. That means my feed is nothing but linguistics stuff, and mini posts that other linguists find interesting. Granted, a lot of these folks post non-linguistic stuff as well, so I do have to sift through those sometimes. But there have been some really valuable gems I’ve found because of Twitter."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#fun-stuff",
    "href": "blog/the-importance-of-twitter/index.html#fun-stuff",
    "title": "The Importance of Twitter",
    "section": "Fun Stuff",
    "text": "Fun Stuff\nFirst, we’ll start with the fun stuff.\n\n\nCheck out @JWGrieve’s wordmapper app (before it gets overwhelmed by traffic!) – plots Twitter usage across U.S.: https://t.co/9nBh3h9z9v\n\n— Ben Zimmer (@bgzimmer) January 29, 2016\n\n\nThis didn’t lead to anything in my work, but it was pretty awesome to see what Jack Grieve had done. In case the link doesn’t load above, it shows an interactive program where you can type in a word and see its regional distribution across Twitter. It’s a lot of fun to play around with."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#datasets",
    "href": "blog/the-importance-of-twitter/index.html#datasets",
    "title": "The Importance of Twitter",
    "section": "Datasets",
    "text": "Datasets\nTwitter has also been good for me to discover new datasets. This tweet for example let me know that the entire contents of Reddit had been extracted and were available for download.\n\n\n1 terabyte corpus of Reddit comments, up to may 2015, from @internetarchive. What a glorious day http://t.co/cHtmhKZyHW\n\n— heather froehlich (@heatherfro) July 9, 2015\n\n\nAbout a month or so later I was starting a course in Digital Humanities, so this corpus became the main tool for my term paper for that class. I ended up downloading all the Reddit comments from its inception (2007) until October 2015. It was a whopping 50 billion words of text sitting on a terabyte of storage. If this were printed on standard book-sized sheets of paper, it would be something like 2½ miles long! And growing at about 4 feet an hour. That’s a lot of data. I don’t have a terabyte of storage available for something like this, so I wrote a Perl script that cut it down to a hundredth of its original size (“only” 500 million words!).\nI ended up having to download it all using lab computers in the student center off and on for a week. In fact, I had four computers going simultaneously, all downloading Reddit files, one month at a time, and running Perl scripts to make them smaller. I wasn’t surprised when IT came over and wondered what the heck I was doing. Turns out it was the fact that my login was used on four computers that triggered their systems, not the fact that I was running four computers at full speed for a couple hours. Once they saw what I was doing they shrugged their shoulders and said it was totally fine.\nHandling this much data, even though it was a hundredth of the original size, was rough. I made a frequency list of all the words, which ended up being about half a million rows long. I wanted to track language across time so I had information about how often each word was used every month for about 100 months. That’s a lot of columns for all those rows. I pushed Excel (and my little laptop) to its limits.\nAnyway, this project turned into a fun term paper that I never published. I wanted to look at the language of the most upvoted comments as compared to all other comments and see if there were any differences. I found a few, but with biggish data like this, statistical significance is everywhere so you have to be more careful about things.\nBottom line: Because of Twitter I got to work with an enormous corpus which was a lot of fun."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#new-methodology",
    "href": "blog/the-importance-of-twitter/index.html#new-methodology",
    "title": "The Importance of Twitter",
    "section": "New Methodology",
    "text": "New Methodology\nOn Twitter people also post new things they see at conferences and other places. During NWAV44, I followed the live tweets and saw this gem:\n\n\n@wgi_02445_temp has given us another gift. Bhattacharyya’s affinity to measure overlap: https://t.co/26byi2KpRk #NWAV44\n\n— Paul De Decker (@pmdedecker) October 25, 2015\n\n\nBasically, Daniel Johnson talked about another way to measure vowel overlap—something I do a lot in my research. In the Shiny App linked above, Johnson compares Pillai scores and something called Bhattacharyya affinity. I ended up using this in a poster (abstract here) I did with Peggy Renwick at LabPhon, and will continue to use this new measure of overlap, not exclusively, but in addition to the other measures out there."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#live-tweeting-conferences",
    "href": "blog/the-importance-of-twitter/index.html#live-tweeting-conferences",
    "title": "The Importance of Twitter",
    "section": "Live Tweeting Conferences",
    "text": "Live Tweeting Conferences\nI’m a lowly grad student and don’t have a ton of funding for conferences, so I can’t attend some of the big ones all the time. Luckily, a lot of people live tweet what’s going on at most major conferences, so I can follow along and feel like a part of the group.\nI myself live tweeted for the first time at a linguistics conference here at UGA. I don’t have a ton of followers, and the conference isn’t super well-known. But I did try to find people’s Twitter handles whenever possible, as well as their department’s, and would include them in the tweets. Well as it turns out I got about half a dozen new followers from that conference. Not a huge deal, but it does spread my name just a little bit further, and maybe onto the right person’s computer screen."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#twitter-is-great",
    "href": "blog/the-importance-of-twitter/index.html#twitter-is-great",
    "title": "The Importance of Twitter",
    "section": "Twitter is great",
    "text": "Twitter is great\nSo in the end, having a Twitter account is a lot of fun. I’ve benefited personally and professionally, and it’s definitely a worthy investment of my time."
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "Joey Stanley",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "todo.html#top-level-pages",
    "href": "todo.html#top-level-pages",
    "title": "Joey Stanley",
    "section": "Top-level pages",
    "text": "Top-level pages\nThings to add to idiolect * American Raising detail. When that’s done, go back to /ar/-raising post and link to it. * /ar/-Raising, with link to the blog post. * The video from this tweet: https://twitter.com/joey_stan/status/1440022805039353856?s=20&t=RLBVBLgh7rnp0TMTUqnObg"
  },
  {
    "objectID": "todo.html#css-and-appearance",
    "href": "todo.html#css-and-appearance",
    "title": "Joey Stanley",
    "section": "CSS and appearance",
    "text": "CSS and appearance\nstyles.css * Make sure the small caps tables in all lexical sets posts look as good as they do in the original (I’ll have to rewrite the tables in raw HTML with my custom CSS classes)\nAll blogs * If I don’t like the cover image on blog posts, switch it to something I do lik (not sure if I can do this with rendered images)\nUpdate images from the joeystanley theme * Testing VOT Durations * /ar/-raising * LCUGA6"
  },
  {
    "objectID": "todo.html#blogs",
    "href": "todo.html#blogs",
    "title": "Joey Stanley",
    "section": "Blogs",
    "text": "Blogs\nLinking to past blog posts * Link to Pillai paper in the normalization and pillai post\nLinking to future posts * Any time a conference paper eventually got published, put a link on the page somewhere with that published version. (Maybe the top of the page?) - Pillai scores has a bit of a thread, starting with ASA181, then NWAV50. - The Gen X–Boomer thing can be traced pretty far back. At least since my LCUGA presentation. But we first showed it more clearly in ADS2019. * In publication announcement posts, standardize the filename. Follow the format of the prevelar paper in AmSp and put the timeline.\nFuture blog posts * That one article and my system for keeping track of my research * Tutorial about colors in the feeding Douglas plot * Showing my Praat scripts for pre- and post-processing MFA. I mention it in /blog/transcribing a sociolinguistic corpus * Retroactively do a blog post about every publication and conference. * Retroactively do a student kudos blog posts. * Retroactively flesh out the interpreting difference smooths blog post from colloquium 2020 * Follow-up on /ar/-raising but with GAMMs. Towards the bottom of the /ar/ post, I say I don’t know GAMMs yet. * Kohler Tapes update based on more complete metadata. * Use that vowel overlap package I tweeted about one time and try out Joe Fruehwald’s package too. * Randomizing a wordlist works, but it’s not super clear. Might be better to have worked-through examples. * any other big tweets * Pillai scores according to me and Betsy (basically the supplemental material). Do this as a “part 3” and a replacement to my current pillai scores tutorials."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I am currently teaching…\nPreviously, I’ve taught…\nThe rest of this page serves as a repository for teaching materials and workshops I have prepared."
  },
  {
    "objectID": "teaching.html#courses",
    "href": "teaching.html#courses",
    "title": "Teaching",
    "section": "Courses",
    "text": "Courses\n\nPhonetics and Phonology\nI taught this course Fall 2017 and Spring 2019 at UGA. Here is a syllabus.\nPraat Tutorial—For homework, I often assign students mini-projects that involve the use of Praat. This assignment serves as a useful tutorial to downloading and running Praat for the first time. In it, I give detailed instructions on how to make and transcribe a sentence.\nFeatures Chart—When I was preparing to teach features for the first time, I found it hard to keep track of all of them so I started making a chart. It turned out to be pretty useful and I thought my students might like it too. It’s meant to act as reference rather than instruction, so you’ll still have to read through the relevant chapters to understand everything. It’s a lot of information to cram into a single page, so it’s still a work in progress to make it look better. It’s sort of one of those things that makes sense to the person who made it and no one else, so you’re milage may vary. I should have an accompanying “How to use this chart” guide. Maybe next time.\n\n\nEnglish Phonetics and Phonology\nI taught this Winter 2022 and Winter 2023. It’s similar to the Phonetics and Phonology course I taught already, but it cuts out most non-English material and goes into more depth into English. Here is a syllabus.\n\n\nIntroduction to Sociolinguistics\nI taught this undergraduate version of sociolinguistics in Winter 2021. You can look through the syllabus here. We followed Allan Bell’s The Guidebook to Sociolinguistics for the most part, though I threw in some additional topics towards the end.\n\n\nSociolinguistics\nI taught this course in Winter 2021, Fall 2021, and Fall 2022. This is the graduate-level version of the Introduction to Sociolinguistics course above (so it’s still introductory course) and is similar to the undergraduate version except we take one day a week to read and discuss articles. Here is my most recent syllabus.\n\n\nSpecial Topics: African American English\nI taught this course Spring of 2023 under the auspices of the senior capstone course, as well as as a linguistics elective, in the linguistics department. (So, technically it was cross-listed as LING 495R: Senior Capstone, ELING 495R: The Senior Course, LING 421R: Studies in Linguistics, and ELING 421R: Studies in Language or Editing).\nThe capstone courses in my deaprtment are designed to be open-ended and end up being special topics courses that cover a wide range of subfields in linguistics. Most of the faculty who had been teaching these capstone courses though had recently retired, so we have been in dire need for new courses to put into the rotation. I had wanted to teaching something about language ideology/discrimination/attitudes but I was asked to do something to do with American English; we settled on African American English, which I think was a nice compromise.\nSo, with the help and input from many friends and colleagues, I designed and prepped the course from scratch, using Lisa Green’s African American English as my main textbook. I went into the course knowing some things here and there about AAE from just being exposed to the literature on it in sociolinguistics, and it turned out to be a very challenging but richly rewarding course for me. Here is the syllabus.\n\n\nSpecial Topics: Sociolinguistic Fieldwork\nThis is a special topics course that I taught with Lisa Johnson in Winter 2023. We used Natalie Schilling’s textbook of the same name as our guide and did a Utah County–based implementation of similar courses taught in other universities. (Our “fieldsite” that we chose was the city of Lehi.) The course involved going out to the field to conduct sociolinguistic interviews. You can see a syllabus here.\n\n\nIntroduction to Varieties of English\nI’ve taught this course four times so far. In Summer 2020, it was completely online and had a focus on the British Isles to make up for the study abroad to the UK and Ireland that was cancelled due to covid. In Fall 2020, Fall 2021, and Fall 2022 we covered around 30 varieties of English in three broad units: North America, the British Isles, and everywhere else. Here is my syllabus from Fall 2022.\n\n\nLinguistics Tools 1\nI taught this course in Fall 2020 and Fall 2021. It covers how to conduct a linguistics study and covers software like Zotero, Word, Excel, Praat, AntConc, Qualtrics, and Jamovi with an introduction to statistics scattered throughout. Here is a syllabus from when I taught it in Fall 2021.\n\n\nSpecial Topics: Linguistic Data Analysis\nThis is a special topics course that I taught with Earl Brown in Fall 2022. We be built upon the version of the course he taught in Winter 2020. Here is the syllabus we started off with (though it did change quite a bit in the end).\n\n\nResearch Design in Linguistics\nI team-taught this course in Winter 2021 with Dan Dewey and then by myself Winter 2022. Students end the semester with a prospectus for their MA thesis and walk away with some introductory statistics. Here is my most recent syllabus."
  },
  {
    "objectID": "teaching.html#workshops",
    "href": "teaching.html#workshops",
    "title": "Teaching",
    "section": "Workshops",
    "text": "Workshops\n\nLaTeX Workshops\nCaleb Crumley, Jonathan Crum, and I led a series of three workshops on LaTeX as a way to introduce the new UGA Dissertation LaTeX template. I discussed basic LaTeX skills, Caleb showed how to use the template, and Jonathan illustrated more advanced topics. This series got the stamp of approval from the UGA Graduate School and had over 100 registered attendees.\n\n\nPraat Scripting Workshops\nLisa Lipani and I led three workshops on Praat and Praat scripting. We discussed the basics of the Praat interface, how to code basic things, and then did one devoted to automatic extraction of formant measurements.\n\n\nData Visualization Workshops\nIn a suite of five workshops, I discussed data visualization. Three of them were focused on ggplot2 (see the R workshops) but two were platform-independent and discussed the use of color and Edward Tufte’s principles of data visualization. Though they’re outside my discipline, I really enjoyed these workshops.\n\n\nR Workshops\nI have led more than a dozen workshops on R on a variety of topics. In addition to just an intro to R, I’ve talked about the tidyverse, RMarkdown, and Shiny. I’ve done a handful just on ggplot2, and I have handouts ready for more detailed workshops on customizing plots in ggplot2 for future workshops.\n\n\nProfessionalization (“Brand Yourself”) Workshops\nIn this workshops, I discuss ways to boost your online presence as an academic. I’ve given this workshop several times over the past few years and it has evolved quite a bit based on my own experiences. The gist: make a website and possibly also get active on Twitter.\n\n\nAcademic Poster Workshop\nI gave a brief—and very opinionated—workshop on how to make an academic poster. I discuss overall design and layout, nit-picky things like font sizes and color, and content.\n\n\nExcel Workshop\nI once gave a workshop on Excel. It was a while ago."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "This page highlights some of my research, organized thematically. I am a dialectologist, so several of my research interests are on specific regions. But intersecting these regions is where the sociolinguist in me comes out, and I try to uncover the meaning in linguistic variants—particularly the infrequent ones. And I go about these questions in these areas as a phonetician, analyzing speech production data and vowel trajectories. Finally, in nearly every study, I put on my quantitative linguist/data scientist/statistician cap when I actually do the analysis and interpretation. I think it’s a great intersection of subdisciplines within linguistics and I’m kept on my toes trying to keep up with each field."
  },
  {
    "objectID": "research/index.html#utah-english",
    "href": "research/index.html#utah-english",
    "title": "Research",
    "section": "Utah English",
    "text": "Utah English\n\n\n\nMy current research focus is on language in Utah. One focus is simply on documenting what linguistic features can be found. There is a more traditional Utah accent that people are aware of. I’d like to document that as best I can before its speakers die out. But then there is a newer, contemporary Utah accent that few people are commenting on. Besides that, there is a lot to be said about people’s perceptions about Utah English, especially when religion is added to the mix. I haven’t published much on Utah English yet, but I expect it to be a central part of my research agenda for a while.\n\n\n\n\n\n\n\nCV Highlights\nJoseph A. Stanley. “Utahns sound Utahn when they avoid sounding Utahn.” The 97th Annual Meeting of the Linguistic Society of America. Denver, CO. January 6. 2023. \nJoseph A. Stanley & Lisa Morgan Johnson. “Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English.” The 96th Annual Meeting of the Linguistic Society of America. Washington, D.C. January 6–9, 2022. \nJoseph A. Stanley (2019). “(thr)-Flapping in American English: Social factors and articulatory motivations.” Proceedings of the 5th Annual Linguistics Conference at UGA, 49–63.  \nJoseph A. Stanley & Kyle Vanderniet (2018). “Consonantal Variation in Utah English.” Proceedings of the 4th Annual Linguistics Conference at UGA, 50–65."
  },
  {
    "objectID": "research/index.html#cowlitz-county-washington",
    "href": "research/index.html#cowlitz-county-washington",
    "title": "Research",
    "section": "Cowlitz County, Washington",
    "text": "Cowlitz County, Washington\n\n\n\nMy dissertation was on English in Cowlitz County, Washington, specifically on the front lax and low back vowels. I used that data to also write about prevelar raising and a few other things. While Washington is not my area of focus anymore, I still use the data for lots of side projects.\n\n\n\n\n\n\n\nCV Highlights\nJoseph A. Stanley. “Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift.” Poster presentation at the 181st Meeting of the Acoustical Society of America (ASA). Seattle, WA. November 29, 2021. \nJoseph A. Stanley (2020). “The Absence of a Religiolect among Latter-day Saints in Southwest Washington.” In Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall (eds.) Speech in the Western States Volume III: Understudied Dialects. (Publication of the American Dialect Society 105), 95–122. Durham, NC: Duke University Press. https://doi.org/10.1215/00031283-8820642. \nJoseph A. Stanley (2020). “Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington.” Ph.D Dissertation. University of Georgia, Athens, GA. \nJoseph A. Stanley (2018). “Changes in the Timber Industry as a Catastrophic Event: bag-Raising in Cowlitz County, Washington” Penn Working Papers in Linguistics, 24(2)."
  },
  {
    "objectID": "research/index.html#the-south",
    "href": "research/index.html#the-south",
    "title": "Research",
    "section": "The South",
    "text": "The South\n\n\n\nHaving firmly established myself as a researcher of Western American English, I was reluctant to start doing research on the South since the amount of existing literature is vast. But, as a research assistant for the Linguistic Atlas Project for four years, I couldn’t help but begin analysis on DASS, a newly transcribed corpus of interviews from the 1970s and 1980s. Together with Peggy Renwick and the others at the Linguistic Atlas team, we’ve dug deep into the phonetics of how southerners, and more specifically Georgians, sounded a couple generations ago. You can see and interact with some of this data by going a site I made called the Gazetteer of Southern Vowels.\n\n\n\n\n\n\n\nCV Highlights\nMargaret E. L. Renwick, Joseph A. Stanley, Jon Forrest, & Lelia Glass (2023). “Boomer Peak or Gen X Cliff? from SVS to LBMS in Georgia English.” Language Variation and Change. https://doi.org/10.1017/S095439452300011X.  \nJoseph A. Stanley (2022). “A comparison of turn-of-the-century and turn-of-the-millennium speech in Georgia.” Proceedings of the 6th Annual Linguistics Conference at UGA. Linguistics Society at UGA, Athens, GA.  \nJoseph A. Stanley, Margaret E. L. Renwick, Katie Ireland Kuiper, & Rachel Miller Olsen (2021). “Back vowel dynamics and distinctions in Southern American English.” Journal of English Linguistics 49(4): 389–418. https://doi.org/10.1177/00754242211043163. \nMargaret E. L. Renwick & Joseph A. Stanley (2020). “Modeling dynamic trajectories of tense vs. lax vowels in the American South.” Journal of the Acoustical Society of America 147(1): 579–595. doi: 10.1121/10.0000549. \nRachel M. Olsen, Michael L. Olsen, Joseph A. Stanley, Margaret E. L. Renwick, & William A. Kretzschmar, Jr. (2017). “Methods for transcription and forced alignment of a legacy speech corpus.” Proceedings of Meetings on Acoustics 30, 060001; doi: http://dx.doi.org/10.1121/2.0000559."
  },
  {
    "objectID": "research/index.html#methods-in-sociophonetic-data-analysis",
    "href": "research/index.html#methods-in-sociophonetic-data-analysis",
    "title": "Research",
    "section": "Methods in Sociophonetic Data Analysis",
    "text": "Methods in Sociophonetic Data Analysis\n\n\n\nI’ve become increasingly becoming preoccupied with how sociophoneticians process their data. Sometimes when I’m in the throes of data analysis, I realize that something about the analysis itself is worth exploring. I end up getting side-tracked, notice something important that I need to tell the field about, and then never get back to what it was I was originally writing about! I quite enjoy these projects but I hope I can get back to doing more sociolinguistic research.\n\n\n\n\nCV Highlights\nJoseph A. Stanley & Betsy Sneller. 2023. Sample size matters in calculating Pillai scores. Journal of the Acoustical Society of America 153(1). 54–67. https://doi.org/10.1121/10.0016757. \nJoseph A. Stanley (2022). “Interpreting the Order of Operations in Sociophonetic Analysis.” Linguistics Vanguard 8(1). 279–289. https://doi.org/10.1515/lingvan-2022-0065. \nJoseph A. Stanley (2022) “Order of Operations in Sociophonetic Analysis,” University of Pennsylvania Working Papers in Linguistics: Vol. 28: Iss. 2, Article 17. Available at: https://repository.upenn.edu/pwpl/vol28/iss2/17."
  },
  {
    "objectID": "research/index.html#vowel-formant-trajectories",
    "href": "research/index.html#vowel-formant-trajectories",
    "title": "Research",
    "section": "Vowel Formant Trajectories",
    "text": "Vowel Formant Trajectories\n\n\n\nTraditional sociophonetic research analyzes vowels using a pair of measurements, usually somewhere near the midpoint of the vowel. We can get a greater understanding of vowels by extracting data at multiple timepoints per vowel. In particular, I think there could be important sociolinguistic meaning encoded in formant trajectories. Much of my recent research has gone this route, and has used generalized additive mixed-effects models to analyze the resulting data. This type of analysis yields complex results and visualizations, but I think we’re starting to get a better idea of how vowels work while uncovering exciting new research questions and possibilities.\n\n\n\n\n\n\n\nCV Highlights\nJoseph A. Stanley & Lisa Morgan Johnson. “Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English.” The 96th Annual Meeting of the Linguistic Society of America. Washington, D.C. January 6–9, 2022. \nJoseph A. Stanley, Margaret E. L. Renwick, Katie Ireland Kuiper, & Rachel Miller Olsen (2021). “Back vowel dynamics and distinctions in Southern American English.” Journal of English Linguistics 49(4): 389–418. https://doi.org/10.1177/00754242211043163.\nJoseph A. Stanley. “Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift.” Poster presentation at the 181st Meeting of the Acoustical Society of America (ASA). Seattle, WA. November 29, 2021. \nJoseph A. Stanley (2020). “The Absence of a Religiolect among Latter-day Saints in Southwest Washington.” In Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall (eds.) Speech in the Western States Volume III: Understudied Dialects. (Publication of the American Dialect Society 105), 95–122. Durham, NC: Duke University Press. https://doi.org/10.1215/00031283-8820642.\nJoseph A. Stanley (2020). “Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington.” Ph.D Dissertation. University of Georgia, Athens, GA. \nMargaret E. L. Renwick & Joseph A. Stanley (2020). “Modeling dynamic trajectories of tense vs. lax vowels in the American South.” Journal of the Acoustical Society of America 147(1): 579–595. doi: 10.1121/10.0000549."
  },
  {
    "objectID": "research/index.html#infrequent-phonological-variables",
    "href": "research/index.html#infrequent-phonological-variables",
    "title": "Research",
    "section": "Infrequent Phonological Variables",
    "text": "Infrequent Phonological Variables\n\n\n\nI’m interested in phenomena on the margins of English phonology. There are some speech patterns that are quite infrequent because the particular sequence of sounds only exists in a handful of words. For example, I’ve found phonological and regional patterns in words with /ɛɡ/ (beg, leg). I’ve also found that people in Utah have a tap in /θɹ/ clusters (three, throw) but Washingtonians don’t. Utahns also insert stops in /ls/ clusters (false, salsa). How infrequent can a variable be and still exhibit language-internal, regional, and sociolinguistic variability? Turns out, I’m discovering, pretty infrequent!\n\n\n\n\n\n\n\nCV Highlights\nJoseph A. Stanley (2022). “Regional patterns in prevelar raising.” American Speech. 97(3): 374–411. 97(3): 374–411. http://doi.org/10.1215/00031283-9308384.\nJoseph A. Stanley & Lisa Morgan Johnson. “Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English.” The 96th Annual Meeting of the Linguistic Society of America. Washington, D.C. January 6–9, 2022. \nJoseph A. Stanley. “Methodological considerations in the study of infrequent phonological variables: The case of English /eɡ/ and /ɛɡ/.” Word-specific phenomena in the realization of vowel categories: Methodological and theoretical perspectives (LabPhon 17 Satellite Workshop). Vancouver, British Columbia[Online]. September, 2020.\nJoseph A. Stanley (2019). “Phonological Patterns in beg-Raising.” UGA Working Papers in Linguistics, 4, 69–91.  \nJoseph A. Stanley (2019). “(thr)-Flapping in American English: Social factors and articulatory motivations.” Proceedings of the 5th Annual Linguistics Conference at UGA, 49–63.  \nJoseph A. Stanley & Kyle Vanderniet (2018). “Consonantal Variation in Utah English.” Proceedings of the 4th Annual Linguistics Conference at UGA, 50–65."
  },
  {
    "objectID": "research/index.html#mormonese",
    "href": "research/index.html#mormonese",
    "title": "Research",
    "section": "“Mormonese”",
    "text": "“Mormonese”\n\n\n\nMormons, more properly referred to as members of the Church of Jesus Christ of Latter-day Saints, were once distinct enough to be considered an ethnic group. Today, their behavior, dress, and culture are much more mainstream, but their speech patterns might not always be. I’m curious about how Mormons talk, particularly in relation to non-Mormons and ex-Mormons. I’d also like to disentangle Mormonese from Utah English, because while they must overlap, I think they’re different in subtle ways. Working at Brigham Young University will certainly give me better access to this population and I hope to soon uncover some new findings on Mormonese.\n\n\n\n\n\n\n\nCV Highlights\nJoseph A. Stanley (2020). “The Absence of a Religiolect among Latter-day Saints in Southwest Washington.” In Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall (eds.) Speech in the Western States Volume III: Understudied Dialects. (Publication of the American Dialect Society 105), 95–122. Durham, NC: Duke University Press. https://doi.org/10.1215/00031283-8820642. \n\n\nStanley, Joseph A. (2016). “When do Mormons Call Each Other by First Name?” University of Pennsylvania Working Papers in Linguistics, 22(1)."
  },
  {
    "objectID": "index.html#what-am-i-up-to-right-now",
    "href": "index.html#what-am-i-up-to-right-now",
    "title": "About Me",
    "section": "What am I up to right now?",
    "text": "What am I up to right now?\nAs of December 13, 2023, I am technically still on parental leave, but next semester’s courses aren’t going to prep themselves, so I’m back to work mostly full-time. So, right now, I am…\n\n👨🏻‍🏫 Prepping Phonetics and Phonology, Applied Phonetics, and Research Methods courses.\n\n📖About to submit the full draft of an edited volume to the publisher (with Peggy Renwick & Monica Nesbitt).\n👨🏻‍🏫 Preparing three presentations for LSA and ADS in January.\n\n\n\n\n\n🎹 Prepping lots of Christmas music to be played on the organ at church."
  },
  {
    "objectID": "blog/a-tutorial-in-calculating-vowel-overlap/index.html",
    "href": "blog/a-tutorial-in-calculating-vowel-overlap/index.html",
    "title": "A tutorial in measuring vowel overlap in R",
    "section": "",
    "text": "Note\n\n\n\nPlease be aware that in the 2023 revision to this tutorial, some of the exact numbers changed because I’m using a slightly different dataset. There was also a very slight change in the SpatialPointsDataFrame function. Otherwise everything should work fine.\nIn the past two days, I’ve had four people ask about the Pillai score or Bhattacharyya’s Affinity. What does it mean, how is it calculated, and how do you get it in R? I figure if there’s a need for a clear tutorial on these two measurements, perhaps I should go ahead and write one.\nIn the past, I’ve done tutorial on visualizing vowel data (part 1 and part 2) and on extracting vowel formants in Praat. Calculating vowel overlap seems to be the next logical step. I’ve got a couple others in the works too, so stay tuned.\nWhen putting this together, I was surprised at how much I had to say. As it turns out, getting the Pillai score and the Bhattacharyya’s Affinity isn’t perfectly straightforward, especially if you want to do it for all your speakers individually. The techniques in this tutorial cover a wide range of R skills, ranging from relatively basic stuff to more advanced things. So, to keep this post as light as possible, I’ve moved all non-essential topics to Part 2. By the end of this one, you’ll be able to get these measures in your own data. If you find it to be buggy, you want to learn more, or you have some additional R background, try looking at the next one too.\nI’ll also say that this is not the first tutorial on calculating these measurements in R. Lauren Hall-Lew has already provided some R code in her 2010 paper. Dan Johnson has code for Bhattacharyya’s Affinity in his 2015 NWAV presentation. And Chris Strelluf’s new 2018 volume comes with the code used for his analysis as well. Hopefully this tutorial will provide some additional clarity in a way that complements what others have done."
  },
  {
    "objectID": "blog/a-tutorial-in-calculating-vowel-overlap/index.html#data-prep",
    "href": "blog/a-tutorial-in-calculating-vowel-overlap/index.html#data-prep",
    "title": "A tutorial in measuring vowel overlap in R",
    "section": "Data prep",
    "text": "Data prep\nAs always, I’ll be using tidyverse code to read in, process, and plot the data. Remember that this is a bit of an umbrella package that includes dplyr, tidyr, and ggplot2, which you might be more familiar with.\n\nlibrary(tidyverse)\n\nAnd I’ll use the same dataset that I’ve used in other tutorials. It’s a bunch of formant measurements of me reading about 300 sentences while sitting at my kitchen counter. The audio was automatically transcribed and processed with DARLA. As part of the process, it was processed with FAVE-extract, so the output file is just like what yours might look like if you also used FAVE. Hopefully this makes this tutorial the most translatable to your own data.\n\nmy_vowels_raw &lt;- read_csv(\"../../data/joey.csv\", show_col_types = FALSE) %&gt;%\n  print()\n\n# A tibble: 3,504 × 43\n   name       sex   vowel stress pre_word word    fol_word    F1    F2 F3    \n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1 LA000-Joey M     AY         1 THE      THAI    SP        826  1520. 2529.5\n 2 LA000-Joey M     AY         1 SP       TIMER   SP        581. 1306  1835.6\n 3 LA000-Joey M     ER         0 SP       TIMER   SP        484. 1449. 1644.6\n 4 LA000-Joey M     IY         1 SP       HERE    WE'LL     235. 2044. 3106.3\n 5 LA000-Joey M     IY         1 HERE     WE'LL   SP        302. 1974. 2549.5\n 6 LA000-Joey M     AA         1 SP       BARRING INJURY    573.  925. 2296.1\n 7 LA000-Joey M     IH         1 BARRING  INJURY  OR        362. 2262. 2591.2\n 8 LA000-Joey M     IY         0 BARRING  INJURY  OR        258. 2222. 3197.5\n 9 LA000-Joey M     ER         0 INJURY   OR      A         370.  872. 1654.5\n10 LA000-Joey M     EY         1 A        CHANGE  OF        428. 2210. 2531.2\n# ℹ 3,494 more rows\n# ℹ 33 more variables: F1_LobanovNormed_unscaled &lt;dbl&gt;,\n#   F2_LobanovNormed_unscaled &lt;dbl&gt;, B1 &lt;dbl&gt;, B2 &lt;dbl&gt;, B3 &lt;chr&gt;, t &lt;dbl&gt;,\n#   beg &lt;dbl&gt;, end &lt;dbl&gt;, dur &lt;dbl&gt;, plt_vclass &lt;chr&gt;, plt_manner &lt;chr&gt;,\n#   plt_place &lt;chr&gt;, plt_voice &lt;chr&gt;, plt_preseg &lt;chr&gt;, plt_folseq &lt;chr&gt;,\n#   pre_seg &lt;chr&gt;, fol_seg &lt;chr&gt;, context &lt;chr&gt;, vowel_index &lt;dbl&gt;,\n#   pre_word_trans &lt;chr&gt;, word_trans &lt;chr&gt;, fol_word_trans &lt;chr&gt;, …\n\n\n\n\n\nNow, I’d like to simplify this dataset a little bit.\n\nI’ll just focus on stressed vowels, so I’ll remove unstressed vowels with filter.\nI don’t need all the FAVE output for this tutorial, so I’ll use select to keep only the columns I need. Most notably here, I’m only keeping the midpoint measurements (F1@50% and F2@50%).\nBut, because those column names are slightly cumbersome to type, I’ll go ahead and rename them simply F1 and F2 using rename.\nFinally, there’s only one speaker here (me), but I want to show how to do this across all speakers in your sample, so I’ll go ahead and add a fake_speaker column that randomly assigns rows the speaker name “Joey” or “Stanley”. Fortunately I have a first name for a last name so this isn’t too weird of a result.\n\n\nmy_vowels &lt;- my_vowels_raw %&gt;%\n    filter(stress == 1) %&gt;%\n    select(vowel, word, dur, `F1@50%`, `F2@50%`, fol_seg, plt_manner, plt_place, plt_voice) %&gt;%\n    rename(F1 = `F1@50%`, F2 = `F2@50%`) %&gt;%\n    mutate(fake_speaker = sample(c(\"Joey\", \"Stanley\"), nrow(.), replace = TRUE))\nhead(my_vowels)\n\n# A tibble: 6 × 10\n  vowel word       dur    F1    F2 fol_seg plt_manner plt_place plt_voice\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n1 AW    WITHOUT   0.12  605. 1210  T       stop       apical    voiceless\n2 AA    TODD      0.17  615. 1065. D       stop       apical    voiced   \n3 EY    PLACES    0.05  363. 1810. S       fricative  apical    voiceless\n4 EH    GUEST     0.06  533  1614. S       fricative  apical    voiceless\n5 IY    SLEEPING  0.13  254. 2336. P       stop       labial    voiceless\n6 EY    PLACE     0.08  344. 2034. S       fricative  apical    voiceless\n# ℹ 1 more variable: fake_speaker &lt;chr&gt;\n\n\nFor this tutorial, I’ll focus on my low back merger (a.k.a. the cot-caught merger or the lot-thought merger). Now, my intuition tells me that lot (= “AA”) and thought (= “AO”) are quite distinct, though I have noticed myself using a backed vowel for some lot words in conversation. I’m definitely merged before tautosyllabic /l/, so that doll and hall rhyme, but definitely not before intervocalic /l/ (so collar and caller are quite distinct). So I’ll exclude tokens before /l/ for clarity. I’ll also exclude tokens before /ɹ/ because words in the north and force lexical sets are transcribed with AO in this data and I’m not particularly concerned about that environment. Finally, I’ll exclude the word on because it was only real stopword included in this sample.\n\nlow_back &lt;- my_vowels %&gt;%\n    filter(vowel %in% c(\"AA\", \"AO\"), \n           !fol_seg %in% c(\"L\", \"R\"),\n           word != \"ON\")\nhead(low_back)\n\n# A tibble: 6 × 10\n  vowel word     dur    F1    F2 fol_seg plt_manner plt_place plt_voice\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n1 AA    TODD    0.17  615. 1065. D       stop       apical    voiced   \n2 AA    GOD     0.09  554  1250. D       stop       apical    voiced   \n3 AO    WATER   0.05  588.  987. T       stop       apical    voiceless\n4 AA    STOCKS  0.13  578. 1074. K       stop       velar     voiceless\n5 AA    LOT     0.06  611. 1066. T       stop       apical    voiceless\n6 AA    CROP    0.1   657. 1097. P       stop       labial    voiceless\n# ℹ 1 more variable: fake_speaker &lt;chr&gt;\n\n\nHere’s what my data looks like. There’s quite a bit of overlap here at the midpoint, but my vowels are distinguished by other means like their trajectories (analyzing those will have to wait until another day).\n\nggplot(low_back, aes(F2, F1, color = vowel, label = word)) + \n    geom_text(size = 3) + \n    scale_x_reverse() + scale_y_reverse()\n\n\n\n\nSo now that we have the data ready to go, let’s look at that Pillai score."
  },
  {
    "objectID": "blog/a-tutorial-in-calculating-vowel-overlap/index.html#pillai-score",
    "href": "blog/a-tutorial-in-calculating-vowel-overlap/index.html#pillai-score",
    "title": "A tutorial in measuring vowel overlap in R",
    "section": "Pillai Score",
    "text": "Pillai Score\nThe Pillai score (a.k.a. Pillai-Barlett Trace) dates back to K. C. Sreedharan Pillai’s (1958) paper. As far as I can tell the first linguists to use it on vowel data were Jennifer Hay, Paul Warren, & Katie Drager in their (2006) paper in Journal of Phonetics where they use it to analyze the merger of near and square in New Zealand English. Measuring the distance between two vowel clusters had been done using Euclidean distance, but the Pillai score was more advantageous because it measures the overlap between the two vowel classes instead of the distance between them. The value ranges from 0 to 1, with values closer to 0 indicating more overlap while values closer to 1 mean complete separation.\nProbably one of the most cited references on Pillai scores in linguistics is Jennifer Nycz and Lauren Hall-Lew’s 2013 paper in the Proceedings of Meetings on Acoustics. It was one of the measures that they looked at in their meta-analysis of vowel merger. If you’re curious about the Pillai score and how it compares to other measures of vowel overlap, I encourage you to take a look at that paper.\nTo get an intuition of how a Pillai score relates to vowel data, here are some example distributions with 100 measurements in each cluster. On the left are two circular distributions. In the middle, one cluster is smaller than the other, so the distance between the two is a little bit smaller to get the same overlap. On the right, one distribution is ellipsoidal, and the distance between them has to be a bit larger to get the same pillai scores as perfectly circular distributions.\n\nThese plots (hopefully) illustrate that Pillai really measures overlap and not distance since the shape and size of the clusters have as much of an effect on the Pillai score as the distance between them.\n\nThe MANOVA Test\nOkay, so how do we actually calculate the Pillai score? As stated in Nycz & Hall-Lew (2013), the Pillai score is an output of a MANOVA test. Basically, what the MANOVA test does is it takes at least two continuous dependent variables and whether they come from the same distribution in that multivariate space. A linear regression with lm (or even a mixed-effects linear regreesion with lmer) takes only one dependent variable and can tell you which of the independent variables are significant predictors. The MANOVA test does the same thing just with more than one dependent variable at the same time.\nThe test itself is done using the manova function. Like lm and lmer, the main argument is a formula, such as y ~ x, where the dependent variable is before the tilde and the independent variable is after. But, because manova can take multiple dependent variables, you have to wrap them all together in this cbind function: cbind(y1, y2) ~ x. In my data, the two dependent variables are F1 and F2, so I’ll wrap those up in cbind. Then I’ll add the dependent variable, which is the vowel column. Finally, I’ll add the data = low_back argument so that the function knows where the data is coming from. When you run all this wrapped up in manova, you get something like this.\n\nmanova(cbind(F1, F2) ~ vowel, data = low_back)\n\nCall:\n   manova(cbind(F1, F2) ~ vowel, data = low_back)\n\nTerms:\n                   vowel Residuals\nF1                 315.6  100402.3\nF2               94265.5  333690.2\nDeg. of Freedom        1        59\n\nResidual standard errors: 41.25208 75.20483\nEstimated effects may be unbalanced\n\n\nSo this output is not particularly useful by itself right now because it doesn’t show the Pillai score. So, like other regression models, if you save the model into an object (my_manova in this case), you can then get all these summary statistics with the summary function.\n\nmy_manova &lt;- manova(cbind(F1, F2) ~ vowel, data = low_back)\nsummary(my_manova)\n\n          Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nvowel      1 0.22138   8.2456      2     58 0.0007053 ***\nResiduals 59                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAha! Now we have all the numbers we need! If you look closely the Pillai score is right there, and for my vowels in this sample it’s about 0.22. Remember that these scores range from 0 to 1, with 0 being completely overlapped and 1 meaning total separation. So a value like 0.22 indicates some but not complete overlap.\n\n2.1.1 Tangent on p-values\nThe model summary does also provide a p-value. The null hypothesis of the MANOVA is that the dependent variable is not a significant predictor of the data. So, the p-value is something like an indication of how surprised you should be to get the result you did, if that were true. Because the p-value is small, that suggests that adding vowel as a predictor to this model is a good idea and that that information is useful in predicting the F1 and F2 values of my low back vowels.\nNow, as a word of caution. I’ve played around with Pillai scores a lot and I’ve found that the p-values are significant a lot. I mean there have been times where the plots show what appear to me to be completely overlapped distributions, but the p-value says that they’re significantly different. I’m not completely familiar with the inner workings of the MANOVA test, so I don’t really know how sensitive it is to outliers, sample sizes, or other things. But in my opinion, it appears to be overly sensitive to minor differences that are probably not perceivable. In other words statistical significance does not necessarily mean social significance.\nBut, we’re here to look at the Pillai score, not the p-value. The problem is there’s no Pillai score threshold for saying something is definitively merged or unmerged. By that I mean we can’t just define a value like 0.05 and say if the Pillai score is less than that then we can conclude that the vowels are merged. As far as I’m aware, the Pillai scores are useful only in comparison to other Pillai scores, either from the same pair of vowels in other speakers, or perhaps from the same speaker but with other pairs of vowels.\n\n\n\n\n\n\nUpdate (Novmeber 23, 2021)\n\n\n\nBased on some recent research with Betsy Sneller (read more here), I’ve learned more about the effect of sample sizes on Pillai scores and whether we should be reporting p-values. At least based on the bivariate normal distribution that our simulations were based on, we found that larger samples will produce smaller Pillai scores. This means that, assuming two speakers have underlying merged vowels, the speaker with less data will have a higher Pillai score. This also means that if you’re comparing across styles (say wordlists to conversational data), the subset with less data will have a higher Pillai score. So what will look like meaningful sociolinguistic differences (either between speakers or between styles within the same speaker) is actually just a product of how the Pillai score is calculated.\nOne way to resolve this is to actually start reporting p-values. Phoneticians do it but for some reason sociolinguists do not. We’re still not sure how this plays out with real vowel data rather than simulated data, but it probably is better to report them than to leave them off. This resolves the issue of coming up with ad hoc thresholds of putting too much weight into the interpretation of the Pillai score itself.\n\n\n\n\n\nMore complex MANOVA formulas\nSo far, we’ve only done the most basic MANOVA test. It includes F1 and F2 as continuous variables and vowel as the dependent variable. The MANOVA test can actually handle more information for that. For one, you can add not just F1 and F2, but also F3 or duration or any number of continuous variables as response variables.\nSo, for example, if I wanted to add duration, that’ll look at the overlap between the two vowels in a three-dimensional space. The two vowels may be completely overlapped in F1 and F2 and differentiated only by duration. If that’s the case, the Pillai score would be higher if duration is added.\n\nmy_manova &lt;- manova(cbind(F1, F2, dur) ~ vowel, data = low_back)\nsummary(my_manova)\n\n          Df  Pillai approx F num Df den Df   Pr(&gt;F)   \nvowel      1 0.22494   5.5143      3     57 0.002143 **\nResiduals 59                                           \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn my case, the Pillai score is hardly any different, so maybe duration doesn’t really play a role in differentiating these vowels for me.\nWe can also add additional predictor variables. So if we wanted to control for various phonetic environments, we could just add them add to the formula. Here I’ll control for place, manner, and voicing of the following segment.\n\nmy_manova &lt;- manova(cbind(F1, F2, dur) ~ vowel + plt_place + plt_manner + plt_voice, data = low_back)\nsummary(my_manova)\n\n           Df   Pillai approx F num Df den Df   Pr(&gt;F)   \nvowel       1 0.244006   5.5945      3     52 0.002111 **\nplt_place   3 0.143178   0.9021      9    162 0.524932   \nplt_manner  1 0.074908   1.4035      3     52 0.252201   \nplt_voice   1 0.195430   4.2103      3     52 0.009689 **\nResiduals  54                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n Now the problem here is that we get a Pillai score for each one. How do we interpret this? To be honest, I’ve never encountered this before in my research. But, I think the way this is interpreted is that the Pillai score for the vowel is a measure of overlap between the two vowels after all the other variables have been controlled for. In my case, it’s a little higher—0.14 instead of 0.12—when environmental effects are considered, which I guess makes sense.What’s weird here is that the p-values don’t really correlate with the Pillai score, so the variable that’s that has the most separation is the one without statistical significance. This is another problem when interpreting p-values in conjunction with Pillai scores. [Update (November 23, 2021): this is likely the result of sample size differences. See my note in section 2.1.1 and link to more recent research on this topic.]\nFor the other variables, they each have their own Pillai scores. So like the plt_manner Pillai score would be a measure of overlap between the various manners of articulation, after the vowel class has been accounted for. To me, that seems a little high considering what the data looks like when it’s colored by voicing of the following segment using the information returned from FAVE.\n\nggplot(low_back, aes(F2, F1, color = plt_voice, label = word)) + \n    geom_text(size = 3) + \n    scale_x_reverse() + scale_y_reverse()\n\n\n\n\n To me, 0.10 actually seems a little high still, but maybe that one token of a word-final vowel (draw) is throwing everything off.I reran it without that one token and the Pillai score didn’t really change.\nFor simplicity, I’d say let’s stick with interpreting the Pillai score for the vowel. Update (November 23, 2021): Again, this is likely the result of sample size differences. See my note in section 2.1.1 and link to more recent research on this topic.\n\n\nExtracting that Pillai score\nSo this is good. We’ve been able to get the Pillai score and that’s great. But what if you wanted R to just give you just that one number instead of that whole summary table? Right now it’s embedded in a small table full of lots of other numbers and it might be distracting to see all of them. Or sometimes, you want to save the Pillai score and use it for something else. Either way, it’s useful to know how to extract just that one number from that summary table.\nAs a reminder here’s that summary table again.\n\nsummary(my_manova)\n\n           Df   Pillai approx F num Df den Df   Pr(&gt;F)   \nvowel       1 0.244006   5.5945      3     52 0.002111 **\nplt_place   3 0.143178   0.9021      9    162 0.524932   \nplt_manner  1 0.074908   1.4035      3     52 0.252201   \nplt_voice   1 0.195430   4.2103      3     52 0.009689 **\nResiduals  54                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s pop the hood on this summary and figure out what’s going on. As it turns out, you can access this summary table by appending $stats at the end of the summary:\n\nsummary(my_manova)$stats\n\n           Df     Pillai  approx F num Df den Df      Pr(&gt;F)\nvowel       1 0.24400611 5.5945415      3     52 0.002110544\nplt_place   3 0.14317786 0.9021218      9    162 0.524931637\nplt_manner  1 0.07490799 1.4035417      3     52 0.252200995\nplt_voice   1 0.19543023 4.2102716      3     52 0.009689201\nResiduals  54         NA        NA     NA     NA          NA\n\n\nWhen you view it this way, you can see that it’s just a dataframe and things like the significance stars aren’t there. R. So that stats object within the summary of the MANOVA test is just a table, which means we can extract specific rows and columns with the same kind of R syntax that you’ve seen in other tables. Specifically, if you put [x,y] at the end of the code, where x is the row number and y is the column name/number, you can get individual cells.\nSo, let’s see if we can extract just the Pillai column.\n\nsummary(my_manova)$stats[,\"Pillai\"]\n\n     vowel  plt_place plt_manner  plt_voice  Residuals \n0.24400611 0.14317786 0.07490799 0.19543023         NA \n\n\nOkay, great. If you look closely, that 0.1404699 is the Pillai score for the vowel. The reason why the word “vowel” is above it is because that’s what row the value is in. Likewise, we can extract just the “vowel” row of the summary table.\n\nsummary(my_manova)$stats[\"vowel\",]\n\n          Df       Pillai     approx F       num Df       den Df       Pr(&gt;F) \n 1.000000000  0.244006107  5.594541473  3.000000000 52.000000000  0.002110544 \n\n\nThere are all those numbers again. So, if we intersect the two, extract the “vowel” row of the “Pillai” column, we should be left with just the Pillai score for vowel.\n\nsummary(my_manova)$stats[\"vowel\",\"Pillai\"]\n\n[1] 0.2440061\n\n\nThere we go. We now have code to extract nothing but our Pillai score.\nNow, from here, we have two options. One is that we can combine the MANOVA test and Pillai score extraction all onto one line. Currently, our workflow looks like this.\n\nmy_manova &lt;- manova(cbind(F1, F2) ~ vowel, data = low_back)\nsummary(my_manova)$stats[\"vowel\",\"Pillai\"]\n\n[1] 0.2213842\n\n\nBut, if you want, you can simplify it even more by embedding the manova function call right in there:\n\nsummary(manova(cbind(F1, F2) ~ vowel, data = low_back))$stats[\"vowel\",\"Pillai\"]\n\n[1] 0.2213842\n\n\nNow there is no my_manova object at all, and in one line of code we run the MANOVA test and extract the Pillai score. Pretty cool.\n\n\nWriting a function\nAt this point, what I like to do is to create a function in R. I don’t like all that typing in the previous line of code because it’s somewhat cumbersome, prone to error, and—most importantly—it’s not immediately transparent what it does. There’s really no indication that that big ol’ line is getting the Pillai score and when you come back to look at your code in a few weeks or months, you might not remember what all that mumbo-jumbo is about. For this reason, I like to create custom functions that do all the heavy lifting for me.\nThe gist of a function is that you take some input parameters (called “arguments”), do something to them, and return some value. So, a function that takes a number and multiplies it by two, might look something like this.\n\ntimes_two &lt;- function(hi_my_name_is_joey) {\n    hi_my_name_is_joey * 2\n}\ntimes_two(5)\n\n[1] 10\n\n\nSo like variable names, the parts of a function are arbitrarily named and they’ll work fine with whatever name you want. This includes the argument (hi_my_name_is_joey) and the name of the function itself (times_two). Do what makes sense to you, but ideally they’ll be something brief but informative (which is sometimes easier said than done).\nNow, I’m not going to get into the details of how to write functions. There are lot of people that have done so, and their explanations are far better than anything I could do. To point you to one source, I learned how to write them primarily through Chapter 19 of R for Data Science, so you’re welcome to take a look there.\nFor the sake of your time, I’ll cut to the chase regarding functions. The cool thing about arguments in R functions is that if all we’re going to do is pass them down into another function (like manova) we don’t even need to bother with naming arguments. You can actually just type ... as the argument, both in the function definition and in the manova function, and it’ll magically take care of everything.\n\npillai &lt;- function(...) {\n    summary(manova(...))$stats[\"vowel\",\"Pillai\"]\n}\npillai(cbind(F1, F2) ~ vowel, data = low_back)\n\n[1] 0.2213842\n\n\nThere. It elegant, straightforward, and clear. The best part is that the pillai function now has the same syntax as manova, only instead of returning the full model, it just returns the Pillai score.\n\n\nMultiple speakers\nOkay, so this seems to work fine on my own data. But I’m just one speaker. You probably have more than one speaker that you’re analyzing and you want to compare all their Pillai scores. So how do you go about calculating the Pillai score for each one?\nFor starters, you could create subsets of the data and calculate them separately.\n\njoey &lt;- low_back %&gt;% \n    filter(fake_speaker == \"Joey\")\nstanley &lt;- low_back %&gt;% \n    filter(fake_speaker == \"Stanley\")\n\nThen, you can use our new handy-dandy function to get the Pillai scores for each one.\n\njoey_pillai &lt;- pillai(cbind(F1, F2) ~ vowel, data = joey)\njoey_pillai\n\n[1] 0.2352627\n\n\n\nstanley_pillai &lt;- pillai(cbind(F1, F2) ~ vowel, data = stanley)\nstanley_pillai\n\n[1] 0.2208865\n\n\nOkay, so that’s cool and it might work if you have a very small number of speakers (like less than five). But if you have a dozen or several dozen or more, this is not the most elegant way of doing things. There’s lots of repetition, it’s unwieldy, and all that copy and pasting code is error-prone.\nInstead, let’s see if we can get a little help with the summarize function. So summarize is part of the dplyr package and according to it’s help file, it “reduces multiple values down to a single value.” We can use summarize for a whole bunch of things, like calculating the average F1 measurement in my low back vowels.\nYou’re going to see this “tibble” thing in my output a lot more now. That’s just a byproduct of using certain tidyverse func­tions. Read more about tibbles here.\n\nlow_back %&gt;%\n    summarize(mean_F1 = mean(F1))\n\n# A tibble: 1 × 1\n  mean_F1\n    &lt;dbl&gt;\n1    626.\n\n\nSo in our case, we have multiple F1 and F2 measurements and we want to reduce those down to a single Pillai score. We can run summarize on our data by creating a new column name (let’s call it low_back_pillai) and then calling the pillai function:\nYou’ll notice you don’t need to add the data = low_back argument anymore. Within tidyverse functions, when you pipe things in using %&gt;%, the data arguments are implied alreday.\n\nlow_back %&gt;%\n    summarize(low_back_pillai = pillai(cbind(F1, F2) ~ vowel))\n\n# A tibble: 1 × 1\n  low_back_pillai\n            &lt;dbl&gt;\n1           0.221\n\n\nOkay, so that’s kind of cool. That’s the same number we saw from above. But that pools the entire dataset (both “Joey” and “Stanley”) together. How do we group the data by the speaker name? Fortunately, there’s another super handy function called group_by, also in the dplyr package, that will group the data by the values in some column. So, when we run group_by(fake_speaker) first, it’ll essentially split the data up into subsets, one for each speaker, and then calculate the Pillai score for each group.\n[Update (November 23, 2021): Here’s some evidence for me and Betsy Sneller’s recent finding that sample size matters. My Pillai score for all data is 0.119, but when I subset it into two smaller groups, the Pillai scores go up to 0.131 and 0.169. See my note in section 2.1.1 and link to more recent research on this topic.]\n\nlow_back %&gt;%\n    group_by(fake_speaker) %&gt;%\n    summarize(low_back_pillai = pillai(cbind(F1, F2) ~ vowel))\n\n# A tibble: 2 × 2\n  fake_speaker low_back_pillai\n  &lt;chr&gt;                  &lt;dbl&gt;\n1 Joey                   0.235\n2 Stanley                0.221\n\n\n Cool! So in just a couple lines of code, I was able to calculate the Pillai score for each “speaker” in my dataset. If you’ve got many more speakers in your data, you’ll be able to do the same thing with each one of them with the exact same amount of code.To see another use of group_by and then summarize, look at how I use it to calculate average formant trajectories per vowel here.\nSo that’s it for the Pillai score. Hopefully this section of the tutorial will help you get those numbers in your own dataset. For most people this will be adequate and you can get great results. If it starts to crash on you and give you weird error messages, check out Part 2 of this tutorial where we look at how to make the function more robust."
  },
  {
    "objectID": "blog/a-tutorial-in-calculating-vowel-overlap/index.html#bhattacharyyas-affinity",
    "href": "blog/a-tutorial-in-calculating-vowel-overlap/index.html#bhattacharyyas-affinity",
    "title": "A tutorial in measuring vowel overlap in R",
    "section": "Bhattacharyya’s Affinity",
    "text": "Bhattacharyya’s Affinity\nSo the Pillai score is pretty mainstream and most people that want to measure vowel overlap use it. However, recently, there have been a few people using this thing called Bhattacharyya’s Affinity. This measurement also dates back quite a ways to when Anil Kumar Bhattachayya published a paper called “On a measure of divergence between two statistical populations defined by their probability distributions” in the Bulletin of the Calcutta Mathematical Society in 1943. One non-linguistic application of this measure was in Fieberg & Kochanny (2005) who use it to measure the overlap in the the home range of some deer in Minnesota.I can’t find this 1943 paper online, but I can find a 1946 paper that looks similar\nAs far as I know, Bhattacharyya’s Affinity in linguistics was first brought up Dan Johnson  in his NWAV presentation in 2015. He explained that it can handle the things that Pillai doesn’t do so well like nested, crossed, skewed, or unequal distributions. His presentation even includes an a overlap simulator where you can play with this yourself (so be sure to follow that link!)It should be noted that Betsy Sneller was very much involved in developing the ideas presented in this talk as well.\nSince then, I’ve seen it in several other studies. Paul Warren (2018) used it to look at mergers in New Zealand vowels. Chris Strelluf uses it in his 2018 volume in the Publications of the American Dialect Society to measure the low-back merger and the pin-pen merger (among other things) and Strelluf used it in a 2016 LCV article to look at overlap in prelateral back vowels. Together with Peggy Renwick, I used it to measure the cord-card merger in an individual over 40 years at LabPhon in 2017.\nBhattacharyya’s Affinity shares some similarities with the Pillai score, but there are also some differences. It too, measures the overlap between two distributions on a scale from 0 to 1. But this time, 1 means complete overlap and 0 means complete separation. However, Bhattacharyya’s Affinity can actaully reach 0 and 1 if there is indeed separation or overlap, unlike the Pillai score. Here’s what these values might look like:\n\nAlso, unlike the Pillai score though, Bhattacharyya’s Affinity can only handle two continuous variables, so you can’t pack on things like F3 or duration. (After all, it was originally designed for animal locations!)\nSo it’s a thing. Whether you want to use Bhattacharyya’s Affinity yourself is up to you, but in this tutorial I’ll show the code so you can run it yourself on your own data.\n\nCalculating Bhattacharyya’s Affinity\nTo get Bhattacharyya’s Affinity you’ll need to install and download an additional R package, adehabitatHR (plus its dependencies), to get this measurement. If you haven’t installed the package already, you’ll need to with install.packages.\nIf you run into problems with the select function in your scripts after loading adehabitatHR, see Part 2 for an explanation and solution.\n\n#install.packages(\"adehabitatHR\")\nlibrary(adehabitatHR)\n\nSo the way this works is that there are three steps to getting Bhattacharyya’s Affinity. Step one is that you’ll need to prep the low_back dataframe. Remember how we created low_back? We took the my_vowels dataframe, which contained data about all the vowels, and got a subset of it. In R, there’s a lot of hidden metadata about dataframes, and sometimes this can bite you down the road. This is one of those times.\nLet’s pop the hood and look at our low_back dataframe. When your data is a factor, meaning R treats it as a categorical variable, it keeps track of what all the possible values are. So, in our vowel column of low_back, we can use table to see all the values attested in our dataframe.\n\ntable(low_back$vowel)\n\n\nAA AO \n43 18 \n\n\nOkay, so I’ve got 43 tokens of “AA”, 18 of “AO”, and a whole bunch of zeros. [If you get a bunch of zeros in addition to these numbers, try running low_back &lt;- droplevels(low_back). When applied to our dataframe, it’ll “forget” all the ones that don’t actually exist anymore.] Okay, great. So that’s step one.\nThe next step is to convert the data into a spatial points dataframe. This is a special kind of dataset that is meant to be processed as spatial data. Using the SpatialPointsDataFrame function, we provide two arguments: the coordinates and the data. If you think of the F1-F2 vowel space as x-y coordinates, then it makes sense why we need to have those as the coordinate data. And for the data, well, the only thing we need to do is supply the vowel data.\nUnfortunately, SpatialPointsDataFrame requires the data to be prepared just right in order for it to work. So, when we send the F1 and F2 data, we have to basically prepare a dataframe that contains just those two columns. We can do this by subsetting the low_back data and only selecting the F1 and F2 columns.\n\njust_formants_df &lt;- low_back[,c(\"F1\", \"F2\")]\nhead(just_formants_df)\n\n# A tibble: 6 × 2\n     F1    F2\n  &lt;dbl&gt; &lt;dbl&gt;\n1  615. 1065.\n2  554  1250.\n3  588.  987.\n4  578. 1074.\n5  611. 1066.\n6  657. 1097.\n\n\nThen, for the data, we need to do the same thing, but only selecting the vowel column.\n\njust_vowels_df &lt;- low_back[\"vowel\"]\nhead(just_vowels_df)\n\n# A tibble: 6 × 1\n  vowel\n  &lt;chr&gt;\n1 AA   \n2 AA   \n3 AO   \n4 AA   \n5 AA   \n6 AA   \n\n\nSo, if we put those two as the arguments to the SpatialPointsDataFrame function, we’re golden. Let’s save that as a new object called low_back_sp.\n\nlow_back_sp &lt;- SpatialPointsDataFrame(just_formants_df, just_vowels_df)\n\nIt’s not particularly important what this new object looks like because it’s just an intermediate step to the next function, which is the kerneloverlap in the adehabitatHR library. This is the function that actually calculates the Bhattacharyya’s Affinity. If we apply this function to our new low_back_sp object, all we need to to is to tell it which method to apply, which is \"BA\":\n\nba_table &lt;- kerneloverlap(low_back_sp, method = \"BA\")\nba_table\n\n          AA        AO\nAA 0.9998734 0.8067602\nAO 0.8067602 0.9998692\n\n\nSo now we have a matrix of Bhattacharyya’s Affinities! So the way you read this is to choose a row and a column and find the cell where the two intersect. That cell contains the Bhattacharyya’s Affinity for those two vowels. So for “AA” and “AA”—identical vowels—it’s 0.9998, which is basically 1. That’s what we’d expect, right? But we’re not interested in those cells, we’re interested in the cells for “AA” and “AO”. Here, you can see that they’re about 0.80.If you look closely, the top left and the bottom right cells are slightly different. I’m not sure why, but that difference is so small it shouldn’t matter.\nWe can extract just this number by pulling just the first row of the second column like this:\n\nba_table[1,2]\n\n[1] 0.8067602\n\n\nAlternatively, you could use the names of the vowels, just like we did with the Pillai score above:\n\nba_table[\"AA\", \"AO\"]\n\n[1] 0.8067602\n\n\nSo that’s it! After all that, we finally were able to extract the Bhattacharyya’s Affinity for my low back vowels. Let’s put it all together just so you can see the necessary steps.\n\nlow_back &lt;- droplevels(low_back)\njust_formants_df &lt;- cbind(low_back$F1, low_back$F2)\njust_vowels_df &lt;- data.frame(low_back$vowel)\nlow_back_sp &lt;- SpatialPointsDataFrame(just_formants_df, just_vowels_df)\nba_table &lt;- kerneloverlap(low_back_sp, method = \"BA\")\nba_table[1,2]\n\n[1] 0.8067602\n\n\nNow of course we can consolidate this somewhat by embedding functions within other functions, if you’d like:\n\nlow_back &lt;- droplevels(low_back)\nlow_back_sp &lt;- SpatialPointsDataFrame(cbind(low_back$F1, low_back$F2), data.frame(low_back$vowel))\nkerneloverlap(low_back_sp, method = \"BA\")[1,2]\n\n[1] 0.8067602\n\n\nSo that’s how you’d do this for one pair of vowels. If that’s all you need, then you’re done!\n\n\nWriting a function\nNow, if you’re like me, you might find that doing it this way is a bit cumbersome. It’s bad enough with just one pair for one speaker. If I needed to do this for many pairs of vowels and/or for many speakers, it would get insane. So, just like with the Pillai scores, let’s wrap all this up into a nice and neat function so that we can apply it to however many groups we want with ease.\nSo the goal for this function is to be as similar to the Pillai one as possible. It’s not going to have identical syntax because of the functions you need to run them (manova verses kerneloverlap), but we can at least come close.\n I’ll go ahead and name the function bhatt. The key ingredients we need for calculating it are the F1, F2, and vowel columns. So as arguments, we’ll have those.These argument names are arbitrary so instead of F1, F2, and vowel, you could do dog, fish, and emu and it’ll work fine. However, it’s useful to keep the argument names informative. But keep in mind that they don’t have to match the column names in your dataframe.\n\nbhatt &lt;- function (F1, F2, vowel) {\n    # This is just the template\n}\n\nWithin the function I’ll now include the three steps I had before.\n\nFirst, I’ll turn the vowel data into its own dataframe. I’ll also wrap droplevels around that.\nThen, I’ll run the SpatialPointsDataFrame function with cbind(F1, F2) method of combining the two formants (the second option presented above).\nFinally, I’ll use kerneloverlap to get the matrix of Bhattacharyya’s Affinity measures and then extract just the first row of the second column.\n\n\nbhatt &lt;- function (F1, F2, vowel) {\n    vowel_data &lt;- droplevels(data.frame(vowel))\n    \n    sp_df &lt;- SpatialPointsDataFrame(cbind(F1, F2), vowel_data)\n    kerneloverlap(sp_df, method='BA')[1,2]\n}\n\nWe can then then run that on whatever dataset you want by piping it into the summarize function.\n\nlow_back %&gt;%\n    summarize(low_back_bhatt = bhatt(F1, F2, vowel))\n\n# A tibble: 1 × 1\n  low_back_bhatt\n           &lt;dbl&gt;\n1          0.807\n\n\nOkay! Now we’ve done it! We can now do this with all the speakers too, as long as your data is prepared the right way.\n\nlow_back %&gt;%\n    group_by(fake_speaker) %&gt;%\n    summarize(low_back_bhatt = bhatt(F1, F2, vowel))\n\n# A tibble: 2 × 2\n  fake_speaker low_back_bhatt\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Joey                  0.810\n2 Stanley               0.818\n\n\nHooray! So, again, this will probably work fine for most people. But, if you find that the function is crashing, go on to the next blog post to see how you can make it more robust to errors.\nIn fact, the way the bhatt and pillai functions are set up now, you can actually extract both the Pillai score and the Bhattacharyya’s Affinity all at once. You just put each on its own line within the summarize function and it’ll take care of it all.\n\nlow_back %&gt;%\n    group_by(fake_speaker) %&gt;%\n    summarize(low_back_pillai = pillai(cbind(F1, F2) ~ vowel),\n              low_back_bhatt  = bhatt(F1, F2, vowel))\n\n# A tibble: 2 × 3\n  fake_speaker low_back_pillai low_back_bhatt\n  &lt;chr&gt;                  &lt;dbl&gt;          &lt;dbl&gt;\n1 Joey                   0.235          0.810\n2 Stanley                0.221          0.818\n\n\nSo that’s handy. That might save you some time trying to do them separately and merging the tables together."
  },
  {
    "objectID": "blog/a-tutorial-in-calculating-vowel-overlap/index.html#conclusion",
    "href": "blog/a-tutorial-in-calculating-vowel-overlap/index.html#conclusion",
    "title": "A tutorial in measuring vowel overlap in R",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s it. Hopefully with this tutorial you are able to calculate the Pillai scores and Bhattacharyya’s Affinity in your data. But we went beyond doing it one speaker at at time and wrote up some functions so that you can calculate these measures each speaker. Again, it’s up to you to figure out which overlap measure to use (by reading the literature and critically analyzing the results in your own data), but at least the coding shouldn’t be an obstacle for you anymore. And with any luck, you’ve gained some additional R skills that may translate (in)directly to other portions of your research.\nFinally, the functions as they’re written now prone to a couple of errors. For example, if you inadvertently apply them to a speaker who doesn’t have very much data, it’ll crash and throw an error message. To learn about how to make the functions more robust and how to apply these functions to multiple vowel pairs at once, continue on to Part 2."
  },
  {
    "objectID": "blog/365-papers-update/index.html",
    "href": "blog/365-papers-update/index.html",
    "title": "#365 Papers (Update)",
    "section": "",
    "text": "At the beginning of 2018, I set the ambitious goal of reading 365 papers during that year. I tweeted about it and blogged about it, but ultimately didn’t achieve my goal. Turns out 365 is a lot. Well, after 1338 days, I can finally say I’ve ready 365 papers! So here’s just some visuals to see what kinds of things I’ve been reading.\nWhat counts as a “paper” and what counts as “reading” it? I didn’t have any hard and fast rules, but these were the guidelines I laid out before starting.\nIn a few cases, I counted full books as a single entry, like if they had short chapters. I think a 3-page chapter of a book shouldn’t count the same as an article in Language, for example. Similarly, a lot of the Masters Theses I read were shorter and about the same as an article reading so those counted as one.\nI will say that re-reading something counts a second time if I do it just as thoroughly. Things like textbook chapters from classes I’m teaching are the main culprit, but it’s nice to revisit things after a few years.\nAnd to be clear, this doesn’t represent all the papers I’ve ever read. In fact, I’d say the bulk of reading for my dissertation happened before I started keeping track. I’ve kept decent notes about what I’ve read since about 2010, but I’ll just focus on the most recent 365 for now."
  },
  {
    "objectID": "blog/365-papers-update/index.html#pace",
    "href": "blog/365-papers-update/index.html#pace",
    "title": "#365 Papers (Update)",
    "section": "Pace",
    "text": "Pace\nIf I wanted to read 365 papers in a year, that’s obviously one paper a day. What was my actual pace and did it change? The following plot answers this. From left to right are the months of the year. The colored lines go up as I finished a paper that year. In dashed gray lines, I have benchmarks for where the colored lines would be if I had maintained a constant rate.\n\nLooks like in 2018 and 2019 (when I was in the throes of dissertation-writing), my pace was usually somewhere around one paper every 4 to 8 days. So about one a week or occasionally two a week, on average. Starting in 2020 and continuing into this year, my pace is quicker and I’m reading a paper at least every three days on average.\nMy pace ebbed and flowed within a single year quite a bit and it’s interesting to see the patterns. In August of 2018 for example, I started really hunkering down and writing my dissertation, so there’s a sudden increase in pace (in the blue line). In early 2019 you can see I read in short bursts (I binge-read several 3rd Wave sociolinguistics papers). In June 2019 I took GIS and Stats courses so that uptick was from those classes. In September I was in a data visualization phase. And it looks like the time between when I submitted my dissertation and when I defended it (in December 2019), I didn’t do much reading at all.\nMy pace went up quite a bit in 2020 as I was transitioning from dissertation work to teaching. I read some material related to my job talk and was working on submitting my chapter in Speech in the Western States: Volume III. The biggest jump was in March 2020. Yes, that’s when COVID hit, but I was also fortunate to be hired as an “instructional designer” for BYU so I was prepping a course and doing a lot of reading. Things waned as I moved to Utah but when Fall semester hit, I kept that pretty quick pace up as I was prepping two new courses. This continued into 2021 as I prepped another two new courses. And you can see my recent uptick as I start getting ready to teach again."
  },
  {
    "objectID": "blog/365-papers-update/index.html#content",
    "href": "blog/365-papers-update/index.html#content",
    "title": "#365 Papers (Update)",
    "section": "Content",
    "text": "Content\nSo now that we’ve got the pace covered, let’s look at the content itself.\n\nYears\nFirst, I’ll show the publication years of the things I read. Note that I do have two colums for “no date” and forthcoming: those are mostly reviews I did or other sneak-peaks at unpublished work.\n\nI’m happy to see that a large proportion of what I read was recent, having come out since I started this little project. Looks like half of the papers I read came out in 2008 or later (or rather, within the last 10–14 years); a third was 2017 or later (the last 1–4 years). I honestly wish I had read even more recent stuff though because I feel a little behind the times. A quarter of what I read was before 1993. It’s good to read the classics, but I think I need to be staying more up to date though. Something that certainly accounts for this older skew is that I read while walking and the things I read are typically older (Trudgill 1978, Petyt 1980, Preston 1989, etc). I’m happy I read some older things, but I wish this plot had been more skewed towards the right.\n\n\nTopics\nNext, here’s a plot of the broad topic the papers fell in. I only gave each paper a single tag, and sometimes the decision to call something sociolinguistics vs dialectology, for example, was somewhat arbitrary. But this should give you a rough idea of what things I read.\n\nIt should come to no surprise that most of what I read was sociolinguistic in nature, followed closely by dialectology. The socio stuff is relevant to research and teaching and the dialectology stuff is mostly for research. Phonetics and statistics coming next are also exactly what I’d expect. I wish I had a bit wider range of topics though so that I can be more well-rounded of a linguist.\n\n\nPublication Type\nNext, here’s a basic plot on the publication type. I’ve divided everything into three broad categories: journal articles (which include conference proceedings), monographs, and edited volumes.\n\nThis is where I think I fall short. I’m happy to see that journal articles were the most common, but I think I should be reading a higher proportion of newer articles than I am. In fact, monographs and edited volumes combined make up 54% of what I read. This may also be because I read as I walk to and from my car and around my building, so I do get more regular book-reading time than sit-and-read-an-article time.\n\n\nPublication Venue\nFinally, the publication venue. Just focusing on the journal articles, here’s a list of the venues I read from the most. (I’ve filtered out venues that I only read from once or twice, for space issues).\n\nBased on my research, it should come as no surprise that American Speech and LVC are the top two. I have a print subscription to American Speech, and I had a habit of reading through all the articles while on the bus. My guess is that the PWPL ones are mostly proceedings from NWAV too. JASA, JEngL, and J. Soc. are also not much of a surprise.\nHowever, this plot again highlights what I think are my shortcomings. I feel like I need to be reading more Language in Society and Journal of Sociolinguistics. I also think I need to be reading about languages other than English too. I feel like I’ve latched on to my venues to my detriment and I’m missing a lot of interesting work by not expanding my horizons."
  },
  {
    "objectID": "blog/365-papers-update/index.html#outlook",
    "href": "blog/365-papers-update/index.html#outlook",
    "title": "#365 Papers (Update)",
    "section": "Outlook",
    "text": "Outlook\nI’m glad I did this exercise ad I’ll certainly continue with it. And looking back at the past 3⅔ years has been enlightening to say the least. My goals for the next 365 papers are the following:\n\nRead a larger proportion of journal articles. Specifically, I want to go from 45% journal articles to 65%.\nRead a larger proportion of newer articles. Specifically, I want over half of what I read to be since 2015 (6–9 years old).\nFinish 365 papers sooner. It took 44 months to do this first batch. Since I’ve been keeping a pretty good pace of a paper every three days, I’ll aim for 36 months and finish by August 31, 2024.\n\nI also want to get through my to-read list. I have about 50 papers on it. It is always growing, and as I read more I find more things to read. But my current list has been nagging me so I want to knock out a bunch of those."
  },
  {
    "objectID": "blog/linguistic-atlas-of-the-pacific-northwest/index.html",
    "href": "blog/linguistic-atlas-of-the-pacific-northwest/index.html",
    "title": "The Linguistic Atlas of the Pacific Northwest",
    "section": "",
    "text": "As a part of my research assistantship this year, I work with the Linguistic Atlas Project, under the direction of Dr. William Kretzschmar. It’s an exciting project to be a part of.\nThere is a lot going on in the lab right now. We’ve got a team of over a dozen undergrad transcribers working dutifully on an NSF grant awarded to Kretzschmar and Dr. Peggy Renwick, not to mention the web developers for the Atlas Project and for Complex Systems.\nOne of the things I’m excited about is that I now have access to all the data for the Atlas Projects from over half a century ago. In a nutshell, what happened is that in the 30s and through the 50s and 60s, Hans Kurath and a team of researchers set out to document the language geography of the United States and Canada. Armed with whatever recording devices they could afford, several hours’ worth of interview questions, and expert phonetic transcribers, they set out to document all the accents and dialects of English in North America.\nThey were partially successful. Starting in New England and in the East, they talked to a couple thousand people, painstakingly analyzed the data, and published a couple multi-volume works specifically focused on certain areas of the United States. Thus, we have the Linguistic Atlases of New England (LANE), the Gulf States (LAGS), and the Upper Midwest (LAUM). However, funding was cut short. Realizing they may have bitten off more than they can chew, the data collected in other portions of the country was never published, other than some brief overviews by some of the researchers. Time passed, and for one reason or another, the majority of this unpublished data disappeared into obscurity.\nBy the late 1970s and into the 1980s, the original handwritten field notes and any extant recordings were scattered across multiple locations. The original researchers’ dream to publish this data for a general audience was never fulfilled, let alone the majority of potential publications for a more specialized audience. The data was always supposed to be accessible to anyone interested, and just a few decades later it was collecting dust in basements, accessible to probably the half a dozen people that knew about it.\nIn 1983 some of the data was under threat of being thrown out. Luckily, William Kretzschmar offered to take all the data from all projects and house it at the University of Georgia. Since then, he has been in the process of realizing the original researchers’ dream of making the data accessible. In the 21st century, that means digitizing it all and putting it online. And there has been success in that endeavor.\nThis is where I come in. As a lowly out-of-state grad student, I’m not particularly concerned with language around Georgia, as interesting as it is. I do however like research on the opposite side of the country: the Pacific Northwest. Only after reading about the Linguistic Atlas of the Pacific Northwest (LAPNW) did I realize that all that data was being housed by my own university. So as soon as I was offered the assistantship in the Linguistic Atlas office, I expressed interest in the LAPNW data. Well, just today, I made a visit to the repository where all the data is held.\nAfter sitting alone for half an hour on the concrete floor literally in the furthest corner in that warehouse, I was quickly able to assess the situation. From what I’ve been able to tell, there are just four boxes of LAPNW data. Compared to the dozens of boxes in the large-scale projects (LANE, LAGS, etc.), it’s a meager project. One box contains the original handwritten notes for about half of the 51 participants, which is great, but I’m a little sad that some of the originals have been lost. But the other three boxes were all copies, including a complete set for all participants and another partial set. I don’t know who did the photocopies or when they were done, but I’m really glad we have them.\nSo, I brought them back to the office and I’ve started to look through them. It’s a bit exciting for me actually. Since being housed at UGA, I don’t know if these boxes have been opened by anyone. As far as I know, there are probably ten people in the world that would be interested in the LAPNW data, and certainly none of them have had the ability to peruse UGA’s repository. So this stuff literally hasn’t seen the light of day for decades. I don’t know what I’m going to do with this goldmine, but I’d sure like to revive it somehow and possibly do what I can to make it accessible. It’s an exciting time for me."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "",
    "text": "In the summer of 2016, I went to Cowlitz County, Washington to do traditional sociolinguistic interviews. I talked to 54 people and gathered my first audio corpus. It took a lot of preparation beforehand and it took a lot of time in the field. What I could not have expected was the amount of time it would take to transcribe that corpus. Now, two years later, I have finally finished transcriptions.\nThis come after a lot of work. Since others might be going through the same thing, I thought I’d share some thoughts on transcribing a sociolinguistic corpus."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#finding-the-motivation",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#finding-the-motivation",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "Finding the motivation",
    "text": "Finding the motivation\nI think my original goal was to have it all transcribed by the end of 2016. So I gave myself about five months. But then I did the first hour of audio and it took me about 5 hours. Yikes!  At that rate, I estimated it would take about 200 hours of work to finish. I think staring down the barrel of any 200 hour task is a motivation killer. So I put it off.I don’t know what I expected—of course it’s going to take a long time to transcribe!\nI wrote a blog post nine months later when I had my first wake up call that I needed to get transcriptions done. I talked about some of the struggles I had getting started but mostly made excuses for why I hadn’t done much. And I got a lot of work done over the next month or so and made it about a quarter of the way through. I remember though just getting burned out after just 10 or 15 minutes of work and would call it a day after half an hour. At that rate, yeah, it’ll take forever.\nSo I put it off for an entire year. In the meantime I was getting a lot done—mostly to distract me from the task I inevitably have to do before graduating. For some reason this distraction was in the form of collecting more audio. I got some laboratory audio, and gathered another corpus using Amazon Mechanical Turk, and in January I went out to Utah to do some more fieldwork. And yet, this audio from 2016 was collecting dust on my computer, just waiting to be analyzed. I think I found that it was easier to collect new data than it was to finish processing the old stuff. Consequently, I had collected something like 150 hours of audio over two years for various projects—and less than 5% of it was processed.\nWhen I finally defended my prospectus in April, it occurred to me that if I wanted to graduate in 2019, I needed to have data to write about. And the only way to do that was to transcribe that darn audio. So, that was what finally got me to crack down and work at this every day. Even then, it took three months of grinding to finally finish. But I’m so glad it’s done."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#a-rite-of-passage",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#a-rite-of-passage",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "A rite of passage",
    "text": "A rite of passage\nI mentioned as a part of my celebratory tweetstorm that doing this kind of work might be something like a rite of passage for sociolinguists.\n\n\nI think putting this much work into a corpus is some sort of rite of passage for sociolinguists. I'm glad I went through it, but ugh, never again.\n\n— Joey Stanley (@joey_stan) July 11, 2018\n\n\nIt seems like a lot of sociolinguists do research on their own corpora, and while the flashy part of statistical analysis, data visualization, or even fieldwork stories are what you see and hear about the most, a significant portion of what we do is the behind-the-scenes tedium on the computer. My university doesn’t have a huge group of sociolinguists and there’s no sort of shared corpus that we can use. So if I want to study contemporary spoken English, I was going to have to collect the audio myself. I think would have done fieldwork myself anyway though. I think it was always something I’ve wanted to do. Plus, there’s this:Yes, the Linguistic Atlas Project has been here since the 80s, but very few of those recordings are transcribed, so they’re of little use in their current state.\n\n\nAlso, shout out to @DrDialect, who I heard say at a Q&A at SECOL in 2015 something like, \"the best career move you can do is to create a corpus: you'll be able to analyze it forever.\" Some of the best advice I've ever heard.\n\n— Joey Stanley (@joey_stan) July 11, 2018\n\n\nAnd from the looks of it, this corpus that I now have is definitely going to last me a while, that’s for sure!"
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#what-software-did-i-use",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#what-software-did-i-use",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "What software did I use?",
    "text": "What software did I use?\nFor transcription, I think there are two ways of doing it. The first method is to find some software that will automatically transcribe it for you, and since it’s not going to be perfect, then spend the time to correct that transcription. I considered doing that, specifically using the transcriber in DARLA. But I found that it took much longer to correct the transcriptions that it would have taken me to just do it by hand. However, DARLA specifically says on their website that their automatic transcriber is not great, so my rate might have been better if I had used a different transcriber. DARLA was what came to mind because it’s easy to use and free. You might have better luck if you use a more sophisticated transcriber.\nThe other option therefore was to just do it myself. As far as I can tell, there are two or three main pieces of software you can use. One is Transcriber. This is one that we use in the Linguistic Atlas Office when we have our undergrads do transcriptions. It’s free and easy to use. One concern is that it’s a little bit tricky to get its output to a TextGrid format. The other concern was that I couldn’t see the spectrogram to accurately place boundaries. Another option is ELAN, which I hear is fantastic. The only reason I didn’t use it was frankly because I didn’t want to take the time to learn a new program.\nWhat I settled on was just plain ol’ Praat. It’s software that I’m comfortable with and I’ve used a lot. I can zoom in as close as I want so I can easy skip over stutters or other noise. Plus, I create a TextGrid right there, which is the format I’m most comfortable working with for scripting purposes. The downside to Praat is that I ended up having to use my trackpad on my laptop more than I wanted to (for scrolling side to side and placing boundaries). I wanted to avoid using my mouse as much as possible because I feel like it hurts my wrist more and I don’t want carpel tunnel.\nBased on my own experience, what I would recommend not doing is hiring out the transcriptions unless you’re not able to do it yourself. For one, I’m cheap, and didn’t want to pay however much per minute of audio. But more importantly, going through my audio a second time gave me a chance to pick up on things that I didn’t catch or forgot about when I was doing the interviews in person. Things like interesting linguistic phenomena  or passages I may want to quote later. Using the Praat textgrids, I just added a separate tier for my own annotations and could make whatever notes I wanted to about a particular section of audio. I learned so much about my people going through it a second time, and I don’t think I would have gotten those intuitions about their speech if I had hired it out. Of course, if you need the transcriptions sooner than you can process them or if you’re not able to do the work yourself, then of course hiring it out might be the better option.I got a token of liketa and two people said I and John instead of John and I which was super cool. I don’t remember those specifically and would never have caught them if I didn’t do the transcriptions myself."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#the-next-steps",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#the-next-steps",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "The next steps",
    "text": "The next steps\nSo while finishing those transcriptions was a monster step, unfortunately the work wasn’t done.\n\n\nNow I've just got to do forced alignment (which includes spell checking) and extract some formants and I'll be ready to go!\n\n— Joey Stanley (@joey_stan) July 11, 2018\n\n\n\nForced alignment\nI’ve been using DARLA for the past few years, but I had some trouble getting the long audio files to process using their web interface. So this gave me a great opportunity to download and install the Montreal Forced Aligner on my own computer. Having this in-house provides lots of benefits like processing the files in bulk and quicker turnaround time since I don’t have to upload the files.\nThe bad news was that I had to do the spell-checking myself. I completely took for granted that DARLA can handle out-of-dictionary words by guessing their pronunciation. So since the Montreal Forced Aligner doesn’t do that, I had to check the words myself. When you run it, it’ll produce a list of out-of-dictionary words for you, so all you need to do is add them to the dictionary or correct the spelling in Praat. It seems simple, but it takes a long time. I had at least 20 or 30 typos or new words in every interview, so I probably spent 15 or 20 hours just doing the spell-checking (I think I added over 1000 new dictionary entries too!).\nLuckily, all this was made easier with the help of some custom Praat scripts I wrote for this project. One does pre-processing to get the files ready for forced-alignment. It splits the audio and textgrid into two halves (it was easier to process that way), it moves these files into a specific directory, and renames the tiers so that they’re consistent. As a bonus, it spits out the command that I need to use to run the aligner on those specific files, so all I need to do is copy and paste that into my terminal and it’ll go on its merry way. This was super helpful because typing path names over and over got old real quick.\nOnce the spell-checking was done and the files were aligned, I had a post-processing script that I used. This one rejoins the two halves into one TextGrid again, adds the new phoneme and word tier to the top of the main TextGrid (so I’ve got the phoneme, word, sentence, and other tiers all in one file), and saves this in that speaker’s directory on my hard drive. Super handy.\nNow ideally, I would go back and hand-check all the boundaries. Maybe one day I’ll have the time to do that, but oh my goodness that’s not going to happen any time soon.\n\n\nFormant extraction\nSo keep in mind that all this work, the nearly 200 hours I’ve put into transcribing and force aligning, was mostly just so I could have Praat know where the vowels were in the audio.\nSo, I modified a couple scripts I wrote to do formant extraction. Of course, I’ve mostly worked with shorter passages of audio (word lists and reading passages and stuff), so what I didn’t anticipate was that Praat kind of has trouble working with audio longer than about 30 minutes. So I had to modify the script so that it splits the audio into roughly five minute chunks, processes each one individually, and then stitches all the output back together.\nAnd of course, the formant measurements ideally should be handchecked. But again, I just spent way too much time transcribing, so I’m not about to spend even more time hand-checking these. Not yet at least."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#the-end-result-a-giant-spreadsheet",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#the-end-result-a-giant-spreadsheet",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "The end result: A giant spreadsheet!",
    "text": "The end result: A giant spreadsheet!\nSo what were the main steps here?\n\nCollect audio.\nTranscription.\nForced alignment.\nFormant extraction.\n\nWhat do I have now? A giant spreadsheet. All this work has been so that I can get a big ol’ spreadsheet that I can then analyze in R. That’s where I am right now. I’ve got the finalized dataset that I’ll use for my dissertation, so I don’t even need to open up Praat much anymore, or even plug in my external hard drive. Almost all my work is in R now. But it is quite satisfying to have this monster spreadsheet of my own data."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#conclusions",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#conclusions",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "Conclusions",
    "text": "Conclusions\nTranscribing (and the subsequent processing of) a sociolinguistic corpus takes a ton of time, patience, diligence, and determination. My eyesight may have suffered a little bit from staring at the computer, my headphones are a little worn down, my keyboard has had to endure well over a million keystrokes, and my wrists and fingers sure took a hit. But, y’know what? It’s a lot better than it used to be. At least we have tools like forced-alignment, FAVE, and Praat to make our lives easier. But in the end, it is really awesome to have completed this corpus."
  },
  {
    "objectID": "blog/reshaping-vowel-formant-data-with-tidyr/index.html",
    "href": "blog/reshaping-vowel-formant-data-with-tidyr/index.html",
    "title": "Reshaping Vowel Formant Data with tidyr 1.0",
    "section": "",
    "text": "Vowel trajectory data can be tricky to work with in R. Sometimes I need to reshape my data into specific format to make a particular type of visual, run some test, or calculate some number. And it can be frustrating. While it has always been possible to accomplish this task in R, with the pivot_longer function from latest version of tidyr, all this reshaping can be done in a single line of code! This post shows you how."
  },
  {
    "objectID": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#introduction",
    "href": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#introduction",
    "title": "Reshaping Vowel Formant Data with tidyr 1.0",
    "section": "Introduction",
    "text": "Introduction\nTo be clear, I understand the frustration that comes when trying to reshape your data. Here’s an actual tweet I posted a few years ago after several hours of wrangling.\n\n\nIn any given project, I think I spend more time trying to figure out how to reshape, melt, and cast my data than anything else.\n\n— Joey Stanley (@joey_stan) November 17, 2016\n\n\nSome of you may be able to deduce that I was working with Hadley Wickham’s reshape2 package, which had the functions melt and cast. In fact, I was so frustrated that I ended up hacking a solution in a Praat script(!) since I couldn’t figure out how do it in R. Fortunately for me, Wickham later released the tidyr package, sort of meant to replace reshape2, and included the functions gather and spread. With the help of the relevant section of R for Data Science I was able to learn to use these functions well and now I use them all the time.\nWell, tidyr has grown up and is now on version 1.0. Some people (not me though) didn’t find the names or syntax of gather and spread intuitive enough, so they were renamed pivot_longer and pivot_wider. But it was more than just a cosmetic change: these functions got some major modifications to make them far more powerful than they were before.\nVowel trajectory data is complicated because we’ve got multiple formant measurements at multiple timepoints. You may also have multiple versions of the measurements, like normalized or transformed into Barks. If you’re working with this kind of data, you need to be comfortable with any sort of reshaping you need so that you can quickly and effectively do what you need to do with your data.\nNote that much of this blog post overlaps with my post on visualizing trajectory data. That tutorial uses the old gather and spread functions. This post shows how to accomplish the same thing with the new pivot_longer function."
  },
  {
    "objectID": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#prep-the-data",
    "href": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#prep-the-data",
    "title": "Reshaping Vowel Formant Data with tidyr 1.0",
    "section": "Prep the data",
    "text": "Prep the data\nFirst, we’ll need to make sure we’ve got the latest version of tidyr.\n\nupdate.packages(\"tidyr\")\n\nI’ll now load it, together with the rest of the tidyverse:\n\nlibrary(tidyverse)\n\nLet’s start with a regular dataset. This is one that I’ve used in lots of my other blog posts. I sat at my kitchen table and read about 300 sentences. The audio was processed using DARLA’s fully automated option: it was automatically transcribed, then force-aligned with ProsodyLab (DARLA used ProsodyLab instead of the MFA back then), and then formants were automatically extracted using FAVE. Not the cleanest method, but it’s good enough for this post. My guess is that if you’ve used FAVE before, the output will look very similar to your own data.\nNote that I’m using the read_csv function instead of read.csv. The one with the underscore will preserve the original column names, like F1@20%, even if it’s hard to work with them down the road. Meanwhile read.csv will replace troublesome characters with periods, so that F1@20% becomes F1.20.. I’ll use the underscore version in this post, which has ramifications for code later on. If things crash for you, check to see that you read your data in with read_csv.\n\njoey_raw &lt;- read_csv(\"http://joeystanley.com/data/joey.csv\", \n                     show_col_types = FALSE) %&gt;%\n    print()\n\n# A tibble: 3,504 × 43\n   name       sex   vowel stress pre_word word    fol_word    F1    F2 F3    \n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1 LA000-Joey M     AY         1 THE      THAI    SP        826  1520. 2529.5\n 2 LA000-Joey M     AY         1 SP       TIMER   SP        581. 1306  1835.6\n 3 LA000-Joey M     ER         0 SP       TIMER   SP        484. 1449. 1644.6\n 4 LA000-Joey M     IY         1 SP       HERE    WE'LL     235. 2044. 3106.3\n 5 LA000-Joey M     IY         1 HERE     WE'LL   SP        302. 1974. 2549.5\n 6 LA000-Joey M     AA         1 SP       BARRING INJURY    573.  925. 2296.1\n 7 LA000-Joey M     IH         1 BARRING  INJURY  OR        362. 2262. 2591.2\n 8 LA000-Joey M     IY         0 BARRING  INJURY  OR        258. 2222. 3197.5\n 9 LA000-Joey M     ER         0 INJURY   OR      A         370.  872. 1654.5\n10 LA000-Joey M     EY         1 A        CHANGE  OF        428. 2210. 2531.2\n# ℹ 3,494 more rows\n# ℹ 33 more variables: F1_LobanovNormed_unscaled &lt;dbl&gt;,\n#   F2_LobanovNormed_unscaled &lt;dbl&gt;, B1 &lt;dbl&gt;, B2 &lt;dbl&gt;, B3 &lt;chr&gt;, t &lt;dbl&gt;,\n#   beg &lt;dbl&gt;, end &lt;dbl&gt;, dur &lt;dbl&gt;, plt_vclass &lt;chr&gt;, plt_manner &lt;chr&gt;,\n#   plt_place &lt;chr&gt;, plt_voice &lt;chr&gt;, plt_preseg &lt;chr&gt;, plt_folseq &lt;chr&gt;,\n#   pre_seg &lt;chr&gt;, fol_seg &lt;chr&gt;, context &lt;chr&gt;, vowel_index &lt;dbl&gt;,\n#   pre_word_trans &lt;chr&gt;, word_trans &lt;chr&gt;, fol_word_trans &lt;chr&gt;, …\n\n\nFor now, I’m going to make the dataset smaller so it’s easier to work with:\n\nFirst, I’ll use filter to exclude some stuff out based on linguistic criteria: only stressed vowels, all vowels except diphthongs and /ɚ/, and vowels that are not followed by a sonorant.\nThen I’ll use select to keep just the columns I need in this tutorial. Today, I’ll just need the columns containing information about the vowel, the word, the time, and all the formant measurements. As a shorthand, I use the function contains and then the string \"@\" to get all the columns that contain the @ symbol. This contains shortcut saves a lot of typing and ensures I don’t miss any.\nThen, because I don’t want an overly large dataset, I’ll just keep the first 25 tokens of each vowel. First, I use group_by to group the data by vowel, then I use top_n to keep the top 25 vowels, with the t column specified to make sure that they’re the first 25 based on time. Then since I’m done grouping them, I’ll use ungroup.\n\nThe final result is a dataframe with 275 rows and 13 columns. Much more manageable than the 3504 rows and 43 columns I had before.\n\njoey &lt;- joey_raw %&gt;%\n    filter(stress == 1, \n           !vowel %in% c(\"AY\", \"AW\", \"OY\", \"ER\"), \n           !fol_seg %in% c(\"R\", \"L\", \"M\", \"N\", \"NG\", \"Y\", \"W\")) %&gt;%\n    select(vowel, word, t, contains(\"@\")) %&gt;%\n    group_by(vowel) %&gt;%\n    top_n(25, t) %&gt;%\n    ungroup() %&gt;%\n    print()\n\n# A tibble: 275 × 13\n   vowel word           t `F1@20%` `F2@20%` `F1@35%` `F2@35%` `F1@50%` `F2@50%`\n   &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 AO    CROSSING    623.     379.     942.     426      950.     534.     978.\n 2 AO    WASHINGTON  703.     609.     937.     612.     940      630.     996.\n 3 AO    COST        738.     871.    1398.     862.    1371.     637.    1040.\n 4 AO    FROST       771.     564.    1034.     621.    1046.     283.    1016 \n 5 UH    LOOKED      885.     439.    1074.     436.    1082.     380.    1210.\n 6 AH    COUPLE      891.     405.    1316.     405.    1316.     492.    1236.\n 7 AH    OTHER       899.     402.    1298      407.    1281.     481.    1103.\n 8 AO    CROSS       904.     630.    1094      611.    1106      595.    1124 \n 9 UH    GOOD        908.     388.    1525.     362.    1518.     376.    1466.\n10 AH    OTHER       910.     466.    2088.     960.    1743.     475.    1340.\n# ℹ 265 more rows\n# ℹ 4 more variables: `F1@65%` &lt;dbl&gt;, `F2@65%` &lt;dbl&gt;, `F1@80%` &lt;dbl&gt;,\n#   `F2@80%` &lt;dbl&gt;"
  },
  {
    "objectID": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#one-row-per-token-f1-f2-single-point-plots",
    "href": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#one-row-per-token-f1-f2-single-point-plots",
    "title": "Reshaping Vowel Formant Data with tidyr 1.0",
    "section": "One row per token: F1-F2 single-point plots",
    "text": "One row per token: F1-F2 single-point plots\nLet’s pause for a second and think about how our data is structured. We have 275 rows in our spreadsheet. What does each row represent? A single vowel token. We have 10 formant measurements per token, each in its own column.\nWhat is this type of data good for? Well, it’s perfect for if you want to make F1-F2 plots and you only want to point a single point per token.\nHere’s a simple plot with this data. I’ll only plot the midpoints. There are some outliers—probably bad measurements—but that’s the nature of automatically processed data. In a later blog post, I’ll show some ways to filter those out.\nNotice that I’ve got the little tick marks (`) around F2@50% and F1@50%. Since the @ and % symbols are hard to work with in R, you have to use the ticks when referring to a column name that includes them.\n\nggplot(joey, aes(`F2@50%`, `F1@50%`, color = vowel)) +\n    geom_point() + \n    stat_ellipse(level = 0.67) + \n    scale_x_reverse() + scale_y_reverse() + \n    theme_bw()\n\n\n\n\nSo, if all you need to do is make that kind of plot, then you’re golden and you don’t need to reshape the data at all. The downside is that if you want to work with trajectories, you can’t really visualize them very well."
  },
  {
    "objectID": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#one-row-per-measurement-spectrogram-like-plots",
    "href": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#one-row-per-measurement-spectrogram-like-plots",
    "title": "Reshaping Vowel Formant Data with tidyr 1.0",
    "section": "One row per measurement: Spectrogram-like plots",
    "text": "One row per measurement: Spectrogram-like plots\nAs it turns out, you can store the exact same data using very different structures. The previous section looked at plots when the data was in a one-token-per-row format. It had 275 rows and 13 columns. We can reshape this data so that each individual formant measurement gets is own row. Basically, we’re collapsing all those F1@20%, F2@20% columns down into one. Here’s what that looks like:\n\n## # A tibble: 2,750 x 5\n##    vowel word         t formant_percent    hz\n##    &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n##  1 AO    CROSSING  623. F1@20%           379.\n##  2 AO    CROSSING  623. F2@20%           942.\n##  3 AO    CROSSING  623. F1@35%           426 \n##  4 AO    CROSSING  623. F2@35%           950.\n##  5 AO    CROSSING  623. F1@50%           534.\n##  6 AO    CROSSING  623. F2@50%           978.\n##  7 AO    CROSSING  623. F1@65%           548.\n##  8 AO    CROSSING  623. F2@65%          1018.\n##  9 AO    CROSSING  623. F1@80%           534.\n## 10 AO    CROSSING  623. F2@80%          1037.\n## # … with 2,740 more rows\n\nNotice that a new column, formant_percent contains the column names F1@20%, F2@20%, etc. In corresponding rows, we have the actual formant measurement in the hz column. This is the exact same data, only now we have 2,750 rows of just 5 columns.\nSo how do we do this transformation—and can we clean it up a little bit better?\nIn a previous tutorial, I show how to accomplish this task using tidyr::gather.\n\njoey %&gt;%\n    gather(formant_percent, hz, contains(\"@\"))\n\n# A tibble: 2,750 × 5\n   vowel word           t formant_percent    hz\n   &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 AO    CROSSING    623. F1@20%           379.\n 2 AO    WASHINGTON  703. F1@20%           609.\n 3 AO    COST        738. F1@20%           871.\n 4 AO    FROST       771. F1@20%           564.\n 5 UH    LOOKED      885. F1@20%           439.\n 6 AH    COUPLE      891. F1@20%           405.\n 7 AH    OTHER       899. F1@20%           402.\n 8 AO    CROSS       904. F1@20%           630.\n 9 UH    GOOD        908. F1@20%           388.\n10 AH    OTHER       910. F1@20%           466.\n# ℹ 2,740 more rows\n\n\nThis method is not obsolete per se, but the gather function is considered “retired” because a new and improved function, pivot_longer, can get the job done better. It is recommended that new code use pivot_longer instead of gather now. If you’d like to learn more about pivot_longer, check out the vignette on pivoting. I’ll just go over some of the most relevant details here.\nSo, if you’re like me and are very used to gather, you can mimic its syntax pretty well with pivot_longer.\n\njoey %&gt;%\n    pivot_longer(contains(\"@\"), \n                 names_to = \"formant_percent\", \n                 values_to = \"hz\")\n\n# A tibble: 2,750 × 5\n   vowel word         t formant_percent    hz\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 AO    CROSSING  623. F1@20%           379.\n 2 AO    CROSSING  623. F2@20%           942.\n 3 AO    CROSSING  623. F1@35%           426 \n 4 AO    CROSSING  623. F2@35%           950.\n 5 AO    CROSSING  623. F1@50%           534.\n 6 AO    CROSSING  623. F2@50%           978.\n 7 AO    CROSSING  623. F1@65%           548.\n 8 AO    CROSSING  623. F2@65%          1018.\n 9 AO    CROSSING  623. F1@80%           534.\n10 AO    CROSSING  623. F2@80%          1037.\n# ℹ 2,740 more rows\n\n\nBut it turns out that pivot_longer has some pretty awesome additional functionality that gather didn’t have!\n First off, we shouldn’t be satisfied with the formant_percent column. It’s a single column containing two pieces of information: what formant the measurement is, and how far into the vowel’s duration it came from. If you want, you can easily split the two up using separate.separate has actually now been replaced by separate_wider_position and separate_wider_delim.\n\njoey %&gt;%\n    pivot_longer(contains(\"@\"), \n                 names_to = \"formant_percent\", \n                 values_to = \"hz\")%&gt;%\n    separate(formant_percent, \n             c(\"formant\", \"percent\"), \n             extra = \"drop\")\n\n# A tibble: 2,750 × 6\n   vowel word         t formant percent    hz\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 AO    CROSSING  623. F1      20       379.\n 2 AO    CROSSING  623. F2      20       942.\n 3 AO    CROSSING  623. F1      35       426 \n 4 AO    CROSSING  623. F2      35       950.\n 5 AO    CROSSING  623. F1      50       534.\n 6 AO    CROSSING  623. F2      50       978.\n 7 AO    CROSSING  623. F1      65       548.\n 8 AO    CROSSING  623. F2      65      1018.\n 9 AO    CROSSING  623. F1      80       534.\n10 AO    CROSSING  623. F2      80      1037.\n# ℹ 2,740 more rows\n\n\nHowever, you can now incorporate that separation into the pivot_longer itself! One potential way to do it is by using the names_sep argument. You provide the string that separates the eventual column names and it’ll take care of splitting it up.\n\njoey %&gt;%\n    pivot_longer(contains(\"@\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_sep = \"@\", \n                 values_to = \"hz\")\n\n# A tibble: 2,750 × 6\n   vowel word         t formant percent    hz\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 AO    CROSSING  623. F1      20%      379.\n 2 AO    CROSSING  623. F2      20%      942.\n 3 AO    CROSSING  623. F1      35%      426 \n 4 AO    CROSSING  623. F2      35%      950.\n 5 AO    CROSSING  623. F1      50%      534.\n 6 AO    CROSSING  623. F2      50%      978.\n 7 AO    CROSSING  623. F1      65%      548.\n 8 AO    CROSSING  623. F2      65%     1018.\n 9 AO    CROSSING  623. F1      80%      534.\n10 AO    CROSSING  623. F2      80%     1037.\n# ℹ 2,740 more rows\n\n\nBut this is a little problematic because the values in percent column still have the “%” symbol attached. With separate we could just toss it with extra = \"drop\" but it doesn’t look so easy with pivot_longer.\nFortunately, pivot_longer has a more sophisticated way to separate columns. Instead of names_sep, we can use names_pattern. Here, we use what’s called a regular expression to capture the necessary groups within the old column names. So, if we think about all our column names F1@20%, F2@20%, F1@35%, etc. we can see that the “template” is “F#@##%”—that is, an F followed by a number, then the @ symbol, two numbers, and then the % symbol. So, knowing that \\\\d is the regular expression for a digit, we can use the search pattern \"(F\\\\d)@(\\\\d\\\\d)%\" to represent our column names. The crucial part here is that we have the information we want to keep, F\\\\d and \\\\d\\\\d, in parentheses. pivot_longer will then use those captured groups as new column names.\n\njoey %&gt;%\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\", \n                 values_to = \"hz\")\n\n# A tibble: 2,750 × 6\n   vowel word         t formant percent    hz\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 AO    CROSSING  623. F1      20       379.\n 2 AO    CROSSING  623. F2      20       942.\n 3 AO    CROSSING  623. F1      35       426 \n 4 AO    CROSSING  623. F2      35       950.\n 5 AO    CROSSING  623. F1      50       534.\n 6 AO    CROSSING  623. F2      50       978.\n 7 AO    CROSSING  623. F1      65       548.\n 8 AO    CROSSING  623. F2      65      1018.\n 9 AO    CROSSING  623. F1      80       534.\n10 AO    CROSSING  623. F2      80      1037.\n# ℹ 2,740 more rows\n\n\nAnd like magic, the 10 columns in the old dataset are turned into tidy formant and percent columns.\nThere’s one more thing that can be done, if you’d like. By default, pivot_longer will make these new columns into character vectors. In our case, we may want to turn them into factors and numeric vectors.\nWe can specify this with the names_ptypes argument. As its value, we provide a list of the column names and then how they should be encoded. For formant, I’m turning it into a factor, making sure to specify that the order of the levels goes “F1” first and then “F2”. For the percent column, R doens’t like going directly from a character type to a numeric, so you’ll have to do this more directly using names_transform rather than names_ptypes.\n\njoey %&gt;%\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\", \n                 names_ptypes = list(formant = factor(levels = c(\"F1\", \"F2\"))),\n                 names_transform = list(percent = as.numeric),\n                 values_to = \"hz\")\n\n# A tibble: 2,750 × 6\n   vowel word         t formant percent    hz\n   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 AO    CROSSING  623. F1           20  379.\n 2 AO    CROSSING  623. F2           20  942.\n 3 AO    CROSSING  623. F1           35  426 \n 4 AO    CROSSING  623. F2           35  950.\n 5 AO    CROSSING  623. F1           50  534.\n 6 AO    CROSSING  623. F2           50  978.\n 7 AO    CROSSING  623. F1           65  548.\n 8 AO    CROSSING  623. F2           65 1018.\n 9 AO    CROSSING  623. F1           80  534.\n10 AO    CROSSING  623. F2           80 1037.\n# ℹ 2,740 more rows\n\n\nWhen these are all incorporated into pivot_longer, we get a single function call that takes care of everything. Before, I had to do gather, then separate, and then mutate to change the column types. So this is a lot handier.\nThe reason you might want to do all this is because this format is ideal if you want to to a spectrogram-like plot. Here’s what would happen if you tried one right now:\n\njoey %&gt;%\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\", \n                 names_ptypes = list(formant = factor(levels = c(\"F1\", \"F2\"))),\n                 names_transform = list(percent = as.numeric),\n                 values_to = \"hz\") %&gt;%\n    ggplot(aes(percent, hz, color = formant, group = t)) + \n    geom_path() + \n    theme_bw()\n\n\n\n\nUnfortunately, we need to do a little more data processing. The key is what ggplot uses as the group variable. We some column that will contain a unique value per line. Right now, we’re close: we have a unique value (t) for each vowel token. But there are 10 rows per vowel token, corresponding to the five measurements for F1 and F2. We need to create a new column that will uniquely identify each formant for each vowel token.\nThe simplest way I know is to just use the unite function. It simply combines two columns into one by concatenating the values together. We have the two columns we need, t and formant, so I’ll combine them to create traj_id. I’ll also add remove = FALSE because I want to keep the original ones (so that I can color the lines by formant). When I set this traj_id as the group variable, it works just as expected.\n\njoey %&gt;%\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\", \n                 names_ptypes = list(formant = factor(levels = c(\"F1\", \"F2\"))),\n                 names_transform = list(percent = as.numeric),\n                 values_to = \"hz\") %&gt;%\n    unite(traj_id, t, formant, remove = FALSE) %&gt;%\n    ggplot(aes(percent, hz, color = formant, group = traj_id)) + \n    geom_path() + \n    theme_bw()\n\n\n\n\nTwo comments on this. First, if you have more than one recording, you’ll want to use a different variable than t to uniquely identify tokens. After all, it’s possible that two tokens from two different recordings will have the exact same time measurement. One solution is to combine three variables (speaker, time, and formant), which will work just fine. I like to just create a new column to give each token a unique id. I’ll do that with rowid_to_column and call the new column id, which I’ll do before pivot_longer since the data is still in the one-token-per-row format. I’ll then create traj_id with id and formant.\nAlso, I’ll go ahead and facet the plot by vowel so you can see the individual vowel curves.\n\njoey %&gt;%\n    rowid_to_column(\"id\") %&gt;% # &lt;- this is the new line\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\", \n                 names_ptypes = list(formant = factor(levels = c(\"F1\", \"F2\"))),\n                 names_transform = list(percent = as.numeric),\n                 values_to = \"hz\") %&gt;%\n    unite(traj_id, id, formant, remove = FALSE) %&gt;%\n    ggplot(aes(percent, hz, color = formant, group = traj_id)) + \n    geom_path() + \n    facet_wrap(~vowel) + \n    theme_bw()\n\n\n\n\nSo, if you’re interested in plotting vowel trajectories this way, then you’ll need to reshape the data a bit to get it to work. The downside is that if you want to plot these in the F1-F2 space, you’ll have to do yet another technique."
  },
  {
    "objectID": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#one-row-per-timepoint-trajectories-in-the-vowel-space",
    "href": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#one-row-per-timepoint-trajectories-in-the-vowel-space",
    "title": "Reshaping Vowel Formant Data with tidyr 1.0",
    "section": "One row per timepoint: Trajectories in the vowel space",
    "text": "One row per timepoint: Trajectories in the vowel space\nThe last main way that you may need to reshape your data is similar to the one-row-per-measurement, except we want F1 and F2 pairs to be on the same row. I’m calling this one the one-row-per-timepoint. Here’s what this format looks like. It’s very similar except F1 ad F2 are spread out into two different columns.\n\n## # A tibble: 1,375 x 6\n##    vowel word           t percent    F1    F2\n##    &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 AO    CROSSING    623.      20  379.  942.\n##  2 AO    CROSSING    623.      35  426   950.\n##  3 AO    CROSSING    623.      50  534.  978.\n##  4 AO    CROSSING    623.      65  548. 1018.\n##  5 AO    CROSSING    623.      80  534. 1037.\n##  6 AO    WASHINGTON  703.      20  609.  937.\n##  7 AO    WASHINGTON  703.      35  612.  940 \n##  8 AO    WASHINGTON  703.      50  630.  996.\n##  9 AO    WASHINGTON  703.      65  627. 1074.\n## 10 AO    WASHINGTON  703.      80  612. 1080.\n## # … with 1,365 more rows\n\nIn a previous tutorial, I show how to accomplish this task using gather, then separate, and then spread all in a row. It was a whole ordeal.\n\njoey %&gt;%\n    gather(\"formant_percent\", \"hz\", starts_with(\"F\")) %&gt;%\n    separate(formant_percent, into = c(\"formant\", \"percent\"), extra = \"drop\") %&gt;%\n    spread(formant, hz)\n\n# A tibble: 1,375 × 6\n   vowel word        t percent    F1    F2\n   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 AA    ADOPTED 1622. 20       498  1621.\n 2 AA    ADOPTED 1622. 35       609. 1382.\n 3 AA    ADOPTED 1622. 50       658. 1184.\n 4 AA    ADOPTED 1622. 65       683. 1160.\n 5 AA    ADOPTED 1622. 80       554. 1237.\n 6 AA    BODY    1785. 20       430. 1149.\n 7 AA    BODY    1785. 35       585. 1054.\n 8 AA    BODY    1785. 50       683. 1103.\n 9 AA    BODY    1785. 65       632. 1169.\n10 AA    BODY    1785. 80       597. 1253.\n# ℹ 1,365 more rows\n\n\nIt worked, but three functions was cumbersome and complicated. I even put a note in that tutorial saying it took me forever to figure out how to do that. I had to do it so often that I even wrote a custom function to accomplish it and just had it at the top of all my scripts.\nUsing the new pivot_longer function, here’s one way that closely approximates my old code. I use the same pivot_longer to make it in the one-measurement-per-row format we had before and then I use the inverse function, pivot_wider, to spread it back out a little bit.\n\njoey %&gt;%\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\"formant\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\", \n                 names_ptypes = list(formant = factor(levels = c(\"F1\", \"F2\"))),\n                 names_transform = list(percent = as.numeric),\n                 values_to = \"hz\") %&gt;%\n    pivot_wider(names_from = formant, values_from = hz)\n\n# A tibble: 1,375 × 6\n   vowel word           t percent    F1    F2\n   &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 AO    CROSSING    623.      20  379.  942.\n 2 AO    CROSSING    623.      35  426   950.\n 3 AO    CROSSING    623.      50  534.  978.\n 4 AO    CROSSING    623.      65  548. 1018.\n 5 AO    CROSSING    623.      80  534. 1037.\n 6 AO    WASHINGTON  703.      20  609.  937.\n 7 AO    WASHINGTON  703.      35  612.  940 \n 8 AO    WASHINGTON  703.      50  630.  996.\n 9 AO    WASHINGTON  703.      65  627. 1074.\n10 AO    WASHINGTON  703.      80  612. 1080.\n# ℹ 1,365 more rows\n\n\nBelieve it or not though, pivot_longer can accomplish this in just one function call now. It’s incredible. If you look at the code in the pivot vignette, specifically in the section called “Multiple observations per row”, you’ll see an example using a dataset called anscombe. This is structurally very similar to our original data frame.\nThe way you pull this off I swear is black magic. In the names_to argument, instead of \"formant\", if you just type the keyword \".value\" (with the period), it will somehow know to spread F1 and F2 into separate columns. Try it!\n\njoey %&gt;%\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\".value\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\",\n                 names_transform = list(percent = as.numeric))\n\n# A tibble: 1,375 × 6\n   vowel word           t percent    F1    F2\n   &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 AO    CROSSING    623.      20  379.  942.\n 2 AO    CROSSING    623.      35  426   950.\n 3 AO    CROSSING    623.      50  534.  978.\n 4 AO    CROSSING    623.      65  548. 1018.\n 5 AO    CROSSING    623.      80  534. 1037.\n 6 AO    WASHINGTON  703.      20  609.  937.\n 7 AO    WASHINGTON  703.      35  612.  940 \n 8 AO    WASHINGTON  703.      50  630.  996.\n 9 AO    WASHINGTON  703.      65  627. 1074.\n10 AO    WASHINGTON  703.      80  612. 1080.\n# ℹ 1,365 more rows\n\n\n I’m still trying to wrap my head around it, but it works beautifully. Of course now that there’s no intermediate step of the hz column, I don’t need the values_to = \"hz\" argument, and I don’t need to include the names_ptype list anymore either.Try swapping “percent” for “.value” instead and see what it does.\nI guess the question then is why we would want to do this in the first place. The main reason is because you may want to create trajectory plots in the F1-F2 space. This data is now perfectly suited for that kind of plot.\n\njoey %&gt;%\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\".value\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\",\n                 names_transform = list(percent = as.numeric)) %&gt;%\n    ggplot(aes(F2, F1, group = t)) + \n    geom_path() + \n    scale_x_reverse() + scale_y_reverse() + \n    facet_wrap(~vowel) + \n    theme_bw()\n\n\n\n\nI never said it’d be pretty, but that is the plot. You can of course incorporate whatever you want into the pipeline after you’ve reshaped. Here I get mean trajectories per vowel and pick some vowels to be colored a certain way. In the ggplot, the t column no longer exists, but since we just have one trajectory per vowel, we can use vowel as the group variable.\n\njoey %&gt;%\n    # Reshape\n    pivot_longer(cols = contains(\"@\"), \n                 names_to = c(\".value\", \"percent\"), \n                 names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\",\n                 names_transform = list(percent = as.numeric)) %&gt;%\n    group_by(vowel, percent) %&gt;%\n    \n    # Additional processing\n    summarize_at(vars(F1, F2), mean, na.rm = TRUE) %&gt;%\n    mutate(is_AE = vowel == \"AE\") %&gt;%\n    \n    # Plot\n    ggplot(aes(F2, F1, group = vowel, color = is_AE)) + \n    geom_path(arrow = arrow(angle = 10, length = unit(0.5, \"cm\"), type = \"closed\")) + \n    scale_x_reverse() + scale_y_reverse() + \n    scale_color_manual(values = c(`TRUE` = \"maroon\", `FALSE` = \"gray75\")) + \n    labs(title = \"TRAP in the context of my vowel space\") + \n    theme_bw() + \n    theme(legend.position = \"none\")\n\n\n\n\n(If you’re looking to add labels to the plot, check out my previous tutorial, which shows how to do that.)"
  },
  {
    "objectID": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#conclusion",
    "href": "blog/reshaping-vowel-formant-data-with-tidyr/index.html#conclusion",
    "title": "Reshaping Vowel Formant Data with tidyr 1.0",
    "section": "Conclusion",
    "text": "Conclusion\nIf you’re able to quickly identify what format you data needs to be in and then seamlessly reshape from one structure to another, it will make your life a lot easier. I reshape my data all the time: it’s not just visualizations that rely on a specific format, but things like normalization and outlier detection are much easier when the data is structured a certain way. These are good skills to learn and I highly encourage you to have them under your fingertips."
  },
  {
    "objectID": "blog/animating_formant_trajectories/index.html",
    "href": "blog/animating_formant_trajectories/index.html",
    "title": "Animating Formant Trajectories",
    "section": "",
    "text": "Last week, I presented some work that Lisa Johnson and I have been working on. We discussed ways that vowel formant trajectories can help us learn more about vowel mergers. What seemed to get the most attention though were the data visuals that I created for the talk.\nHere’s what that image looks like up close:\nI haven’t done a cool tutorial for a while and I thought it might make for a good one. So, in this blog post, I’ll show you my step-by-step process for how I made that animation. It’s a little long because I’ve tried to explain the reasoning for why I did what I did. I guess it just shows how much work goes into a nice visual I suppose. Anyway, hopefully you can make it too with your own data!\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(gganimate)\nlibrary(mgcv)\nlibrary(itsadug)"
  },
  {
    "objectID": "blog/animating_formant_trajectories/index.html#load-the-data",
    "href": "blog/animating_formant_trajectories/index.html#load-the-data",
    "title": "Animating Formant Trajectories",
    "section": "Load the data",
    "text": "Load the data\nI’ll start by loading the data that I used in the presentation.\n\nheber &lt;- read_csv(\"lsa2022.csv\") %&gt;%\n  print()\n\n\n\n# A tibble: 8,409 × 9\n# Groups:   token_id [807]\n   token_id    speaker word  allophone percent F1_norm F2_norm   yob start_event\n   &lt;chr&gt;       &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;      \n 1 UT008-Jani… UT008-… kneel ZEAL          0     0.464   2.82   1958 TRUE       \n 2 UT008-Jani… UT008-… kneel ZEAL          0.1   0.434   2.88   1958 FALSE      \n 3 UT008-Jani… UT008-… kneel ZEAL          0.2   0.478   2.81   1958 FALSE      \n 4 UT008-Jani… UT008-… kneel ZEAL          0.3   0.524   2.27   1958 FALSE      \n 5 UT008-Jani… UT008-… kneel ZEAL          0.6   0.619   1.11   1958 FALSE      \n 6 UT008-Jani… UT008-… kneel ZEAL          0.8   0.387   0.960  1958 FALSE      \n 7 UT008-Jani… UT008-… kneel ZEAL          0.9   0.392   1.04   1958 FALSE      \n 8 UT008-Jani… UT008-… kneel ZEAL          1     0.631   1.30   1958 FALSE      \n 9 UT008-Jani… UT008-… built GUILT         0.1   0.430   2.39   1958 TRUE       \n10 UT008-Jani… UT008-… built GUILT         0.2   0.455   2.36   1958 FALSE      \n# ℹ 8,399 more rows\n\n\nThis data has already been processed a little bit. Formants were extracted using FastTrack, and from there I did a bit of cleaning and normalization and whatnot. For simplicity in this blog post, I’ll only work with the allophones  ZEAL and GUILT (which are /il/ and /ɪl/, respectively). To make it possible to run the GAMM, I’ll then convert charactater vectors to factors, and I’ll add the start_event column. For more information on prepping for GAMMs, see Sóskuthy (2017).See this blog post where I introduce these Wells-inspired labels for prelateral vowels."
  },
  {
    "objectID": "blog/animating_formant_trajectories/index.html#fit-the-gamms",
    "href": "blog/animating_formant_trajectories/index.html#fit-the-gamms",
    "title": "Animating Formant Trajectories",
    "section": "Fit the GAMMs",
    "text": "Fit the GAMMs\nNow it’s time to actually fit the GAMMs. This one is, relatively speaking, not too complicated. First, the dependent variable is the normalized F1 measurements. As predictors, I have allophone as a fixed effect which allows the model to have each allophone’s predicted measurements be at different heights (in normalized Hz). I then have percent as a smooth by allophone, which allows the model to fit a curve from the onset to the offset of the vowel, independently for each allophone. I also have a smooth for birth year (yob) by allophone, which allows the model to fit a curvey from the earliest birth year to the latest birth year, independently for each allophone. The next line (with the ti() function), is the juicy one: it’s the interaction between percent and birth year (yob), which allows the curved line fit from the onset to the offset of the vowel to vary freely as it progresses across birth years. The last two rows include speaker and word as random smooths, by allophone.\nI’ll fit the exact same model twice: once for F1 and again for F2. To avoid repetitive code and potential human error, I’ll use update to change the dependent variable of the model.\n\nf1_gamm &lt;- bam(F1_norm ~ allophone + \n      s(percent, by = allophone) + \n      s(yob, by = allophone) + \n      ti(percent, yob, by = allophone) +\n      s(speaker, percent, by = allophone, bs = \"fs\", m=1) + # I removed xt = \"cr\",\n      s(word,    percent, by = allophone, bs = \"fs\", m=1), \n    data = heber, \n    discrete = TRUE)\n\nf2_gamm &lt;- update(f1_gamm, F2_norm ~ .)\n\nTo avoid autocorrelation in the data, I’ll get the rho values from each model and rerun them with this rho parameter. See Sóskuthy (2017) for more information.\n\nf1_gamm_rho &lt;- start_value_rho(f1_gamm)\nf2_gamm_rho &lt;- start_value_rho(f2_gamm)\n\nf1_gamm &lt;- update(f1_gamm, rho = f1_gamm_rho, AR.start = heber$start_event)\nf2_gamm &lt;- update(f2_gamm, rho = f2_gamm_rho, AR.start = heber$start_event)\n\nOkay, now that we’ve got some GAMMs fit to the data, we can extract predicted measurements. Here’s where where use the itsadug package. I’ll get predicted measurements for both ZEAL and GUILT, for every year from 1920 to 2000, and—here’s the kicker—at a thousand equidistant points within the vowel. Actually, 1001 to avoid fencepost errors reasons, and after chopping off the first and last 10% of the vowel to avoid consonantal effects. I’ve found that kind of fidelity is what makes for some really smooth-looking plots. It also makes so that it takes a several minutes to extract predicted measurements.\n\nf1_preds &lt;- get_predictions(f1_gamm,\n                            cond = list(\n                              allophone = c(\"ZEAL\", \"GUILT\"),\n                              yob = 1920:2000,\n                              percent = seq(0.1, 0.9, length = 1001)),\n                            rm.ranef = TRUE,\n                            print.summary = FALSE)\nf2_preds &lt;- get_predictions(f2_gamm,\n                            cond = list(\n                              allophone = c(\"ZEAL\", \"GUILT\"),\n                              yob = 1920:2000,\n                              percent = seq(0.1, 0.9, length = 1001)),\n                            rm.ranef = TRUE,\n                            print.summary = FALSE)\n\nAt this point, we’re ready to combine the two datasets and do a little bit of light processing.\n\npreds &lt;- bind_rows(`F1` = f1_preds, `F2` = f2_preds, .id = \"formant\") %&gt;%\n  select(-rm.ranef, -speaker, -word) %&gt;%\n  rename(hz_norm = fit)\nhead(preds)\n\n  formant allophone percent  yob   hz_norm         CI\n1      F1      ZEAL  0.1000 1920 0.4013465 0.05088054\n2      F1     GUILT  0.1000 1920 0.4853472 0.05443935\n3      F1      ZEAL  0.1008 1920 0.4012991 0.05085728\n4      F1     GUILT  0.1008 1920 0.4856286 0.05441697\n5      F1      ZEAL  0.1016 1920 0.4012518 0.05083414\n6      F1     GUILT  0.1016 1920 0.4859101 0.05439473\n\n\nWe now have 324,324 rows. The reason it’s so huge is because there are 1001 predicted formant measurements, for each year from 1920 to 2000, for each formant, for each allophone. That’s a lot of data."
  },
  {
    "objectID": "blog/animating_formant_trajectories/index.html#basic-plots",
    "href": "blog/animating_formant_trajectories/index.html#basic-plots",
    "title": "Animating Formant Trajectories",
    "section": "Basic plots",
    "text": "Basic plots\nOkay, now we’re ready for some visuals. As a sanity check, let’s plot this data in the F1-F2 space. That means I’ll quickly have to reshape the data so that F1 and F2 are in separate columns. I’ll use the predicted measurements from 1920 for this plot.See this blog post on all about using pivot_wider on vowel data.\n\npreds %&gt;%\n  pivot_wider(names_from = formant, values_from = c(hz_norm, CI)) %&gt;%\n  filter(yob == 1920) %&gt;%\n  ggplot(aes(hz_norm_F2, hz_norm_F1, color = allophone)) + \n  geom_path(arrow = joeyr::joey_arrow()) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_bw()\n\n\n\n\nOkay, looks decent enough. Let’s plot that same data but in a way that imitates a spectrogram. I’ll facet it by formant so that F1 and F2 have equal space in the plot.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  ggplot(aes(percent, hz_norm, color = allophone)) + \n  geom_path() + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") + \n  theme_bw()\n\n\n\n\nOkay, now that we’ve got the basic gist of the plot, we’re ready to make it a nicer!"
  },
  {
    "objectID": "blog/animating_formant_trajectories/index.html#sprucing-up-the-plots",
    "href": "blog/animating_formant_trajectories/index.html#sprucing-up-the-plots",
    "title": "Animating Formant Trajectories",
    "section": "Sprucing up the plots",
    "text": "Sprucing up the plots\nIf you’ve seen my vowel plots, you know that I like to make things a little bit fancier than what basic ggplot2 does for you. Some things are driven by principles of data visualization, some are aesthetics, and some are just for personal taste.\n\nI’ll reverse the order of F1 and F2 so that F2 is on top, better imitating a spectrogram. I do that using fct_rev within mutate as a pre-processing stage, before the actual ggplot call.\nI’ll change the colors so that they’re a little nicer and colorblind friendly using the scale_color_ptol function in the ggthemes package. This uses Paul Tol’s color schemes, which you can read more about here.\nI’ll also add a title and make the y-axis label a little less code-y with labs.\n\nI’ll change the x-axis ticks so that they line up with values that are more meaningful (what is halfway between 0.5 and 0.25?) with scale_y_continuous.\n\nPretty sure the first time I saw these themes was in this presentation that Joe Fruehwald gave.From here on out I’ll only put comments in the code where code has changed. Or to divide major sections of code.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, color = allophone)) + \n  geom_path() +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) + \n  scale_color_ptol() + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") + \n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") + \n  theme_bw()\n\n\n\n\nAlready, that version of the plot looks a lot crisper than the earlier one.\n\nAdding confidence intervals\nFrom this point on, I’ll do a little more of a step-by-step walk-through of the plot so that you can see what each part does and why I did it that way.\nOnce crucial element for our plot is that we want to see the confidence intervals. We’ll do that with geom_ribbon and use the CI column in our data to help determine the width.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, color = allophone)) + \n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI)) + \n  geom_path() +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) + \n  scale_color_ptol() + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") + \n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") + \n  theme_bw()\n\n\n\n\nOkay, so that’s not pretty. Let’s make it lighter. I’ll set the color of these bands intervals to be \"gray75\" which is kinda like a shade of gray three-quarters of the way from black to white. I always forget this when I make plots, but this is done with the fill argument rather than the color argument.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, color = allophone, group = allophone)) + \n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\") + \n  geom_path() +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) + \n  scale_color_ptol() + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") + \n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") + \n  theme_bw()\n\n\n\n\nOKay that almost works, but when the two overlap the red on is on top. So, I’ll set the alpha level (= transparency) to be 0.5 so that we can see overlapping areas a little better.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, color = allophone)) + \n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) + \n  geom_path() +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) + \n  scale_color_ptol() + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") + \n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") + \n  theme_bw()\n\n\n\n\nThere we go, now we can see the bands. The problem I have now is that I don’t really want the edges of the ribbons to be colored—I’d rather they just be gray. If you look at the code, the color = allophone argument is in the main ggplot function, which means it’ll apply to all layers in the plot where relevant (here, geom_ribbon and geom_path). Instead, I only want it to apply to one layer, geom_path. So I’ll just move it to that function instead.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm)) + \n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) + \n  geom_path(aes(color = allophone)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) + \n  scale_color_ptol() + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") + \n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") + \n  theme_bw()\n\n\n\n\nWell great, what happened here? The ribbons are now really faded or something. As it turns out, when geom_ribbon was inheriting the color = allophone argument, that was what kept the two ribbons distinct from each other. Now, there’s nothing in the geom_ribbon function or its inherited aesthetics from ggplot that is distinguishing ZEAL from GUILT. I don’t want to put color back in. So instead, I’ll put group = allophone. That’ll tell it to group things by allophone. I’ll go ahead and put that in the main ggplot call because it won’t affect geom_path.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, group = allophone)) + \n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) + \n  geom_path(aes(color = allophone)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) + \n  scale_color_ptol() + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") + \n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") + \n  theme_bw()\n\n\n\n\nOkay, so now we’ve got some decent-looking ribbons.\n\n\nIndicate significance\nIn my LSA presentation I had several things in the plot to help indicate where the confidence intervals overlapped. Let’s take a look at that real quick.\n\nSo, there are several things going on here.\n\nWhere the confidence intervals overlap, the color of the line turns to a darker gray.\nIf you look closely, the size of the gray line is thinner than the colored lines.\nA dotted vertical black line indicates the crossing point.\n\nSo, what we need to do is figure out not only where along the duration of the vowel the confidence intervals overlap, but we also need to figure out that crossing point. The first task is pretty straightforward. The second is a bit more complicated. The third is surprisingly tricky. Let’s start with the first.\n\nFinding where confidence intervals overlap\nTo begin, let’s look at the structure of our data as is so we know what we’re working with.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  head()\n\n  formant allophone percent  yob   hz_norm         CI\n1      F1      ZEAL  0.1000 1920 0.4013465 0.05088054\n2      F1     GUILT  0.1000 1920 0.4853472 0.05443935\n3      F1      ZEAL  0.1008 1920 0.4012991 0.05085728\n4      F1     GUILT  0.1008 1920 0.4856286 0.05441697\n5      F1      ZEAL  0.1016 1920 0.4012518 0.05083414\n6      F1     GUILT  0.1016 1920 0.4859101 0.05439473\n\n\nWe’ve got F1 and F2 predictions in the same column (hz_norm). Since we need F1 and F2 to talk to each other to see if they overlap, the first step that we’ll need to do is to reshape the data so that F1 and F2 are in separate columns. This puts the predicted F1 and F2 measurements from any vowel token from any percent on the same row. We’ll do that the same way that we did it previously. We really want the predicted formant values and the confidence intervals so be sure to put both hz_norm and CI in the values_from argument of pivot_wider.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n  print()\n\n# A tibble: 2,002 × 7\n   formant percent   yob hz_norm_ZEAL hz_norm_GUILT CI_ZEAL CI_GUILT\n   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 F1        0.1    1920        0.401         0.485  0.0509   0.0544\n 2 F1        0.101  1920        0.401         0.486  0.0509   0.0544\n 3 F1        0.102  1920        0.401         0.486  0.0508   0.0544\n 4 F1        0.102  1920        0.401         0.486  0.0508   0.0544\n 5 F1        0.103  1920        0.401         0.486  0.0508   0.0544\n 6 F1        0.104  1920        0.401         0.487  0.0508   0.0543\n 7 F1        0.105  1920        0.401         0.487  0.0507   0.0543\n 8 F1        0.106  1920        0.401         0.487  0.0507   0.0543\n 9 F1        0.106  1920        0.401         0.488  0.0507   0.0543\n10 F1        0.107  1920        0.401         0.488  0.0507   0.0542\n# ℹ 1,992 more rows\n\n\nAt this point, it’s pretty straightforward math: if the predicted line for ZEAL plus its confidence interval is lower than the predicted line for GUILT minus its confidence interval, than the difference is significant.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  mutate(is_significant = hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT) %&gt;%\n  \n  print()\n\n# A tibble: 2,002 × 8\n   formant percent   yob hz_norm_ZEAL hz_norm_GUILT CI_ZEAL CI_GUILT\n   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 F1        0.1    1920        0.401         0.485  0.0509   0.0544\n 2 F1        0.101  1920        0.401         0.486  0.0509   0.0544\n 3 F1        0.102  1920        0.401         0.486  0.0508   0.0544\n 4 F1        0.102  1920        0.401         0.486  0.0508   0.0544\n 5 F1        0.103  1920        0.401         0.486  0.0508   0.0544\n 6 F1        0.104  1920        0.401         0.487  0.0508   0.0543\n 7 F1        0.105  1920        0.401         0.487  0.0507   0.0543\n 8 F1        0.106  1920        0.401         0.487  0.0507   0.0543\n 9 F1        0.106  1920        0.401         0.488  0.0507   0.0543\n10 F1        0.107  1920        0.401         0.488  0.0507   0.0542\n# ℹ 1,992 more rows\n# ℹ 1 more variable: is_significant &lt;lgl&gt;\n\n\nAt this point, we can now just revert the shape, and we’re ready to plot.  Reverting the shape with pivot_longer isn’t trivial, because it uses some regex and a special trick with \".value\" that lets you split the resulting column into two. I encourage you to look at a previous blog post of mine about reshaping data.I again encourage you to look at this blog post on all about using pivot_wider on vowel data.\nI’ll incorporate this new information by changing the color of the lines to map to the is_significant column. I’ll also add the size aesthetic and map it to is_significant too. Finally, I’ll control the size by adding scale_size_manual. I’ll purposely exaggerate the difference so we can make sure they’re doing what we want.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  mutate(is_significant = hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT) %&gt;%\n  \n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = is_significant, size = is_significant)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(1, 3)) + \n  scale_color_ptol() +\n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw()\n\n\n\n\nWell shoot. What’s wrong with this plot? I see two things. First, while the significance was accurately detected for F1, it was not for F2 for some reason. The second problem is that we’ve got color for significance rather than by allophone. Let’s fix both of these, starting with the first.\n\n\nA more robust method\nThe code we did for detecting significance was this. Keep in mind that this is just one chunk of our pipeline that I’ve separated for illustrative purposes and it’s not going to run by itself.\n\n# Calculate the difference\nmutate(is_significant = hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT) %&gt;%\n\nThe problem is that this works only if ZEAL is lower than GUILT. Based on the plots, it’s clear that it is, but only for F1. How can we make the calculation more general so that it detects overlaps without us predetermining which one is supposed to be higher?\nThere are probably a lot of solutions out there, but here’s the solution I came up with. Given each formant, we need to programatically determine which allophone is higher. We could do it as something like this:\n\nmutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT)\n\nThis would create a new column called ZEAL_is_higher. If ZEAL is indeed higher, the value in that column for that row is TRUE. Otherwise, it’s FALSE. We can now use this information to calculate the confidence interval a little more robustly. I’ve chosen to use case_when instead of if_else: I find the extra typing worth it for the clarity that it offers.\n\nmutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n       is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                  !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n\nSo what this does is it first determines if ZEAL is higher than GUILT. If it is, then it’ll see if the confidence interval of ZEAL is higher than the confidence interval of GUILT. If it’s lower, it’ll see if it’s lower.\nFor this to work, we’ll need to do this independently for each formant. And for good measure, we should do it independently for each birth year and each percent as well.\n\ngroup_by(formant, yob, percent) %&gt;%\nmutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n       is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                  !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n\nIf we incorporate this into our pipeline, we should have made an improvement to our plot.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = is_significant, size = is_significant)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(1, 3)) + \n  scale_color_ptol() +\n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw()\n\n\n\n\nSuccess! The plot is still bad, but it at least has correctly detected regions of significance for both F1 ad F2. Now we need to figure out how to fix the colors.\n\n\nAdd gray for non-significance\nThe core of the problem is that we are actually mapping two pieces of information to a single aesthetic: allophone and significance. This is something that is normally not done in ggplot2. So, we’re going to go about it in a bit of a hacky way. Basically, what we need to do is expand our case_when function above to three levels: “ZEAL”, “GUILT”, and “not significant.” Fortunately, this is pretty straightforward to do. Here’s what the code looks like:\n\nmutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                      TRUE ~ as.character(allophone)),\n       allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\")))\n\nWhat it does is it creates a new column, allophone_plus_sig. For each row in the spreadsheet, if it’s not significant, or rather if is_significant is FALSE, then the value in the new allophone_plus_sig column is set to “not significant”. Otherwise, it’ll just take the value from the allophone column, which, in our case, is \"ZEAL\" or \"GUILT\". The reason why allophone is wrapped in as.character() is because case_when requires all output values to be of the same type, and since \"not significant\" is a string, specifically a character vector, I have to turn allophone, a factor, into a character vector. Don’t worry too much about it. But, I guess what I need to do is then turn that three-way allophone_plus_sig back into a factor, which allows me to control the ordering.\nOkay, so now we’re ready to incorporate that back into the pipeline. Because we’re using information about the allophone column, it has to happen after our retransformation back into the original format. Here’s the completed pipeline, and plot.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(1, 3)) + \n  scale_color_ptol() +\n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw()\n\n\n\n\nHooray! It looks like it works. Now what we’ll do is set the colors ourselves. Because I’m adding gray to the mix, I can’t use scale_color_ptol anymore and I’ll have to use scale_color_discrete and tell it the colors I want manually. I’ll use ptol_pal() to access those same blue and red colors though and then I’ll add gray as a third option.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  # Revert the shape\n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant)) %&gt;%\n  ggplot(aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw()\n\n\n\n\nHey, we’re looking pretty good! The plot is really starting to come together!\n\n\nFinding transition points\nThe last thing we need to do for indicating significance is finding the transition points and adding that black vertical dotted line. For some reason this is a harder task than I feel like it should be. It’s easy to spot visually, but not quite as easy for the computer to do it. Maybe I’m doing it wrong, but here’s how I do it.\nAs a reminder, here’s what our current dataset looks like just before it gets plotted.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  # Revert the shape\n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant)) %&gt;%\n  print()\n\n# A tibble: 4,004 × 9\n# Groups:   formant, yob, percent [2,002]\n   formant percent   yob ZEAL_is_higher is_significant allophone hz_norm     CI\n   &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;          &lt;lgl&gt;          &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 F1        0.1    1920 FALSE          FALSE          ZEAL        0.401 0.0509\n 2 F1        0.1    1920 FALSE          FALSE          GUILT       0.485 0.0544\n 3 F1        0.101  1920 FALSE          FALSE          ZEAL        0.401 0.0509\n 4 F1        0.101  1920 FALSE          FALSE          GUILT       0.486 0.0544\n 5 F1        0.102  1920 FALSE          FALSE          ZEAL        0.401 0.0508\n 6 F1        0.102  1920 FALSE          FALSE          GUILT       0.486 0.0544\n 7 F1        0.102  1920 FALSE          FALSE          ZEAL        0.401 0.0508\n 8 F1        0.102  1920 FALSE          FALSE          GUILT       0.486 0.0544\n 9 F1        0.103  1920 FALSE          FALSE          ZEAL        0.401 0.0508\n10 F1        0.103  1920 FALSE          FALSE          GUILT       0.486 0.0544\n# ℹ 3,994 more rows\n# ℹ 1 more variable: allophone_plus_sig &lt;fct&gt;\n\n\nThe key is that is_significant column. At some point, all those FALSE values will switch to TRUE. We need to add some new column that indicates where that switch happens. The problem is that tidyverse functions—which this workflow is heavily entrenched in—generally run one row at a time. It’s difficult get some function to interact with some other row without flat out writing a for loop. I’d rather keep all my data processing in a single pipeline if possible, so we’ll have to think of some way to do this.\nThe secret, I’ve found is with the lag or lead functions. To illustrate what they do, let’s take a single, arbitrary list of colors:\n\ncolors &lt;- tibble(colors = c(\"red\", \"yellow\", \"blue\", \"green\"))\ncolors\n\n# A tibble: 4 × 1\n  colors\n  &lt;chr&gt; \n1 red   \n2 yellow\n3 blue  \n4 green \n\n\nWhat lag does is it takes that list, shifts all the elements of that list down by one, and puts an NA at the beginning. Basically, it returns what the next element of the list is. lead does the opposite and shifts all the elements up by one slot. For more information on lead and lag, see the documentation.\n\ncolors %&gt;%\n  mutate(prev_color = lag(colors),\n         next_color = lead(colors))\n\n# A tibble: 4 × 3\n  colors prev_color next_color\n  &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     \n1 red    &lt;NA&gt;       yellow    \n2 yellow red        blue      \n3 blue   yellow     green     \n4 green  blue       &lt;NA&gt;      \n\n\nSo, this can be very useful for us. We want to look at that is_significant column and find rows where the next row is not the same as the current row. So we want to find places where lead(is_significant) is not the same as is_significant.\nSo, the key piece of code we need is this:\n\nmutate(switch = is_significant != lead(is_significant))\n\nNow, we have to be careful about how this is done. Because we have data from lots of years and both formants all pooled together, we want to make sure that they don’t mess each other up. So, we’ll add a group_by function to make sure that the code runs independently for each formant and each year of birth. Otherwise, we might have cases where the offset of one year is compared against the onset of another year, and that’s not what we want. For good measure I’ll also make sure the data is sorted by percent just in case things got out of order at some point.\nSo the code we need to insert now is this block:\n\ngroup_by(formant, yob) %&gt;%\narrange(formant, yob, percent) %&gt;%\nmutate(switch = is_significant != lead(is_significant)) %&gt;%\nungroup() \n\nI’ll go ahead and insert that right after we find the significance. Here’s the full data processing pipeline now:\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  # Find where the switches happen\n  group_by(formant, yob) %&gt;%\n  arrange(formant, yob, percent) %&gt;%\n  mutate(switch = is_significant != lead(is_significant)) %&gt;%\n  ungroup() %&gt;%\n  \n  # Revert the shape\n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n\n  \n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant)) %&gt;%\n  print()\n\n# A tibble: 4,004 × 10\n   formant percent   yob ZEAL_is_higher is_significant switch allophone hz_norm\n   &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;          &lt;lgl&gt;          &lt;lgl&gt;  &lt;chr&gt;       &lt;dbl&gt;\n 1 F1        0.1    1920 FALSE          FALSE          FALSE  ZEAL        0.401\n 2 F1        0.1    1920 FALSE          FALSE          FALSE  GUILT       0.485\n 3 F1        0.101  1920 FALSE          FALSE          FALSE  ZEAL        0.401\n 4 F1        0.101  1920 FALSE          FALSE          FALSE  GUILT       0.486\n 5 F1        0.102  1920 FALSE          FALSE          FALSE  ZEAL        0.401\n 6 F1        0.102  1920 FALSE          FALSE          FALSE  GUILT       0.486\n 7 F1        0.102  1920 FALSE          FALSE          FALSE  ZEAL        0.401\n 8 F1        0.102  1920 FALSE          FALSE          FALSE  GUILT       0.486\n 9 F1        0.103  1920 FALSE          FALSE          FALSE  ZEAL        0.401\n10 F1        0.103  1920 FALSE          FALSE          FALSE  GUILT       0.486\n# ℹ 3,994 more rows\n# ℹ 2 more variables: CI &lt;dbl&gt;, allophone_plus_sig &lt;fct&gt;\n\n\nWe now have a new switch column that is true only if the next timepoint’s significance is different from the current timepoint.\nWe can now incorporate that into the plot. I’ll add the vertical line with geom_vline.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  # Find where the switches happen\n  group_by(formant, yob) %&gt;%\n  arrange(formant, yob, percent) %&gt;%\n  mutate(switch = is_significant != lead(is_significant)) %&gt;%\n  ungroup() %&gt;%\n  \n  # Revert the shape\n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n\n  \n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant)) %&gt;%\n  \n  # Plot\n  ggplot(aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw()\n\n\n\n\nOkay! We’re looking pretty good now. We’ve got a nice looking plot.\n\n\n\nLabels on the plots\nBut we’re not done! Not even with the static plot—and we haven’t even gotten to the animations yet! As a reminder, here’s a plot I used in my LSA talk:\n\nThe only difference now is that we have a legend still. That legend has been bothering me this whole time and we need to get rid of it. The size is something that probably doesn’t need to be in the legend, and we can do a better job at indicating which color is which vowel. I try to avoid legends if at all possible and instead opt to annotate the plot directly.\nAs a first step, we can get rid of the legend by adding theme(legend.position = \"none\") at the end of our plotting code.\n\npreds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  # Find where the switches happen\n  group_by(formant, yob) %&gt;%\n  arrange(formant, yob, percent) %&gt;%\n  mutate(switch = is_significant != lead(is_significant)) %&gt;%\n  ungroup() %&gt;%\n  \n  # Revert the shape\n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n\n  \n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant)) %&gt;%\n  \n  # Plot\n  ggplot(aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\n\nThat at least takes care of removing the legend. But now we need to figure out how to annotate the plot.\nA simple solution is to manually put some labels in. That works well if there’s only one plot you need to do. But since you may want to plot many vowel pairs, it gets cumbersome to have to do that every time. Plus, we’ll be animating these, which means the lines could move around and we don’t want to have to reposition the label every time. A better solution would be to programatically generate plot labels.\nSo what we’ll need to do is take our existing data and generate a new dataset that contains the information for where we want to put the labels. So let’s save the plotting data without actually plotting it.\n\nplotting_data &lt;- preds %&gt;%\n  filter(yob == 1920) %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  # Find where the switches happen\n  group_by(formant, yob) %&gt;%\n  arrange(formant, yob, percent) %&gt;%\n  mutate(switch = is_significant != lead(is_significant)) %&gt;%\n  ungroup() %&gt;%\n  \n  # Revert the shape\n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n\n  \n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant))\n\nI’ve found that a good position for the label is to put it near the onset of the vowel, slightly outside of the confidence intervals. So, let’s get those vowel onsets.\n\nlabel_data &lt;- plotting_data %&gt;%\n  filter(percent == min(percent)) %&gt;%\n  print()\n\n# A tibble: 4 × 10\n  formant percent   yob ZEAL_is_higher is_significant switch allophone hz_norm\n  &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;          &lt;lgl&gt;          &lt;lgl&gt;  &lt;chr&gt;       &lt;dbl&gt;\n1 F1          0.1  1920 FALSE          FALSE          FALSE  ZEAL        0.401\n2 F1          0.1  1920 FALSE          FALSE          FALSE  GUILT       0.485\n3 F2          0.1  1920 TRUE           FALSE          FALSE  ZEAL        2.08 \n4 F2          0.1  1920 TRUE           FALSE          FALSE  GUILT       1.81 \n# ℹ 2 more variables: CI &lt;dbl&gt;, allophone_plus_sig &lt;fct&gt;\n\n\nNow, we could just put both labels above the line. Here I’ll take the confidence interval times 1.5 and use that as the vertical position. That way it’s above the shaded area.\n\nggplot(plotting_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = hz_norm + 1.5*CI)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\n\nBut that doesn’t look too great and are in fact misleading. A better solution would be to put the upper one above the line and the lower one below it. Fortunately, we already know which line is higher because we got the ZEAL_is_higher already figured out. So, with another case_when statement, we can get a higher position for the top one and a lower position for the bottom one. Here’s\n\nlabel_data &lt;- plotting_data %&gt;%\n  filter(percent == min(percent)) %&gt;%\n  mutate(label_position = case_when(allophone == \"ZEAL\"  &  ZEAL_is_higher ~ hz_norm + 1.5*CI,\n                                    allophone == \"GUILT\" &  ZEAL_is_higher ~ hz_norm - 1.5*CI,\n                                    allophone == \"ZEAL\"  & !ZEAL_is_higher ~ hz_norm - 1.5*CI,\n                                    allophone == \"GUILT\" & !ZEAL_is_higher ~ hz_norm + 1.5*CI)) %&gt;%\n  print()\n\n# A tibble: 4 × 11\n  formant percent   yob ZEAL_is_higher is_significant switch allophone hz_norm\n  &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;          &lt;lgl&gt;          &lt;lgl&gt;  &lt;chr&gt;       &lt;dbl&gt;\n1 F1          0.1  1920 FALSE          FALSE          FALSE  ZEAL        0.401\n2 F1          0.1  1920 FALSE          FALSE          FALSE  GUILT       0.485\n3 F2          0.1  1920 TRUE           FALSE          FALSE  ZEAL        2.08 \n4 F2          0.1  1920 TRUE           FALSE          FALSE  GUILT       1.81 \n# ℹ 3 more variables: CI &lt;dbl&gt;, allophone_plus_sig &lt;fct&gt;, label_position &lt;dbl&gt;\n\n\nNow, there are more efficient ways of coding that, but I like to be explicit, so I’ve opted for the slightly more verbose method.\n\nggplot(plotting_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = label_position)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\n\nThere we go. That’s a little bit better. For good measure, I’m going to also color the text using the same color assigned to the allophones. My first time doing this, I made the mistake of using the allophone_plus_sig column just like the lines. The problem is now I had gray labels, which is not what I wanted.\n\nggplot(plotting_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = label_position, color = allophone_plus_sig)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\n\nInstead, I should color it using just plain allophone. That way ZEAL is always blue and GUILT is always red, regardless of the significance of the vowel at the onset.\n\nggplot(plotting_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = label_position, color = allophone)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\n\nOkay, now that we’ve got a decent looking plot (at least least it matches the plot I used in my presentation), we can start to work on animating it!"
  },
  {
    "objectID": "blog/animating_formant_trajectories/index.html#animate",
    "href": "blog/animating_formant_trajectories/index.html#animate",
    "title": "Animating Formant Trajectories",
    "section": "Animate!",
    "text": "Animate!\nThe gganimate package is awesome and with just a few lines of code, we can make an animation. The simplest way to think about the animations is that whatever variable you would use in a facet_wrap, you use that as the frames in the animation. In our plot, we are already faceting by formant (F1 and F2), but we want the animation to happen by birth year. Currently we’re only looking at one birth year (1920). In our animation, we would take out that one-year filter and animate along that variable instead.\nFirst, what I’m going to do is regenerate my data so that all birth years are in there.\n\nanimating_data &lt;- preds %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  # Find where the switches happen\n  group_by(formant, yob) %&gt;%\n  arrange(formant, yob, percent) %&gt;%\n  mutate(switch = is_significant != lead(is_significant)) %&gt;%\n  ungroup() %&gt;%\n  \n  # Revert the shape\n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n\n  \n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant))\n\nlabel_data &lt;- animating_data %&gt;%\n  filter(percent == min(percent)) %&gt;%\n  mutate(label_position = case_when(allophone == \"ZEAL\"  &  ZEAL_is_higher ~ hz_norm + 1.5*CI,\n                                    allophone == \"GUILT\" &  ZEAL_is_higher ~ hz_norm - 1.5*CI,\n                                    allophone == \"ZEAL\"  & !ZEAL_is_higher ~ hz_norm - 1.5*CI,\n                                    allophone == \"GUILT\" & !ZEAL_is_higher ~ hz_norm + 1.5*CI))\n\nNow, I could make a static plot that includes all the data at once.\n\nggplot(animating_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = label_position, color = allophone)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\n\nA mess. But that’s fine. We could also make a version of the plot where there’s one panel per birth year.\n\nggplot(animating_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = label_position, color = allophone)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(yob~formant, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\")\n\n\n\n\nSo here’s all the information at once. What we need to do now is animate across those birth years. It literally takes one line of code to do this: transition_time(yob). I’ll save the plot into an object called a. I’ll then take that a object and animate it with animate. I’ll specificy the height, width, and resolution. This line of code may take a minute or so. Finally, I’ll export the plot with anim_save, which lets me specify the path and the frames per second.\n\na &lt;- ggplot(animating_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), aes(xintercept = percent), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = label_position, color = allophone)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = \"Predicted trajectories for ZEAL and GUILT: birth year = 1920\",\n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  transition_time(yob)\nanimate(a, height = 7.5, width = 13.33, unit = \"in\", res = 150)\nanim_save(filename = \"./plots/lsa2022/lsa2022_temp.gif\", fps = 30)\n\n\nOkay, this is looking pretty good, but there are a few details we’ll want to change. One is that we need to change the title now because it’s not just 1920. Fortunately gganimate has a way to incorporate the year into the title itself. So I’ll chage the title we had in labs so that it incorporates {frame_time}.\nAnother smaller detail is the vertical black line. You’ll notice that it sort of moves quickly across the screen from left to right even though there’s no corresponding overlap in the confidence intervals. If you look at each plot individually, that line isn’t there. So what’s up with that? What’s actually happening in the data is the black line from the earlier in the gif disappears and then appears later on. So gganimate fills in the gaps because it assumes all the black lines are the same underlying element, so it’ll add transitions between them.\nWith a little of trial-and-error on my part, I found that that the problem is the group argument in aes. Basically, what it’s doing it assuming that the vertical line “object” should be something that persists across the entire time frame represented here. In reality, we don’t need the thing to persist. There’s no reason why it should be there if it doesn’t have to. Anyway, long story short, to fix it, I’ll add group = yob in geom_vline. That way it’ll treat the vertical from each year as a distinct unit without attempting go connect the dots across time.\n\na &lt;- ggplot(animating_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), \n             aes(xintercept = percent, group = yob), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = label_position, color = allophone)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = paste(\"Difference between ZEAL and GUILT (birth year: {frame_time})\"), \n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  transition_time(yob)\nanimate(a, height = 7.5, width = 13.33, unit = \"in\", res = 150)\nanim_save(filename = paste0(\"./plots/lsa2022/lsa2022_temp.gif\"), fps = 10)\n\nAt this point, if you have a keen eye, you’ll notice that the black vertical line still kinda hangs out there on the left side for a while. I’ll admit I hadn’t seen that before my LSA presentation so it’s there in the “official” plot too. I don’t quite know how to fix that."
  },
  {
    "objectID": "blog/animating_formant_trajectories/index.html#pulling-it-all-together",
    "href": "blog/animating_formant_trajectories/index.html#pulling-it-all-together",
    "title": "Animating Formant Trajectories",
    "section": "Pulling it all together",
    "text": "Pulling it all together\nSo, we now have our visual. Here is the code from start to finish. First, we got the data we wanted to visualize:\n\nanimating_data &lt;- preds %&gt;%\n  \n  # Reshape\n  pivot_wider(names_from = allophone, values_from = c(hz_norm, CI)) %&gt;%\n\n  # Calculate the difference\n  group_by(formant, yob, percent) %&gt;%\n  mutate(ZEAL_is_higher = hz_norm_ZEAL &gt; hz_norm_GUILT,\n         is_significant = case_when(ZEAL_is_higher  ~ hz_norm_ZEAL - CI_ZEAL &gt; hz_norm_GUILT + CI_GUILT,\n                                    !ZEAL_is_higher ~ hz_norm_ZEAL + CI_ZEAL &lt; hz_norm_GUILT - CI_GUILT)) %&gt;%\n  \n  # Find where the switches happen\n  group_by(formant, yob) %&gt;%\n  arrange(formant, yob, percent) %&gt;%\n  mutate(switch = is_significant != lead(is_significant)) %&gt;%\n  ungroup() %&gt;%\n  \n  # Revert the shape\n  pivot_longer(cols = c(matches(\"hz_norm\"), matches(\"CI\")),\n               names_to = c(\".value\", \"allophone\"),\n               names_pattern = \"(.+)_([A-Z]+)\\\\Z\") %&gt;%\n  \n\n  # Add three-way significance + allophone\n  mutate(allophone_plus_sig = case_when(!is_significant ~ \"not significant\",\n                                        TRUE ~ as.character(allophone)),\n         allophone_plus_sig = factor(allophone_plus_sig, levels = c(\"ZEAL\", \"GUILT\", \"not significant\"))) %&gt;%\n  \n  mutate(formant = fct_rev(formant))\n\nThen we got a subset of that for the labels, and did some slight processing on that.\n\nlabel_data &lt;- animating_data %&gt;%\n  filter(percent == min(percent)) %&gt;%\n  mutate(label_position = case_when(allophone == \"ZEAL\"  &  ZEAL_is_higher ~ hz_norm + 1.5*CI,\n                                    allophone == \"GUILT\" &  ZEAL_is_higher ~ hz_norm - 1.5*CI,\n                                    allophone == \"ZEAL\"  & !ZEAL_is_higher ~ hz_norm - 1.5*CI,\n                                    allophone == \"GUILT\" & !ZEAL_is_higher ~ hz_norm + 1.5*CI))\n\nFinally, we plotted it.\n\na &lt;- ggplot(animating_data, aes(percent, hz_norm, group = allophone)) +\n  geom_ribbon(aes(ymin = hz_norm - CI, ymax = hz_norm + CI), fill = \"grey75\", alpha = 0.5) +\n  geom_path(aes(color = allophone_plus_sig, size = is_significant)) +\n  geom_vline(data = . %&gt;% filter(switch == TRUE), \n             aes(xintercept = percent, group = yob), linetype = \"dotted\") +\n  geom_text(data = label_data, aes(label = allophone, y = label_position, color = allophone)) + \n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  scale_size_manual(values = c(0.5, 1)) + \n  scale_color_manual(values = c(ptol_pal()(2), \"gray50\")) + \n  facet_wrap(~formant, ncol = 1, scales = \"free\") +\n  labs(title = paste(\"Difference between ZEAL and GUILT (birth year: {frame_time})\"), \n       y = \"frequency (normalized)\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  transition_time(yob)\nanimate(a, height = 7.5, width = 13.33, unit = \"in\", res = 150)\nanim_save(filename = paste0(\"./plots/lsa2022/lsa2022_temp.gif\"), fps = 10)\n\n\nNow if you want to get fancy, you could wrap all this up into a function so that it can be applied to whatever vowel pair you want to visualize in your data. That’s what I ended up having to do, which is how I got my four identically-formatted gifs in my presentation. I really like wrapping data viz code into functions because it lets me make lots of plots at once without copying and pasting. Anyway, I’ll let you work that out on your own. 99% of the code is the same and the only differences places where there are explicit mention of the vowel classes. You also of course have to prep the data as well.\nAnd that’s it! Okay, now I expect more people to have animated vowel plots at the next LSA!"
  },
  {
    "objectID": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html",
    "href": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html",
    "title": "Assigning Pseudonyms in R with the babynames package",
    "section": "",
    "text": "options(dplyr.summarise.inform = FALSE)\nRecently, on RWeekly.org, I saw that Hadley Wickham’s babynames package had been updated. I had never heard of it, but when I saw that it contained Social Security data for births in the US from 1880 to 2017, I immediately thought that it would make coming up with pseudonyms a lot quicker in the future. So here’s a tutorial on how I’ve done that.\nlibrary(tidyverse)"
  },
  {
    "objectID": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#pseudonyms",
    "href": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#pseudonyms",
    "title": "Assigning Pseudonyms in R with the babynames package",
    "section": "Pseudonyms",
    "text": "Pseudonyms\nLots of linguistic research involves the use of anonymized data. As part of the process of ensuring our participants’ identities, a common practice is to use pseudonyms when referring to them in research papers. My IRB asks that I destroy records of their actual names once data collection is complete, so these pseudonyms are the only way that I can refer to these folks now.\nAs long as the pseudonym isn’t the person’s actual name, you’re free to call them technically whatever you want. Some people use identifiers that contain lots of metadata (Proj_003_F_1991), but are not particularly personable or easy to read sometimes.I suppose you could use whimsical names like Spiderman and Batman, but that might not be the most professional or respectful.\nI like to take a more personal approach and give them real, plausible names, leaving the metadata to other spreadsheets. Tagliamonte (2006:51) and Schilling (2013:253–254) both recommend this approach. It gives a little more life to any quotations you might pull from the interviews and the names are easier to remember, which is helpful not only for yourself but also for your readers if you use multiple quotes from any one person (especially when spread across multiple papers).Tagliamonte, Sali A. (2006) Analysing sociolinguistic variation. Cambridge: Cambridge University Press.Schilling, Natalie. (2013) Sociolinguistic fieldwork. Cambridge: Cambridge University Press.\nExactly how you choose these pseudonyms is up to you, but typically you pick something that is consistent with the age, gender, and ethnicity of the speaker. As it turns out, you can use the babynames package to help automate some of this name selection. (Edit: The code for this post is now available on GitHub.)"
  },
  {
    "objectID": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#the-babynames-package",
    "href": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#the-babynames-package",
    "title": "Assigning Pseudonyms in R with the babynames package",
    "section": "The babynames package",
    "text": "The babynames package\nSome R packages contain lots of new functions that perform some task. Others, like the babynames package, just contain data. Specifically, it contains all the baby names registered by Social Security from 1880 to 2017 There are four datasets here, but the one that I’ll focus on is simply called babynames. Let’s load the data and see what it looks like:Okay, almost all of them: names that were used less than five times in a year are not released by Social Security for privacy concerns. This makes up 2%–9% of people each year.\n\n#install.packages(\"babynames\")\nlibrary(babynames)\n\nTo access the data, I’ll type babynames::babynames. The first babynames refers to the package, and the second refers to the dataset called babynames within that package. Slightly confusing, but no biggie.\n\nhead(babynames::babynames)\n\n# A tibble: 6 × 5\n   year sex   name          n   prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n1  1880 F     Mary       7065 0.0724\n2  1880 F     Anna       2604 0.0267\n3  1880 F     Emma       2003 0.0205\n4  1880 F     Elizabeth  1939 0.0199\n5  1880 F     Minnie     1746 0.0179\n6  1880 F     Margaret   1578 0.0162\n\n\nA you can see, it contains five columns. The year, sex, and name columns are straightforward. The last two, n and prop, help give you an idea of how common the name was, with n being the number of babies born that year with that name and prop being the proportion of babies that year with that name for that sex.\nAs it turns out, this dataset is a lot of fun to play with. I’ll let you explore what you want, but this is nice to see trends for a specific name over time and other stuff. I’ve got a bonus section down at the bottom where I make a couple plots."
  },
  {
    "objectID": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#assigning-a-single-pseudonym",
    "href": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#assigning-a-single-pseudonym",
    "title": "Assigning Pseudonyms in R with the babynames package",
    "section": "Assigning a single pseudonym",
    "text": "Assigning a single pseudonym\nSo, with this dataset, we can easily filter out names from a specific year and sex and then sort them by frequency to get a quick list of what the most common names were that year:\n\nbabynames::babynames %&gt;%\n    filter(year == 1989, sex == \"M\") %&gt;%\n    arrange(-prop) %&gt;%\n    head(n = 11)\n\n# A tibble: 11 × 5\n    year sex   name            n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;\n 1  1989 M     Michael     65382 0.0312\n 2  1989 M     Christopher 53176 0.0254\n 3  1989 M     Matthew     45371 0.0217\n 4  1989 M     Joshua      44090 0.0210\n 5  1989 M     David       35216 0.0168\n 6  1989 M     Daniel      34998 0.0167\n 7  1989 M     Andrew      34811 0.0166\n 8  1989 M     Justin      32842 0.0157\n 9  1989 M     James       32698 0.0156\n10  1989 M     Robert      30064 0.0143\n11  1989 M     Joseph      29869 0.0143\n\n\nOh, hey, me, there in 11th place. Other than showing my name there, this code actually illustrates the gist of what I want to show. If I had interviewed a man who was born the same year as me, this is a list of 11 possible pseudonyms I could choose from.\nWe could take this a little further and actually turn it into a function. Doing so can really simplify the script quite a bit when I search for lots of names. I’ll also add an argument specifying how many names to show, with 10 as the default.\n\nshow_nyms &lt;- function(yob, sx, n = 10) {\n    babynames::babynames %&gt;%\n        filter(year == yob, sex == sx) %&gt;%\n        arrange(-prop) %&gt;%\n        head(n = n)\n}\nshow_nyms(1989, \"M\")\n\n# A tibble: 10 × 5\n    year sex   name            n   prop\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;  &lt;dbl&gt;\n 1  1989 M     Michael     65382 0.0312\n 2  1989 M     Christopher 53176 0.0254\n 3  1989 M     Matthew     45371 0.0217\n 4  1989 M     Joshua      44090 0.0210\n 5  1989 M     David       35216 0.0168\n 6  1989 M     Daniel      34998 0.0167\n 7  1989 M     Andrew      34811 0.0166\n 8  1989 M     Justin      32842 0.0157\n 9  1989 M     James       32698 0.0156\n10  1989 M     Robert      30064 0.0143\n\nshow_nyms(1995, \"F\", 5)\n\n# A tibble: 5 × 5\n   year sex   name         n   prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1  1995 F     Jessica  27935 0.0145\n2  1995 F     Ashley   26602 0.0138\n3  1995 F     Emily    24380 0.0127\n4  1995 F     Samantha 21645 0.0113\n5  1995 F     Sarah    21376 0.0111\n\n\nFrom here, you can choose a pseudonym that you feel is appropriate for that person. Unfortunately, because race and ethnicity data are not included here, if your speaker is part of a minority group and you would like their pseudonym to reflect their identity that more visibly, these names might not be the most appropriate since White Americans dominate this dataset. In such cases, you might set n to a larger number and scan through to find a more suitable name for your speaker if you want."
  },
  {
    "objectID": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#ignore-names",
    "href": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#ignore-names",
    "title": "Assigning Pseudonyms in R with the babynames package",
    "section": "Ignore names",
    "text": "Ignore names\nAt some point, you may run into a name that you just don’t want to use. There were some names I just didn’t like for whatever reason. More likely though was that there were alternative spellings of names that I’ve already used. Like Corey instead of Cory or maybe Kathy instead of Cathy. For these homophonous names, I just don’t want to use them to avoid potential confusion. In other cases, I chose a name but decided to go for the shortened version, like Pat instead of Patrick, so I’d need a way to ignore Patrick even though Pat is in my metadata spreadsheet. What I’ll need to do is come up with a separate list of names that I have not used but I want my function to ignore. You have two options: create a separate spreadsheet like your metadata spreadsheet, or just create and maintain the list in the R script itself. It’s up to you which one you do but I’ll create the list in R.\nWhat names should I ignore? For this walk-through, I’ll take a look at girls’ names born in 1995.\n\nshow_nyms(1995, \"F\")\n\n# A tibble: 10 × 6\n    rank  year sex   name          n    prop\n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1    43  1995 F     Sydney     7358 0.00383\n 2    49  1995 F     Erin       6564 0.00342\n 3    52  1995 F     Brooke     6374 0.00332\n 4    53  1995 F     Marissa    6082 0.00317\n 5    55  1995 F     Andrea     6009 0.00313\n 6    57  1995 F     Miranda    5978 0.00311\n 7    58  1995 F     Paige      5733 0.00298\n 8    59  1995 F     Katelyn    5573 0.00290\n 9    60  1995 F     Sierra     5494 0.00286\n10    61  1995 F     Gabrielle  5447 0.00284\n\n\nSo here, I’d want to ignore Erin (not to confuse with Aaron) and Katelyn (I’ve already got Kaitlyn), so I’ll create a list called ignored_names with those two as starters.\n\nignored_names &lt;- c(\"Erin\", \"Katelyn\")\n\nI can now update my show_nyms function one last time to ignore these too:\n\nshow_nyms &lt;- function(yob, sx, n = 10) {\n    # Read in the used names\n    used_names &lt;- read_csv(\"sample_metadata.csv\", show_col_types = FALSE)\n    \n    # Find the candidate names\n    babynames::babynames %&gt;%\n        filter(year == yob, sex == sx) %&gt;%\n        arrange(-prop) %&gt;%\n        rowid_to_column(\"rank\") %&gt;%\n        filter(!name %in% used_names$name,\n               !name %in% ignored_names) %&gt;% # &lt;- ignore these names\n        head(n = n)\n}\nshow_nyms(1995, \"F\")\n\n# A tibble: 10 × 6\n    rank  year sex   name          n    prop\n   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n 1    43  1995 F     Sydney     7358 0.00383\n 2    52  1995 F     Brooke     6374 0.00332\n 3    53  1995 F     Marissa    6082 0.00317\n 4    55  1995 F     Andrea     6009 0.00313\n 5    57  1995 F     Miranda    5978 0.00311\n 6    58  1995 F     Paige      5733 0.00298\n 7    60  1995 F     Sierra     5494 0.00286\n 8    61  1995 F     Gabrielle  5447 0.00284\n 9    62  1995 F     Julia      5411 0.00282\n10    63  1995 F     Vanessa    5403 0.00281\n\n\nNow those two are ignored from my list. So it’s as easy as just creating a list of names you want to ignore. If you’d like you can incorporate that list into the function itself, which will ensure that the function takes into account the most up-to-date list:\n\nshow_nyms &lt;- function(yob, sx, n = 10) {\n    # Read in the used names\n    used_names &lt;- read_csv(\"sample_metadata.csv\", show_col_types = FALSE)\n    \n    # List of ignored names\n    ignored_names &lt;- c(\"Erin\", \"Katelyn\")\n    \n    # Find the candidate names\n    babynames::babynames %&gt;%\n        filter(year == yob, sex == sx) %&gt;%\n        arrange(-prop) %&gt;%\n        rowid_to_column(\"rank\") %&gt;%\n        filter(!name %in% used_names$name,\n               !name %in% ignored_names) %&gt;%\n        head(n = n)\n}\n\nSo now with this show_nyms function, all I need to do is call it once and it’ll show me the best names to choose from. When I choose a name, I’ll add it to my metadata spreadsheet and save. If there’s a name I see and want to ignore, I’ll add it to my ignored_names list. I’d then go through one at a time and choose pseudonyms for all my participants."
  },
  {
    "objectID": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#automate-all-this",
    "href": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#automate-all-this",
    "title": "Assigning Pseudonyms in R with the babynames package",
    "section": "Automate all this",
    "text": "Automate all this\nCurrently, the show_nyms function is great for one-at-a-time stuff. If you’re actively doing fieldwork, you might only add a couple people a day, and it would be a piece of cake to run this function a couple times. Furthermore, since you know these people personally, you might want to spend a moment to decide on the most appropriate pseudonym given the top 10 or whatever.\nOther times, you might just want to automate all this and assign whatever name shows up at the top of the list. Like if you’ve got 1,000 pseudonyms to dish out, you might not want to sit there and decide on every one. We can modify our existing code to do this task, making sure to not give two people the same name.\nOkay, so the first thing I’ll do is create a new function. The old one was show_nyms. This one is slightly different so I’ll call it assign_nyms because it’ll return the top one. Because I don’t need to specify how many to print, I’ll remove the n argument. I’ll also take out the head(n = n) line, and replace it with pull(name) and then first() so that it gets just the first name itself and not any other information associated with it.\nIn order to pull off not assigning the same name twice, I’ll need to add a new argument, ignore. This will be the running list of names I’ve already assigned. I’ll give it a default list that is 1 element long (\"Joey\") so the code doesn’t break. In the filter function, I’ll add yet another filter saying to only return names that are not in this list.\n\nassign_nym &lt;- function(yob, sx, ignore = c(\"Joey\")) {\n    # Read in the used names\n    used_names &lt;- read_csv(\"sample_metadata.csv\", show_col_types = FALSE)\n    \n    # List of ignored names\n    ignored_names &lt;- c(\"Erin\", \"Katelyn\")\n    \n    # Find the candidate names\n    babynames::babynames %&gt;%\n        filter(year == yob, sex == sx) %&gt;%\n        arrange(-prop) %&gt;%\n        rowid_to_column(\"rank\") %&gt;%\n        filter(!name %in% used_names$name,\n               !name %in% ignored_names, \n               !name %in% ignore) %&gt;%  # &lt;- ignore names I just assigned \n        pull(name) %&gt;%\n        first()\n}\n\nOkay, so to use this new function, I’ll create a dummy list of randomly generated birth years between 1990 and 2000, with alternating sexes. I’ll also create an empty list for the name because the code depends on there being something there the first time though.\n\nto_be_named &lt;- tibble(yob = sample(1990:2000, 100, replace = TRUE),\n                      sex = rep(c(\"F\", \"M\"), 50),\n                      name = rep(\"\", 100))\nhead(to_be_named)\n\n# A tibble: 6 × 3\n    yob sex   name \n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n1  1999 F     \"\"   \n2  1991 M     \"\"   \n3  1999 F     \"\"   \n4  1990 M     \"\"   \n5  1990 F     \"\"   \n6  1993 M     \"\"   \n\n\nNow, I’m a fan of using functions to iterate through things in R and I’ve done it many times. In general, for loops are ignored (and even frowned upon) in R code because they’re supposedly slow. But, Hadley Wickham has said that they are not slow, and since I’ve already allocated memory space for the names to go (the new name column in my to_be_named data frame), it shouldn’t be too bad. Furthermore, he said that loops are the only solution when one iteration depends on the values in some other row, which is the case here.I tried using purrr::map for this, but just couldn’t get it figured out and it wasn’t worth the effort for this post.\nSo, I’ll write a for loop that assigns a new name for every row of my 100-row dataframe of generated data. The first two arguments of assign_nym are the same as we’ve seen before (I’ve added the argument names just to be clear what I’m doing). But the third one is where I make sure it doesn’t assign two names twice. I’ve included the argument ignore = to_be_named$name[1:i] because that’s the list of names that have already been assigned in this loop. When that gets sent to assign_nym, it becomes the ignore argument (replacing the default \"Joey\"), and then when the function is filtering out names, it’ll make sure not to let those go through.\n\nfor (i in 1:nrow(to_be_named)) {\n    to_be_named$name[[i]] &lt;- assign_nym(yob    = to_be_named$yob[[i]], \n                                        sx     = to_be_named$sex[[i]], \n                                        ignore = to_be_named$name[1:i])\n}\n\nThe result is that to_be_named now has the name column populated with new, unique pseudonyms that are appropriate for their age and sex.\n\nto_be_named\n\n# A tibble: 100 × 3\n     yob sex   name  \n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n 1  1999 F     Emma  \n 2  1991 M     Travis\n 3  1999 F     Sydney\n 4  1990 M     Jesse \n 5  1990 F     Erica \n 6  1993 M     Alex  \n 7  1994 F     Andrea\n 8  1994 M     Juan  \n 9  1995 F     Brooke\n10  1998 M     Noah  \n# ℹ 90 more rows\n\n\nFortunately, because this list is deterministic and it’ll be the same every time you run the code, if you scan through and see names you don’t like, you can update your ignored_names list and rerun it. So, I found that Corey, Briana, and Shawn were in there and I don’t want to confuse them with the homophonous names Cory, Breanna, and Sean. When I remove those, every thing up until them will be the same, but from then on there will be some differences.\n\nassign_nym &lt;- function(yob, sx, ignore = c(\"Joey\")) {\n    # Read in the used names\n    used_names &lt;- read_csv(\"sample_metadata.csv\", show_col_types = FALSE)\n    \n    # List of ignored names\n    ignored_names &lt;- c(\"Erin\", \"Katelyn\", \"Corey\", \"Briana\", \"Shawn\")\n    \n    # Find the candidate names\n    babynames::babynames %&gt;%\n        filter(year == yob, sex == sx) %&gt;%\n        arrange(-prop) %&gt;%\n        rowid_to_column(\"rank\") %&gt;%\n        filter(!name %in% used_names$name,\n               !name %in% ignored_names, \n               !name %in% ignore) %&gt;% \n        pull(name) %&gt;%\n        first()\n}\nfor (i in 1:nrow(to_be_named)) {\n    to_be_named$name[[i]] &lt;- assign_nym(to_be_named$yob[[i]], \n                                        to_be_named$sex[[i]], \n                                        to_be_named$name[1:i])\n}\nto_be_named\n\n# A tibble: 100 × 3\n     yob sex   name   \n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  \n 1  1999 F     Sydney \n 2  1991 M     Jesse  \n 3  1999 F     Emma   \n 4  1990 M     Travis \n 5  1990 F     Amy    \n 6  1993 M     Juan   \n 7  1994 F     Marissa\n 8  1994 M     Alex   \n 9  1995 F     Andrea \n10  1998 M     Hunter \n# ℹ 90 more rows\n\n\nSo that was quick. This could theoretically be expanded to include a thousand names and it would work just fine. You might end up with some unusual names, but this will in principle work so long as you haven’t exhausted all the names. And it would take a ton of people do to do. Here’s what the last 10 names are in my sample of 1000 people.\n\nto_be_named &lt;- tibble(yob = sample(1990:2000, 1000, replace = TRUE),\n                      sex = rep(c(\"F\", \"M\"), 500),\n                      name = rep(\"\", 1000))\nfor (i in 1:nrow(to_be_named)) {\n    to_be_named$name[[i]] &lt;- assign_nym(to_be_named$yob[[i]], \n                                        to_be_named$sex[[i]], \n                                        to_be_named$name[1:i])\n}\ntail(to_be_named, n = 10)\n\n# A tibble: 10 × 3\n     yob sex   name   \n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  \n 1  1991 F     Shana  \n 2  1992 M     Earl   \n 3  2000 F     Zoey   \n 4  1995 M     Kurtis \n 5  2000 F     Alayna \n 6  1991 M     Norman \n 7  2000 F     Lilly  \n 8  1991 M     Bret   \n 9  1993 F     Paris  \n10  1996 M     Raekwon\n\n\nSo with just a couple lines of code, you can very quickly assign age- and sex-appropriate pseudonyms to all your participants."
  },
  {
    "objectID": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#conclusion",
    "href": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#conclusion",
    "title": "Assigning Pseudonyms in R with the babynames package",
    "section": "Conclusion",
    "text": "Conclusion\nFinding pseudonyms is not the most important part of your research, but it is likely a necessary step. I figure it’s nice to use R to help you out even on the little stuff."
  },
  {
    "objectID": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#bonus",
    "href": "blog/assigning-pseudonyms-in-r/assigning-pseudonyms-in-r.html#bonus",
    "title": "Assigning Pseudonyms in R with the babynames package",
    "section": "Bonus",
    "text": "Bonus\nHere are just some fun plots I made when playing around with the data. This first one shows the trends for the name Parker, which my wife and I considered for our daughter. This plot shows that there would have been many more boys her age with that name than girls.\n\nbabynames::babynames %&gt;%\n    filter(name == \"Parker\") %&gt;%\n    ggplot(aes(year, y = prop, color = sex)) + \n    geom_line(size = 1) + \n    scale_color_manual(values = c(\"pink\", \"lightblue\")) + \n    theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nMy given name, Joseph, has historically been among the most common male names. Through about the 1920s, at least 1 in 50 baby boys were called Joseph. There was a drop in frequency between the World Wars and in the 1980s it started becoming less common. Ever since then there have been fewer and fewer Josephs born. If this trend continues, Joseph will become old-fashioned as I age, which is kinda fun to think about.\n\nbabynames::babynames %&gt;%\n    filter(name == \"Joseph\") %&gt;%\n    ggplot(aes(year, y = prop, color = sex)) + \n    geom_line(size = 1) + \n    scale_color_manual(values = c(\"pink\", \"lightblue\")) + \n    theme_bw()\n\n\n\n\nFinally, the plot below was just an interesting one that shows the diversity in names. I add up the total proportion of people per year are represented in this dataset. Remember that this dataset does not include names if fewer than five people in a year had that name. So if all the data adds up to only 0.96, then 4% of people that year had somewhat unusual names. The higher number is, the fewer uncommon names there were. This chart shows that compared to the 1960s, there are more unusual names, particularly among the women. I’ll let the folks in onomastics interpret this, but I thought it was kind of a fun graph.\n\nbabynames::babynames %&gt;%\n    group_by(year, sex) %&gt;%\n    summarize(total = sum(prop)) %&gt;%\n    ungroup() %&gt;%\n    ggplot(aes(year, total, color = sex)) + \n    geom_line() + \n    theme_minimal()\n\n\n\n\nThat’s all! Have fun with babynames!"
  },
  {
    "objectID": "blog/jealousy-list-1/index.html",
    "href": "blog/jealousy-list-1/index.html",
    "title": "Jealousy List 1",
    "section": "",
    "text": "This year, FiveThirtyEight started a monthly Jealousy List, which is essentially a list of really cool articles they saw other people do that they wish they had been the ones to write. This is an idea they got from Bloomberg and I think others are starting to do their own as well. It’s kind of a fun way to showcase some of the best stuff that has come out recently and to share others’ work. I kinda like the idea so I thought I’d start an occasional jealousy list of my own.\nI’m a linguist at heart, but because I’m interested learning more about R and statistics, I tend to keep tabs on a lot of what’s going on in the data science world. I follow a lot of data scientists on Twitter and I’m also subscribed to something like 50 academics’ blogs that I read when I have a spare minute. Consequently, most of what I’ll post here will probably not be directly related to linguistics, but they’ll show off some really cool R skills. Things that I find really cool or I wish I could have been the one to do write it. I tend to share some of this kind of stuff on Twitter, but I thought I’d start compiling them here.\nSo, in no particular order, here is a list of some of the things I liked the most in the past few weeks/months. Some are tutorials, others are just information, but they’re all stuff I jealously wish I could have written:\n\nHelen Graham. “Now that’s what I call text mining and sentiment analysis”\nThis is an analysis of all the lyrics in the 100 Now! music albums. (Yeah, those are still a thing, and the UK series just released their hundredth album!) First, they use rvest for web scraping and geniusR for extracting the lyrics from genius.com. But the real fun comes from tidytext and looking at how word usage, sentiments, and other trends change over time. I’ve never done we scraping but I feel like maybe it’s not so hard after reading this.\nListen Data. “15 Types of Regression you should Know”\nThis is just a quick overview of lots of different kinds of regression analysis. I took a course in regression so I felt pretty confident going into this, but I learned about some cool types of models that I hadn’t heard of like Lasso, Poisson, and other fancier ones. There’s some brief R code and a simple explanation of how each is different from the others. I’m particularly interested in the Poisson regression and wonder how that might be used in corpus studies.\nLaura Ellis. “Highlighting with ggplot2: The Old School and New School Way”\nSomething that you might need to do in ggplot2 is to highlight data. This author explains two ways to do that. I had been doing what they refer to as the “old school” way, which is essentially overlay a second layer using just a subset of the data (which is exactly what I did here). After reading this, I think I should switch to the new school way, which uses Hiroaki Yutani’s gghighlight package. I can’t wait to try it out.\nJosef Fruehwald. “Why does Labov have such weird transcriptions?”\nThe title of this post is literally something I’ve asked myself. If I had really dug around for an answer, I probably could have found some of the early sources that explain it, but this blog post summarizes it all up nicely for you. One of the reasons is that the “transcriptional system is now encoding a phonological analysis.”\nTimo Grossenbacher. “Categorical spatial interpolation with R”\nI recently got a fair amount of spatial data and I’ve been meaning to learn how to visualize some of what I got. This post gives a complete tutorial for how to do k-nearest neighbor categorical interpolation using the kkNN package. Because this involves some pretty complicated calculations, I also learned I could utilize the multiple cores I have on my computer for super intense stuff, which is always fun. I ended up creating this map, thanks to this tutorial:\n\nAs far as what this map shows, well, no spoilers, but I’m waiting to hear back from ADS about whether I get to present on this data.\n\nI can say now that it shows prevelar raising. I presented on it at ADS and eventually published the results in American Speech.\nSo that’s my jealousy list. Text analysis, regression, ggplot2, vowel transcriptions, and GIS. That’s actually a nice smattering of the things I read about the most."
  },
  {
    "objectID": "blog/simulating_chutes_and_ladders/index.html",
    "href": "blog/simulating_chutes_and_ladders/index.html",
    "title": "Simulating Chutes and Ladders",
    "section": "",
    "text": "We tried teaching our little almost-three-year-old Chutes and Ladders today. She wasn’t very good at counting tiles. But, as I was sitting there climbing up and sliding down over and over, I wondered what the average number of turns it would take to finish the game. So I decided to take a stab at simulating the game. So here’s a post on a simple simulation of Chutes and Ladders that demonstrates absolutely nothing about linguistics and instead shows off some R skills.\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(scico)\nlibrary(readxl)\nlibrary(gganimate)\nI’m trying to increase my Github presence, so the code for this project can be found on my Github."
  },
  {
    "objectID": "blog/simulating_chutes_and_ladders/index.html#the-game",
    "href": "blog/simulating_chutes_and_ladders/index.html#the-game",
    "title": "Simulating Chutes and Ladders",
    "section": "The game",
    "text": "The game\nFor those of you deprived people who have never played Chutes and Ladders, the game is quite simple. There are 100 tiles arranged in a 10 by 10 board. With 1–3 of your closest friends, you start at Tile 1, roll a die, and advance that number of tiles. Players take turns moving up the board boustrophedonically. until one person reaches 100. The catch is that there are various “chutes” and “ladders” on the board. If you land on the bottom of one of about half a dozen ladders, you climb to the top, advancing anywhere from 10 to 54 tiles. But, if you land at the top of about a dozen chutes, you slide down anywhere from 4 to 63 tiles. There is no skill and it’s 100% luck—perfect for small kids.I’ll admit that half the reason I wrote this post was so I could use this word!\nHere’s a simplified version of the board. First, I’ll read in the data I prepared ahead of time. It just gives the x-y coordinates for each of the 100 tiles.\n\ntile_data &lt;- readxl::read_excel(\"chutes_ladders_data.xlsx\", sheet = 1)\nhead(tile_data)\n\n# A tibble: 6 × 3\n   tile     x     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1\n2     2     2     1\n3     3     3     1\n4     4     4     1\n5     5     5     1\n6     6     6     1\n\n\nNow, I’ll read in information about the chutes and ladders themselves, as in where they start and where they stop.\n\nchutes_and_ladders_data &lt;- readxl::read_excel(\"chutes_ladders_data.xlsx\", sheet = 2) %&gt;%\n  rowid_to_column(\"id\") %&gt;%\n  gather(position, tile, start, end) %&gt;%\n  left_join(tile_data, by = \"tile\")\nchutes_and_ladders_data\n\n# A tibble: 38 × 6\n      id type   position  tile     x     y\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 ladder start        1     1     1\n 2     2 ladder start        4     4     1\n 3     3 ladder start        9     9     1\n 4     4 ladder start       21     1     3\n 5     5 ladder start       28     8     3\n 6     6 ladder start       36     5     4\n 7     7 ladder start       51    10     6\n 8     8 ladder start       71    10     8\n 9     9 ladder start       80     1     8\n10    10 chute  start       16     5     2\n# ℹ 28 more rows\n\n\nJust some information about the lines for the chutes and ladders, mostly for visual purposes.\n\nlines_data &lt;- read_excel(\"chutes_ladders_data.xlsx\", sheet = 3) %&gt;%\n  rowid_to_column(\"id\") %&gt;%\n  gather(point, value, beg_x, beg_y, end_x, end_y) %&gt;%\n  separate(point, into = c(\"location\", \"axis\")) %&gt;%\n  spread(axis, value)\nhead(lines_data)\n\n# A tibble: 6 × 5\n     id type    location     x     y\n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     1 outside beg        0.5   0.5\n2     1 outside end       10.5   0.5\n3     2 outside beg        0.5  10.5\n4     2 outside end       10.5  10.5\n5     3 outside beg        0.5   0.5\n6     3 outside end        0.5  10.5\n\n\nNow make a basic plot.\n\nggplot(tile_data, aes(x, y)) + \n  geom_line(data = lines_data, aes(group = id, linetype = type)) + \n  geom_text(aes(label = tile), size = 3, nudge_x = -0.25, nudge_y = 0.25) +\n  labs(title = \"A Simplified Chutes and Ladders Board\",\n       caption = \"joeystanley.com\") +\n  geom_path(data = chutes_and_ladders_data, aes(group = id, linetype = type),\n            arrow = arrow(angle = 20, length = unit(0.1, \"in\"), type = \"closed\")) +\n  scale_linetype_manual(values = c(\"solid\", \"solid\", \"dashed\", \"solid\", \"dotted\", \"dotted\")) +\n  coord_fixed(ratio = 1) + \n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nThe game we bought is actually a knock-off (I’m a poor grad student—what do you expect?), but I found an image of the authentic version online so I’ll go with that for this blog post. And for simplicity, I’ll just simulate a one-person game."
  },
  {
    "objectID": "blog/simulating_chutes_and_ladders/index.html#the-simulation",
    "href": "blog/simulating_chutes_and_ladders/index.html#the-simulation",
    "title": "Simulating Chutes and Ladders",
    "section": "The simulation",
    "text": "The simulation\nSimulating the game is relatively straightforward. All you really need is a way to keep track of what tile you’re on, a way to roll the die, and sequence of if-else statements to simulate the chutes and ladders. The die rolling is pretty simple with the help of sample.\n\n\n\n\nsample(6, 1)\n\n[1] 1\n\n\nWe could wrap that up into a function to make it slightly more transparent too:\n\nroll_die &lt;- function() { \n  sample(6, 1) \n}\nroll_die()\n\n[1] 4\n\nroll_die()\n\n[1] 1\n\nroll_die()\n\n[1] 2\n\n\nNow, in this game, every time you roll a die, you’ll need to advance your token by that many pieces. I could write a separate advance_token function, but with functions this simple, I’ll just combine them into one. This time, it’ll take an argument, spot, which is the current tile number (from 1 to 100) that you’re on. The function takes this spot, rolls a die, and adds that value to it, returning the tile you’ll land on.\n\nroll_die &lt;- function(spot) { \n  spot + sample(6, 1)\n}\nroll_die(1)\n\n[1] 6\n\nroll_die(10)\n\n[1] 13\n\nroll_die(50)\n\n[1] 56\n\nroll_die(80)\n\n[1] 82\n\n\nAwesome. Now with this, it makes for a pretty uneventful game, so we’ll have to simulate the chutes and ladders. I know base R has switch syntax, but I’ve never been able to get it to work, so I’ll use case_when from the dplyr package to do this. I’ll of course wrap it up in another function verbosely called check_for_chute_or_ladder. Here, I input the start and end points to all the chutes and ladders. So for example, if you land on the very first tile, there’s a ladder that’ll take you to tile 38. If you land on tile 16, you’ll slide down a chute to tile 6. The function will return the new tile you’ll end up on. If you don’t land on any of them, the function will return the same number you sent in.\n\ncheck_for_chute_or_ladder &lt;- function(spot) {\n  case_when(\n    \n    # Ladders (9)\n    spot ==  1 ~  38,\n    spot ==  4 ~  14,\n    spot ==  9 ~  31,\n    spot == 21 ~  42,\n    spot == 28 ~  84,\n    spot == 36 ~  44,\n    spot == 51 ~  67,\n    spot == 71 ~  91,\n    spot == 80 ~ 100,\n    \n    # Chutes (10)\n    spot == 16 ~   6,\n    spot == 47 ~  26,\n    spot == 49 ~  11,\n    spot == 56 ~  53,\n    spot == 62 ~  19,\n    spot == 64 ~  60,\n    spot == 87 ~  24,\n    spot == 93 ~  73,\n    spot == 95 ~  76,\n    spot == 98 ~  78,\n    \n    # No change\n    TRUE ~ spot)\n}\ncheck_for_chute_or_ladder(1)\n\n[1] 38\n\ncheck_for_chute_or_ladder(4)\n\n[1] 14\n\ncheck_for_chute_or_ladder(5)\n\n[1] 5\n\n\nGreat. Now we’ve got a full turn. For kicks, I can wrap all this up into yet another function that’ll simulate taking a turn in the game:\n\ntake_turn &lt;- function(spot) {\n  spot %&gt;%\n    roll_die() %&gt;% \n    check_for_chute_or_ladder()\n}\ntake_turn(1)\n\n[1] 14\n\ntake_turn(3)\n\n[1] 6\n\ntake_turn(6)\n\n[1] 7\n\ntake_turn(10)\n\n[1] 15\n\ntake_turn(6)\n\n[1] 11\n\n\nSo as it turns out, these are all the functions I need to simulate an entire game. But, the way it’s set up now, I have to run each turn one at a time, check the output, and run it again. It would be better if I could automate the whole thing and save the results into a dataframe or something.\nSo I’ve created the larger simulate_game function below. When I run this function, it’ll simulate an entire (1-player) game. First, it’ll create a mostly empty data frame that will be populated as the turns advance. I know that some programming languages are slow if you try to append rows to a dataframe with each iteration of a loop, so I wanted to make sure there was room for a full game before we do anything else. Also, in theory, the game could last forever because of looping chutes and ladders, so I made it large enough to handle a game with 1000 turns—probably way too many for this, but I wanted to make sure. In that dataframe, I have columns for the turn number, what the dice roll was (those are all predetermined), where you landed, whether it has a chute or a ladder, and finally, where you ended up after traveling on that chute or ladder.I think Perl doesn’t care, and I miss that…I started with 100 turns, but that actually wasn’t enough turns for some of the simulated games!\nI’ve never done a simulation in R before, so I don’t know what the protocol is for looping through something an unknown number of times, so I did this whole while(keep_playing) thing. The keep_playing object is initially true, and at the end of each iteration, I check to see if we’ve gotten to 100; if so, I’ll set that to false, which’ll kill the loop. However, I wanted some sort of iterating number (like in a for loop), so I added i myself.I tried just looping through the turn_num column, but I couldn’t get the loop to stop after the player hit tile 100.\nOkay, so then inside that loop, there are several main chunks. Most of it is fluff for handling the data and keeping track of stuff and the actual game portion is just two lines in the middle.\n\nFirst, if it’s the first iteration of the loop, set the start tile number to 0. Otherwise, set it to wherever we ended up last time.\nThen, I add the dice roll to to the start tile to get the (potentially) temporary land tile. I then send that number off to check_for_chute_or_ladder and get the actual end tile.\nI then do another conditional to tell whether I had a chute or ladder. In theory, I should just be able to tell that with the check_for_chute_or_ladder function, but I’m not sure how to return two values at once in R like I can with Perl.\nFinally, I’ll do one more conditional to see if we’ve reached tile 100. If not, go on to the next iteration of the loop. If so, we’re done.\n\nAfter the looping is done, we’ve completed a game. Remember that I started by declaring enough space for 1000 turns. I don’t need all the extra rolls, so just before I return the dataframe with all the game information, filter out the rolls that didn’t happen.\n\nsimulate_game &lt;- function(game_num = 0) {\n  \n  # Declare space for the full game.\n  n &lt;- 1000\n  turns &lt;- tibble(turn_num = 1:n,\n                  start    = NA,\n                  roll     = sample(6, n, replace = TRUE),\n                  land     = NA,\n                  chute_or_ladder = NA,\n                  end      = NA)\n  \n  # Loop until the game is over\n  i &lt;- 1\n  keep_playing &lt;- TRUE\n  while(keep_playing) {\n    \n    # Step 1: Start at zero\n    if (i == 1) {\n      turns$start[[i]] &lt;- 0\n      \n      # Otherwise, start where the last turn ended.\n    } else {\n      turns$start[[i]] &lt;- turns$end[[i - 1]]\n    }\n    \n    # Step 2: This is where the game actually happens.\n    # Add dice roll to the start tile\n    turns$land[[i]] &lt;- turns$start[[i]] + turns$roll[[i]]\n    # Check for chute or ladder\n    turns$end[[i]] &lt;- check_for_chute_or_ladder(turns$land[[i]])\n    \n    # Step 3: Keep track of whether I had a chute or ladder.\n    if (turns$land[[i]] &gt; turns$end[[i]]) {\n      turns$chute_or_ladder[[i]] &lt;- \"ladder\"\n    } else if (turns$land[[i]] &lt; turns$end[[i]]) {\n      turns$chute_or_ladder[[i]] &lt;- \"chute\"\n    } else {\n      turns$chute_or_ladder[[i]] &lt;- NA\n    }\n    \n    # Step 4: Check if it's game over.\n    if (turns$end[[i]] &gt;= 100) {\n      keep_playing &lt;- FALSE\n    } else {\n      i &lt;- i + 1\n    }\n  }\n  \n  turns %&gt;%\n    filter(turn_num &lt;= i) %&gt;%\n    return()\n}\n\nNow let’s see it in action!\n\nsimulate_game()\n\n# A tibble: 21 × 6\n   turn_num start  roll  land chute_or_ladder   end\n      &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1        1     0     2     2 &lt;NA&gt;                2\n 2        2     2     6     8 &lt;NA&gt;                8\n 3        3     8     6    14 &lt;NA&gt;               14\n 4        4    14     2    16 ladder              6\n 5        5     6     1     7 &lt;NA&gt;                7\n 6        6     7     5    12 &lt;NA&gt;               12\n 7        7    12     5    17 &lt;NA&gt;               17\n 8        8    17     1    18 &lt;NA&gt;               18\n 9        9    18     1    19 &lt;NA&gt;               19\n10       10    19     6    25 &lt;NA&gt;               25\n# ℹ 11 more rows\n\n\nHooray! A complete game. This one appears to have been completed in just 15 moves after hitting four chutes and one ladder."
  },
  {
    "objectID": "blog/simulating_chutes_and_ladders/index.html#the-end",
    "href": "blog/simulating_chutes_and_ladders/index.html#the-end",
    "title": "Simulating Chutes and Ladders",
    "section": "The End",
    "text": "The End\nSo that’s it. Thanks for joining me on my journey of simulating Chutes and Ladders!"
  },
  {
    "objectID": "blog/custom-themes-in-ggplot2/index.html",
    "href": "blog/custom-themes-in-ggplot2/index.html",
    "title": "Custom Themes in ggplot2",
    "section": "",
    "text": "Note\n\n\n\nFor additional detail and updated information on creating custom themes, see this handout which accompanies a workshop I gave in September 2019. For examples on everything the theme function can do, see this supplemental handout.\n\n\nI’ve always enjoyed the R package ggplot2 because it allows limitless flexibility in how a plot looks—if you’re willing to put in the time. After a while though I found myself adding the same commands to all my plots so that they all match. Shouldn’t there be an easier way to do that?\nYes! I recently discovered how to make a custom theme in ggplot2. It was relatively straightforward to create, a cinch to implement, and everything looks great.\n\nWhy make a custom theme?\nFirst, I don’t like it when I go to conferences and I see plots that have the default R settings, like the gray background, the white grid, and the standard font. To me, it’s like seeing a document in default the Helvetica or Calibri: they’re almost so bad you’d think they did it on purpose to force you to change it. And yet, they remain.\nBy not using the default R theme, you’re showing the world that you know your way around R. It shows that you didn’t just blindly copy and paste someone else’s code, but that you know how to control and customize things they way you want.\nIt shows that you have some attention to detail. The purpose of a plot is to present data in a clean and effective way. Yes, the type of plot and the colors and all that matters. But so does the color of the background and the font. By applying a new style to your plots, you can control everything to make sure your audience sees what you want them to see.\nI like having my own unique theme because it’ll add some continuity between my presentations. I already have a custom PowerPoint template that I’ve been using for a while now, so all my presentations look like they go together. It also makes it easier to moves plots around from one presentation to another if they all match.As of 2023, I still use that exact same template!\nFinally, using a custom theme, especially in tandem with a custom powerpoint template, will make it look like everything goes together. My theme colors and font both match my slideshow colors and font (and this website’s!), so incorporating these visualizations into my slides is visually seamless.\n\n\nHow I figured this stuff out (and how you can learn more)\nI discovered how to do this when I was looking at the code for theme_bw(). In case you haven’t used it, if you add this function to your ggplot() command, it’ll make a nice black and white theme for you, which I always found to be nice looking. (It’s actually just one of several themes: see the help page at ?theme_bw for other themes to try out.) Here’s the code straight from R for convenience:\n\nlibrary(ggplot2)\ntheme_bw\n\nfunction (base_size = 11, base_family = \"\", base_line_size = base_size/22, \n    base_rect_size = base_size/22) \n{\n    theme_grey(base_size = base_size, base_family = base_family, \n        base_line_size = base_line_size, base_rect_size = base_rect_size) %+replace% \n        theme(panel.background = element_rect(fill = \"white\", \n            colour = NA), panel.border = element_rect(fill = NA, \n            colour = \"grey20\"), panel.grid = element_line(colour = \"grey92\"), \n            panel.grid.minor = element_line(linewidth = rel(0.5)), \n            strip.background = element_rect(fill = \"grey85\", \n                colour = \"grey20\"), legend.key = element_rect(fill = \"white\", \n                colour = NA), complete = TRUE)\n}\n&lt;bytecode: 0x13b069dc8&gt;\n&lt;environment: namespace:ggplot2&gt;\n\n\nWhat I see is a function that is based on theme_grey() with lots of modified elements to control different parts of a plot. When I was looking through the code, I figured out what was going on and through some trial and error, was able to make my own custom theme. So I’ll try to explain it for you and you might find it useful too.\n\n\nThe nuts and bolts\nSo the way this works is I create a function called theme_joey() and base on theme_bw(). The following block does that, but it changes the typeface to one I use called Avenir.\n\ntheme_joey &lt;- function () { \n    theme_bw(base_size = 12, base_family = \"Avenir\")\n}\n\nRight now, other than the typeface, theme_joey() is just a copy of theme_bw(). What I want to do now is change just a few of the properties in this function. To do that, I use the %+replace% command, which, in all honesty, I have no idea how it works. What I want to replace though are elements of the theme() function within theme_bw(), so I add that to the function:\n\ntheme_joey &lt;- function () { \n    theme_bw(base_size=12, base_family=\"Avenir\") %+replace% \n        theme(\n            # change stuff here\n        )\n}\n\nNow, I just need to specify which elements of theme() I want to change. This took some trial and error, a peek at ggplot2’s help pages, as well as some googling.Update: Three years after writing this original tutorial, I learned a lot more about theme() and gave a workshop on it. Here is the handout.\nThe first thing I wanted to to do was remove the background. I didn’t want a white one, so take it out entirely:\n\npanel.background  = element_blank(),\n\nThen I wanted to change was the background to make it what I’ve seen described as “whitesmoke”. I’d prefer a transparent background in my plots so that the whatever I copy and paste the image into will always match, but I couldn’t figure out how to do that. Here’s how I specified the color:\n\nplot.background = element_rect(fill = \"gray96\", color=NA)\n\nThe legend background and key were both white in theme_bw() still, so I wanted to make those transparent. So I’ll change those to transparent so they take on the global “gray96” specified above:\n\nlegend.background = element_rect(fill = \"transparent\", color = NA)\nlegend.key        = element_rect(fill = \"transparent\", color = NA)\n\nSo if we put all these elements in the new theme, we get a fully-functional, custom theme:\n\ntheme_joey &lt;- function () { \n    theme_bw(base_size=12, base_family=\"Avenir\") %+replace% \n        theme(\n            panel.background  = element_blank(),\n            plot.background = element_rect(fill = \"gray96\", color = NA), \n            legend.background = element_rect(fill = \"transparent\", color = NA),\n            legend.key = element_rect(fill = \"transparent\", color = NA)\n        )\n}\n\nSo all I need to do is make sure R knows about this function when I start a new session. What I do is I put it at the top of my R scripts in the same chunk of code where I load my packages and read in my data. I can then just add theme_joey() to any ggplot() command and like magic all my plots match. It’s pretty cool.\n\n\nSample plots\nSo here are some sample plots that show the differences between no theme, theme_bw(), and my new theme_joey():\n\n# Sample data\ndf &lt;- data.frame(x = factor(rep(letters[1:3], each = 10)), y = rnorm(30), color=(rep(c(\"A\", \"B\"), each=5)))\nplot &lt;- ggplot(df, aes(x = x, y = y, color=color)) + geom_point()\n\nplot + ggtitle(\"No theme\")\n\n\n\n\n\nplot + ggtitle(\"Black and White\") + theme_bw()\n\n\n\n\n\nplot + ggtitle(\"Custom Theme\") + theme_joey()\n\n\n\n\nAssuming your browser is rendering this webpage like mine is, the last plot should be the same background color as this webpage, and the font should match my header fonts.Update: I’ve changed the look of my webpage, so it probably doesn’t match anymore.\nNote that some fonts require different settings if you want to export plots that use them as a pdf with ggsave(). You just need to add the argument device=cairo_pdf to it like so:\n\nggsave(\"themes_joey.pdf\", device=cairo_pdf, width = 6, height = 6)\n\n\n\nConclusion\nAnd that’s it! Give it an hour or so and I think you can start to make a custom theme. And let me know how it turns out. I’d love to see what you’ve done."
  },
  {
    "objectID": "blog/ads-and-lsa-2019/index.html",
    "href": "blog/ads-and-lsa-2019/index.html",
    "title": "LSA and ADS 2019",
    "section": "",
    "text": "Thanks for attending my presentations. At the 2019 annual meetings of the American Dialect Society and the Linguistic Society of America in New York City, I was fortunate to present three presentations!"
  },
  {
    "objectID": "blog/ads-and-lsa-2019/index.html#thursdays-lsa-poster-on-southern-vowels",
    "href": "blog/ads-and-lsa-2019/index.html#thursdays-lsa-poster-on-southern-vowels",
    "title": "LSA and ADS 2019",
    "section": "Thursday’s LSA poster on southern vowels",
    "text": "Thursday’s LSA poster on southern vowels\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nThursday, Peggy Renwick and I presented our poster on social patterns in static and dynamic measurements of Southern American English vowels. Our dataset was the Digital Archive of Southern Speech (DASS), a collection of 64 interviews from the 1970s. We looked at Pillai scores to measure the degree of “swapping” between pairs of front vowels (/i ɪ/ and /e ɛ/) and we used vector length, trajectory length, and spectral rate of change to see how dynamic the vowels were.\nWe found a bunch of cool patterns! You can see the poster for all of them, but one of the cooler ones was that /e/ and /ɛ/ swapped more in younger speakers. This plot shows the trajectories of these two vowels split up by generation and you can see how they get closer together (though keep in mind that because the trajectories are drastically different, these aren’t merging).\n\nThis is to be expected for speakers with the Southern Vowel Shift. But, the African American Vowel Shift doesn’t have this same swapping. So sure enough, if we look at the data split by ethnicity, we see that the African American speakers had less speakers than the European Americans.\n\nSo our results are mostly what we expected to find. But this corpus of older recordings give us a unique peek into the past while these changes were developing. And by using both static and dynamic measurements, we can get a more complete picture of what’s going on."
  },
  {
    "objectID": "blog/ads-and-lsa-2019/index.html#fridays-ads-presentation-on-prevelar-raising",
    "href": "blog/ads-and-lsa-2019/index.html#fridays-ads-presentation-on-prevelar-raising",
    "title": "LSA and ADS 2019",
    "section": "Friday’s ADS presentation on prevelar raising",
    "text": "Friday’s ADS presentation on prevelar raising\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nEarly Friday morning, I talked about regional patterns in beg- and bag-raising in North American English. There’s been a lot of research on these phenomena, but only in places like the Upper Midwest, Western Canada, and the Pacific Northwest. Perhaps there is prevelar raising in other areas too?\nI used the same dataset that I used for my NWAV47 presentation: I set up an online survey and asked people how they pronounced several dozen prevelar words. For geographic data, I used GPS coordinates of people’s childhood homes. I ended up with over 500,000 data points!\nIt was clear that there were differences between the two vowels. bag-raising was pretty clear cut: either people had it or they did not. And most people didn’t. There just weren’t too many people that were sort of on the middle ground. However, with and beg-raising, while there were lots of people with no raising, there were tons of people with varying amounts of raising. Basically, beg was much more variable than bag.\n\n\nAnd then I showed some plots. The first was bag-raising which was reported in pretty much all the areas we expected.\n\nBut then other map showed that bag-raising was pretty much everywhere except for the South. It’s particularly widespread in the West and the Midlands.\n\nWhen you combine the two maps, you can start to see where one occurs without the other. Here, green areas are those that have bag-raising but not beg-raising and purple are areas with beg-raising without bag-raising.\n\nBasically, the purpose of this study was to see whether there were differences between beg-raising and bag-raising in where they occur regionally. I think these maps show that there are differences. Now I just want to go confirm these patterns with phonetic data!"
  },
  {
    "objectID": "blog/ads-and-lsa-2019/index.html#sundays-ads-presentation-on-the-perception-of-southern-american-english",
    "href": "blog/ads-and-lsa-2019/index.html#sundays-ads-presentation-on-the-perception-of-southern-american-english",
    "title": "LSA and ADS 2019",
    "section": "Sunday’s ADS presentation on the perception of Southern American English",
    "text": "Sunday’s ADS presentation on the perception of Southern American English\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nMy last presentation of the weekend was Sunday morning at the American Dialect Society. On behalf of my coauthors, Rachel Olsen, Mike Olsen, Lisa Lipani, and Peggy Renwick, I talked about our research on comparing acoustic data with fieldworker transcriptions in the Digital Archive of Southern Speech. Doing this kind of comparison is not new, and people have found that Linguistic Atlas transcriptions are not really that reliable, but we wanted to look into this for ourselves in our newly transcribed corpus.\nSo for now, we’re just focusing on the canonical diphthongs /aɪ, aʊ, ɔɪ/ because they’re quite a bit more monophthongal in the South than in other areas. To measure this acoustically, we used trajectory length. For the perception, we looked at the original fieldworker’s protocols they created for each informant, which has example tokens and how those people would pronounce them (in IPA).\nBasically there was no correlation between how monophthongal the vowels were acoustically and how they were transcribed. Besides that, in transcriptions the vowels were more monophthongal when they were before /r/, but acoustically they were more monophthongal before /l/ and among European Americans. The two give different results.\nSo we’ve at least found that whatever the fieldworkers heard when they transcribed these words, it wasn’t trajectory length. Perhaps in the future we can explore some other acoustic measures and see if they correlate better with the transcriptions."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html",
    "title": "Making vowel plots in R (Part 1)",
    "section": "",
    "text": "Note\n\n\n\nThis post was written in 2018 and used code that was up-to-date at the time. In November 2023, I updated the code to reflect changes in dplyr.\nLast week I was approached by a fellow graduate student who asked how they might go about making vowel plots in R. I’ve made my share of these plots and have learned some tricks along the way, so I thought it might make for an interesting blog post. Actually, I thought it would make for an interesting series of blog posts. In this first one, I’ll stick with scatterplots and look at the code you’ll need for them. In the next one I show how to plot vowel trajectories.\nFor this workshop, we’ll need just two packages: dplyr and ggplot2. Let’s load those now.\nlibrary(dplyr)\nlibrary(ggplot2)\nJust FYI, there are actually some phonetic-specific packages that make it easier to do this (I’m thinking the vowels package by Tyler Kendall and Erik Thomas), but I like the flexibility of doing it from scratch in ggplot2."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html#read-in-and-process-data",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html#read-in-and-process-data",
    "title": "Making vowel plots in R (Part 1)",
    "section": "Read in and process data",
    "text": "Read in and process data\nThe dataset I’ll be working with comes from me reading 300 sentences while sitting at my kitchen table. This was transcribed manually and force-aligned using DARLA, with formants extracted using FAVE. You can access this dataset yourself with my joeysvowels package. I’ve removed a lot of the outliers already, so the remaining data that we’ll use here is relatively clean.\nAs with any R script, the first step (after loading your packages) is to read in and prepare your data. For maximal reproducibility in your own data, I’m going to work with the FAVE output as is, so you can see how I process the data. This means that we’ll be seeing the vowels in “ARPABET”, rather than IPA. Since the focus of this post isn’t necessarily on the minutia of the data processing, I’ll keep that part to a minimum.\n\nmy_vowels &lt;- read.csv(\"http://joeystanley.com/data/joey.csv\") %&gt;%\n  filter(stress == 1, \n         !vowel %in% c(\"AY\", \"AW\", \"OY\", \"ER\"),\n         !word %in% c(\"TO\", \"US\", \"ON\")) %&gt;%\n  mutate(word = tolower(word))\n\n\n\n\nSo what this chunk does is it reads in file called joey.csv that I have saved in a folder called data. It then filters the data by keeping just the vowels with primary stress, removing diphthongs and /ɚ/, and removing a couple stopwords. Then it changes all the words so that they’re lowercase. Here’s what the dataset looks like\n\nhead(my_vowels)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nsex\nvowel\nstress\npre_word\nword\nfol_word\nF1\nF2\nF3\nF1_LobanovNormed_unscaled\nF2_LobanovNormed_unscaled\nB1\nB2\nB3\nt\nbeg\nend\ndur\nplt_vclass\nplt_manner\nplt_place\nplt_voice\nplt_preseg\nplt_folseq\npre_seg\nfol_seg\ncontext\nvowel_index\npre_word_trans\nword_trans\nfol_word_trans\nF1.20.\nF2.20.\nF1.35.\nF2.35.\nF1.50.\nF2.50.\nF1.65.\nF2.65.\nF1.80.\nF2.80.\nnFormants\n\n\n\n\nLA000-Joey\nM\nAA\n1\nWITHOUT\ntodd\nEARLY\n662.8\n1162.4\n2596.3\n1.1671501\n-1.0629811\n287.2\n419.8\n221.4\n12.917\n12.86\n13.03\n0.17\no\nstop\napical\nvoiced\noral_apical\n\nT\nD\ninternal\n2\nW IH0 TH AW1 T\nT AA1 D\nER1 L IY0\n775.3\n1336.7\n740.5\n1162.1\n614.6\n1065.2\n637.6\n1109.4\n545.2\n1314.1\n6\n\n\nLA000-Joey\nM\nAE\n1\nTHE\nlast\nTIME\n658.1\n1389.0\n2496.6\n1.1377223\n-0.4825991\n505.4\n208.3\n81.6\n13.713\n13.68\n13.78\n0.10\nae\nfricative\napical\nvoiceless\nliquid\ncomplex_coda\nL\nS\ninternal\n2\nDH IY0\nL AE1 S T\nT AY1 M\n638.3\n1513.1\n642.6\n1445.0\n613.8\n1005.4\n645.1\n1395.4\n615.9\n1237.3\n6\n\n\nLA000-Joey\nM\nEY\n1\nALL\nplaces\nTHAT\n391.0\n1664.2\n2589.7\n-0.5346548\n0.2222603\n85.1\n216.4\n115.7\n17.452\n17.44\n17.49\n0.05\ney\nfricative\napical\nvoiceless\nobstruent_liquid\none_fol_syll\nL\nS\ninternal\n3\nAO1 L\nP L EY1 S IH0 Z\nDH AH0 T\n391.0\n1664.2\n391.0\n1664.2\n363.4\n1810.2\n354.7\n1944.9\n354.7\n1944.9\n6\n\n\nLA000-Joey\nM\nEH\n1\nMY\nguest\nSIL\n501.3\n1674.3\n2416.1\n0.1559599\n0.2481291\n84.9\n92.5\n442.6\n19.830\n19.81\n19.87\n0.06\ne\nfricative\napical\nvoiceless\nvelar\ncomplex_coda\nG\nS\ninternal\n2\nM AY1\nG EH1 S T\nSIL\n468.4\n1719.3\n472.9\n1711.9\n533.0\n1613.9\n549.3\n1549.6\n548.4\n1545.5\n6\n\n\nLA000-Joey\nM\nIY\n1\nSP\nsleeping\nFREEZING\n254.8\n2279.9\n2658.2\n-1.3874357\n1.7992296\n82.9\n110.1\n124.1\n24.073\n24.03\n24.16\n0.13\niy\nstop\nlabial\nvoiceless\nobstruent_liquid\none_fol_syll\nL\nP\ninternal\n3\n\nS L IY1 P IH0 NG\nF R IY1 Z IH0 NG\n300.6\n1595.5\n261.7\n2216.3\n253.8\n2335.6\n260.8\n2351.3\n244.0\n2229.0\n5\n\n\nLA000-Joey\nM\nEH\n1\nDOES\ncollectors\nMONEY\n521.9\n1342.8\n2471.9\n0.2849414\n-0.6009294\n113.6\n62.9\n106.4\n31.917\n31.90\n31.95\n0.05\ne\nstop\nvelar\nvoiceless\nobstruent_liquid\ncomplex_one_syl\nL\nK\ninternal\n3\nD IH0 Z\nK L EH1 K T ER0 Z\nM AH1 N IY0\n512.0\n1317.0\n512.0\n1317.0\n530.2\n1377.6\n488.7\n1436.6\n488.7\n1436.6\n5\n\n\nLA000-Joey\nM\nEY\n1\nHAVE\nsnakes\nAND\n472.7\n1823.0\n2584.5\n-0.0231116\n0.6289888\n341.8\n197.7\n183.4\n34.419\n34.39\n34.48\n0.09\ney\nstop\nvelar\nvoiceless\nnasal_apical\ncomplex_coda\nN\nK\ninternal\n3\nHH AE1 V\nS N EY1 K S\nAH0 N D\n372.5\n1855.8\n468.8\n1828.8\n408.6\n1608.8\n355.7\n1510.0\n314.5\n1652.2\n6\n\n\nLA000-Joey\nM\nIY\n1\nHE\nfeeds\nWIRE\n261.5\n2044.8\n2771.1\n-1.3454854\n1.1970768\n58.3\n75.0\n154.6\n41.217\n41.17\n41.31\n0.14\niy\nstop\napical\nvoiced\noral_labial\ncomplex_coda\nF\nD\ninternal\n2\nHH IY1\nF IY1 D Z\nW AY1 R\n1387.6\n2206.5\n256.6\n2035.2\n252.6\n1895.6\n230.6\n2086.2\n236.1\n1999.6\n6\n\n\nLA000-Joey\nM\nIH\n1\nTHE\ntrigger\nSP\n351.0\n1643.2\n2198\n-0.7851044\n0.1684738\n62.5\n87.6\n207.6\n43.740\n43.72\n43.78\n0.06\ni\nstop\nvelar\nvoiced\nobstruent_liquid\none_fol_syll\nR\nG\ninternal\n3\nDH IY0\nT R IH1 G ER0\n\n346.6\n1602.3\n347.5\n1607.4\n353.2\n1689.6\n340.0\n1768.0\n337.6\n1777.7\n5\n\n\nLA000-Joey\nM\nIY\n1\nON\nspeaking\nAS\n302.9\n1948.8\n2459.5\n-1.0862700\n0.9511956\n63.8\n124.2\n92.2\n50.653\n50.63\n50.70\n0.07\niy\nstop\nvelar\nvoiceless\noral_labial\none_fol_syll\nP\nK\ninternal\n3\nAO1 N\nS P IY1 K IH0 NG\nEH1 Z\n274.2\n1791.6\n293.1\n1856.2\n284.1\n1898.8\n273.7\n2009.4\n263.3\n2190.1\n6"
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html#building-a-basic-scatterplot",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html#building-a-basic-scatterplot",
    "title": "Making vowel plots in R (Part 1)",
    "section": "Building a basic scatterplot",
    "text": "Building a basic scatterplot\nThe way things work in ggplot2 is we be build a visualization layer by layer. The base layer can be created by just using the ggplot() function.\n\nggplot()\n\n\n\n\nIt’s just a blank, gray rectangle, but it is valid code. To make this actually useful, we can tell it to work with the my_vowels data.\n\nggplot(my_vowels)\n\n\n\n\nOkay still no visualization, but we’re on our way. The next part of a ggplot function is what’s called the mapping argument. This is where you tell ggplot which columns of your data should correspond to what parts of the visualization. Traditionally in vowel plots, we want F2 along the x-axis and F1 along the y-axis. We can do that using the aes() function and specify that we want to work with the columns called F1 and F2 from our spreadsheet.\n\nggplot(my_vowels, aes(x = F2, y = F1))\n\n\n\n\nWe’re getting closer. What ggplot has done at this point is added some information to your plot already. There are now x- and y-axis labels, ticks, and a grid with major and minor lines. All we need to do is populate this with some data.\n\nTangent: Column names\nSide note. In FAVE output, there are several column names with formant data. The F1 and F2 columns have measurements at slightly different points depending on the vowel. If you want to plot the midpoints specifically, you’ll have to use different column names. If you open the file in Excel, the column names are F1@50% and F2@50%. However, R doesn’t really like having the @ or % in the column names, so if you read it in using read.csv like I did, those characters will be changed to periods, meaning the column names are actually F1.50. and F2.50.. So if you want to use midpoints, be sure to do use those columns instead:See Labov, Rosenfelder, & Fruehwald’s 2013 article in Language for details on these columns.\n\nggplot(my_vowels, aes(x = F2.50., y = F1.50.))\n\n\n\n\nSide-side note. If you read your data in using read_csv (with an underscore) from the readr package (which is part of the “tidyverse”), it actually handles the real name of the column. However, you’ll have to put little ticks (that apostrophe-looking thing next to your “1” key) around them:\n\nmy_vowels_readr &lt;- readr::read_csv(\"http://joeystanley.com/data/joey.csv\")\nggplot(my_vowels_readr, aes(x = `F2@50%`, y = `F1@50%`))\n\n\n\n\nWe’ll stick with the basic F1 and F2 columns, but I thought you might find it handy to know what the different columns in your FAVE output mean.\n\n\nAnyway, back to the scatterplot\nAll we need to do at this point is to add the scatterplot. We can do that by adding a separate layer to the ggplot function. To do this, just add a plus sign (+) at the end of the line, start a new line, and add the function geom_point, which is the function for making a scatterplot in ggplot2.\n\nggplot(my_vowels, aes(x = F2, y = F1)) + \n  geom_point()\n\n\n\n\nAha! We now have a scatterplot! It’s not the most useful one because we can’t tell what vowel or word each point came from. But it is a start."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html#themes",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html#themes",
    "title": "Making vowel plots in R (Part 1)",
    "section": "Themes",
    "text": "Themes\nRight now, you might be wondering why we have a gray background. This is on purpose by the designers of ggplot2 because it makes colors pop out. You can change the overall look and feel of your plot using various theme functions.  I like theme_bw(), theme_classic(), and theme_minimal() myself, so I’ll stick with theme_classic() for today.You can explore the other themes by typing the command ?theme_classic and looking at the other options.\n\nggplot(my_vowels, aes(x = F2, y = F1)) + \n  geom_point() + \n  theme_classic()"
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html#coloring-vowels",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html#coloring-vowels",
    "title": "Making vowel plots in R (Part 1)",
    "section": "Coloring vowels",
    "text": "Coloring vowels\nBecause English has so many vowels, there’s no really good way to show them all on a plot. Typically, I use color, but it’s hard to get a set of 11 colors that are all easily distinguishable and easy on the eyes. There’s no real way to win here. For now, let’s just add the default colors and see how it looks.\nSo how do we add color? If you think about it, what we want ggplot to do is to change the color of the dot depending on what the vowel is. Since the vowel is stored in a column called vowel in our spreadsheet, in a practical sense we want to tell ggplot to simply change the color of the dot so that each value in the vowel column has its own color.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel)) + \n  geom_point() + \n  theme_classic()\n\n\n\n\nOkay, so let’s look at the result. The most obvious thing we see is that there is now color, but there’s also a legend too. Each unique vowel in our data is now represented in this legend, and the name of the column in our spreadsheet, vowel, is the title of that legend. One subtler change is that the plotting area is actually a little bit narrower to make room for the legend.\nHow is this color assigned? First, it puts all the vowels in alphabetical order. But keep in mind that this is based on the ARPABET notation, which might not be the order you want. In IPA, it ends up being /ɑ, æ, ʌ, ɔ, ɛ, e, ɪ, i, o, ʊ, u/. It then takes that order and, going around the color wheel from red to pink, picks 11 equidistant, maximally-distinct colors. Because of the nature of how color works, there are several shades of blue and green, but not very many warm colors. We’ll see how to fix the order of these colors, as well as the specific color values, in just a sec.\n\nTangent: reversing the axes\nNow wait a second. The high front vowel /i/—represented by the digraph “IY”—is in the bottom right of the plot when it traditionally is in the top left. Vowel plots typically reverse both the x- and the y-axes so that high vowels are at the top, and front vowels are on the right. This is just convention but it has to do with the inverse relationship with the actual formant values and our perception of sounds. Anyway, the functions you’re looking for are scale_x_reverse() and scale_y_reverse(), which should each be added as their own layer. (Unlike most other layers, I typically put these on one line.)\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_classic()\n\n\n\n\nOkay, much better. Now we can see that the bright blue IY vowel is in the top left, the pink UW is in the top right-ish, and my unmerged AA and AO (/ɑ/ and /ɔ/, as in cot and caught) are in the bottom right.\n\n\nChanging the order\nIf you want to change the order of the colors and the order in the legend, there are two ways to do that. The first is by leaving your underlying data alone and making superficial changes only within ggplot itself. This is a useful thing to know how to do, but I won’t cover that here. If you’re interested, I’d highly recommend this page on that, or you can peruse one of the ggplot2 workshops I did.\nAt least for the order of the vowels, what I think is the most useful option is to actually modify your dataset and then plot the modified version. The way to do this to overwrite the vowel column in our my_vowels dataset, and, using the factor function, manually specifying the order you want them to be in. The actual data itself doesn’t change, but what we’re doing is modifying how R treats this column under the hood. This is the order that I typically do, but you’re of course free to do whatever you want.\n\nmy_vowels &lt;- my_vowels %&gt;%\n  mutate(vowel = factor(vowel,\n                        levels = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                   \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")))\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_classic()\n\n\n\n\nThe benefit to this is that now the vowels are in a somewhat logical order in the legend. The downside is that the colors of each vowel are very close to other vowels near them in the vowel space. What would be better is to have vowels near each other to be different visually.\nJust for funsies, I tried a different order essentially by just choosing every third vowel.\n\nmy_vowels &lt;- my_vowels %&gt;%\n  mutate(vowel = factor(vowel, \n                        levels = c(\"IY\", \"EH\", \"AO\", \"UH\", \"IH\", \n                                   \"AE\", \"AH\", \"UW\", \"EY\", \"AA\", \"OW\")))\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() +\n  scale_y_reverse() + \n  theme_classic()\n\n\n\n\nThis is the first time I’ve done this and I kinda like it. I’ll stick with it. The good part is that the vowels are for the most part relatively easy to distinguish from their neighbors. The major downside is that the legend is the exact order I specified, which is useless for finding something. What we need to do is actually modify the legend order. We can do that with the scale_color_discrete function added to our growing stack of ggplot code and then supply the order you want it to be in as the breaks argument.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) + \n  theme_classic()\n\n\n\n\nGreat. Now the colors are distinct from one another and the order of the legend is back to an order we might expect."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html#adding-vowel-means",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html#adding-vowel-means",
    "title": "Making vowel plots in R (Part 1)",
    "section": "Adding vowel means",
    "text": "Adding vowel means\nThe problem with the plot the way it is, is you still have to constantly check back and forth between the legend and the plot to see what vowel you’re looking at. An easier solution would be to plot the name of the vowel itself inside of its cluster.\nOne solution that I think I’ve seen before is to use the stat_summary function. Supposedly this works, and if you know how to use it, by all means go for it. I’ve never gotten it to work and I found a workaround that I like that I think offers more flexibility anyway. It involves creating a separate dataset and essentially overlaying a second scatterplot over the main one.\nTo create this, I pull out some black magic from the dplyr package. First, I start with the my_vowels dataset. I then “pipe” it (the %&gt;% function) to the summarise function. This function makes it easy to get summary statistics from your data. We’re creating a new, arbitrarily-named column called mean_F1, which is calculated as the mean of the values in the F1 column. Same thing for mean_F2. However, as it is, we’ll end up with two numbers: the average F1 and F2 of all your data, which would probably be somewhere near the middle of your vowel space.\n\nmeans &lt;- my_vowels %&gt;%\n  summarise(mean_F1 = mean(F1),\n            mean_F2 = mean(F2)) %&gt;%\n  print()\n\n   mean_F1  mean_F2\n1 457.5832 1528.889\n\n\nWhat we actually want is the mean F1 and F2 per vowel. So, what we do is insert the group_by function just before summarise. By itself, group_by doesn’t really do much except change some stuff about the dataframe under the hood. But these changes are especially useful when that is then “piped” (%&gt;%) to summarise. Because I did group_by(vowel) first, whatever summary information you want from your dataset will apply to each vowel independently. So, instead of the average overall, you’re getting the average per group. The result is a new dataframe that we’re calling means, that has all the information we want. (I’m then piping it to a print function so we can see the output.)\n\nmeans &lt;- my_vowels %&gt;%\n  group_by(vowel) %&gt;%\n  summarise(mean_F1 = mean(F1),\n            mean_F2 = mean(F2))\n\n# Here's a shortcut using more modern code\nmeans &lt;- my_vowels %&gt;%\n  summarise(mean_F1 = mean(F1),\n            mean_F2 = mean(F2),\n            .by = vowel)\n\nprint(means)\n\n   vowel  mean_F1  mean_F2\n1     AA 621.6889 1130.740\n2     AE 622.7078 1565.124\n3     EY 407.6831 1827.070\n4     EH 494.6517 1598.229\n5     IY 299.7768 2018.850\n6     IH 386.6250 1673.486\n7     UH 399.4735 1246.662\n8     AO 612.7316 1051.511\n9     UW 328.4848 1537.948\n10    AH 531.0722 1227.483\n11    OW 428.7913 1124.385\n\n\nThis new dataset, means, is a perfectly good, stand-alone dataset that we can plot by itself. Note that because we called the columns mean_F1 and mean_F2, we’ll have to use those in the ggplot2 function.\n\nggplot(means, aes(x = mean_F2, y = mean_F1)) + \n  geom_point() + \n  theme_classic()\n\n\n\n\nThe points themselves aren’t very enlightening. To add some pizzazz, I’m going to use geom_label. This is essentially the same thing at geom_point because it makes a scatterplot, but instead of dots it’ll print these nice little labels. Of course, you have to tell ggplot what text to use for these labels, so we’ll tell it to use the text in the vowel column in the means dataset.\n\nggplot(means, aes(x = mean_F2, y = mean_F1, label = vowel)) + \n  geom_label() + \n  theme_classic()\n\n\n\n\nOoh! Okay, so now we’re getting somewhere. Here it becomes obvious that we need to reverse the x- and y-axes. Let’s do that too.\n\nggplot(means, aes(x = mean_F2, y = mean_F1, label = vowel)) + \n  geom_label() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_classic()\n\n\n\n\nPerfect. So we’ve seen how to plot the points themselves, and now we’ve seen how to plot the means. Now comes the fun part of actually overlaying them into one plot.\nIt’s perfectly possible to plot two (or more) different datasets in a single visualization, but you’ll have to be careful about the aes() functions. Anything in the ggplot(aes()) function will apply to all other layers, unless they’re overridden. That’s why we didn’t need to provide any additional information in geom_point because it inherited all its information (the data, the axes, the color) from ggplot.\nIf we want to add the means, we’re using a different dataset, so that right off that bat has to be overridden in our geom_label function:\n\n# Don't plot this yet...\n...\ngeom_label(data = means) + \n  ...\n\nBecause we’re using geom_label, we’re going to need to put label = vowel somewhere. You can put it in the main ggplot(aes()) function with everything else and that’ll work out fine:\n\n# Don't plot this yet...\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  ...\ngeom_label(data = means) + \n  ...\n\nHowever, we’re going to have to supply our own aes() function within geom_label. The reason for that is because right now we’ve got x = F2 and y = F1 as global settings. Our new means dataframe doesn’t have columns with those names. So we’ll have to override these by adding a second aes() function, this time within geom_label:\n\n# Still don't run this.\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  ...\ngeom_label(data = means, aes(x = mean_F2, y = mean_F1)) + \n\nAdd all the other pieces to the plot, and let’er rip.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) +\n  geom_point() +\n  geom_label(data = means, aes(x = mean_F2, y = mean_F1)) +\n  scale_x_reverse() + scale_y_reverse() +\n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\",\n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\"))  +\n  theme_classic()\n\n\n\n\nAha! So now we have a vowel plot that has the points, and on top of them it has the labels for these vowels right where the averages are. Pretty cool.\nA couple things to note here. In the legend, notice that the dots have now all turned into little a’s. This is because we’re using geom_label now. You can change them back to dots if you’d like by adding show.legend = FALSE to the geom_label() function. A better solution though is to actually remove the legend entirely because now it’s not providing any additional clarity. We can do that with guides(color = FALSE).\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  geom_point() + \n  geom_label(data = means, aes(x = mean_F2, y = mean_F1)) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\nAnother thing to notice is that the labels are automatically colored the same as the vowels! How did it do that? We’ll, as it turns out, geom_label inherited the color = vowel argument from the main ggplot(aes()) function. It worked because it just so happens that the column vowel exists in both the means and the my_vowels datasets. Pretty cool. If you want to override it, perhaps by making them all black, you can certainly do so. Just put it within geom_label but not inside of aes:\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  geom_point() + \n  geom_label(data = means, aes(x = mean_F2, y = mean_F1), color = \"black\") + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nIf course, now it’s not quite as clear which cluster the labels belong to. It’s up to you.\n\nTangent: An alternative approach\nSide note, we could have saved ourselves some headache by planning ahead. When we created the means dataframe, we could have called the new columns F1 and F2 to match the columns in my_vowels. By doing that, we wouldn’t need to override the x and y arguments. All of that code would look like this.\n\nmeans &lt;- my_vowels %&gt;%\n  group_by(vowel) %&gt;%\n  summarise(F1 = mean(F1),\n            F2 = mean(F2))\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  geom_point() + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) + \n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nI’ll use this version of the means dataset moving forward since it’s closer to what I do in my actual code and because it makes everything a little easier."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html#ellipses",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html#ellipses",
    "title": "Making vowel plots in R (Part 1)",
    "section": "Ellipses",
    "text": "Ellipses\nOne final thing that would be good to show in a vowel plot are ellipses. These are often used to get an idea of the distribution of the vowels or to show degree of overlap. Fortunately, they’re pretty easy to implement (a lot easier than means at least). The main function that takes care of these is stat_ellipse.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) +\n  geom_point() +\n  geom_label(data = means, color = \"black\") +\n  stat_ellipse() +\n  scale_x_reverse() + scale_y_reverse() +\n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\",\n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) +\n  theme_classic()\n\n\n\n\nEasy-peasy. By default, these ellipses cover about a 95% confidence interval (or approximately two standard deviations) around the means of each vowel. We can change that to whatever we want using the level function. I usually set mine to 0.67, which corresponds to about one standard deviation. This only changes the size of the ellipses, leaving shape/orientation the same.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  geom_point() + \n  geom_label(data = means, color = \"black\") + \n  stat_ellipse(level = 0.67) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\n\nTangent: Ordering\nYou might be wondering why I order the layers in the block of code the way I do. For the most part, the order doesn’t matter, but for some things it does. So if you look carefully at the above plot, you’ll see that the ellipses lines actually cover the labels. The reason for that is simply because the stat_ellipse function came after geom_label. I think it looks better with the labels on top, so you can switch those.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  geom_point() + \n  stat_ellipse(level = 0.67) + \n  geom_label(data = means, color = \"black\") + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nAs far as I can tell, many of the other things like scale_x_reverse, scale_y_reverse, and scale_color_discrete can go anywhere. Generally, I order my block with the important things first (like geom_point since this is a scatterplot after all), then the small cosmetic changes (like scale_x_reverse), and then any themes.\n\n\nMaking the ellipses the focus\nSometimes, you just have too much data and you lose the forest for the trees with all those points. You can easy remove them and leave just the means and the ellipses by removing (or just commenting out) the geom_point line.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  #geom_point() + \n  stat_ellipse(level = 0.67) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nThat’s one way to clean it up. We could also keep them but make them a bit transparent by adding the alpha argument. The range of values for alpha is from 0 to 1, with 1 being completely opaque and 0 being invisible. An alpha level of 0.2 means that it takes 5 (0.2 = 1/5) overlapping points to become completely opaque. In other words it’s only shaded in 20%.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  geom_point(alpha = 0.2) + \n  stat_ellipse(level = 0.67) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nWe could also change the size so that they’re smaller. I’ve found that the exact size depends on how much data you’re displaying, but if you make it smaller than the default of 1.5, you might have a slightly cleaner plot. I’ll make mine about half the default size by adding size = 0.75 within geom_point.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  geom_point(size = 0.75) + \n  stat_ellipse(level = 0.67) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nYou could even change the size to a lot smaller, like 0.1 or even 0.01. You could also keep them relatively large but change the transparency as well (size = 2, alpha = 0.5). The sky’s the limit!\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  geom_point(size = 1, alpha = 0.5) + \n  stat_ellipse(level = 0.67) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\n\n\nShaded ellipses\nI want to touch on how to shade the ellipses in. This takes slightly more finagling within stat_ellipse, but the result is pretty cool.\nThe main argument that you need to add is geom = \"polygon\" within stat_ellipse. Try that and see what the result is:\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  stat_ellipse(level = 0.67, geom = \"polygon\") + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nOkay, so that has the effect of filling them all in black. That’s probably not what we had in mind. Let’s make them all a bit transparent by adding the alpha = 0.2 argument as a part of stat_ellipse.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nFinally, we can actually color these ellipses based on the vowels themselves by adding fill = vowel to our aes function:\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel, fill = vowel)) + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nWhoops! What did that do? Yes, the ellipses are shaded in correctly, but now the labels are too! Turns out, in geom_label, the fill argument also modifies the background color, which is normally white. We could either override the fill on geom_label and set it to \"white\", or, better yet, let’s just add aes(fill = vowel) just to stat_ellipse to prevent any other potential bugs.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1, aes(fill = vowel)) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE) + \n  theme_classic()\n\n\n\n\nAha. Now we get the desired result. Note though that our legend is back. This is because we told ggplot2 to not display a legend as it relates to color using guides(color = FALSE), but we didn’t say anything about this new fill aesthetic. We can just add fill = FALSE there and it’ll take it out.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1, aes(fill = vowel)) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  guides(color = FALSE, fill = FALSE) + \n  theme_classic()\n\n\n\n\nReally though, the guides option is only good if you want to remove specific aspects of the legend but not all of it. Alternatively, we can just use theme(legend.position = \"none\") instead and that’ll remove the legend no matter what else we add to the plot. However, this has to come after any themes you might apply to the plot.\n\nggplot(my_vowels, aes(x = F2, y = F1, color = vowel, label = vowel)) + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1, aes(fill = vowel)) + \n  geom_label(data = means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \n                                  \"AA\", \"AO\", \"OW\", \"UH\", \"UW\", \"AH\")) +\n  theme_classic() + \n  theme(legend.position=\"none\")\n\n\n\n\nSo shaded ellipses are cool, but because we have so many vowels in English they can get a little muddy. If you’re only working with a subset of English vowels or a language with fewer vowels, it’ll look a little crisper, even if you add the points back in. I’ll just take five vowels and plot them, and I’ll even add shape in there too for fun. (Can you see how I did that?)\n\nmy_five_vowels &lt;- my_vowels %&gt;% \n  filter(vowel %in% c(\"IY\", \"EH\", \"AA\", \"OW\", \"UW\"))\nfive_means &lt;- my_five_vowels %&gt;%\n  summarise(F1 = mean(F1),\n            F2 = mean(F2),\n            .by = vowel)\nggplot(my_five_vowels, aes(x = F2, y = F1, color = vowel, label = vowel, shape = vowel)) + \n  geom_point() + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1, aes(fill = vowel)) + \n  geom_label(data = five_means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"IY\", \"EH\", \"AA\", \"OW\", \"UW\")) +\n  theme_classic() + \n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html#text-instead-of-points",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html#text-instead-of-points",
    "title": "Making vowel plots in R (Part 1)",
    "section": "Text instead of points",
    "text": "Text instead of points\nOkay, last thing, I promise. If your dataset is relatively small, a really slick trick is to plot the words themselves rather than points. We saw how to do this above when we were plotting the means, but let’s apply that to the regular data. For this example, I’ll just zoom in on my “AA” and “AO” vowels (except for the ones before /ɹ/) because I’ve been reading about the cot-caught merger recently.\n\ncot_caught &lt;- my_vowels %&gt;%\n  filter(vowel %in% c(\"AA\", \"AO\"),\n         fol_seg != \"R\")\ncot_caught_means &lt;- cot_caught %&gt;%\n  group_by(vowel) %&gt;%\n  summarise(F1 = mean(F1),\n            F2 = mean(F2))\n\nSo here’s what this would look like with points.\n\nggplot(cot_caught, aes(x = F2, y = F1, color = vowel, label = vowel, shape = vowel)) + \n  geom_point() + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1, aes(fill = vowel)) + \n  geom_label(data = cot_caught_means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"AA\", \"AO\")) +\n  theme_classic() + \n  theme(legend.position=\"none\")\n\n\n\n\nThe trick here is to use geom_text instead of geom_point. Note that geom_text is very similar to geom_label, which is what we used for the means. The only difference I’ve been able to see is that there’s a nice little box around geom_label and not one for geom_text.\n\nggplot(cot_caught, aes(x = F2, y = F1, color = vowel, label = vowel, shape = vowel)) + \n  geom_text() + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1, aes(fill = vowel)) + \n  geom_label(data = cot_caught_means) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"AA\", \"AO\")) +\n  theme_classic() + \n  theme(legend.position=\"none\")\n\n\n\n\nOops! That’s not what we wanted! For the means, yes, we want the vowel. But for the points we want the actual word. This means we have to add label = word to our code somewhere. For clarity, I’ll move label = vowel out of ggplot(aes() and into geom_label(aes()). That way there is no default label and every time we call geom_text or geom_label we need to specify label individually. (Also, I’ll get rid of shape = vowel since that’s not being used anymore.)\n\nggplot(cot_caught, aes(x = F2, y = F1, color = vowel)) + \n  geom_text(aes(label = word)) + \n  stat_ellipse(level = 0.67, geom = \"polygon\", alpha = 0.1, aes(fill = vowel)) + \n  geom_label(data = cot_caught_means, aes(label = vowel)) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  scale_color_discrete(breaks = c(\"AA\", \"AO\")) +\n  theme_classic() + \n  theme(legend.position=\"none\")\n\n\n\n\nAha! There we go. So this is a way to make a plot look cooler. Especially if individual lexical items are part of your analysis and particularly if you don’t have a lot of data to show at once. If you want, try it with the full dataset just to see how not helpful it is, but be aware that it can be slow to render if you have a lot of data to show."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-1/index.html#final-remarks",
    "href": "blog/making-vowel-plots-in-r-part-1/index.html#final-remarks",
    "title": "Making vowel plots in R (Part 1)",
    "section": "Final remarks",
    "text": "Final remarks\nThe way you present your data is all up to you. I often prefer a set of settings when I’m playing around with my data, but then switch to a different set when I want to copy and paste into a presentation or paper. It’s good to be comfortable enough with ggplot2 so that you know what is going on and what changes you can make. Hopefully this post made a few things clearer.\nIf you’re interested in plotting trajectories, feel free to look at Part 2 of this tutorial. In the future, I’d like make some other posts on some slightly more advanced topics in vowel plots. I hope this one at least has helped you and your research."
  },
  {
    "objectID": "blog/reviewer-feedback/index.html",
    "href": "blog/reviewer-feedback/index.html",
    "title": "Reviewer Feedback",
    "section": "",
    "text": "Yesterday I got the reviewer feedback for the paper I’m going to be presenting at the American Dialect Society in January.\nThis is not the first time I’ve gotten reviewer feedback: I’ve submitted several things to big enough conferences where reviewers give their feedback. But they’re usually something like, “Okay yeah that’s cool and all, but here are some obvious things you should consider. I’ll reluctantly give you a pass, but I expect major changes at the conference.” Either that, or they’re brutally honest and say it’s garbage.\nI’m grateful for the feedback every time because it’s completely spot-on, and reading an uncomfortable email alone at my desk is merciful compared to what I might potentially hear at conferences. Or worse, what I might not hear but what people think.\nBut, I’m happy to report I had three glowing reviews this time! On top of that, I was offered advice on some studies to look up and some ways to strengthen my argument. Maybe this is an interesting topic after all!"
  },
  {
    "objectID": "blog/prevelar-raising-survey-results/index.html",
    "href": "blog/prevelar-raising-survey-results/index.html",
    "title": "Prevelar Raising Survey Results",
    "section": "",
    "text": "In April and May this year, I posted a survey to a bunch of different subreddits that asked people how they pronounced certain words. If you took the survey, THANK YOU! The number of responses I got was overwhelming and took much longer to analyze than I could have ever anticipated. So, after many months, I’m finally ready to post the results for you. Hopefully you’ll find them interesting.\nTL;DR I asked a bunch of people how they pronounced words that rhyme with bag or beg. Basically, people from the northern US and Canadians tend to pronounce them with the same vowel as the word bake. Also, if you do though, it’s less likely in longer words."
  },
  {
    "objectID": "blog/prevelar-raising-survey-results/index.html#bag-words",
    "href": "blog/prevelar-raising-survey-results/index.html#bag-words",
    "title": "Prevelar Raising Survey Results",
    "section": "bag words",
    "text": "bag words\nI’ll start with the bag words. As I said before, bag-raising generally occurs in Canada and in the northern US. Here’s a map of what I found in my survey:\n\nIn this map, darker, bigger circles represent people who had more bag-raising while smaller, white circles are for those that had less. So my survey results were pretty much what I expected based on existing linguistic research.\nHowever, sometimes people had bag-raising on some words but not on others. Were there any sort of patterns? Here’s a colorful plot that puts the bag in order of how much they were raised, with Volkswagon sounding like bake or deck the most often and Mary Magdalene the least.\n\nIn this chart, each word is on its own row. Within that row, the width of the red box is the number of people that indicated that they say that word with the same vowel as bake. Yellow is “somewhere between bake and deck”. The small green ones are deck and the blue ones are back. Purple are where people said it had some other vowel. This chart shows that not all bag words are equally likely to be pronounced the same, but the difference between the Volkswagon at the top and Mary Magdalene at the bottom is admittedly not that huge.\nSo now lets turn our attention to the beg words."
  },
  {
    "objectID": "blog/prevelar-raising-survey-results/index.html#beg-words",
    "href": "blog/prevelar-raising-survey-results/index.html#beg-words",
    "title": "Prevelar Raising Survey Results",
    "section": "beg words",
    "text": "beg words\nSo bag words are cool and all, but a lot less research has been done on beg words. This was the main thing I wanted to find out: where do people have beg-raising and what beg words are more (or less) likely to be raised?\nTo answer the first question, here are the results on a map:\n\nAgain, darker, bigger circles represent people who said more beg-words had the same vowel as bake while smaller, white circles are for those that said it was more like deck. So this shows that you’ll hear people pronouncing beg words like bake at least some of the time pretty much everywhere but the South. Linguists already kinda knew this happened in the Pacific Northwest, but the fact that it’s in pretty much all Western and Midwestern states (especially in that stretch from Illinois to Pennsylvannia) was news to me.\nJust as surprising was the following plot which shows how often beg words were raised:\n\nHere, we see that there’s a major difference between the top and bottom of the chart. Okay so technically, words like vague, pagen, plague, and fragrant don’t belong on this chart because pretty much everyone would pronounce them with a “long a” sound. I’ll ignore this for now. But even between words that are spelled with eg, there’s a big difference between them. A word like segregate is said with the same vowel as bake less than 10% of the time. But a word like egg is almost 20% and omega is almost 40%. And, if you look closely, most of the words towards the bottom all have consonants following the g—integrity, segment, pregnant. That’s pretty interesting if you ask me."
  },
  {
    "objectID": "blog/prevelar-raising-survey-results/index.html#both-in-one-map",
    "href": "blog/prevelar-raising-survey-results/index.html#both-in-one-map",
    "title": "Prevelar Raising Survey Results",
    "section": "Both in one map",
    "text": "Both in one map\nWhen you combine the two maps, you can start to see where one vowel is pronounced like bake and the other is not. Here, green areas are those that have bag-raising but not beg-raising and purple are areas with beg-raising without bag-raising.\n\nBasically, the whole reason I did this survey was to produce this map. I wanted to see if there would be purple areas without green and green areas without purple. And there are!"
  },
  {
    "objectID": "blog/general-update/index.html",
    "href": "blog/general-update/index.html",
    "title": "General Update",
    "section": "",
    "text": "Because I know I have such a massive following, I thought I’d give an update on my research since it’s been a few months since the last time I wrote.\n\nPublications\nAt the Linguistic Atlas Office, we’re working hard on publishing some of our preliminary results. Currently, I’m on two papers submitted to Proceedings of Meetings on Acoustics that are in various stages of reviewing. I’m excited to see these come out.\nI’m also working on some of my own research. I’ve got three manuscripts going right now: one on near-mergers in Washington, one on language change within a speaker’s lifespan, and another on Amazon Mechanical Turk. I still haven’t submitted a paper to an actual journal so mentally this is a big hurdle for me to get over.Edit (September 2023): Lol, none of these manuscripts were even finished.\n\n\nConferences\nI’m happy to say I’ve been accepted into two conferences that’ll happen over the next few months. The first is a paper called “Consonantal variation in Utah English: What el[t]se is happening[k]?” that I’m doing with Kyle Vanderniet, a fellow grad student here at UGA. Using the MTurk data I collected recently, we focused on just the 14 speakers from Utah and gathered a lot of really interesting tokens of non-standard variants. I’ve also been accepted to present a poster at NWAV on some of my findings in Washington.\nI’m also still waiting to hear back from three other conferences: ASA and two at ADS.\n\n\nTeaching\nSo I’m teaching for the first time this semester. Because our normal phonetician will be gone, I was asked to teach Phonetics & Phonology. This is pretty cool because normally grad students at UGA don’t get to teach that class. I’ve really enjoyed it so far! I’ve got 29 students who are all linguistics majors or minors. It takes quite a bit of time, but I’m having a great time.\n\n\nR Series\nFortunately, my funding through the DigiLab at the UGA Main Library continues this school year. I’m really excited be giving a whole series of workshops on R. Next week I’ll start with just a basic introduction to R. Next month I’ll do a day on ggplot2 and in November will be one on the rest of the tidyverse. I plan on producing some detailed handouts that will be available on this website as a product of these workshops. I’m really looking forward to them.\n\n\nGrant\nI applied for a small grant a week or so ago that I’m waiting to hear back about. If I get it, it’ll pay for some fieldwork out West I’d like to do. Stay tuned. (Update: I got the grant.)\n\n\nDissertation\nAh, the dissertation. This should be my primary task right now, and I’m putting as much time as I can into it. I originally wanted to do something on my Washington data. I could do a purely descriptive work and it would make for a fine dissertation. I’ve read through some of those and they’re perfectly good.\nThe problem with that is if I would do that, it would be of interest only to people interested in Washington English. A very small group of dialectologists. Not that my ultimate goal is to boost the number of citations, but I feel like a dissertation, which is supposed to make a real contribution to the field, should be more than that.\nI’ve read a couple other dissertations that are heavily based in dialectology and sociolinguistic fieldwork, but they make a bigger statement. They make some advancement on the study of language change, using the data they’ve collected as an illustration. For example, Ruth Herold’s (1990) UPenn dissertation is based on the cot-caught merger parts of Pennsylvania. Instead of simply describing the data, she makes some very cool statements about how language change works, especially in relation to mergers and generational changes, using the data she collected as evidence. The data is supporting the claim rather than the claim itself.\nI’ve got around 180 hours of my own data right now, gathered from lots of sources. All of it has to do with mergers in some way. I feel like I’ve got a dissertation bubbling inside of me relating to vowel mergers. I can use my data as support, illustration, and evidence for some claim that I’d like to make.Update: I stuck with the Washington data for my dissertation, but Betsy Sneller and I did eventually write something about mergers using the Washington data, so I guess that scratched that itch.\nSo I’m working on my prospectus right now and will be pitching it to my committee this semester. I’ll keep you informed."
  },
  {
    "objectID": "blog/365-papers/index.html",
    "href": "blog/365-papers/index.html",
    "title": "#365papers",
    "section": "",
    "text": "Around the first of the year, I saw that several academics I follow on Twitter made a goal to read 365 papers during 2018. They tweet about their papers and use the hashtag #365papers. I don’t stand a chance at reaching that goal of 365 papers, but I decided to join in."
  },
  {
    "objectID": "blog/365-papers/index.html#whats-the-point",
    "href": "blog/365-papers/index.html#whats-the-point",
    "title": "#365papers",
    "section": "What’s the point?",
    "text": "What’s the point?\nFirst off, why bother doing this over Twitter? I can think of several reasons.\n\nTwitter is an interesting place for academics and this just sort of fits in with what goes on there.\nIf I read papers written by people I know on Twitter, I’ll @ them. It not only lets them know I’m reading their work, it’s a quick way to incorporate them into the conversation.\nIn general, keeping track of the papers I read is healthy because it’s easier to go back and find something I read when I have a spreadsheet.\nKeeping track in a way for others to see is even more motivating because I don’t want to quit halfway through.\n\nEssentially, I find it useful to keep track of things. I always feel like I need to read more academic work, so this is a nice way to motivate myself to do so."
  },
  {
    "objectID": "blog/365-papers/index.html#twitter-feed",
    "href": "blog/365-papers/index.html#twitter-feed",
    "title": "#365papers",
    "section": "Twitter feed",
    "text": "Twitter feed\nThe rest of this post will be updated periodically to include my tweets. For brevity, I’ll only post parent tweets here, but sometimes I do a tweetstorm and include some commentary, which sometimes includes comments from others.\n\n\nAll right, all you #365papers have convinced me. I'm behind, but I'll mention what I remember reading so far this year:#1–6: @americandialect's PADS vol 2 (https://t.co/Yoy5YoIfaX) with @aliciabwassink, @dialect, @cotterw, @sociolx, etc. etc.\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\n@aliciabwassink. 2015. Sociolinguistic patterns in Seattle English. Language Variation and Change 27(01). 31–58. https://t.co/4e4xSylA2B It somehow slipped through the cracks in my research on Pacific Northwest English. #365papers #7\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\nTrudgill, Peter & Tina Foxcroft. 1978. On the sociolinguistics of vocalic mergers: Transfer and approximation in East Anglia.Labov (1994) cites this as where mergers by approximation and transfer came from. Thought I'd go to the source. #365papers #8\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\n@dialect 2013. ’Flip-flop’and mergers-in-progress. English Language and Linguistics 17(02). 359–390.It's been on my to-read list for a while. Was not disappointed. #365papers #9\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\nZeller, Christine. 1997. The investigation of a sound change in progress : /æ/ to /e/ in Midwestern American English. Journal of English Linguistics 25(2). 142–155.I'm researching BAG-raising. This was one of the first. #365papers #10\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\nBauer, Matt & Frank Parker. 2008. /æ/-raising in Wisconsin English. American Speech 83(4). 403–431.More research on BAG-raising. Also, ultrasound is sweet. Also, add Ohala (2003) to my to-read list. #365papers #11\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\nFaber & Di Paolo. 1990. Phonation differences and the phonetic content of the tense-lax contrast in Utah English. LVC 2(02). 155–204. doi:10.1017/S0954394500000326.Their last statement says it all: \"There is more to vowels than their formant frequencies.\" #365papers #12\n\n— Joey Stanley (@joey_stan) March 20, 2018\n\n\n\n\nDinkin. 2016. Phonological Transfer as a Forerunner of Merger in Upstate New York. Journal of English Linguistics 44(2). 162–188. doi:10.1177/0075424216634795.Introduces a new type of merger, distinct from Approximation, Transfer, and Expansion. #365papers #13\n\n— Joey Stanley (@joey_stan) March 21, 2018\n\n\n\n\nLisker, Leigh. 1986. “Voicing” in English: A Catalogue of Acoustic Features Signaling /b/ versus /p/ in Trochees. Language and Speech 29(1). 3–11.There are no fewer than 16 acoustic cues to differentiate /p/ and /b/ in \"rapid\" vs. \"rabid.\" #365papers #14 1/6\n\n— Joey Stanley (@joey_stan) March 23, 2018\n\n\n\n\nMilroy, James & Lesley Milroy. 1978. Belfast: Change and variation in an urban vernacular. In Peter Trudgill (ed.), Sociolinguistic Patterns in British English, 19–36. London: Edward Arnold. #365papers #15 1/2\n\n— Joey Stanley (@joey_stan) April 11, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 1: Introduction. I need to learn more about phonetics, so this will be a great book. #365papers #16 (I'm counting book chapters as papers.)\n\n— Joey Stanley (@joey_stan) April 15, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 2: American English Phonemes. #365papers #17\n\n— Joey Stanley (@joey_stan) April 15, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 3: Speech and Sound. #365papers #18A nice overview of the acoustics of sound is always good every once in a while.\n\n— Joey Stanley (@joey_stan) April 17, 2018\n\n\n\n\nGerry O. Knowles. 1978. The Nature of Phonological Variables in Scouse. In Peter Trudgill (ed.), Sociolinguistic Patterns in British English, 19–36. London: Edward Arnold. #365papers #19\n\n— Joey Stanley (@joey_stan) April 18, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 4: Static Properties of Speech Sounds #365papers #20\n\n— Joey Stanley (@joey_stan) April 19, 2018\n\n\n\n\nJames Milroy. 1981. Regional accents of English: Belfast. Chapter 5: Belfast Vowels in Detail #365papers #21.Thanks, @EstherAsprey, for the recommendation!\n\n— Joey Stanley (@joey_stan) April 19, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 5: Vowel Transitions. #365papers #22\n\n— Joey Stanley (@joey_stan) April 20, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 6: Obstruent and Vowel Transitions. #365papers #23\n\n— Joey Stanley (@joey_stan) April 30, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 6: Consonantal Sonorants and Vowels. #365papers #24\n\n— Joey Stanley (@joey_stan) April 30, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 8: Consonant Interactions. #365papers #25\n\n— Joey Stanley (@joey_stan) May 7, 2018\n\n\n\n\n@karmaglow. 2018. A remedial path to merger: Merger by phonological transfer in British Columbia English. Toronto Working Papers in Linguistics 40(1). https://t.co/bonUfob51K #365papers #26Working on pre-velar raising myself right now so this was perfect for me!\n\n— Joey Stanley (@joey_stan) May 10, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 9: Acoustic Variability. #365papers #27Basically, this whole book can be summed up in their last two sentences:\n\n— Joey Stanley (@joey_stan) May 11, 2018\n\n\n\n\nLabov, William. 1994. Principles of Linguistic Change (Volume 1). Chapter 14: The Suspension of Phonemic Contrast. #365papers #28\n\n— Joey Stanley (@joey_stan) May 17, 2018\n\n\n\n\n@JoFrhwld 2017. Generations, lifespans, and the zeitgeist. Language Variation and Change 29(1). 1–27. doi:10.1017/S0954394517000060.Added to my todo list: 1) Acquire big corpora 2) Be able to use and explain advanced statistical models like a boss. #365papers #29\n\n— Joey Stanley (@joey_stan) May 22, 2018\n\n\n\n\nLabov, William. 1994. Principles of Linguistic Change (Volume 1). Chapter 5: General Principles of Vowel Shifting. #365papers #30Hey, it's only taken me almost 6 months to get to 30, but that's a start!\n\n— Joey Stanley (@joey_stan) May 22, 2018\n\n\nAt this point, I decided to stop Tweeting. It was more of a commitment than I was willing to get myself into. Plus, I was reading some foundational things that I felt embarrassed to admit I hadn’t read yet. 😅 But I did finish, and you can read my post about when I read my 365th paper here."
  },
  {
    "objectID": "blog/geomtextpath/index.html",
    "href": "blog/geomtextpath/index.html",
    "title": "Curved Text in ggplot2 with geomtextpath",
    "section": "",
    "text": "I recently saw a tweet by @timelyportfolio that mentions an R package, geomtextpath, by Allan Cameron. The function overlays text over curved lines, giving you the possibility to add nice labels to the data.\nSince I plot vowel trajectories a lot and have to label them, I thought I’d try it on some vowel data to see how well it works.\nlibrary(tidyverse)\nlibrary(joeysvowels)\nlibrary(joeyr)\n\n# remotes::install_github(\"AllanCameron/geomtextpath\")\n#    alternatively...\n# install.packages(\"geomtextpath\")\nlibrary(geomtextpath)"
  },
  {
    "objectID": "blog/geomtextpath/index.html#data-prep",
    "href": "blog/geomtextpath/index.html#data-prep",
    "title": "Curved Text in ggplot2 with geomtextpath",
    "section": "Data prep",
    "text": "Data prep\nI’ll use some trajectory data I have in my joeysvowels package. I’ll mostly work with data from the coronals object. This contains a bunch of formant trajectory data from me reading all my vowels flanked by a bunch of combinations of coronal consonants (they were mostly nonce words like /sneɪz/, /nɔdz/, and /dʊz/).\n\ncoronals\n\n# A tibble: 14,446 × 13\n   vowel_id start   end     t percent    F1    F2    F3    F4 word  pre   vowel\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt;\n 1        1  2.06  2.41  2.06       0  387. 1701. 2629. 3164. snoʊz sn    GOAT \n 2        1  2.06  2.41  2.07       5  483. 1591. 2454. 3310. snoʊz sn    GOAT \n 3        1  2.06  2.41  2.09      10  525. 1466. 2526. 3343. snoʊz sn    GOAT \n 4        1  2.06  2.41  2.13      20  530. 1297  2616. 3330  snoʊz sn    GOAT \n 5        1  2.06  2.41  2.14      25  497. 1223. 2562. 3280. snoʊz sn    GOAT \n 6        1  2.06  2.41  2.16      30  461. 1172. 2559. 3252  snoʊz sn    GOAT \n 7        1  2.06  2.41  2.18      35  414. 1120  2625. 3247. snoʊz sn    GOAT \n 8        1  2.06  2.41  2.20      40  423  1072. 2655. 3175. snoʊz sn    GOAT \n 9        1  2.06  2.41  2.22      45  396. 1074  2623. 3248. snoʊz sn    GOAT \n10        1  2.06  2.41  2.23      50  368. 1018. 2602. 3168. snoʊz sn    GOAT \n# ℹ 14,436 more rows\n# ℹ 1 more variable: fol &lt;chr&gt;\n\n\nThere’s a lot of data and I don’t need to plot all trajectories of all observations, so I’ll boil them down to one trajectory per vowel by taking the median per timepoint per vowel. Also, because these vowels are flanked by coronals, the edges of the vowel trajectories, particularly the second halves, all converged towards the high front portion of the vowel space, so I’ll just get a middle portion of the trajectory.\n\navg_trajs &lt;- coronals %&gt;%\n  group_by(vowel, percent) %&gt;%\n  summarize(across(c(F1, F2), median)) %&gt;%\n  filter(percent &gt;= 10, percent &lt;= 60) %&gt;%\n  print()\n\n# A tibble: 143 × 4\n# Groups:   vowel [13]\n   vowel percent    F1    F2\n   &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 LOT        10  646. 1279.\n 2 LOT        15  660. 1240.\n 3 LOT        20  664. 1170.\n 4 LOT        25  658. 1175.\n 5 LOT        30  645  1141.\n 6 LOT        35  640. 1136.\n 7 LOT        40  631. 1154.\n 8 LOT        45  634. 1168 \n 9 LOT        50  629. 1167.\n10 LOT        55  632. 1178.\n# ℹ 133 more rows"
  },
  {
    "objectID": "blog/geomtextpath/index.html#basic-trajectory-plots",
    "href": "blog/geomtextpath/index.html#basic-trajectory-plots",
    "title": "Curved Text in ggplot2 with geomtextpath",
    "section": "Basic Trajectory Plots",
    "text": "Basic Trajectory Plots\nHere’s what this data looks like.\n\nmy_arrow &lt;- arrow(ends = \"last\", type = \"closed\", angle = 25, length = unit(0.3, \"cm\"))\nggplot(avg_trajs, aes(F2, F1, color = vowel, group = vowel)) +\n  geom_path(arrow = my_arrow) + \n  scale_x_reverse() +\n  scale_y_reverse() + \n  theme_joey_legend()\n\n\n\n\nOkay, normally what I’d do here is create a separate dataset that has just the vowel onsets and plot it separately like this. I can remove the legend too at this point since it contributes no new information.\n\navg_trajs_onsets &lt;- avg_trajs %&gt;%\n  filter(percent == min(percent))\nggplot(avg_trajs, aes(F2, F1, color = vowel, group = vowel)) +\n  geom_path(arrow = my_arrow) + \n  geom_text(data = avg_trajs_onsets, aes(label = vowel)) + \n  scale_x_reverse() +\n  scale_y_reverse() + \n  theme_joey() + \n  theme(legend.position = \"none\")\n\n\n\n\nHowever, with this geomtextpath package, I might be able to save myself the work of creating a new dataframe."
  },
  {
    "objectID": "blog/geomtextpath/index.html#f1-f2-plots",
    "href": "blog/geomtextpath/index.html#f1-f2-plots",
    "title": "Curved Text in ggplot2 with geomtextpath",
    "section": "F1-F2 plots",
    "text": "F1-F2 plots\nOkay, now we’re ready to try out geomtextpath. Here’s what it looks like with the function in its most basic form.\n\nggplot(avg_trajs, aes(F2, F1, color = vowel)) +\n  geom_textpath(aes(label = vowel), arrow = my_arrow) + \n  scale_x_reverse() +\n  scale_y_reverse() + \n  theme_joey() + \n  theme(legend.position = \"none\")\n\n\n\n\nSo, a few things to note here. The behavior of the function is most easily seen in goose and choice. It chooses a spot in the middle of the line and overlays the label. The line itself is cut into two pieces (a behavior that can be overriden with cut_path = FALSE if I wanted) with the label centered in the gap. Overall, I’m not a huge fan of the out-of-the-box look, but we can adjust things to make it look better.\nOne main problem with this look is that the lines go through the label, which isn’t great. We can adjust the vertical position of the label with the vjust argument, which will position it slightly above the line. While we’re adjusting the position of the label, I’ll set hjust to 0, which will move it to the onset of the vowel.\n\nggplot(avg_trajs, aes(F2, F1, color = vowel)) +\n  geom_textpath(aes(label = vowel),\n                hjust = 0,\n                vjust = -0.25,\n                arrow = my_arrow) +\n  scale_x_reverse() +\n  scale_y_reverse() + \n  theme_joey() + \n  theme(legend.position = \"none\") \n\n\n\n\nFor some vowels, this looks pretty well. However, because of the jaggedness of the lines, the label itself is going to be pretty jagged. Lines like mouth, price, and fleece are illegible.\n\nFortunately, the argument straight, when set to TRUE, will force the text to be straight. According to the documentation, it is helpful for “noisy paths”, which I think perfectly applies to our this data.\n\nggplot(avg_trajs, aes(F2, F1, color = vowel)) +\n  geom_textpath(aes(label = vowel),\n                hjust = 0,\n                vjust = -0.25,\n                arrow = my_arrow,\n                straight = TRUE) +\n  scale_x_reverse() +\n  scale_y_reverse() + \n  theme_joey() + \n  theme(legend.position = \"none\")\n\n\n\n\nThis is better, but I’m not a fan of how some of them cross the lines themselves (like thought, fleece, choice).\nA potential solution I came up with is to fit a GAM to the data. Here’s some quick and dirty code that fits GAMs to both F1 and F2 for each vowel, extracts predicted measurements, and plots them instead.\n\nlibrary(mgcv)\nlibrary(itsadug)\n\ncoronals_gam_preds &lt;- coronals %&gt;%\n  # Subset the data\n  filter(percent &gt;= 10, percent &lt;= 60) %&gt;%\n  select(vowel, percent, F1, F2) %&gt;%\n  # Add an id column\n  rowid_to_column() %&gt;%\n  # Reshape it to a \"long\" formant\n  pivot_longer(cols = c(F1, F2), names_to = \"formant\", values_to = \"hz\") %&gt;%\n  # Group for the GAMs\n  group_by(vowel, formant) %&gt;%\n  nest() %&gt;%\n  # Fit the GAM and extract predicted measurements\n  mutate(mdl = map(data, ~bam(hz ~ percent + s(percent), data = .)),\n         preds = map(mdl, ~get_predictions(., cond = list(percent = 10:60), print.summary = FALSE))) %&gt;%\n  # Post-processing\n  select(-data, -mdl) %&gt;%\n  unnest(preds) %&gt;%\n  rename(hz = fit) %&gt;%\n  select(-CI) %&gt;%\n  pivot_wider(names_from = formant, values_from = hz) %&gt;%\n  print()\n\n# A tibble: 663 × 4\n# Groups:   vowel [13]\n   vowel percent    F1    F2\n   &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 GOAT       10  449. 1232.\n 2 GOAT       11  447. 1218.\n 3 GOAT       12  445. 1204.\n 4 GOAT       13  444. 1189.\n 5 GOAT       14  442. 1176.\n 6 GOAT       15  440. 1162.\n 7 GOAT       16  438. 1149.\n 8 GOAT       17  436. 1136.\n 9 GOAT       18  434. 1123.\n10 GOAT       19  432. 1111.\n# ℹ 653 more rows\n\nggplot(coronals_gam_preds, aes(F2, F1, color = vowel)) +\n  geom_textpath(aes(label = vowel),\n                vjust = -0.5,\n                hjust = 0,\n                arrow = my_arrow,\n                straight = TRUE) +\n  scale_x_reverse() +\n  scale_y_reverse() + \n  theme_joey() + \n  theme(legend.position = \"none\")\n\n\n\n\nThe results show a smoother line and indeed smoother labels, but there are still some positions I’m not a fan of, like kit, choice, and thought.\nOverall, I’m haven’t quite found a plot that I’m satisfied with. Perhaps with a different dataset it could work, but at least for this one, this F1-F2 plot isn’t quite doing what I was hoping."
  },
  {
    "objectID": "blog/geomtextpath/index.html#spectrogram-plots",
    "href": "blog/geomtextpath/index.html#spectrogram-plots",
    "title": "Curved Text in ggplot2 with geomtextpath",
    "section": "Spectrogram plots",
    "text": "Spectrogram plots\nAnother potential use for geomtextpath is to create spectrogram-like plots. Here’s a simple one with all the vowels plotted, split up by formant.\n\navg_trajs %&gt;%\n  pivot_longer(cols = c(F1, F2), names_to = \"formant\", values_to = \"hz\") %&gt;%\n  unite(traj_id, vowel, formant, remove = FALSE) %&gt;%\n  ggplot(aes(percent, hz, color = vowel)) +\n  geom_textpath(aes(group = traj_id, label = vowel), vjust = -0.1, hjust = 0.3) + \n  scale_y_log10() + \n  facet_wrap(~formant, nrow = 1, scales = \"free\") + \n  theme_joey() + \n  theme(legend.position = \"none\")\n\n\n\n\nIt’s not bad. With the help of color to help tell the lines apart, I think it does a pretty good job to be honest. Although, I’ll mention that I tried this plot out with a couple different aspect ratios, and taller ones work better."
  },
  {
    "objectID": "blog/geomtextpath/index.html#ellipses",
    "href": "blog/geomtextpath/index.html#ellipses",
    "title": "Curved Text in ggplot2 with geomtextpath",
    "section": "Ellipses?",
    "text": "Ellipses?\ngeomtextpath has a few other functions, including geom_textdensity, geom_textsmooth, geom_textcontour, geom_textdensity2d, which correspond to adding labels to density plots (geom_density), 2D density plots (geom_textdensity2d), smooths (stat_smooth), and contour plots (geom_contour). What I would love to see is a function that correspoinds to stat_ellipse. I often make plots like this where I add dots and ellipses in the same plot and then add labels at vowel averages with a separate data frame:\n\nmidpoints_means &lt;- midpoints %&gt;%\n  group_by(vowel) %&gt;%\n  summarize(across(c(F1, F2), mean))\nggplot(midpoints, aes(F2, F1, color = vowel)) + \n  geom_point(alpha = 0.25) + \n  stat_ellipse(level = 0.67) + \n  geom_text(data = midpoints_means, aes(label = vowel)) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_joey() + \n  theme(legend.position = \"none\")\n\n\n\n\nIt would be pretty cool to see these ellipses get the labels on them rather than plotting at the centers."
  },
  {
    "objectID": "blog/geomtextpath/index.html#conclusion",
    "href": "blog/geomtextpath/index.html#conclusion",
    "title": "Curved Text in ggplot2 with geomtextpath",
    "section": "Conclusion",
    "text": "Conclusion\nThat’s all. I just wanted to try something new. Maybe this might be useful to you as you plot your vowel trajectory data."
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html",
    "href": "blog/new-publication-in-pwpl/index.html",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "",
    "text": "I’ve just been informed that a manuscript I submitted to the Penn Working Papers in Linguistics has been published! It’s called “Order of Operations in Sociophonetic Analysis” and is available here. In a nutshell, I discuss the various processing steps that are typical of a sociophonetic analysis an I show that changing the order that you run them can have a non-negligible impact on the overall results. I end the paper with a recommended order that we can use."
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html#overview",
    "href": "blog/new-publication-in-pwpl/index.html#overview",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "Overview",
    "text": "Overview\nI originally started this paper because I noticed, by accident, that I was getting different results after moving some code around. I was in the middle of an analysis (of the very dataset I use in the paper) and had already gotten some results, but I had to change a few things in the processing and ended up moving a chunk of code up a little bit so that it happened before another chunks. Lo and behold, the results changed. Nothing about the underlying data changed, and nothing about the functions I ran changed—other than their order.\nSo this sent me down a bit of a rabbit hole. I identified seven processing steps that I did in my analysis pipeline. I then wrote some code that arranged those seven steps into all possible orderings, all 5,040 of them. I then took the exact same input dataset, and processed it 5,040 times, once for each permutation. With each resulting spreadsheet, I then calculated things like how shifted or overlapped people’s vowels were. Again, to be clear, the underlying data was identical, and the functions I ran were identical. The only thing that changed was the order that I ran them.\nWell, as you can read in the paper, the results were a little concerning. First, I got many unique values for each person. Not 5,040 unique values, because some pipeline result in identical outputs. But sometimes hundreds of unique values. Some of these were admittedly very similar, but others were quite drastic. In one case, I point out that if I use some pipelines, a particular individual may be interpreted as being a linguistic innovator since she had advanced measures of some of the shifts. But in other pipelines, she would be seen as linguistically conservative since those same measures returned less-advanced measures. Taking the entire group of 53 speakers collectively, some pipelines showed that about half had shifted vowels while other pipelines showed that nearly all of them did. So, just by changing the order that I run my code can have an impact on how my dataset is interpreted!"
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html#takeaways",
    "href": "blog/new-publication-in-pwpl/index.html#takeaways",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "Takeaways",
    "text": "Takeaways\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that this is a different order than what I suggest in my NWAV49 talk! The version shown here is what I currently stand by.\n\n\nSo, in addition to pointing out the problem, I do offer a potential solution by prescribing an order of operations. Here’s what that is:\n\nReclassify your data into allophones\nThis is important because subsequent steps really need to run by allophone rather than by phoneme. I’d even say that we should separate the data into allophones even for vowels we’re not interested in (like allophones of /u/ even if we’re only interested in front vowels) because it has implications for what data gets excluded which could affect the centering when you normalize.\nRemove “bad” data\nThis is when you remove outliers. Again, I argue that it should be done by allophone. I also recommend that stopwords and unstressed vowels be treated as their own separate category as well. But, I’m no married to that idea.\nNormalize\nRegardless of what normalization procedure you use, I think it is at this point in the pipeline that the procedure should happen. The normalization should include all good data (even if it’s not pertinent to the study) and not include any bad data.\nRemove “good” but otherwise uninteresting data.\nFinally, here is where you should subset your data to focus on just the stuff you’re interested. So, get rid of vowels or allophones you don’t want, stopwords, unstressed vowels, etc. Importantly though, it is only at this point in the pipeline that you should toss vowel trajectories! If you never extracted them in the first place, or just ignored those columns in FAVE’s output, then you’re excluding trajectory data as step 1 and that has an impact on your results!\n\nFor now, these steps are based more on logic and theory (and I appreciate the input from Rich Ross and Thomas Kettig for helping me out with that)! My publication in Linguistics Vanguard dives more into these steps and provides some more quantitative justification for them."
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html#my-soapbox",
    "href": "blog/new-publication-in-pwpl/index.html#my-soapbox",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "My Soapbox",
    "text": "My Soapbox\nFinally, towards the end, I do get a little preachy. First, I recommend that Order of Operations be explained in detail in methods sections so that we can evaluate them better. In fact, it was only after I submitted this paper that I found Brand, Hay, Clark, Watson, & Sóskuthy’s (2021) paper that does exactly what I want to see! That’s exactly the kind of transparency that we need. Of course, at this point, it’s hard to interpret what effect one order has upon the data as opposed to another order, though, again, my publication in Linguistics Vanguard tries to unpack that a little bit.\nI then say that quantitative linguists should be more mindful of how the data is being processed. I basically subtweet some methods that I’m not particularly fond of (Lobanov transformations, ANAE “benchmarks”) without calling them out specifically. I also subtweet a reviewer for a different paper who said to me that I shouldn’t use New Technique X and should instead use Old Technique Y for comparability with previous studies, even though recent research has shown that Old Technique Y is flawed and New Technique X is better. Anyway, so if those paragraphs sound cryptic, now you know some background.\nFinally, I encourage quantitatively-minded linguists to continue writing methods papers. Whether it be developing a new technique or comparing existing techniques against each other, all that stuff is good for the field.\nAnyway, I took the opportunity in this non-peer-reviewed methods paper to stand on my soapbox a little bit and talk about some of my views on the current state of quantitative linguistics. And I stand by what I wrote in the paper."
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html#conclusion",
    "href": "blog/new-publication-in-pwpl/index.html#conclusion",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s it! This was a fun one to write because it literally has nothing to do with language or how language works: it’s purely methods. In fact, it’s not even about one specific method. It’s about how we should run our R code! Very niche. But I hope it does some small part in improving quantitative linguistics papers.\nPS: In my notes I refer to “Order of Operations” as “OoO”. I’m 1000% okay with having OoO be a new abbreviation in people’s code and in methods sections now. Who wants to be the first? :)"
  },
  {
    "objectID": "blog/divar/index.html",
    "href": "blog/divar/index.html",
    "title": "DiVar",
    "section": "",
    "text": "I’m excited to announce I’ve been accepted to present at the first iteration of the Diversity and Variation in Language Conference (DiVar1), which will be held at Emory University in Atlanta February 10–11. I’m excited to hear that many of my colleagues at UGA have also been accepted, so it should be a fun day for us.\nIn a nutshell I’m showing that in word lists people pronounce pull and pole the same but Mary and merry different, but in a minimal pair task, pull and pole are separate while Mary and merry are the same. I spent the summer in Washington state, and I’m only looking at a little less than 3% of everything I recorded for this conference.\nI’ll post a summary of the presentation and the slides once the conference is over. Stay tuned."
  },
  {
    "objectID": "blog/lcuga4/index.html",
    "href": "blog/lcuga4/index.html",
    "title": "LCUGA4",
    "section": "",
    "text": "This weekend, I had the opportunity to present twice at the 4th Annual Linguistics Conference at UGA. One was planned and the other was a last-minute fill-in for someone who couldn’t make it. I was happy to do both.\nFriday’s presentation was called The linguistic effects of a changing timber industry: Language change in Cowlitz County, WA. Here, I talk about some of the sudden linguistic changes that I found in apparent time and suggest that they had to do with changes in the timber industry around that time. Because this was last-minute, it is basically a precursor to (and a slideshow version of) my NWAV46 poster that I’ll be giving in a few weeks. You can download the slides for this talk here\nSaturday’s presentation with Kyle Vanderniet was called Consonantal variation in Utah English: What el[t]se is happening[k]?. We talked about three variables that seem to be particularly salient in Utah English\n\nThe various pronunciation of words like mountain, button, or satin with the last syllables as [ʔn̩], [ʔɨn], or [tʰɨn].\nInsertion of [t] between /ls/ clusters, as in fal[t]se or el[t]se.\nRealizing word-final ing as [ɪŋk].\n\nYou can see our slides for this presentation here. Edit: we later presented additional findings from this research at ADS2018.]"
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html",
    "href": "blog/kohler-tapes-update/index.html",
    "title": "Kohler Tapes (Update)",
    "section": "",
    "text": "In February, I acquired a goldmine of data that I can use for linguistic analysis. Here’s an update on that project, now that I have some more solid numbers."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#cataloguing",
    "href": "blog/kohler-tapes-update/index.html#cataloguing",
    "title": "Kohler Tapes (Update)",
    "section": "Cataloguing",
    "text": "Cataloguing\nSoon after getting the tapes, I had to organize them in some way. Since I don’t really know what I’m doing, I reached out to a few people I know who have worked with collections comparable in size to ask for their advice. (I also had to ask some Gen Xers how to handle and store cassette tapes because, well, I’ve never done that before 🤷🏻‍♂️).\nSo, over the course of a couple months, I took the tapes home in batches of fifty or so and cataloged them. By this, I mean that I labeled each with a sequential identifier and took photos of all sides of the tape, its case (if it had one), and any other slips of paper it came with.Thanks, Charlie, for that tip!\nWhile I was there, I wrote down whatever information was written on the outside of the tape. Things like the student’s name, the interviewee’s name, the date of the interview, their age, and the relationship between the two people (grandparent, great-grandparent, etc.). It looks like most of the tapes were done between 1986 and 1999. The ones from the 80s weren’t documented as consistently and I think Mr. Kohler caught on to that and asked his students to do a better job because the ones in the 90s were much more well-documented.\nThere are a couple really fun gems for me as I went through the names. First, I went to Heber in 2018 to collect some audio data myself. I talked to several older people then. As it turns out, some of these tapes contain recorded interviews with some of the people I talked to! Again, it’s a small town, so not a complete surprise, but it’s still pretty cool. Also, my sister-in-law has family from Heber, and sure enough, the collection contains interviews with about 10 of her distant relatives (great-grand-uncle, etc.). I’m sure I’ll discover some other gems as I dig deeper into this collection."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#digitizing",
    "href": "blog/kohler-tapes-update/index.html#digitizing",
    "title": "Kohler Tapes (Update)",
    "section": "Digitizing",
    "text": "Digitizing\nThanks to a great tip from my colleague Chris Rogers, I found out that the Humanities Learning Resource Center in my building can digitize tapes. So I dropped everything and talked to them. Not only can they do it, but they said they do it for free! Wow!\nSo, I dropped off a box of tapes for them and two weeks later the files magically appeared in my Box drive! So I went and collected them, dropped off another several dozen and repeat the process over and over. It was a good morning when I woke up to the notification from Box saying that another 100 files were ready to download.\nI want to just pause and do a huge shout-out to the student employees who processed all this audio! They had to hear the whir of the digitizing machine going for 8–9 hours days for 86 straight work days. Not to mention get up and flip the tape over or insert a new tape every 30–60 minutes, trim the audio, and upload the file. As someone who can’t stand unnecessary white noise or interruptions, that sounds like awful work to me. I’m sure if I had had to do it myself, it’d take me three or four times as long to get it all done and I wouldn’t be a happy camper."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#file-structure",
    "href": "blog/kohler-tapes-update/index.html#file-structure",
    "title": "Kohler Tapes (Update)",
    "section": "File Structure",
    "text": "File Structure\nSomething that I haven’t yet figured out though is the best way to store all this. When I worked with the Linguistic Atlas Project, it was simple: one speaker per file. Most interviews were longer and were recorded on multiple reels, so a single speaker may be on as many as ten or so files, but it was otherwise pretty well-organized.\nThis collection though is a hot mess. Here’s a list of the kinds of things I’m working with:\n\nMany students did the cleanest route and interviewed one person and turned in one tape. Great.\nHowever, some students went overboard and turned in multiple (as many as five!) tapes for a single interview. I welcome more data, so that’s not too bad.\nTo complicate things, sometimes the same person was interviewed by different students on different years. One person was interviewed six different times!\nSometimes, if the first interview wasn’t long enough, a student would conduct another interview with a different person. So there are two interviews and two different people on a single tape.\nThe messier route was if a student interviewed two grandparents at the same time. So the husband and wife would alternate back and forth. So one interview, but two people. This will be the trickiest to process.\nAnd, of course, there are a few cases where multiple joint interviews are tagged on to each other, and this collection of interviews was spread across multiple tapes. Ugh.\n\nCurrently, my file structure is one-folder-per-tape, but as I get my hands dirty, I’m realizing I need to switch to a one-folder-per-person structure.\nAnd of course, all this necessarily will need to come with multiple spreadsheets and some minor databasing to keep track of it all. Currently, I’ve got a spreadsheet for tapes, a spreadsheet for sides of tapes, and a spreadsheet for individuals. It’s not as clean as a single spreadsheet but it works."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#some-numbers",
    "href": "blog/kohler-tapes-update/index.html#some-numbers",
    "title": "Kohler Tapes (Update)",
    "section": "Some numbers",
    "text": "Some numbers\nSo now that they’ve all been cataloged and digitized, I can give some more concrete numbers than I did in my previous blog post.\n\nNumber of tapes: 751\nMy estimate before was 600–700, so it’s a little more than I expected (which is great!)\n\n\nHours of audio: 631\nI know that some of that is music (some students taped over mix tapes) so the number may go down as I listen to it all. I anticipated “only” 467 hours of audio at first, so this is 33% more than what I originally thought.\n▼ I tried to estimate how much I’d end up with before they were all done, and it looks like by around 200 tapes or so I had a pretty good idea. The blue line is the predicted number and the black lines are some error. The pink line shows the true total.\n\n\n\nMinutes per tape: 51.7 (on average).\nMy estimate in February was 40 minutes, so not only did I end up with more tapes than I expected, but they were 30% longer than I expected. I think the assignment was to have 30 minutes, but I didn’t expect so many students to go that much longer.\n▼ Here’s the distribution of how many minutes of audio were on each tape (both sides). I’m pretty sure the peaks at 30, 65, and 95 or so reflect how much audio a cassette tape can hold.\n\n\n\nDigitizing pace: 8.46 tapes a day (on average)\nSince digitization happens in real-time, that means these students had this going for like nine hours a day. It took them 86 work days to do it all.\n▼ Based on the creation date of the files, here’s how much work they did per day. They worked Monday through Saturday every week. You can see that after the semester ended in April it was slightly less consistent. Sometimes I wasn’t on campus the day they finished a box so they had to wait a day or two to get the next batch.\n\n\n\nNumber of people: 806\nThat’s just interviewees. If you add the 667 students, that’s 1473 total people. My guess is that that number will go down a small amount as I clean up the metadata. I’ve already had to change “Mr. Norman” and “Mrs. Norman” to their full names once I listened to them. Correcting any typos may change the number too if people were interviewed multiple times. I mentioned this in my last blog post, but Heber only had a few thousand people living in it at the time, so this is decent proportion of the total population of Heber. And if you just focus on the age group that this collection represents, that proportion goes up quite a bit!\nThere are a fair number of common family names like Jones, Smith, Johnson, McDonald, Thompson, Anderson, Davis of course. But there are also a lot of names like Giles, Bethers, Jensen, Web, Allred, Broadhead, Casper, Duke, Probst, and Young, which I presume are local families.\nI will say right now that some of the students’ speech has some pretty distinctive linguistic features. These would be people born between about 1972 and 1987, or Gen Xers and early Millennials,  who grew up in Heber. Unfortunately, I won’t have enough data from them to do much of an analysis.Geriatric Millennials?\n\n\nInterview years: 1986–2001\nThat’s at least based on the 418 tapes where the interview date was written on the outside. Almost all were in April or May of each year. I suppose if I really wanted to I could track down some old yearbooks and find when the students were in 8th grade and get an exact year. I may find this information in the audio, but I haven’t listened to all of them yet. I may also be able to deduce it from people’s ages and birth years if they mention them.\n▼ Notice there are many more in the 90s. I think it’s a sampling bias though. My guess is that later on in the project, the students received more explicit instructions to write that information than the students in the 1980s did.\n\n\n\nBirth Years: 1905–1953\nAgain, that’s at least based on the 28 tapes I’ve listened to. This information was not written on the outside of the tape, so I can only get it in the audio itself. If they don’t say it explicitly, I can usually get enough information about the person to look up census records and get a confirmed date.\n▼ Here’s the spread of confirmed birth years. They’re color-coded by generation cohort in case that’s meaningful to you. I estimated everyone would be born between 1900 and 1940, so that was pretty close being right.\n\n\n\nPlaces of birth: mostly Wasatch County\nOnce again, that’s based on the 28 tapes so far. Wasatch County includes Heber, Charleston, Daniel, and Walsburg. Keep in mind these interviews took place in Heber, the county seat of Wasatch County.\n▼ Several other places in Utah are represented so far too. 14 unique cities in just 28 tapes. Only one person so far was born outside of Utah."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#looking-ahead",
    "href": "blog/kohler-tapes-update/index.html#looking-ahead",
    "title": "Kohler Tapes (Update)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nI won’t rehash what I wrote in my earlier blog post, but now that I have some more solid numbers, I can have better estimate for what I need to get this project done.\nWith 631 hours of audio and a rough estimate of 10 hours of work for every hour of audio, that’s 6,310 hours of manual labor needed to transcribe this all. Again, that’s about 35% more than I anticipated in February. At $15 per hour of work, that’s $94,650 in student wages.\nIf I can get some more stellar RAs like I had this semester, who worked 10 hours a week, that’s 631 student-weeks. At 15 weeks a semester, that’s 42 student-semesters of work. If I want this done in two years, that’ll take an average of 14 RAs each semester, including summers. Anyone who has worked with transcribers knows that retention is not great, so I’ll most certainly need more than 14 student helpers and they’ll most certainly not last two years.\nThis doesn’t even include my pipe dream of getting some grad student workers to form a core group of researchers. They’d help with supervising and training the transcribers, quality control, other aspects of the data processing (force-aligning, formant-extraction, file management), and analysis.\nAm I already looking at external grants? Yes, yes I am. And am I already looking at different types of transcription software, methods, speech-to-text programs, and other things to speed this up and/or make it less costly? Yes, yes I am."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#immediate-plans",
    "href": "blog/kohler-tapes-update/index.html#immediate-plans",
    "title": "Kohler Tapes (Update)",
    "section": "Immediate Plans",
    "text": "Immediate Plans\nMy first task though is to get as many of the gaps in my metadata spreadsheet filled as quickly as possible. I only know the birth years and places for 28 of the 806 people. I’d like to get a much more complete picture of what this collection is like before I start prioritizing which tapes to transcribe first. Sometimes this information is near the start of the interview but sometimes it’s not. Not quite sure how to get that information without just listening to all of them.\nFortunately, I got a grant from the Redd Center for Western Studies to go towards this project. It’s about the right amount needed to process enough data for a preliminary linguistic analysis. So I hope to get the ball rolling on some transcriptions and metadata extraction. Stay tuned for the first results at a conference near you!"
  },
  {
    "objectID": "blog/nwav51/index.html",
    "href": "blog/nwav51/index.html",
    "title": "NWAV51",
    "section": "",
    "text": "This weekend, some students that I advise in different capacities presented their research at NWAV51 in New York! I am co-author on them, but it’s about 95% their work. For reasons I won’t get into here, I was not able to attend the conference myself, so these three students presented by themselves!"
  },
  {
    "objectID": "blog/nwav51/index.html#thursdays-presentation-on-ukrainian-language-ideolgies",
    "href": "blog/nwav51/index.html#thursdays-presentation-on-ukrainian-language-ideolgies",
    "title": "NWAV51",
    "section": "Thursday’s Presentation on Ukrainian Language Ideolgies",
    "text": "Thursday’s Presentation on Ukrainian Language Ideolgies\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nKateryna Kravchenko presented her research, “An analysis of Ukrainians’ language attitudes and ideology: A conflict-catalyzed identity shift.” Katya has been tracking Ukrainian language ideologies since the Russian invasion of Ukraine in February 2022 and has noticed that many Ukrainians are vowing to never use Russian again, even though it is their first language. It appears to start with social media and then move to other domains after that. She has conducted interviews with Ukrainians who have had to flee the country since February 2022 and has some really fascinating first-hand insights into what we’re calling a “conflict-catalyzed identity shift.”"
  },
  {
    "objectID": "blog/nwav51/index.html#fridays-presentation-on-southern-utah-english",
    "href": "blog/nwav51/index.html#fridays-presentation-on-southern-utah-english",
    "title": "NWAV51",
    "section": "Friday’s Presentation on “Southern” Utah English",
    "text": "Friday’s Presentation on “Southern” Utah English\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nChad Huckvale has been working on perceptual dialectology research in Utah, and has been focused on this idea that people describe parts of Utah as sounding “Southern,” even though Utah English and Southern American English have little in common. In his presentation, “Perceptions of ‘Southern’ Utah English,” he presents results from two experiments on this topic. First, he did a class draw-a-map task and found that, unsurprisingly, people labeled the urban area along the Wasatch Front as “urban” (and related words) and everywhere else as “rural” (and related words). In his second experiment, people listened to speech samples and were asked where in Utah they thougt they were from. (The twist is that only a few of the speakers were actually from Utah!) People placed speakers with features of Southern American English in the rural parts of the state. There’s a lot more in Chad’s MA thesis, but this was"
  },
  {
    "objectID": "blog/nwav51/index.html#fridays-presentation-on-tourism-and-its-effect-on-language",
    "href": "blog/nwav51/index.html#fridays-presentation-on-tourism-and-its-effect-on-language",
    "title": "NWAV51",
    "section": "Friday’s presentation on tourism and its effect on language",
    "text": "Friday’s presentation on tourism and its effect on language\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nFinally, Zoe Elredge just graduated with her BA, but she’s been working hard on perceptions of Park City English. In her presentation, “Exploring the Effects of Cross-Cultural Variation and Tourism in Utah English,” she presents just some of the results of a very rich data collection she has gathered. The results are messy, but it seems like Parkite English is perceived as less professional and less laidback. That seems contradictory, but it kinda fits the stereotype of being a “Mountain Surfer Dudes” as one participant put it: superficial and posh. With Park City being a ritsy, touristy area, perhaps people are less willing to invest in friendships. Again, the results are messy, and Zoe is still digging to see what it can say and I look forward to see what she finds."
  },
  {
    "objectID": "blog/laboratory-research/index.html",
    "href": "blog/laboratory-research/index.html",
    "title": "Laboratory Research",
    "section": "",
    "text": "Recently, I’ve presented on words like pool, pull, and pole and how the difference between them can be really hard to describe, both by me and the non-specialist alike. Based on my findings in Washington, I decided I wanted to dig a little deeper into what these words are like, so I started a study that is less sociolinguistic and more laboratory phonology-based, which is a little unusual for me.\nBroadly, I want to look at the phonetics of English vowels before coda laterals. So, after making a list of lots and lots of possible words, cutting them down based on frequency and other factors, I’ve got a decent list of targeted words.\n\n\nList of all ~401 monosyllabic English words with a coda lateral, organized by vowel and syllable structure? Check. My favorite? Squelched.\n\n— Joey Stanley (@joey_stan) May 7, 2017\n\n\nI got IRB approval just a little too late into the semester to recruit people and offer them extra credit in courses, so I had to wait a few weeks to get started. Now that Maymester has started, I’ve approached some professors and asked them to offer participation in my research as an extra credit opportunity. I even hand out little business cards after I’ve done my pitch to the class, so they have my contact information—an idea that proved very effective for me in Washington:\n\n\n\nRecruitment business card\n\n\nSo I’m now meeting with students in the linguistics laboratory that we have here at UGA. It’s unfortunately under-utilized but nonetheless very good. Inside the already very quiet recording studio is a tiny booth where the best recordings can be made. I have participants reading a bunch of carefully selected sentences that target key sounds and then taking a quick follow-up survey. It amounts to about 30–40 minutes of speech from each person, which is kind of a lot.\nI don’t have a specific goal for how many people I want to get, but I should have 20 by the end of the month and potentially up to 50 by the end of the summer. My only limitation is how much time I can put into this. I’ll do some preliminary analyses on those and see if I need to recalibrate the sentences or maybe collect more data. This will probably be an ongoing project for a while: the IRB and consent forms are purposely pretty open-ended to allow me to modify things where needed without much hassle.\nAnyway, it’s been fun being in the lab, and I’m excited to analyze really clean audio for a change."
  },
  {
    "objectID": "blog/nwav50/index.html",
    "href": "blog/nwav50/index.html",
    "title": "NWAV50",
    "section": "",
    "text": "Today I gave a talk that Betsy Sneller and I have been working on called “How Sample Size Impacts Pillai Scores – and What Sociophoneticians Should Do About It” at the 50th New Ways of Analyzing Variation conference in San Jose! This is an updateto what we presented at ASA2021.\nWe have three things you can download.\n\nFirst is the actual powerpoint file. In the notes of each slide though you can see the actual script I read, so you can read every word that was said during the talk.\nNext, if that’s too much for you, you can download just a PDF of the talk. In case you want this ligherweight version of the slides.\nFinally, here is the current manuscript that is under review with the Journal of the Acoustical Society of America. The final product will likely change somewhat, but most of the information is there. [Edit (December 14, 2022): Here is the accepted version.]\n\nIf you need to calculate Pillai scores in R, I’ve got a two-part tutorial for you (here and here). I also did a blog post (here) about how Pillai scores don’t seem to change after normalization."
  },
  {
    "objectID": "blog/prevelar-raising-american-speech/index.html",
    "href": "blog/prevelar-raising-american-speech/index.html",
    "title": "Prevelar raising paper published in American Speech",
    "section": "",
    "text": "I’m thrilled to announce that a paper of mine has been published in American Speech! It’s called “Regional patterns in prevelar raising.” This one has been in the works for quite some time, so it’s very exciting to see it finally published."
  },
  {
    "objectID": "blog/prevelar-raising-american-speech/index.html#summary",
    "href": "blog/prevelar-raising-american-speech/index.html#summary",
    "title": "Prevelar raising paper published in American Speech",
    "section": "Summary",
    "text": "Summary\nSome people pronounce words like bag, flag, and dragon with a raised vowel, so that bag rhymes with vague. There are also some people who pronounce words like beg, peg, and leggings with that same raised vowel. People have studied this in a few places, most notably Wisconsin and the Pacific Northwest, but not really much anywhere else. I’m from St. Louis and I’ve got beg-raising pretty strong. So, where else do you get prevelar raising? Whenever I have a question like that, I just ask my 7000 closest friends what they do, as one does :)\nI set up a survey that asks people to indicate how they pronounce something like 70 prevelar words. I distributed it on Reddit to get good geographic coverage. Using fancy GIS skills (thanks, Jack Grieve for helping me with that!), I was able to produce some pretty cool maps showing where you get raising. As it turns out, the regions where people reported bag-raising lines up with where bag-raising has been reported in the ANAE based on acoustic data, which is good because it means the data I have is at least somewhat reliable. For beg-raising though, I found that it’s much more widespread. Pretty much everywhere except the South and Michigan you’ll find people who have it, and you’re more likely to find it in places like Ohio, Indiana, the Upper Rockies, and Canadian Praries.\nAnyway, I had a lot of fun with this one. There’s less work on prevelar raising at the moment compared to like five years ago when I first had the idea for the project. But that’s okay. It was fun to learn how to make cool maps."
  },
  {
    "objectID": "blog/prevelar-raising-american-speech/index.html#timeline",
    "href": "blog/prevelar-raising-american-speech/index.html#timeline",
    "title": "Prevelar raising paper published in American Speech",
    "section": "Timeline",
    "text": "Timeline\nI take careful notes of when my projects hit major milestones. I figued I’d share those with you here:\n\nNovember 2017: I had the idea for the study. I was teaching a phonetics and phonology class and noticed that I have beg-raising in all but about six words, and that most of my exceptions have a sonorant following the /ɡ/: integrity, interregnum, etc. On November 10, I sent out a pilot study to my friends on Facebook. I got about 350 people to take it, which was pretty cool. In my notes I said I was hesitant to do a blog post about it because “these are legit results that I could publish.”\nApril 2018: I presented this preliminary data at an informal “TinyTalks” event to the linguistics grad students at UGA. I started making the full version of the survey and began filling out IRB forms. Approval was soon granted.\nApril–May 2018: I distributed the survey on Reddit and collected 7000 responses in two weeks. I made a plan to submit an abstract about phonological factors to NWAV and submit an abstract on regional factors to ADS.\nSummer 2018. I submitted both abstracts as planned. I’m amazed at myself that the precise plans I made in the early days of the project all happened to a tee.\nOctober 2018–January 2019: I presented at NWAV47 and ADS2019 as planned. I wrote up preliminary findings and submitted a manuscript to our UGA Working Papers in Linguistics. I also wrote up the results in layperson’s terms in a blog post to send to my participants.\nSeptember 13, 2019: Submitted the manuscript to American Speech. Not quite sure why it took me so long to finish the darn thing, but it might because it was my first Real Paper™. I was motivated to get this done so that I could include it in job applications.\nDecember 4, 2019: Got an R&R from American Speech.\n2020: I put off working on this for a full year. I had my dissertation defense, job interviews, a campus visit, dissertation revisions, an contract job with BYU during covid lockdown, graduation, moving to Utah, starting a new job at BYU in June, and teaching three new preps over the next six months. In November 2020 I set a goal to get this returned by the end of the year.\nDecember 29, 2020: Resubmitted to American Speech.\nApril 8, 2021: Accept with revisions! Yay!\nJune 8, 2021: Returned the manuscript. I was told by American Speech that there’s a bit of a backlog, so it’d come out in 2022.\nAugust 26, 2022: Got proofs from American Speech.\nOctober 19, 2022: Got the physical copy in the mail!\nNovember 2, 2022: I noticed it was published online!\n\nSo from the start of the idea until publication was just about five years. I don’t really work on prevelar raising anymore, and this was a side project I wanted to get done quickly in grad school. It’s just interesting that a project I started so long ago and presented first as a grad student was used as a writing sample for job applications, and now it’ll go in my tenure portfolio! I’ve had a few papers that I started after this one come out already, but I guess I’m particularly excited about this because it was my first manuscript submitted to a peer-reviewed journal."
  },
  {
    "objectID": "blog/prevelar-raising-american-speech/index.html#misc-topics",
    "href": "blog/prevelar-raising-american-speech/index.html#misc-topics",
    "title": "Prevelar raising paper published in American Speech",
    "section": "Misc topics",
    "text": "Misc topics\n\nUnofficial erratum\nFigure 9 was a very tricky one to do and I created probably dozens of versions of the plot trying to find the right one. Online, you’ll see the one I settled on, which does a pretty cool thing with color where I map two different variables to color.\n\nThis is basically impossible to print in black and white though, so I was asked to make a print-friendly one. The compromise that either I or the editors came up with was to do diagonal slashes in the regions.\n\nUnfortunately, it looks like the Figure 9 in my physical copy is just a black-and-white version of the online map, rather than the one specifically for print. This is especially confusing because the print version specifically mentions the slashes even though they’re not there. Oh well. More people will read the online version anyway.\n[Update: Pretty soon after writing this, I got more physical copies in the mail. I suspected it was to correct this Figure 9 and sure enough it was. So, if you got an extra copy of that issue of American Speech, you can blame my darn complicated plot. I hope it didn’t cost the ADS too much money…]\n\n\nBoustrophedonic plots\nA while ago, I tweeted what would become one of my most viewed tweets:\n\n\nOkay, I'm trying something out. I have this histogram with one very tall bar and many shorter ones. So to save space, I made that tall bar follow the edge of the plotting area boustrophedonically—my favorite word!—but I'm not sure if I like it. Thoughts? #dataviz pic.twitter.com/9b9QYqQJLg\n\n— Joey Stanley (@joey_stan) December 17, 2020\n\n\nI even wrote a blog post about the suggestions people gave about alternative plots.\nI’ll let you in on a little secret: I was working on this paper when I came up with the idea for that plot! The data shows the distribution of beg-raising across my entire sample. I wanted to show that there were lots of people without it, but also show that there is lots of variation among those who do have it.\nUnfortunately, the plot didn’t make it into the manuscript. I just couldn’t do it. Figure 10 is my compromise:\n\nSo, not as fun, but prehaps more normal. Do I regret not including the boustrophedonic plot? Yeah, maybe a little."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html",
    "href": "blog/testing-vot-durations/index.html",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "",
    "text": "So I’m teaching phonetics and phonology this semester and we’re using Ladefoged & Johnson’s A Course in Phonetics textbook. As I was preparing to teach about stops, I thought it might be a good idea as a homework assignment for students to gather their own data to see if some of these ideas panned out. Here’s my quick study."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html#hypotheses",
    "href": "blog/testing-vot-durations/index.html#hypotheses",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "Hypotheses",
    "text": "Hypotheses\nThe four hypotheses I wanted to test come from Chapter 3 from my 6th edition of A Course in Phonetics:\n\nWord-initially, /p, t, k/ have longer aspiration than /b, d, g/.\nAfter onset /s/, /p, t, k/ have about as much aspiration as word-initial /b, d, g/.\nWord-finally, voiced obstruents have an overall longer duration (closure + burst + aspiration) than voiceless obstruents.\nVowels preceding voiced obstruents are longer than those preceded by voiceless obstruents."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html#methods",
    "href": "blog/testing-vot-durations/index.html#methods",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "Methods",
    "text": "Methods\nEach student was asked to record a friend reading the following words: tack, soap, days, pad, steep, sit, code, tab, bees, scope, dice, goes, bus, seep, cab, spit, gas, peg. I chose these words to maximize onset and coda obstruents in as few words as possible. Vowel quality is assumed to have no effect.\nFor stops, student measured durations of aspiration and closure; for fricatives it was duration of the fricative itself. This was done in Praat. They have worked with Praat once before in this course, and were taught how to identify boundaries for these, but were otherwise relatively untrained linguistics undergraduates. I provided them with a template spreadsheet to fill out.\nI ended up with measurements from 432 words: 18 unique words each from 24 students. I then combined the spreadsheets and wrote up the R code."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html#results",
    "href": "blog/testing-vot-durations/index.html#results",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "Results",
    "text": "Results\n\nWord-initial stop aspiration\nThe first hypothesis is one that is commonly taught in intro to linguistics courses: word-initial voiceless stops have aspiration and word-initial voiced stops have little, if any. In my data, this turned out to be the case based on the words pad, peg, tab, tack, cab, code, bees, bus, days, dice, gas, and goes.\n\nA mixed-effects regression model that predicts this aspiration with the underlying voicing of the stop as a fixed effect and student and word as random effects suggests that this difference is significant.\nsummary(lmer(aspiration ~ underlying_voicing + (1|student) + (1|word), data=wi_stops))\n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n student  (Intercept) 1.039e-03 0.03224 \n word     (Intercept) 9.550e-06 0.00309 \n Residual             1.084e-03 0.03293 \nNumber of obs: 285, groups:  student, 24; word, 12\n\nFixed effects:\n                            Estimate Std. Error t value\n(Intercept)                 0.047309   0.007241   6.534\nunderlying_voicingvoiceless 0.037331   0.004291   8.699\nConclusion: Yep. Based on this data, the model predicts that voiced stops get around 47ms of aspiration while voiceless stops get about 85ms. Cool.\n\n\nStops following /s/\nWe learn that voiceless stops following /s/ in the same syllable are not aspirated in English. In our sample, this also proved to be correct based on the same words as above with the addition of spit, steep, and scope.\n\nI ran a mixed-effects regression model like the one described above but with position (=environment) as the main effect and voiced word-initial stops as the reference level.\nsummary(lmer(aspiration ~ position + (1|student) + (1|word), data=onset_stops))\n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n student  (Intercept) 0.001153 0.03395 \n word     (Intercept) 0.000000 0.00000 \n Residual             0.001150 0.03391 \nNumber of obs: 355, groups:  student, 24; word, 15\n\nFixed effects:\n                                Estimate Std. Error t value\n(Intercept)                     0.047309   0.007485   6.321\npositionvoiceless word-initial  0.037395   0.004019   9.304\npositionfollowing /s/          -0.007779   0.004948  -1.572\nWe get the same coefficients for word-initial stops. Only this time, we can see how stops followed by /s/ fit in. The model shows that the duration of aspiration was not significantly different from word-initial voiced stops. There’s some indication that the stops following /s/ have even less aspiration, but this didn’t reach significance.\n\n\nDuration of word-final stops\nWhat I didn’t know before preparing for this class was that the overall duration of word final stops (closure + burst + aspiration) is longer for voiceless obstruents than it is for voiced obstruents. Based on all 18 words (scope, seep, soap, steep, sit, spit, tack, cab, tab, code, pad, peg, bus, dice, gas, bees, days, goes), this was true.\n\nThe difference is small but significant: in a mixed-effects regression model that predicts duration with voicing and manner of articulation as fixed effects and student and word as random effects, the difference reached statistical significance.\nsummary(lmer(duration ~ underlying_voicing + manner + (1|student) + (1|word), data=wf))\n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n student  (Intercept) 3.002e-03 0.054790\n word     (Intercept) 3.745e-06 0.001935\n Residual             7.550e-03 0.086893\nNumber of obs: 427, groups:  student, 24; word, 18\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)               0.194404   0.012836  15.145\nunderlying_voicingvoiced -0.035321   0.008540  -4.136\nmannerfricative           0.096638   0.008976  10.766\n\nThis model shows that the voiceless stops were about 194ms long, and voiced stops were 159ms (35ms shorter). Furthermore, and I didn’t expect this until I saw the plot, fricatives were generally about 97ms longer than stops. I don’t do a lot of nitty-gritty phonetics work like this too often, especially on consonants, so this was news to me.\n\n\nVowel length\nFinally, it is pretty well known that vowels before voiced obstruents are said to be longer than vowels before voiceless obstruents. As expected, the data showed this to be the case.\n\nOne last regression model, similar to what was done previously, showed not only that this voicing difference is significant but that the manner of articulation mattered as well.\nsummary(lmer(vowel ~ underlying_voicing + manner + (1|student) + (1|word), data=wf))\n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n student  (Intercept) 0.001937 0.04401 \n word     (Intercept) 0.001114 0.03337 \n Residual             0.002274 0.04769 \nNumber of obs: 431, groups:  student, 24; word, 18\n\nFixed effects:\n                         Estimate Std. Error t value\n(Intercept)               0.18905    0.01513  12.495\nunderlying_voicingvoiced  0.09595    0.01654   5.799\nmannerfricative           0.05349    0.01744   3.067\nHere, we see that vowels followed by voiceless stops are predicted to have a duration of 189ms. If the following segment is voiced, the model predicts it to actually be 285ms (an increase of 95ms). What stands out is that vowels followed by /g/ were quite a bit shorter. Due to an oversight in my data, there was just one /g/-final word, peg, and it was listed last on the stimulus. No doubt this had an effect. In fact, a more rigorous study that would include more tokens and randomization might find the difference to be even greater, assuming /g/ falls more in like with /b/ and /d/.\nI learned something new here as well: fricatives lengthen the vowel even more. The model predicts that vowels preceding fricatives will be 53ms longer than stops with the same voicing. I didn’t know that."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html#conclusion",
    "href": "blog/testing-vot-durations/index.html#conclusion",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "Conclusion",
    "text": "Conclusion\nThis study was short, unsystematic, and full of methodological issues. Despite its flaws, it shows evidence to support what Ladefoged and Johnson say in A Course in Phonetics. I also learned a few things: in addition to themselves being longer than stops, fricatives cause their preceding vowels to lengthen as well.\nSpecial thanks goes to my Fall 2017 LING 3060 class at the University of Georgia, who bothered their friends with this silly assignment and painstakingly measured durations in software they barely know how to use."
  },
  {
    "objectID": "blog/brother-joseph/index.html",
    "href": "blog/brother-joseph/index.html",
    "title": "Brother Joseph",
    "section": "",
    "text": "I had the fun opportunity to be a guest in a podcast today! Faith Promoting Rumors is a new podcast that my brother and dad started that explores Mormon myths and culture. Having published on an interesting linguistic quirk about Mormon culture—the alternation between calling someone either as “Brother Jones” or as “Bob”—I was asked to talk about my research and about this convention in Mormon culture generally."
  },
  {
    "objectID": "blog/brother-joseph/index.html#background",
    "href": "blog/brother-joseph/index.html#background",
    "title": "Brother Joseph",
    "section": "Background",
    "text": "Background\nThere is a robust practice of using titles among Mormons. Kids and teenagers are expected to refer to all adults using the appropriate title (usually Brother or Sister, though in some cases Elder, Bishop, or President—see below), plus their last name. Adults reciprocate by calling minors by their first name, establishing a clear superiority between kids and adults, though this can be flouted for comedic, sarcastic, or reverential effects.\nBut between adults the rules are less straightforward. Sometimes adults use first name for each other and other times they use titles. Familiarity is the strongest factor but age and status within the congregation play a role as well. I had a hunch that things like situation, audience, family make up, and place in the social network had to do with it too. This is essentially the same as how some languages have their T-V distinction in pronouns. This is a classic sociolinguistic variable since it doesn’t appear to be a conscious decision by speakers and it’s an extremely common linguistic feature.\nSo what started off as a mild curiosity during Sunday meetings ended up being a term paper in both my sociolinguistics and social network analysis courses, two conference presentations, a spot in the 2016 Penn Working Papers in Linguistics, and my first qualifying paper—I got a lot of mileage out of that study. I was just getting into quantitative methods, so it’s a little statistics-heavy, but I did find some interesting results."
  },
  {
    "objectID": "blog/brother-joseph/index.html#summary-of-the-study",
    "href": "blog/brother-joseph/index.html#summary-of-the-study",
    "title": "Brother Joseph",
    "section": "Summary of the Study",
    "text": "Summary of the Study\nEssentially, what I did to answer this question was give a survey out to members of my own congregation and ask them to indicate what they would call other members of the congregation in four different situations. I also asked them to tell me how well they knew each person.\nAs I mention in the podcast, some of the main findings are pretty intuitive. The better someone knows another person, the more likely they are to use their first name. There was more first name among people of the same sex, especially women. Holding all other variables constant, more peripheral members of the social network of the congregation generally use more titles and get called by titles more than the core members. There were some differences between the sexes and the situation: men use more titles for present company while women use more first name in face-to-face situations.\nOne thing I tested was whether southerners use more titles. It’s a pretty common stereotype that southerners are more polite and call people “Miss Betty” more often. It turns out this carries over into Mormon circles as well: southerners generally used titles slightly more than northerners or Utahns.\nSurprisingly, age wasn’t a factor. I thought there would be a clear effect of increased usage of titles for older people, but this didn’t pan out. What did appear to be the case was that people roughly within a couple years of each other use more titles for each other, but this was more an effect of familiarity than anything else: people are friends with others their age so they use fewer titles.\nOne of the more interesting findings was that people who have children were called by their titles more than those that didn’t. I’ve heard anecdotes where unmarried people get titles far less often than married people their same age, and it seems like having kids moves a person up another step in the “adult” category. One of my conclusions was that it seems like a Mormon is truly considered an adult until they are married and have children. Interestingly, the number of kids didn’t matter, just whether someone had kids. This is explainable by the family-centered religion and culture that Mormons are a part of, and it seems to made manifest in how people address each other—at least in my Georgia congregation."
  },
  {
    "objectID": "blog/brother-joseph/index.html#titles-first-name",
    "href": "blog/brother-joseph/index.html#titles-first-name",
    "title": "Brother Joseph",
    "section": "Titles + First Name?",
    "text": "Titles + First Name?\nThe title of the episode, “Brother Joseph”, alludes to the practice that early church leaders had in calling people by a title and their first name. I don’t know exactly when the change from first to last name happened, but it appears to be sometime in the mid-to-late 1800s. It also might be that certain individuals had this special use of the title: Brother Brigham [Young], Brother Joseph [Smith], Brother John [Taylor], Brother George [Cannon], and Sister Eliza [Snow] were some of the top hits in the mid 1800s. Though Brother [John] Taylor and Brother [George] Cannon were also common in the corpus I looked through. There are a lot of unknowns about forms of address in the early days of Mormonism, but we try to look into it a little bit."
  },
  {
    "objectID": "blog/brother-joseph/index.html#other-titles",
    "href": "blog/brother-joseph/index.html#other-titles",
    "title": "Brother Joseph",
    "section": "Other Titles",
    "text": "Other Titles\nAs we mention in the podcast, there are other titles too that you might hear occasionally. Elder is reserved for male full-time missionaries, whether they be the young guys you might see on the streets or for men in global leadership positions. Women who serve missions retain their generic title of Sister. What’s interesting about the women though is that in some languages this is a unique title: in Portuguese for example the title is Síster rather than the generic Irmã (‘sister’) that other women have.\nPresident is for leaders of specifically organized groups of men. This can apply to the president of the church, the leader of the greater local area (what we call “stakes”), or the group of 14–15-year old boys. This is a case when calling a 15-year-old boy “President Jones” is attested in certain circumstances. This title can also be used for the president’s assistants or “counselors”, though this applies more to the larger groups and is much less common at the local level.\nBishop is for the presiding authority of a congregation. This title, as well as those for full-time missionaries, have a unique position syntactically: they can stand on their own. In other words, it’s perfectly acceptable to approach to missionaries and say “Hey, Sisters!” or to approach a bishop and say “Hey, Bishop” instead of “Hey, Bishop Jones.” President can be used sometimes in this way though it’s less common."
  },
  {
    "objectID": "blog/brother-joseph/index.html#conclusions",
    "href": "blog/brother-joseph/index.html#conclusions",
    "title": "Brother Joseph",
    "section": "Conclusions",
    "text": "Conclusions\nThis is an interesting part of Mormonism, and in the podcast we discuss some of the cultural implications of it. Linguistically though I still think there’s a lot more to be said and I’d be curious to see other research on this."
  },
  {
    "objectID": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html",
    "href": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html",
    "title": "Extending Wells’ Lexical Set to Prelateral Vowels",
    "section": "",
    "text": "At some point in the past few years, I’ve analyzed pretty much every English vowel before laterals. They’re pretty cool because they’re understudied, they’re somewhat infrequent, and they’re involved in a lot of different mergers in different parts of the country. When referring to these prelateral vowels, several labels have been used in the past, but none do the job quite right. So, I think prelaterals should get a standardized set of Wells-style labels, like prerhotics have. The problem is figuring out what they should be. In this post, I explain why existing labels aren’t great and then propose a complete set of new labels for prelateral vowels.\nTo cut to the chase, this table—particularly the column labeled “Prelateral Keyword”—shows the labels I’ve come up with:\nYes, this is probably another case of competing standards, but it illustrates what I think lexical sets for allophones should look like.\nFor additional background how and why I came up with these labels, read on."
  },
  {
    "objectID": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#wells-lexical-sets",
    "href": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#wells-lexical-sets",
    "title": "Extending Wells’ Lexical Set to Prelateral Vowels",
    "section": "Wells Lexical Sets",
    "text": "Wells Lexical Sets\nAs a small bit of background, John C. Wells. 1982. Accents of English.John C. Wells came up with standardized labels for sets of words that historically share the same vowel. The labels are also used to refer to the vowel itself. So something like fleece may refer to words like creep, speak, leave, feel, key, and people but also to the phoneme /i/. This is useful in cross-dialect comparisons when the actual phonetic quality of that vowel may systematically differ in that set of words.\nThe keywords themselves are not just arbitrary words within that lexical set, but rather they were carefully selected:\n\nThe keywords have been chosen in such a way that clarity is maximized: whatever accent of English they are spoken in, they can hardly be mistaken for other words. Although fleece is not the commonest of words, it cannot be mistaken for a word with some other vowel; whereas beat, say, if we had chosen it instead, would have been subject to the drawback that one man’s pronunciation of beat may sound like another’s pronunciation of bait or bit. (Wells 1982:123)\n\nPresumably then, a good set of keywords for prelateral vowels should be deliberately chosen to follow this same principle of maximal clarity.\nTo be clear, what I am proposing is a set of keywords that refer to the prelateral allophones of each English vowel. That is, a label like zeal would refer to /i/ when followed by an /l/, as in the words feel, peel, and deal. As explained in my previous post on non-canonical allophonic extensions to Wells’ lexical sets, zeal would be a subset of the umbrella label fleece, which refers to all tokens of /i/ regardless of phonetic environment."
  },
  {
    "objectID": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#existing-prelateral-labels",
    "href": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#existing-prelateral-labels",
    "title": "Extending Wells’ Lexical Set to Prelateral Vowels",
    "section": "Existing Prelateral Labels",
    "text": "Existing Prelateral Labels\nOf the research I’ve seen on prelateral vowels, if labels are given to them, they usually take the form of a minimal pair or set (or as close to one as possible). Some of the more common labels include pool, pull, and pole for back vowels, or feel, fill, fail, and fell for front vowels. I’ve used them myself, actually.See, for example, Bowie (2000), Thomas (2004), and Tillery & Bailey (2004).\nTo my knowledge, the only attempt at standardizing prelateral labels was by Thomas & Yeager-Dror (2009). In their table, reproduced below, they list the labels they use in that volume. In addition to standardizing other allophones’ labels, prelateral vowels get an entire column, and a nearly complete set of labels.I presented an abbreviated version of this table already in a previous post since it’s what popularized the b_t frame.\n\n\n\n\n\n  IPA  \n\n\n    Keyword    \n\n\n    [_r]    \n\n\n    [_l]    \n\n\nSpecific Formulations  \n\n\n\n\n\n\n/i/\n\n\nbeet\n\n\nbeer\n\n\npeel\n\n\n\n\n\n\n/ɪ/\n\n\nbit\n\n\n\n\nbill\n\n\nbin [_N]\n\n\n\n\n/e/\n\n\nbait\n\n\nbear\n\n\nbail\n\n\n\n\n\n\n/ɛ/\n\n\nbet\n\n\n\n\nbell\n\n\nben [_N]; beg [_ɡ]\n\n\n\n\n/æ/\n\n\nbat\n\n\n\n\n\n\nback [_k]; bag [_ɡ]; ban [_N]; tap [_p]; tab [_b]; bad, for Milwaukee [_d], for New York see p. 109.\n\n\n\n\n/ɑ/\n\n\nbot\n\n\nbar\n\n\n\n\n\n\n\n\n/ɔ/\n\n\nbought\n\n\nborder\n\n\nball\n\n\n\n\n\n\n/o/\n\n\nboat\n\n\nboar\n\n\nbowl\n\n\n\n\n\n\n/ʌ/\n\n\nbut\n\n\n\n\ncull\n\n\n\n\n\n\n/ʊ/\n\n\nbook\n\n\nboor\n\n\npull\n\n\n\n\n\n\n/u/\n\n\nboot\n\n\n\n\npool\n\n\ntoot [C_coronal_]\n\n\n\n\n/aɪ/\n\n\nbite\n\n\npyre\n\n\nbile\n\n\nbide [_Cvd]; buy [_#]; pine [_N]\n\n\n\n\n/aʊ/\n\n\nbout\n\n\nhour\n\n\nhowl\n\n\nbough [_#]\n\n\n\n\n/ɔɪ/\n\n\nboy\n\n\n\n\nboil\n\n\n\n\n\n\n/ɚ/\n\n\nbird\n\n\n\n\n\n\nburr [_#]; bother [-stress]\n\n\n\n\n\n\nThe lexical set, including prelateral and other allophonic expansions, from Thomas & Yeager-Dror (2009:6).\n\n\nIt appears the authors wanted to select as many keywords as possible that started with /b/. The now somewhat widespread b_t frame is established, and most prerhotic labels start with /b/ as well. The prelateral vowels had the most exceptions: there are seven keywords with b_l, but there are several consonant onsets to fill in the gaps (three with /p/ and one each with /k/ and /h/). This inconsistency in the prelaterals creates a somewhat haphazard set of labels.\nThe draw of using minimal sets for prelaterals is tempting, but I believe they have the same problems that the b_t frame has. In a previous post, I’ve already shared my thoughts on why I think the b_t frame isn’t very helpful, so I won’t rehash it further. The gist is this: if you hear someone say [poɫ] in isolation, are they saying pole or are do they merge /ʊl/ and /ol/ and are actually saying pull? This ambiguity goes against Wells’ original intentions of the words being “hardly be mistaken for other words.”\n\n\n\n\n\n\nMy Hot Take\n\n\n\nTo put it another way, just because we call something the pull-pole merger, that doesn’t mean we should refer to the entire lexical sets as pull and pole. In fact, I think we should actively avoid referring to them as pull and pole for the very reason that they do form a minimal pair."
  },
  {
    "objectID": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#why-a-minimal-set-wont-work-for-prelaterals",
    "href": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#why-a-minimal-set-wont-work-for-prelaterals",
    "title": "Extending Wells’ Lexical Set to Prelateral Vowels",
    "section": "Why a minimal set won’t work for prelaterals",
    "text": "Why a minimal set won’t work for prelaterals\nLet’s entertain the idea of using a minimal set to unite all prelateral vowels. We’ve got f_l for front vowels and p_l for some of the back vowels. Can we find one frame that works for all vowels?\nNo. The following table, which is grouped approximately by natural class, lists the frames that have the most words and would therefore be the best candidates for new labels:\n\n\n\n\n\nIPA\n\n\nb_l\n\n\np_l\n\n\nf_l\n\n\nd_l\n\n\nh_l\n\n\nk_l\n\n\nt_l\n\n\ng_l\n\n\n\n\n\n\n/il/\n\n\n\n\npeel/peal\n\n\nfeel\n\n\ndeal\n\n\nheal, heel\n\n\nkeel\n\n\nteal\n\n\n\n\n\n\n/ɪl/\n\n\nbill\n\n\npill\n\n\nfill\n\n\ndill\n\n\nhill\n\n\nkill\n\n\ntill\n\n\ngill\n\n\n\n\n/el/\n\n\nbale/bail\n\n\npale/pail\n\n\nfail\n\n\nDale\n\n\nhail\n\n\nkale\n\n\ntale/tail\n\n\ngale\n\n\n\n\n/ɛl/\n\n\nbell\n\n\npell\n\n\nfell\n\n\ndell\n\n\nhell\n\n\n\n\ntell\n\n\n\n\n\n\n/ul/\n\n\nboule\n\n\npool\n\n\nfool\n\n\nduel\n\n\nwho’ll\n\n\ncool\n\n\ntool\n\n\nghoul\n\n\n\n\n/ʊl/\n\n\nbull\n\n\npull\n\n\nfull\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/ol/\n\n\nbowl\n\n\npole/poll\n\n\nfoal\n\n\ndole\n\n\nhole/whole\n\n\ncoal\n\n\ntoll\n\n\ngoal\n\n\n\n\n/ʌl/\n\n\n\n\n\n\n\n\ndull\n\n\nhull\n\n\ncull\n\n\n\n\ngull\n\n\n\n\n/æl/\n\n\n\n\npal\n\n\n\n\n\n\nHal\n\n\nCal\n\n\n\n\ngal\n\n\n\n\n/ɑl/\n\n\n\n\n\n\n\n\ndoll\n\n\n\n\n\n\n\n\n\n\n\n\n/ɔl/\n\n\nball\n\n\nPaul\n\n\nfall\n\n\n\n\nhall\n\n\ncall\n\n\ntall\n\n\ngall\n\n\n\n\n/ɑɪl/\n\n\nbile\n\n\npile\n\n\nfile\n\n\ndial\n\n\n\n\nKyle\n\n\ntile\n\n\nguile\n\n\n\n\n/ɑʊl/\n\n\nbowel\n\n\n\n\nfoul/fowl\n\n\ndowel\n\n\nhowl\n\n\n\n\ntowel\n\n\n\n\n\n\n/ɔɪl/\n\n\nboil\n\n\n\n\nfoil\n\n\nDoyle\n\n\n\n\ncoil\n\n\ntoil\n\n\nGoyle\n\n\n\n\n\n\nMinimal sets of and potential keywords for prelateral vowels. Words in gray are not ideal for a variety of reasons (infrequent words, proper nouns, or inconsistent vowel assignments). Blank cells are, as far as I can tell, accidental gaps in the English lexicon.\n\n\nAs you can see, no one frame works for all vowels. The main culprits are /ʌl/ and /ɑl/ and maybe also /ʊl/ and /æl/. Let’s look at each frame individually:\n\nb_l—It’s closest to the established b_t frame, but other than the diphthongs (which are probably least studied), none of the natural classes gets a full minimal set. In fact, from what I’ve been able to find, there are no monosyllabic English words with /#bil/, so you can’t even add extra coda material to make it work.\np_l—If the b_l frame is the goal, p_l makes for a good backup since it’s articulatorily closest. For the back vowels, it’s probably the least bad option. (If so, I prefer pole over poll.) For the front vowels, it works fine except the word pell isn’t exactly that common.\nf_l—This one is best choice for front vowels with its four common words. Conveniently, it works for the diphthongs too. It’s a little less ideal for the back vowels because foal isn’t a particularly common word, and some people confuse it with foul.\nd_l—This one works well for a lot of vowels, but only if you use a couple proper names like Dale and Doyle. In a lot varieties of American English dual works fine here, but not for non-American varieties. Furthermore, dole isn’t a very well-known word I don’t think. The benefit of looking at d_l though is that it is the only frame that contrasts /ɑl/ and /ʌl/.\nh_l—It works for the front vowels, but I don’t really want to plaster hell all across my papers. Besides, who’ll isn’t ideal either because if we’re okay with contractions, we might as well include bee’ll, boo’ll, or any number of words to fit the frame. I will say though that the benefit to using h_l is that is has a nice minimal pair contrasting /ɔl/ and /ʌl/, which some varieties merge.\nk_l—Most of the benefits of h_l apply to k_l but none of the broad natural classes are complete.\nt_l—It’s comparable to f_l, except there’s no /ʊl/.\ng_l—The benefits are similar to k_l, except there’s no /ɡil/, and I had to fudge /ɔɪ/ with Goyle (though Harry Potter fans may enjoy it).\n\nBy natural classes, I’m referring to front vowels, back vowels, low vowels, and diphthongs—environments within which mergers are most commonly reported.I don’t speak hypothetically here: I’ve included foal in wordlists and people sometimes do mistake it for foul. See also Wade (2017:7) and Arnold (2015:3) who explicitly mentione this problem too.Several frames work pretty well, but they all have their problems. More importantly, no one frame works well for all front vowels (if you include /æl/), no one frame works for all back vowels (if you include /ʌl/), and no one frame works for the low vowels (thanks, /ɑl/). To get a complete set of labels, you’re going to have to pick three different frames—at a minimum. Such a patchwork of frames is inconsistent and confusing.\nSo, should we ditch the minimal sets and go for unambiguous, truly Wells-style lexical sets? Yes."
  },
  {
    "objectID": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#unambiguous-labels-for-prelateral-vowels",
    "href": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#unambiguous-labels-for-prelateral-vowels",
    "title": "Extending Wells’ Lexical Set to Prelateral Vowels",
    "section": "Unambiguous labels for prelateral vowels",
    "text": "Unambiguous labels for prelateral vowels\nMy goal in all this was to select prelateral vowel keywords that are, in Wells’ words, “unmistakable no matter what accent one says them in.” So, rather than finding the optimal minimal set, I decided to choose words that were were very different from one another.\nTo identify potential words, I would need a list of all monosyllabic English words with a prelateral vowel. Fortunately, I’ve got that (and I’ve had it for some time)!\n\n\nList of all ~401 monosyllabic English words with a coda lateral, organized by vowel and syllable structure? Check. My favorite? Squelched.\n\n— Joey Stanley (@joey_stan) May 7, 2017\n\n\nThis spreadsheet is based on the CMU dictionary. I searched for all words that contained just one vowel and where that vowel was followed by an L. I then just organized them by their consonants in a spreadsheet. It’s pretty handy.\nWith this spreadsheet, I can easily identify candidate representative words. So, loosely based on Wells’ original keywords, here were the criteria that I established for selecting a word, approximately in order of importance:I’ve outlined more general criteria for selecting a new Wells-inspired label for an allophone in a previous post.\n\nThey must be monosyllabic.\nThey must be monomorphemic.\nWords should not form another English word when the vowel is replaced with another (i.e. it should have no minimal pairs).\nOnsets should ideally have just one consonant. Glides are avoided and liquids are acceptable as a last resort.\nCodas consist of a single voiceless obstruent following the /l/. \nNouns are preferred.\n\nI have relaxed the coda requirement a little bit from what Wells says (1982:123):“As far as possible the keywords have been chosen so as to end in a voiceless alveolar or dental consonant: a voiceless consonant minimizes the likelihood of diphthongal glides obscuring a basic vowel quality, while coronality (alveolar or dental place) minimizes the possible allophonic effect of the place of a following consonant. An exception here is trap for the /æ/ correspondence, where no items in /-t, -s, -θ/ are altogether suitable; another one is palm.”I figure the presence of the lateral is enough of a buffer between the vowel and any following consonant (though phoneticians are welcome to correct me).With this organized spreadsheet, applying the six criteria for each vowel class was relatively straightforward. The below table shows the list I came up with:\n\n\n\n\n\n  IPA  \n\n\nWellsKeyword\n\n\nPrelateralKeyword\n\n\n \n\n\nExample words\n\n\n\n\n\n\ni\n\n\nfleece\n\n\nzeal\n\n\n\n\nfeel, peel, deal, kneel, meal, seal, yield, shield\n\n\n\n\nɪ\n\n\nkit\n\n\nguilt\n\n\n\n\nill, pill, dill, gill, shrill, drill, kilt, quill, thrill\n\n\n\n\ne\n\n\nface\n\n\nflail\n\n\n\n\nfail, tail, whale, scale, jail, trail, grail, shale, ale\n\n\n\n\nɛ\n\n\ndress\n\n\nshelf\n\n\n\n\nfell, bell, weld, gel, smell, swell, dwell, delve, realm\n\n\n\n\nu\n\n\ngoose\n\n\nspool\n\n\n\n\nfool, cool, tool, ghoul, stool, school, drool, cruel, Yule\n\n\n\n\nʊ\n\n\nfoot\n\n\nwolf\n\n\n\n\nfull, pull, bull, wool, wolf\n\n\n\n\no\n\n\ngoat\n\n\njolt\n\n\n\n\nhole, coal, bowl, goal, cold, scold, troll, molt, gold\n\n\n\n\nʌ\n\n\nstrut\n\n\nmulch\n\n\n\n\nhull, dull, gull, pulse, skull, cult, gulf, lull, sulk, sculpt\n\n\n\n\næ\n\n\ntrap\n\n\ntalc\n\n\n\n\npal, gal, shall, scalp, talc, valve, Hal, Alps\n\n\n\n\nɑ\n\n\nlot\n\n\ngolf\n\n\n\n\ndoll, alm, golf, qualm, psalm\n\n\n\n\nɔ\n\n\nthought\n\n\nfault\n\n\n\n\nall, hall, fall, scald, bald, shawl, crawl, vault, false\n\n\n\n\naɪ\n\n\nprice\n\n\nchild\n\n\n\n\naisle, dial, pile, tile, bile, mile, child, trial, vile, guile\n\n\n\n\naʊ\n\n\nmouth\n\n\nprowl\n\n\n\n\nfoul, howl, towel, bowels, jowl, scowl, growl, owl\n\n\n\n\nɔɪ\n\n\nchoice\n\n\nbroil\n\n\n\n\noil, foil, boil, coil, toil, spoil, broil, soil, royal(?)\n\n\n\n\nɚ\n\n\nnurse\n\n\ntwirl\n\n\n\n\nEarl, girl, curl, hurl, pearl, whirl, swirl, twirl\n\n\n\n\n\n\nThe prelateral keywords I propose in this post.\n\n\nAllow me to share a few comments about each one, starting with the front vowels.\n\nzeal—The only other word with /il/ and no minimal pairs was squeal, and I opted for the one with the simpler onset.\nguilt—Lots of other words fit the critera, like filth, lilt, milk, and tilt, so I arbitrarily chose guilt as the representative word. Also, milk wouldn’t work because a lot of people have /ɛ/ in that word.\nflail—This was an easy choice because it’s the only one with no minimal pair. I’m not a fan of the complex onset, especially with the liquid, but it’s the only option. That’s okay: since Wells used fleece, I think flail is fine.\nshelf—Again, there were lots of other options: belch, help, health, pelt, self, wealth, and yelp. Of these, shelf just felt like the best choice—though belch would have been a great alternative!\n\nIf I wanted to relax the coda requirement, I really like chill as an alternative option.For the back vowels, it’s especially important that minimal pairs do not exist since there are a variety of mergers that exist among prelateral back vowels, including four-way mergers.\n\nspool—There were no words with /ul/ that didn’t form a minimal pair with other prelateral vowels. And in fact, none of the monomorphemic words had any other coda material besides /l/. Given these limitations, I chose spool as the representative word. It forms minimal pairs with spiel, spill, spell, and spoil, but I’m not aware of any variety of English that would merge spool with any of those words. Other candidates were school and cruel, but they form minimal pairs with skull and crawl and I wanted to avoid any words with other back vowels in the same frame.\nwolf—There really are very few words with /ʊl/, especially monosyllabic ones. As far as I know, there are five: full, pull, bull, wool, and wolf. Is it coincidental that they all start with labial sounds? Not sure. Probably. Of these, full, pull, and bull form lots of minimal pairs with other back vowels. Wool has no obstruent in the coda. That leaves wolf. I’m pretty sure my idiolect assigns /o/ to this word, so it goes against my intuition, but Wells himself assigns /ʊ/ to wolf and it’s the only real option we’ve got.\njolt—There are lots of words with /ol/, but only two lack minimal pairs: stroll and jolt. Only jolt fits the other criteria.\nmulch—A handful of words fit the criteria: gulp, hulk, mulch, pulp, pulse. And unless you count the name of the department store Belk as a word, then bulk also fits. Any of these work well (I’ve personally used pulp in the past), but I chose mulch to add a bit of variety in coda consonants to the lexical set.\n\nI’m pretty sure I’ve seen school before but I can’t find the source.With the low vowels, the number of options was limited, and I had to relax some of the rules.\n\ntalc—The only other word that fit all the criteria was scalp, so I chose the one with the simpler onest.\ngolf—Okay, this one was tricky. The CMU and searchable phonetic dictionaries I have access to didn’t have too many words with /ɑl/. Plus I merge /ɑ/ with /ɔ/ before coda laterals, so I don’t have intuition to back me up. In fact, the best list I know of is Dinkin 2016. The word golf was the most common word, and though it forms a minimal pair with gulf, it’s probably the best option. The next best might be psalm, but that only works for speakers that pronounce the [l] and who don’t have palm there. Another option was doll, but it’s part of a large minimal set.\nfault—The only two words that formed no minimal pairs, sprawl and false, aren’t ideal because they didn’t seem nouny enough. Plus, sprawl has has a complex onset, and seeing false in capital letters just seems like an error in computer code. The word fault forms a minimal pair with felt but fits all the other criteria, so I think it’ll do.\n\nAnother option is valve, and if I wanted to relax the voiceless obstruent requirement, I’d opt for that.For diphthongs and pre-/ɹ/, the choices were also limited. You’ll notice that only one has a consonant in the coda, and its voiced, violating my rule 5. The reason is because none of the monomorphemic words with these vowels had anything else in the codas, other than child, wild, and mild.\n\nchild—There was only one word with no minimal pairs, whilst, which didn’t seem like a good fit. Though child has a voiced coda, it only forms a minimal pair with chilled so it seems pretty good otherwise.\nprowl—It’s not the most common word, but it doesn’t have any minimal pairs.\nbroil—There were really no good options for /ɔɪl/. They all had minimal pairs with other words, so I chose a word based on the minimal pairs it did have. Spoil forms a set with spool, spell, spill, and spiel; soil forms a set with sole/soul/Seoul, Saul, Sal, sell/cell, sale/sail, sill, and boil, toil, coil, Doyle, foil, and oil have even larger minimal sets (see the table above). In comparison, broil doesn’t seem to bad since it forms a set with just brawl and Braille.\ntwirl—I’m not a big fan of the /w/, but it’s the only one that doesn’t form a minimal pair.\n\nI believe the resulting list forms a decent set of labels to refer to prelateral vowels.\nUsing this new lexical set, we can now refer to things like the zeal-guilt merger, the flail-shelf merger, or the dress-talc merger among the front vowels. Among the back vowels, we might find the spool-wolf merger, the wolf-jolt merger, the jolt-mulch merger, or possibly, the wolf-mulch or the mulch-fault mergers. I personally have the golf-fault merger, though only if the /l/ is in the coda (collar and caller are still distinct)."
  },
  {
    "objectID": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#conclusion",
    "href": "blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html#conclusion",
    "title": "Extending Wells’ Lexical Set to Prelateral Vowels",
    "section": "Conclusion",
    "text": "Conclusion\nI have proposed a new set of labels akin to the Wells Lexical set for referring to prelateral vowels. The ad hoc labels in existing literature are inconsistent, and the attempts at using lexical sets are unsuccessful. This new set should make labels more consistent in sociolinguistic research going forward."
  },
  {
    "objectID": "blog/nwav47/index.html",
    "href": "blog/nwav47/index.html",
    "title": "NWAV47",
    "section": "",
    "text": "Today, I gave a poster presentation on prevelar raising. As it turns out, despite beg and bag being relatively small lexical classes, I found phonological, morphological, and lexical effects on the degree of raising, and that the two vowel classes reacted to these influences differently.\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\n\n\n(Photo by Maciej Baranowski.) \n\nSo for some speakers, mostly in the Pacific Northwest, the Upper Midwest, and Canada, they raise the dress (beg) and trap (bag) vowels before voiced velars. The idea for this research mostly came about because I was trying to figure out whether I have beg-raising. Turns out, I do, except for in a few words like integrity, negligible, and segregate which seems somewhat unexpected.\nSo I set up a categorization task that I distributed via Reddit. I showed people a bunch of beg and bag words and asked them whether the vowel in their pronunciation is most like that in bake, deck, or back. Almost 7,000 people took the survey and I got over 500,000 observations.\nAs it turns out, there were clear phonological effects for beg. If the /ɡ/ was followed by a sonorant (as in regular or segment), particular a liquid (negligible or segregate), it was indicated to be raised less. Adding suffixes had different effects: people indicated a raised vowel more when -ed was added to verbs, but only for bag. There were lexical effects too: for bag, more frequent words were raised more, borrowings with beg were raised more, and beg words with orthographic &lt;ex&gt; (like exile or exit) were virtually never raised.\nThe takeaway is that even though these are relatively marginal lexical classes, they still have interesting language-internal factors. And, even though beg- and bag-raising are generally considered to be the same phenomenon of prevelar raising, the two are different. There are some major limitations here, particularly because reported speech is never completely reliable, so I’d like to follow up with an acoustic study. But, I think it’s important to include many more words in a word list because some of these patterns would not have been found if a just small one was used."
  },
  {
    "objectID": "blog/nwav49/index.html",
    "href": "blog/nwav49/index.html",
    "title": "NWAV49",
    "section": "",
    "text": "I’m at New Ways of Analyzing Variation 49 online right now! Other than an quick online satellite session of LabPhon last summer, I haven’t attended a conference since November 2019 when we hosted LCUGA at UGA. Anyway, I’m excited to be conferencing again and while I miss seeing colleagues in-person, this online format isn’t bad. Anyway, on this page you’ll find links to the slides and YouTube videos of my two talks."
  },
  {
    "objectID": "blog/nwav49/index.html#order-of-operations-in-sociophonetic-analysis",
    "href": "blog/nwav49/index.html#order-of-operations-in-sociophonetic-analysis",
    "title": "NWAV49",
    "section": "Order of Operations in Sociophonetic Analysis",
    "text": "Order of Operations in Sociophonetic Analysis\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nI’m very excited and nervous about this Order of Operations one. As it turns out, even if you start with the exact same spreadsheet and use the exact same functions, if you do those function in different orders, it’ll produce different results. Sometimes drastically different results. I did this by processing a spreadsheet 5,040 unique ways and got a whole range of results. To me at least, it’s making me rethink how I process my data and how I can interpret others’ results when the order isn’t explicitly reported in the methods section of a paper."
  },
  {
    "objectID": "blog/nwav49/index.html#years-of-georgia-english",
    "href": "blog/nwav49/index.html#years-of-georgia-english",
    "title": "NWAV49",
    "section": "100 Years of Georgia English",
    "text": "100 Years of Georgia English\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nAs a continuation of some work I did as a grad student, Peggy Renwick and I presented our research on Georgia English vowels and how they’ve changed over 100 years. Basically, all of them have. The Southern Vowel Shift seems to have undergone a rise and fall, perhaps peaking in those born around WWII. Meanwhile, back vowels are fronting. Younger people today have something like the Low-Back-Merger Shift, but flavored with some Southernisms still. You’ll hear more about this project in later conferences too."
  },
  {
    "objectID": "blog/lots-of-transcribing/index.html",
    "href": "blog/lots-of-transcribing/index.html",
    "title": "Lots of Transcribing",
    "section": "",
    "text": "Last summer, I collected roughly 40 hours of conversation from people in Washington State last year. Not an enormous corpus, but I’m quite proud of that dataset. Well, my goal was to transcribe one speaker gradually over the course of the year, finishing around now. Well, in 9 months, I’ve done about… one hour. I realized this week that I really really need to get these done.\nNow, I haven’t just been slacking off doing nothing for the past school year. I’ve been very busy with my research assistantships and with other tasks. I’ve given workshops and seminars here at UGA, and my name has been on a lot of conference presentations in the past few months, even if I haven’t been able to attend them. I’ve put together this website as well.\nI’ve got my second qualifying paper coming up (edit: see my post about it here and it occurred to me that if I want to graduate in a year, I had better get these recordings transcribed!\nBecause I am the way I am, I kept track of my transcription rate. I did the first hour or so in about 5 hours. Okay, so at that rate, it’ll take me around 200 hours of work to finish the transcriptions. Okay, so at an hour a day that’ll take me like 8 months. At two hours a day I’ll finish in like August. Yikes. That’s a lot of work.\n\n\n39 hours of audio left. At two hours of transcribing a day I'll finish by… Thanksgiving. Am I old enough to hire undergrads yet?\n\n— Joey Stanley (@joey_stan) April 28, 2017\n\n\nSo I decided to just get to it. I can calculate and project and write about it all I want, but it’s not going to get done unless I just go for it. And I’ve got to be the one to do it. Even though there are automatic transcribers out there, I’d have to double-check their work anyway, and if I need to listen to it all I might as well do it all myself. Plus, it’s nice to go through them again and listen to linguistic features I didn’t catch before. And there are a lot!\n\nEdit: I’ve decided to tweet every so often to keep track of my progress and to keep myself motivated.\n\n\nI put in four hours today and finished an entire speaker, averaging 3.2 minutes of work time for every minute of transcription time. Nice.\n\n— Joey Stanley (@joey_stan) May 2, 2017\n\n\n\n\nI'm 15% finished transcribing. Just finished listening to my interview with a professional radio announcer. Wow. Talk about clear audio.\n\n— Joey Stanley (@joey_stan) May 12, 2017\n\n\n\n\nJust hit the 20% mark for transcribing my interviews. I'm currently reliving the one with a guy who worked falling trees for 18 years.\n\n— Joey Stanley (@joey_stan) May 16, 2017\n\n\n\n\n25% done with transcribing. Would have hit it earlier, but I’ve got two other projects going right now, which means more transcribing later…\n\n— Joey Stanley (@joey_stan) May 31, 2017\n\n\n\n\nAfter a bit of a hiatus, I'm back to transcribing and just hit the 30% mark for my corpus! Hooray!\n\n— Joey Stanley (@joey_stan) August 3, 2017\n\n\n\n\nAfter another long hiatus, I'm back to transcribing. I just hit the 35% mark of my corpus. I'm averaging around 4.6 hours of work for every hour of audio, and I've got about 26 hours of interviews left. https://t.co/AyjOL9gIXX\n\n— Joey Stanley (@joey_stan) April 17, 2018\n\n\n\n\nOkay I'm now 40% finished transcribing. I've put 78 hours into this already and I've got approximately 105 hours of work left. I'm hoping to be done by July. This is crazy. #amtranscribing\n\n— Joey Stanley (@joey_stan) April 25, 2018\n\n\n\n\nI'm now 45% done with transcription. It'll be really satisfying to heat that 50% mark in a week or so. Only 60 workdays left! #amtranscribing\n\n— Joey Stanley (@joey_stan) May 2, 2018\n\n\n\n\nHooray! I'm HALFWAY done with transcribing these interviews! At slightly more than two hours a day and my current pace of 4.5 hours of work for every hour of audio, I should be done by July. #amtranscribing\n\n— Joey Stanley (@joey_stan) May 8, 2018\n\n\n\n\nJust hit 55%. Learning a lot about welding right now. 1022 minutes of interviews left to transcribe, but who's counting anyway? #amtranscribing\n\n— Joey Stanley (@joey_stan) May 16, 2018\n\n\n\n\nOkay, so now I'm 60% done. I'm enjoying all the technical terms I'm learning about: welding on the last interview and quilting on this one. Only 65 more hours of work left! #amtranscribing\n\n— Joey Stanley (@joey_stan) May 22, 2018\n\n\n\n\nOkay, I'm now 65% done with transcribing my corpus! About 6 more weeks and I'll be finished. #amtranscribing\n\n— Joey Stanley (@joey_stan) May 29, 2018\n\n\n\n\nAlright, I just hit the 70% mark. Just 10½ hours of interview left, which should take me about 45 hours to do. In less than a month I'll be done! #amtranscribing\n\n— Joey Stanley (@joey_stan) June 7, 2018\n\n\n\n\nHooray! 75% done! Just finished an interview with a native Washingtonian who totally has existential \"it\" and \"liketa.\" That was cool. #amtranscribing\n\n— Joey Stanley (@joey_stan) June 13, 2018\n\n\n\n\nIt's been a busy week of transcribing, and even though I hit 80% on Tuesday, I just finished out the week at 85%. I did an entire interview yesterday and another one (plus some change) today! #amtranscribing\n\n— Joey Stanley (@joey_stan) June 29, 2018\n\n\n\n\nSo apparently, I've been miscalculating my percentage. Turns out I'm 95% done! I only have two more interviews to transcribe! The end is near! #amTranscribing #NotForMuchLonger #CantWaitToStartAnalysis\n\n— Joey Stanley (@joey_stan) July 10, 2018\n\n\n\n\nI saved the funnest interview for last. This guy's a fourth generation euphonium player and I did trombone before switching to linguistics, so most of the interview is us geeking out on low brass topics. #amtranscribing #ButIllBeDoneTODAY #StayTuned\n\n— Joey Stanley (@joey_stan) July 11, 2018\n\n\nTo see my excited tweetstorm for when I finished, see this new post."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html",
    "href": "blog/using-phonic-for-sociophonetics/index.html",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "",
    "text": "A few months ago, I was looking for ways to collect audio within a Qualtrics survey and I stumbled upon this answer which lead me to Phonic.ai. It looked good, so I decided to try it out for a new project. I’m happy to report that I’m overall very satisfied with how it all turned out. In this post, I’ll explain how to use Phonic, tell about some of my experiences with using it, and give you some tips.\nNote that this page may update if I think of more things to add. And, in an ideal world, I’d post screenshots and do more of a walkthrough for you, but I just don’t quite have the time for that right now. So, I hope this text-based post provides the information you need to get started using Phonic."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html#project-background",
    "href": "blog/using-phonic-for-sociophonetics/index.html#project-background",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "Project Background",
    "text": "Project Background\n To give you a sense of what I used Phonic for, I should provide a little bit of background on my project. I wanted to collect some audio remotely from perhaps 100 people. Since this is largely exploratory data, all I had people do was read some wordlists and respond to some open-ended questions. I had 200 words that I had them read, split up into four sets of 50. I also had about 5 open-ended questions (with an additional 3 or 4 more, depending on their demographic background).I’m deliberately being vague here because I don’t want to spoil anonymity in upcoming abstracts and manuscript.\nI ran the survey for about four months, actively recruiting on Reddit and other places the whole time. In the end, I ended up with data from a whopping 324 people, which is far more than I ever expected. I’m very excited about the project!\nSome of you may know that I’ve been looking for a good way to collect audio remotely for a while. About five years ago I used Amazon Mechanical Turk to do so after reading about how the folks at Dartmouth used it to collect data across New England. It was—a bit of a nightmare for me. So I am very happy that Phonic is much more manageable."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html#what-is-phonic",
    "href": "blog/using-phonic-for-sociophonetics/index.html#what-is-phonic",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "What is Phonic?",
    "text": "What is Phonic?\nAccording to its homepage, Phonic is a “research platform designed for voice and video.” Basically, it’s a survey-building site that allows you to collect audio and video data. It also has a pretty nice interface that lets you interact with your data in useful ways, which I’ll describe later.\nTo be clear, you do have to pay to use Phonic.  The more you pay, the more people you can collect data from. When I started using Phonic in March 2022, they had different subscription tiers. The one that looked more appropriate to me was the academic tier, which was about $37 a month for 200 participants. That was a little more than I needed, but I definitely needed more than the 25-a-month available in the lower tier. Recently, they rolled out a different system where you buy tokens as needed. In theory, it comes out to be more economical.I suppose Qualtrics is a paid software too, but I’ve been able to get a free account through every institution I’ve been at, so it feels free to me.\nPhonic is also a survey site in and of itself. What you can integrate with Qualtrics is, as far as I can tell, only a small portion of its full capabilities. However, I am more familiar with Qualtrics and I know that things like survey flow work well in it, so to me it was worth it to use Phonic embedded within a Qualtrics survey rather than go full native Phonic. This of course adds some increased complexity when it comes to processing your data because you’ll need to merge two datasets—one from Phonic and one from Qualtrics—but to me it was worth it."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html#using-phonic-within-qualtrics",
    "href": "blog/using-phonic-for-sociophonetics/index.html#using-phonic-within-qualtrics",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "Using Phonic within Qualtrics",
    "text": "Using Phonic within Qualtrics\nI won’t walk you through how to integrate Phonic into Qualtrics questions because they’ve already got tutorials for that. I used the instructions on this page but it looks like they’ve recently made a video tutorial as well. I haven’t watched it, but I presume it has all the information you need.\nThe gist is this. First, you need to create a skeleton survey in Phonic, with one question for each instance of audio collection you want to do in Qualtrics. I just made a really simple survey using the most basic audio collection type and put “Wordlist 1” or something simple as the question text. Each survey you make is given a unique ID, and each question within that survey is also given a unique ID.\nYou then go over to your Qualtrics survey and create it like normal. Then, wherever you want to add audio collection to your survey, you open up the survey question in Qualtrics in the HTML view and add some invisible code at the bottom that links that question to a specific question using the survey ID and question ID generated in Phonic. You also need to put some HTML code in your survey header. Again, this is all explained in the documentation on Phonic’s website.\nOnce it’s all set up, I’ve found that it works like a charm. When I tested it out, I noticed that the audio appeared in Phonic very soon after it was submitted in Qualtrics. You should know though that there are a few combinations of browers ond hardware that aren’t going to work, which they point out here. For example, it won’t work on an iPhone in any browser except Safari, and it won’t work directly within Facebook Messenger."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html#my-one-bad-experience-and-how-they-fixed-it",
    "href": "blog/using-phonic-for-sociophonetics/index.html#my-one-bad-experience-and-how-they-fixed-it",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "My one bad experience and how they fixed it",
    "text": "My one bad experience and how they fixed it\nI will say that I did have some troubles at the start of my data collection. I noticed that people were completing the survey, but the audio didn’t go through. I was pretty upset about it too because maybe a dozen people’s recordings didn’t go through at all. For another dozen or so, only some of the recordings went through. Fortunately, I wasn’t paying people, so no harm done I suppose, but it sure stinks losing data.\nI wrote to Phonic about the issue, showing them clear evidence that these people had indeed completed the survey but that the audio wans’t going through. It took them a few weeks, but they did eventually fix the issue. They even kept me up to date on the status of their progress too. I haven’t had any problems since then. So, hats off to a good customer service experience and to them fixing an issue that would have deterred me from using Phonic in the future."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html#how-the-phonic-interface-works",
    "href": "blog/using-phonic-for-sociophonetics/index.html#how-the-phonic-interface-works",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "How the Phonic interface works",
    "text": "How the Phonic interface works\nOkay, so with all that in mind, lets say you’ve got a survey going and you’re starting to collect some data. What do you get on your end? You actually get, quite a lot!\nFor one, it automatically transcribes your data!  Better yet, it did a pretty good job! Now, to be clear most of the folks I collected data from spoke something close to Standard American English, so take that for what it’s worth. These transcriptions were super helpful when scanning through the data too. It’s so much faster to look through a rough transcription and get the gist of what they’re saying than it is to listen to the recording all the way through.I only collected spoken American English, so I have no idea how it works on other varieties or other languages.\nSo, with these transcriptions readily available, the Phonic interface lets you quickly navigate through your data. You can look at them by-person and see all the answers each person gave. You can also look at individual questions and see all the answers to each question.\nFor each response then, you get the transcription and the waveform together. When you play the audio, the word that is being said in the transcription gets highlighted, so it’s easy to follow along. Furthermore, when you click on a word, it’ll take you straight to that part of the audio—a very cool feature. You can also click on the waveform and it’ll start playing from there.\nThe Phonic interface is very handy. However, you lose access to all that as soon as your subscription runs out. So, take advantage of it while you can. I wish I could recreate the interface in Praat or Shiny or something, but it wouldn’t quite be the same."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html#managing-downloaded-data",
    "href": "blog/using-phonic-for-sociophonetics/index.html#managing-downloaded-data",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "Managing downloaded data",
    "text": "Managing downloaded data\nSo, interacting with your data online is nice, but as a sociophonetician, I needed to download the audio so I can do acoustic analysis on it. I also need to link it to the metadata I collect in Qualtrics. This is where you really have to flex your data management skills because it’s not super straightforward to get all your data in one place.\n\nDownloading audio\nFirst off how is the audio quality? Not bad. Yes, you do get varying sound qualities because each person is recording from their own devices, and I encourage you to look at the literature on how that affects acoustic analysis. But, you do have the option to record and download in WAV format, which is handy. (The other option is mp3.) I suspect though that Phonic runs it through a denoising algorithm, but it’s hard to tell what Phonic does and what is the consequence of people’s computer/phone microphones.Edit: Sam Hellmuth pointed out to me on Twitter that the WAV files are in fact compressed and only go up to 8000Hz! Thanks for letting me know, Sam!\nThere are two ways to download audio, and I’ll admit that both are not great. The first is to manually go through and download each audio file. While slow and tedious, it has the benefit of you knowing exactly what it is you’re downloading. The part that takes the longest is renaming each file since they all get downloaded as “response.wav.” I ended up writing a script in R to set up the file structure and generate a bunch of dummy files where the audio would end up. I then copied the name of the dummy file and pasted it as the name of the audio. It’s a little tedious to set up, but it worked well for me.Keeping in mind that I had eight or so audio clips from each person, it took me about 90 seconds to download all the audio, rename them, and put them in their right folders, for one person. I had 350 people to download, so it took me several days to download it all this way.\nThe second way to download audio is to do it in bulk. I did it this way at first, but after a while it stopped working. I’m not sure if it was because I was sitting on a lot of data, or because of some error on Phonic’s end. Regardless, when it did work, it sent me an email with a link to a zip file. The problem is the audio files were all named with some unique ID and it was impossible to link that to the participant and question without referring to the metadata spreadsheet. So from there, I had to then set up an R script where I linked the code to the person and question and then renamed the files that way. Not a super straightforward task if you’re not code-savvy. But my goodness was it much faster—when it worked.\n\n\nDownloading metadata\nAll the other data you collect as part of your survey can be downloaded in two spreadsheets. If you’ve worked with Qualtrics before, you’ll be familiar with how you download the data. The spreadsheet contains survey metadata as well as responses from any of the othe questions you ask in Qualtics itself. In my case, I had a few demographic questions before the audio-recording portion started, so that information will of course be in the Qualtrics spreadsheet.\nFor the Phonic data, you get similar kind of spreadsheet. Like Qualtrics, it exports the spreadsheet as one row per participant, which means it has many many columns. For each question you get the transcription (see next section) and the unique ID associated with that audio file (see previous section). You also get a bunch of sentiment analysis data. I don’t use it and I’m not sure how they generate those numbers, but it’s there in case you need it.\nThe key piece of information you get in both spreadsheets is the column called “response_id.” This is crucial because it’s the only piece of information that links the Qualtrics and Phonic datasets. (So, when you go to download the Phonic data, be sure to check the box that says to include it!) Using R or whatever other software you use, you’ll need that column to link the data together database-style.\n\n\nDownloading transcriptions\nWhen I first saw that Phonic transcribes things automatically, I was very excited because it could save me a lot of time. Correcting rough transcriptions is usually much faster than creating fresh transcriptions from scratch. Fortunately, the transcriptions are indeed available as part of the metadata file.\nTo help with correcting the transcriptions and generating good TextGrids, I wrote an R script that compiles all the transcriptions into a more streamlined spreadsheet. I have that open when I work with the files in Praat. I then just find transcription that corresponds to the audio file I’m working on, paste it into Praat, and then correct when needed and add utterance-level boundaries to break it up (which generally makes MFA work better in the next step of the pipeline). For things like wordlists (or potentially a reading passage if you use one of those), it’s probably better to pull up the original script and go from there since that’ll be closer to what people actually said than the automatic transcriptions.\nUnfortunately, word-level ones are not available. In theory, Phonic has that data because it has to know when to higlight each word when you’re playing the audio. But I wrote to them and they said that word-level detail is unavailable. Bummer."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html#tips",
    "href": "blog/using-phonic-for-sociophonetics/index.html#tips",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "Tips",
    "text": "Tips\nSo, with all that in mind, here are a few miscellaneous tips that didn’t fit anywhere else.\nDownload all your data before your plan runs out! As soon as your subscrioption is up, you’re blocked from .your survey. You won’t be able to download your data or even access the handy Phonic interface. Depending on how your payment plan works, if you end up collecting more data than you have paid for, people will still be able to complete your survey and upload recordings, but you won’t have access to the audio.\nI’m not sure if this is relevant anymore now that they’ve migrated to a token-based payment plan, but I had to write to them and specifically request the academic tier. I could sign up for the basic or more advanced tiers throught he website itself, but they wanted to ensure that I had a .edu email address before giving me the reduced rate for academics. Anyway, so if you run into a weird bug on the website where the academic tier is not available or whatever, write to them because that’s what I had to do.\nOne token is one person. So, it doesn’t matter whether your survey has one question or twenty, whether the audio files are short or long, or whether someone completes the survey or not. If they start to record something, that eats up one of your tokens. So, if you can integrate many questions into a single survey, do so because it won’t cost you any more money.\nRelatedly, if you’re testing your survey, each time you you through your own Qualtrics survey and record some test audio, that also counts as one token. I find that kind of annoying because it’s really nice to be able to do some robust testing before releasing the survey into the wild. So, plan accordingly. I didn’t need that many tests (maybe two or three) before being satisified with the results.\nPhonic has other features that I didn’t end up using that much. But, one that I discovered later than I should have is that you can upload any audio you want and it’ll transcribe it for you. It can even handle multiple speakers! It counts as one token, so if you’re paying by the token, this might not be as good of a deal. But if you’re like me and paid for a certain number of tokens per month without rollover, it could be more useful. I didn’t ever use all 200 of my monthly tokens, so at the end of my billing cycle, I lost a bunch. Meanwhile, I’m sitting on hundreds of recordings from other projects that need to be transcribed. If I had seen the feature sooner, on the last day of my billing cycle I would have uploaded as many of these recordings as I could. It would have been nice to have a couple dozen of those done.\nYou need to be careful when editing your Qualtrics questions once you’ve put Phonic codes at the bottom of them. So, for example, if you find a typo and want to change the text of a question, unless you edit it in the HTML viewer, you’ll immediately lose the hidden Phonic HTML code at the bottom. I learned the hard way that I accidentally did this when someone mentioned in one of their comments that the previous question didn’t have a record button.\nSpeaking of editing, it’s okay to add more questions to the Phonic survey itself. I did this twice without any loss of data. I did this because I decided to replace one of the questions in my survey with another one partway through data collection. I just added a new question in the Phonic survey, changed the code in the Qualtrics question, and I was ready to go. Of course, what’ll happen is that anyone who took the first version of the survey will have that question as blank, and anyone who takes the survey after the change will have the first question blank. But that’s fine.\nRelatedly, you can even link different Qualtrics surveys to the same Phonic survey. I also did this because I wanted to create a pared down version of my main survey to send out to a general audience, sort of as a control group. So, I made a new Qualtrics survey, included only what I needed to, but then included the same Phonic IDs in the hidden HTML codes. The result is that I have all my data in one place on the Phonic website, regardless of what survey it came from."
  },
  {
    "objectID": "blog/using-phonic-for-sociophonetics/index.html#conclusion",
    "href": "blog/using-phonic-for-sociophonetics/index.html#conclusion",
    "title": "Using Phonic for Collecting Sociophonetic Data",
    "section": "Conclusion",
    "text": "Conclusion\nSo, Phonic is cool and I’m a fan. Hopefully this collecting of rambling thoughts has informed you a little bit to help you decide if you want to use it or not. Please feel free to reach out to me about how I set up my survey or how I handled the data afterwards."
  },
  {
    "objectID": "blog/mturk/index.html",
    "href": "blog/mturk/index.html",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "",
    "text": "Note\n\n\n\nThe following is a manuscript I put together around November 2017. It was intended for publication somewhere, but I felt it was too informally written and too specific to a particular method to be useful. Besides, now it’s a little dated. Rather than collecting dust on my computer in perpetuity, I’ve decided to release it as a blog post. Hopefully it can be useful to someone out there.\nFor well over a century, dialectologists have needed to collect linguistic data from people in a large geographic area. The gold standard is to conduct in-person interviews, but the time commitment and costs associated with travel are prohibitively expensive for most researchers. This paper discusses Amazon Mechanical Turk as an innovative alternative to conducting fieldwork, and provides detail on the kinds of things to consider when using this new tool."
  },
  {
    "objectID": "blog/mturk/index.html#data-collection-methods",
    "href": "blog/mturk/index.html#data-collection-methods",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "Data Collection Methods",
    "text": "Data Collection Methods\nBefore discussing why any new methodology is even worth considering, it is prudent to briefly summarize the various ways in which dialectologists have collected data to show that techniques have evolved with technology several times.\nPetyt (1980) explains that one of the earliest dialectology project in modern times began in 1877 in Düsseldorf, Germany. With the help of government funding, Gerog Wenker mailed questionnaires to every village that had a school, asking teachers to “translate” 40 sentences into the local dialect. A staggering 52,000 questionnaires were returned, but only a fraction of them were published before the project was discontinued in 1956.\nTowards the end of the 19th century, Jules Gilliéron headed the new Atlas Linguistique de la France (Gilliéron 1902), and recruited Edmond Edmont to do the fieldwork. In what might have been the most serene sociolinguistic fieldwork ever, middle-aged Edmont bicycled across France and French-speaking Europe for eight years, documenting the language of 639 rural areas. The questionnaire eventually included 1900 items, arranged in a semi-conversational structure. Later researchers in Europe would improve on this format all the way through the 1970s.\nComparing the two projects, Petyt (1980) concludes that while the German survey outstrips the French project in geographic coverage, Gilliéron’s is superior in the amount of data per person. In addition, German schoolteachers were required to use a modified orthography to convey pronunciation differences while Edmont recorded utterances phonetically. Using the postal service had its advantages though: it saved time, money, and personnel.\nIt was around the mid-twentieth century that linguists began taking advantage of developing technology and started using portable recording equipment in fieldwork. The Linguistic Atlas of New England (Kurath 1939) was completed too early for audio recording, but interviews after 1950 in the Linguistic Atlas of the Middle and South Atlantic States (McDavid, Jr. et al. 1980) and all of both the Linguistic Atlas of the Gulf States (Pederson, McDaniel & Adams 1986) and the Dictionary of American Regional English (Cassidy 1985) were recorded.\nThe use of this technology provided numerous benefits over traditional pencil-and-paper transcription. First, it made it so the interviewer did not have to carefully transcribe during the interview but instead could be an active participant in the conversation. It allowed for transparency in data collection as well as preservation of the audio so that one could double-check the fieldworker’s transcriptions. It also made it possible for acoustic analysis of the audio rather than relying on transcriptions. Finally, it allowed for the analysis of other portions of the interview, or “collateral data” (Antieau 2017), besides the targeted lexical items, even if this analysis occurs decades later (see, for example, Olsen et al. 2017). However, the number of fieldworkers and the time commitment required for even small-scale dialectology projects was still an inhibiting factor in data collection (which was now coupled with the added cost of recording equipment).\nBy the 1980s, technology had advanced to allow linguists to record interviews that took place over the telephone. This was a major breakthrough and was a new way to cut costs associated with data collection. Labov (1984) describes procedures used for an early telephone survey, showing that most aspects of an in-person interview (a short questionnaire, grammaticality judgements, words lists, and minimal pairs) could be done over the phone. Labov, Ash, & Boberg (2006:36) explain that collecting data via telephone offers the advantage of collecting large amounts of data without the cost of sending fieldworkers all over the country. Convenience comes at the expense of audio quality, but they explain that their analysis was satisfactory and mostly comparable to in-person interviews. This comparison is remarkably similar to the early dialectology projects in Europe: Wenker’s questionnaires provided more data but were less reliable while Gilliéron’s sample was smaller but more accurate.\nToday, the internet provides nearly limitless data and dialectologists are using innovative techniques for data collection. For example, the primary instrument in the Harvard Dialect Survey (Vaux & Golder 2003) was a questionnaire distributed it over the internet rather than through the mail (see also Katz 2016). Boberg (2016) collected data by teaming up with a popular newspaper in Canada in order to study preferences between Canadian and American spelling. And Grieve (2015) compiled a corpus of more than 200,000 letters to the editor to study regional variation in written American English.\nThe advantage of these projects is that it is relatively easy to collect an enormous amount of data, even for a single researcher, in relatively little time. However, similar to using telephones and questionnaires in previous projects, this comes at the expense of quality. Furthermore, it is difficult to gather a large amount of data, including metadata, from any one person, making individual-level analysis limited, if possible at all.\nThe other major limitation is that it is difficult to collect audio data by scraping the web or distributing questionnaires online. This makes intuitive sense: because text a simpler data format, it is much easier to extract and analyze with automatic methods. It certainly is possible to use existing recordings to compile a corpus: Stanley & Renwick (2016) analyze the speech of one speaker’s religious sermons over several decades using 39 hours of publically available recordings. However, as is true of any convenience sample, the biggest challenge is to build an audio corpus that is representative and evenly sampled across a population.\nIn-person fieldwork will likely always be the gold standard, but an alternative that is less time- and resource-intensive for gathering audio in dialectology is needed. Like distributing questionnaires, a method that does not require the researcher to be physically present could facilitate audio collection in a short period of time. One possible solution to this problem is Amazon Mechanical Turk."
  },
  {
    "objectID": "blog/mturk/index.html#what-is-amazon-mechanical-turk",
    "href": "blog/mturk/index.html#what-is-amazon-mechanical-turk",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "What is Amazon Mechanical Turk?",
    "text": "What is Amazon Mechanical Turk?\nAmazon Mechanical Turk (hereafter “MTurk”) is a US-based crowdsourcing internet marketplace owned by Amazon. It is a platform for businesses and individuals (“requesters”) to post tasks called Human Intelligence Tasks (“HITs”) for workers to complete. Typical HITs are simple tasks that cannot (yet) be automated like completing surveys, tagging images, data cleanup, writing product descriptions, and transcribing audio. Workers complete these for a small monetary payment (called a “reward”). In exchange for their services, the site charges requesters an additional 20%–40% of the worker’s compensation for each task.\nRequesters have full control their HIT. They determine how many workers they would like to perform the task. They set the compensation amount, based on the amount of work required to complete the task. They also give a maximum amount of time allotted for the task (perhaps one hour for very short tasks or a day for longer ones). With justification, they can reject payment for a particular worker if the task is incomplete or dissatisfactory or they can reward workers with extra money for a job well done. Finally, requesters can set up their tasks so that only workers who meet the qualifications can see the HIT. Filters can target specific age groups, ethnicities, genders, and—critical for dialectology—geographic areas (specific countries or US states).\nFrom the worker’s side, MTurk is more straightforward. After creating a free MTurk account, workers can browse existing HITs and complete whatever tasks they choose. When they finish a HIT, they are provided with a completion code that is unique to them and/or the HIT. Requesters use this to link workers to data and to prevent spoofers from getting paid. Within a short time, typically less than three days, workers receive payment, which can be transferred to a bank account.\nWhile a HIT can be created using the MTurk interface, it is common to provide workers with a link to a third-party survey distributor. A popular distributor is Qualtrics , which allows researchers to create surveys with a wide variety of customizable features. While providing analyses of their own, the data and metadata collected on these surveys are available for download in spreadsheet form, allowing the researcher to perform their own analysis.\nIn summary, MTurk is an online space where people are willing to complete small tasks for payment. Unlike other kinds of studies, this makes recruiting very easy: willing workers are ready and are waiting for tasks. Linguists can take advantage of this platform to find participants for their own studies faster and easier than through many other ways of recruitment."
  },
  {
    "objectID": "blog/mturk/index.html#mturk-for-linguistics-research",
    "href": "blog/mturk/index.html#mturk-for-linguistics-research",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "MTurk for Linguistics Research",
    "text": "MTurk for Linguistics Research\nThe field of linguistics has already benefited from MTurk. Gibson, Piantadosi, Fedorenko (2011) show how the platform can be used to gather grammaticality judgements (see also Sprouse 2011). Karttunen (2014) provides evidence for semantic interpretations of sentences using MTurk workers. Wood et al. (2015) successfully used MTurk workers for acceptability judgements in non-standard syntactic constructions.\nKim et al. (2016) was perhaps the first to collect audio using MTurk in order to dialect features. As a supplement to answers from questionnaires that asked about regional lexical items and phonological features, a separate HIT was set up to collect audio. Focusing on New Englanders, they asked workers to read 12 sentences twice each and paid them $2–$4. The researchers received roughly 10–15 recordings a day from the relatively small targeted geographic area, though it included major urban centers such as Boston, Hartford, and Providence. After two months, they were able to collect audio from around 800 participants. Processing this much audio takes time, but drawing from just 390 speakers (52,418 vowel tokens), they show that their sample contained fine-grained sociophonetic variation in New England comparable to previous research using traditional methods.\nIn June 2017, I adopted a similar methodology and collected audio from MTurk workers in the western United States. Aiming for more data from each speaker, two similar HITs were set up: the first asked participants to read 132 sentences and a 288-item word list and the second had 125 sentences and a 260-item word list. Each of these tasks paid workers $4.50. Not all workers completed both tasks, but in less than four weeks I was able to collect 84 hours of audio from 212 speakers in the targeted area, providing me with 619,738 vowel tokens.\nThese studies show that researchers have already started taking advantage of MTurk for linguistic studies. In particular, a staggering amount of audio can be gathered for dialectology using this platform. The question is to consider whether this is good data and whether this technique is a viable option for linguistic research."
  },
  {
    "objectID": "blog/mturk/index.html#technical-aspects-of-mturk",
    "href": "blog/mturk/index.html#technical-aspects-of-mturk",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "Technical Aspects of MTurk",
    "text": "Technical Aspects of MTurk\nAs with any data collection technique, MTurk has its pros and cons. The benefits are obvious: it is easy to collect audio from many speakers in a specific geographic area in a short amount of time. With that said, it is not free: a budget of at least several hundred dollars to a couple thousand is required to collect a large sample, depending on the task. But traveling to interview hundreds of people spread over New England—or especially the West—is expensive, certainly more than a few dollars per person which is what an MTurk study would cost. In addition to being cheaper, using MTurk is faster than traditional methods and can be done entirely on a computer by a single researcher. However, this efficiency is not without its drawbacks. The following sections describe some of the limitations of using an MTurk study that researchers should be aware of before when considering a project using this platform."
  },
  {
    "objectID": "blog/mturk/index.html#demographics",
    "href": "blog/mturk/index.html#demographics",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "Demographics",
    "text": "Demographics\nProper sampling procedures are crucial when designing a study. If the sample is not representative of the population, the results are not generalizable. This section considers who MTurk workers are generally, how they compare to the general population, and issues with demographics on MTurk.\nThe majority of MTurk workers are based in India or the United States (Ross et al. 2010; Pavlick et al. 2014). This is good news for dialectologists wanting to study language in those two countries because it is easy to find workers there. There are workers in many other countries, but they are far less common, making MTurk-based dialectological work outside of these two major countries less feasible. Fortunately, as was mentioned above, researchers can set up HITs so that they are visible only to workers whose IP addresses locate them in the targeted area.\nSpatially, within the United States, workers are relatively representative of the country’s population distribution. Most workers come from urban centers but some come from very rural areas. In my sample of 212 people across the western United States, all major cities were represented and I was still able to get some workers from Wyoming, Montana, and South Dakota (none, however, came from North Dakota). If the researcher wishes to have representation in rural areas of the United States, it may take several months to get sufficient coverage and a large sample is required, but it is possible.\nHow is this geographic information gathered? MTurk has information on users’ IP addresses to allow for filtering by location, but this information is not available to requesters. The simplest way is to include a survey question that asks for current location, where they grew up, or residential history. Another option is to use some sort of online survey distributor such as Qualtrics, which collects participant’s IP addresses by default. But workers must be made aware in consent forms if this information is to be collected.\nFocusing just on the United States, compared to the national average, workers tend to be lower in socioeconomic status, more educated, younger (μ = 31.6 years), less representative of ethnic minorities (particularly African Americans), less likely to be married, and significantly more liberal and Democratic (Levay, Freese & Druckman 2016 and sources within). Because of the computer-based nature of the platform, workers are presumably literate and more comfortable with computers, which means older and people lower in socioeconomic status are probably underrepresented. These trends have implications for the kinds of conclusions that are drawn from MTurk studies, assuming all data is pooled together. However, if a sufficiently large sample is gathered, a balanced subset that controls for certain demographic factors can be analyzed instead, which can still reasonably be generalized to the non-MTurk population.\nAs always, demographic data from MTurk must be taken with a grain of salt. Unfortunately, some workers will try to cheat their way through the task in order to get paid in less time. Chandler & Paolacci (2017) found that workers will falsify their own information in order to get through prescreening questions, so putting an exact description of the required qualifications is not recommended . IP addresses can be spoofed as well, meaning geolocation is not always completely reliable. For user-submitted recordings, it’s necessary to listen to the audio to make sure workers completed the task properly before approving payment.\nThus, an audio sample collected from MTurk workers is, strictly speaking, not representative of all Americans. Only a very carefully designed random sampling procedure can accomplish this, which is becoming more difficult with increasing mobility (Bailey 2017). Together with the restriction to reading tasks, this demographic bias should be carefully considered when drawing conclusions from MTurk studies."
  },
  {
    "objectID": "blog/mturk/index.html#types-of-tasks",
    "href": "blog/mturk/index.html#types-of-tasks",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "Types of Tasks",
    "text": "Types of Tasks\nWhile MTurk makes it easy to reach a particular group of people, there are limitations to the types of tasks they can completed. Questionnaires that include grammaticality judgements, intuitions of rhyming pairs, and lexical choices for certain concepts can be easily adapted to an MTurk task. But the kind of audio that can be collected using MTurk is limited.\nCollecting recordings of spontaneous, naturalistic, or conversational speech is probably not feasible in this setting. First, another person is required, meaning the worker must be involved in recruiting that second person, and there is no guarantee that ethical recruitment procedures (as determined by the researcher’s institution’s ethics board) will be followed or that informed consent will be given. Even if the researcher asks the worker to talk to themselves for a few minutes, this is a very different speech style than what many sociolinguists consider naturalistic data.\nThus, researchers are essentially limited to reading tasks to collect audio, but there are several different kinds that can still be done. Workers can read carefully crafted stories, paragraphs, sentences, or word lists to collect many tokens of some variable. Workers can also be asked to read minimal pairs, which can complement intuition data collected in a later portion of the survey.\nThis restriction to reading tasks is a limitation of MTurk and should be taken into consideration when designing a study and interpreting its results. For example, Stanley & Vanderniet (2018) found that non-mainstream consonantal variants were infrequent and hypercorrect forms were more common in MTurk data. The research question determines the methodology, and MTurk may not be appropriate for those that approach non-standard language."
  },
  {
    "objectID": "blog/mturk/index.html#audio-quality",
    "href": "blog/mturk/index.html#audio-quality",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "Audio Quality",
    "text": "Audio Quality\nAn obvious drawback to using MTurk for audio is that the sound quality is completely uncontrolled. Substandard audio can still provide meaningful results (Labov, Ash & Boberg 2006; Kim et al. 2016), but MTurk audio will never approach laboratory data. This makes it even more imperative to perform procedures such as normalization on vowel data when making interspeaker comparisons to tease out these speaker and recording differences (Rathcke et al. 2017)\nIn my study, I asked participants to use an external microphone if they had one and to specify the model. Some workers reported using decent condenser microphones (Blue Yeti, Blue Snowball, Samson C03U) or gaming headsets, all of which provided clean audio. Others used the microphone on iPhone earbuds or some other inexpensive external microphone. But several recordings were noisy, very quiet, or (especially if they used the built-in microphones on their laptops) punctuated with clicks on the keyboard and taps on the trackpad.\nNot only is it important to consider the microphone being used, but users have to some sort of recording software as well since MTurk does not have a way to record audio. Kim et al. (2016) provided instructions for QuickTime or similar built-in computer software and asked workers to use those. In an attempt to control for quality and file formats, I asked users to use the free recording software Audacity and provided instructions for downloading, installing, and using it. This software records with a sampling frequency of 44,100 Hz with 32-bit depth by default, even if the microphone cannot match those specifications, ensuring at least consistency in the recorder even if the microphones are all different. Importantly, the instructions showed users how to save the audio as a .WAV file. However, having to set up a rudimentary “recording studio” is an atypical amount of prep work for a HIT: some users were not comfortable using new computer software it was a source of much frustration.\nAs an alternative, researchers could consider whether workers should just use their smartphones as recording devices. Admittedly, this is probably easier for the workers in some ways, but makes it more difficult on the analyst’s end. First, this assumes that all workers have smartphones, which is not guaranteed. It may be easier to record, but it takes time to transfer recordings from phones to computers and explicit instructions must be given for different phone-computer combinations. Finally, the formatting is inconsistant across smartphones and likely not one that is immediately useable for linguistic research. For example, the Voice Memo app on iPhones records in a .M4A format and is compressed. This must be converted to .WAV or some other format to be useable in most linguistic software which necessarily means some loss in fidelity. The recording quality of smartphones has continued to improve in recent years, but currently it is probably better to use a computer-based recording device for MTurk studies. Having any control over audio quality is difficult and is a primary concern for MTurk-based audio. Not all workers have good recording equipment, and it is a challenge to control software and formatting. With that said, the audio is not unusable, and with the volume of data available on MTurk, clear patterns can be found among the noise."
  },
  {
    "objectID": "blog/mturk/index.html#file-uploads",
    "href": "blog/mturk/index.html#file-uploads",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "File Uploads",
    "text": "File Uploads\nJust as MTurk does not have a way to record audio, it does not have a file-upload system either, so any study that requires user-submitted audio needs a way to get the files from the workers’ computers to the researcher’s. Kim et al. (2016) linked workers to a custom designed webpage that had recording and uploading capabilities. A custom site like this takes considerable preparation for the researchers, but allows complete control over all the data gathered from the workers.\nLacking the skills to build such a site, I provided users a link to a Qualtrics survey for uploading files. Currently, the option to allow users to upload files is an add-on feature and does not come standard on the basic Qualtrics license. Even with the feature, individual files must be relatively small (no more than a couple minutes of uncompressed audio), but currently there is no storage limit imposed on a Qualtrics account or survey, meaning all audio for the project can be safely stored on their servers.\nIt is possible to get an entire recording session in a single file (especially if software like Audacity is used), but the problem is uploading such a large file. If a worker’s internet connection is slow, this takes a lot of their valuable time. Kim et al. (2016) asked users to record, save, and upload one sentence at a time (there were 12 sentences total). Having over 100 sentences in my HITs, this was not feasible. Instead, I asked workers to record ten sentences at a time (corresponding to one page of the provided script). Again, this meant more work and tedium for the workers, which was reflected in their compensation.\nIt is unusual for Qualtrics surveys to include a large amount of uploaded data, so as a safety precaution, I also asked users to submit their files using the cloud-based file transferring site WeTransfer. This site is free and easy to use: users simply drag and drop files onto the webpage (up to 2GB for free—more than enough for a single person’s audio), put in sender and recipient email addresses, and submit the form. The site then takes the files and store them on their server for a week. Immediately after uploading and submitting the form, the sender and recipient get emails with a link to download the files. Since it is against MTurk policy to collect user emails, I set up a sender and a recipient email account and asked users to use those emails instead . This not only protected their anonymity but ensured me with two copies of the files in case of typos or some other error.\nThese extra precautions were worth the extra time: some files failed to upload in Qualtrics and others failed to come via WeTransfer. Between the Qualtrics uploads and the two copies from WeTransfer, I fortunately did not lose any audio. Whatever method is chosen as a file upload system should be reliable so that workers’ time is not wasted."
  },
  {
    "objectID": "blog/mturk/index.html#organization",
    "href": "blog/mturk/index.html#organization",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "Organization",
    "text": "Organization\nEven if all the files are received by the researcher, any large project necessarily requires some degree of organization or else data can be lost. Dialectology projects often have dozens or hundreds of speakers each with various recordings and some amount of metadata. Organization is especially important for MTurk projects because audio, speaker metadata, HIT metadata, MTurk IDs, and completion codes must all be linked together.\nA lot of organization relies on careful tracking of worker IDs, which are unique to each worker and are available to requesters. In my project, I asked workers to give their files a specific name that included their MTurk ID (e.g. “ID12345678_1-10.wav”) before uploading them. This created a link between audio files and the specific worker. This also connected workers to their metadata provided in the Qualtrics survey since these uniquely named audio files were uploaded as a part of their survey.\nAnother thing to consider is the completion codes, the key that links a worker to the completed task when requesters approve payment. Theoretically, codes can be anything the requester sets up, including words, numbers, or a random string of characters. These should be randomly generated in some way or else people will be able to provide a “correct” completion code without actually doing the task.\nQualtrics makes it easy to provide completion codes at the end of a survey using a random number generator. This number is saved in the downloadable Qualtrics spreadsheet and thus links it to the audio, metadata, and worker ID. When it comes time to approve payment, it is then easy to see whether the MTurk ID and completion code match what is on the Qualtrics spreadsheet.\nFinally, all this should be connected to the metadata about the HIT itself. Most of the metadata on the HIT, such as start and completion times, is not useful for the researcher. But it does contain unique IDs for the HIT itself which is required when asking MTurk to help resolve disputes with workers.\nKeeping track of all this information is not easy or straightforward. There are several spreadsheets that need to be maintained, and both update frequently as workers complete the tasks. Currently, the MTurk website is not the most intuitive interface, though in 2017 an API was launched which is accessible through the MTurkR package in R (Leeper 2017), making it easier to organize HITs, approve payment, and communicate with workers through the programming language R (R Core Team 2014)."
  },
  {
    "objectID": "blog/mturk/index.html#the-ethnics-of-mturk-workers",
    "href": "blog/mturk/index.html#the-ethnics-of-mturk-workers",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "The Ethnics of MTurk Workers",
    "text": "The Ethnics of MTurk Workers\nThough they are anonymous, MTurk workers are people. Most do not rely on income from completing tasks (Ross et al. 2010), but they deserve fair pay for their time. As with any study on human subjects, researchers must clearly explain what the task will require them to do, what the estimated time is, and what their compensation will be. Recording audio requires workers to have a microphone. It also implies that they need to be in a semi-private setting and not in a public library or similar place. Installing software requires access to a computer with administrator privileges. To get a gauge of how long the task would take to complete, I ran a pilot study on some MTurkers and found that it took about twice as long as my original estimates. (I then gave those who participated in the pilot study a bonus.)\nMost workers are motivated to do a good job. This is in part because requesters can always deny payment if a task is not completed or done properly. There is no reason for them to invest time into doing something wrong and not get payment. This motivation comes from the fact that some requesters make their HITs available only to workers who have had a history of many approved payments, and that a rejected HIT is permanently on a worker’s profile and can exclude them from other tasks. The majority of workers for a single project will complete the job as requested with no additional work on the researcher’s part.\nHowever, no matter how much compensation you offer or how long your estimated time is, there will be workers who feel like they were taken advantage of. If they are less comfortable with computers, then using (and installing) software will take much longer, as well as saving and uploading files. Some people have trouble reading out loud and will take longer to make the recordings. There will be technical errors on your end, their end, and with MTurk. Some workers may want to spread the task over two days, which will cause the HIT to time out. One worker asked if his wife could complete the task as well, so I had to temporarily lift the ban on multiple submissions from the same IP address. Though communication between the two parties is limited, workers are able to send these concerns to requesters, but the tone can be harsh.\nMTurk has ways of dealing with some of these issues. For example, upon completion of a task, requesters can offer a bonus to workers that do an especially good job. This can be used as incentive to complete the task on a case-by-case basis if specific workers feel like they are underpaid. It is also possible to set up “dummy HITs” that are only good for a specific worker ID as a way to pay people because of technical errors. It is recommended that part of the budget for an MTurk project be set aside for rewards and other extra payment to workers. When it comes to working with your participants, the 80-20 rule apples: most will take minimal effort, but some will take a lot of extra attention.\nBetween keeping track of the various spreadsheets of metadata, ensuring good audio quality, file organization, and resolving concerns with MTurk workers, it is nearly a full-time job for me for at least a week or two for this kind of project. Researchers wishing to use MTurk should be aware of the time commitment required to essentially be the accountant and human resources of a large project."
  },
  {
    "objectID": "blog/mturk/index.html#conclusions",
    "href": "blog/mturk/index.html#conclusions",
    "title": "Data Collection in Dialectology using Amazon Mechanical Turk",
    "section": "Conclusions",
    "text": "Conclusions\nThe gold standard for dialectology research is in-person fieldwork because the researcher has complete control over the sampling, demographics, recording equipment, and can collect different kinds of styles besides reading tasks. However, this is prohibitively time-intensive and expensive for most researchers. An alternative is to use Amazon Mechanical Turk for collecting audio.\nThis paper has outlined some of the positive and negative aspects of MTurk. Most notably, this platform provides a way to get recordings of speech from a specific geographic area without the researcher having to travel to the field site. Though there are costs associated with paying workers and fees to MTurk, it can be considerably cheaper to collect this data since the cost there are no costs associated with travel or equipment.\nThe most obvious negative aspect of MTurk-based audio is the sound quality. It is near-impossible to control for recording equipment and to ensure audio fidelity. However, just as meaningful results can be drawn from questionnaires (Boberg 2016) and telephone interviews (Labov, Ash & Boberg 2006), MTurk data can be used for meaningful dialectology research as well (Kim et al. 2016; Stanley & Vanderniet 2018).\nAmazon Mechanical Turk is not the solution to all problems in dialectology research, but it is a viable alternative to traditional methods. As tech-nology advances, linguistic methodology should follow closely behind, and linguists should be aware of the strengths, weaknesses, assumptions, and techniques associated with each new tool. Amazon Mechanical Turk is a feasible option for collecting audio data for linguistic analysis and should be considered when a researcher lacks the means for a full-scale dialectology project."
  },
  {
    "objectID": "blog/full-house-at-my-first-latex-workshop/index.html",
    "href": "blog/full-house-at-my-first-latex-workshop/index.html",
    "title": "Full house at my first LaTeX workshop!",
    "section": "",
    "text": "Today I had the opportunity to teach LaTeX for the first time. Caleb Crumley, an RA for the DigiLab at UGA, has been working on a dissertation template in LaTeX that conforms with UGA’s formatting check. He finished it, and it’s got the stamp of approval from the Graduate School. To advertise the template, Caleb, Jonathan Crum, and I put on a three-part series to introduce the template and teach a little LaTeX as well.\nAs always, marketing these workshops is tricky and it’s rare to get more than about a dozen attendees. But, with some persistence from the DigiLab’s coordinator, Emily McGinn, the Grad School agreed to advertise a little for us, and sent out the information to the approximately 6000 grad students at UGA!\nWithin minutes, we had dozens of people registering for the workshop. After day or two, we doubled, and then tripled the registration cap! Sure enough, the DigiLab was fuller than I had ever seen it, and we decided to host repeat sessions of the three workshops as soon as the first cycle is over.\n\n\n\nAction shot of me showing how to make a bulletted list!\n\n\nKnowing there would be so many people there, I was a little nervous prepping the workshop because I’m not as comfortable with LaTeX as I am with R. I’ve been dabbling with it for a couple year but I’ve only been using it seriously for about a year.  All things considered, I think it went well, and I enjoyed it a quite a lot.I began using it when I switched my dissertation over from Word. It took about 20 hours to do so, but seriously, best decision ever.\n\n\nThis'll be my first time teaching LaTeX, but I had a lot of fun putting this workshop together with @CalebCrumley and Jonathan Crum. The materials for today's workshop can be found at https://t.co/uyGNAq1EMe https://t.co/uhnz7sptRz\n\n— Joey Stanley (@joey_stan) January 31, 2020\n\n\nYou can see more details about the workshop series here, the handout for the workshop on my github, and the UGA LaTeX dissertation template that Caleb made on the DigiLab’s github."
  },
  {
    "objectID": "blog/randomizing-a-wordlist/index.html",
    "href": "blog/randomizing-a-wordlist/index.html",
    "title": "Randomizing a Wordlist",
    "section": "",
    "text": "A few weeks ago I did some fieldwork in Utah and used a wordlist as part of my data collection. I’ve needed to compile wordlists several times in the past and have never been completely satisfied with how they’ve been randomized. I thought I’d share what I did today.\n\nBackground\nA lot of my research has to do with mergers involving infrequent lexical classes, such as /ʊl/ in words like pull or bull or /eɡ/ in words like vague and flagrant. These words do come up in conversation, but usually not enough for a robust, person-level statistical analysis. For this reason, I often supplement my interviews with relatively extensive word lists (150+ words), so that they contain enough tokens from each lexical class for a meaningful analysis.\nBecause I often target lots of mergers/shifts in any one project, I usually have words from at least a dozen or so lexical classes all interspersed in the wordlist. To put them in some order that distracts speakers from the underlying questions, what I’ve done in the past is use Excel’s rand() function to put the words in a random order. Then I’d go through the list myself to find anything I didn’t like, like pairs of words from the same lexical class. But inevitably, when I transcribe them and listen to the list over and over, there’s always some sequence of words I wish I could have changed.\nSo, for this trip to Utah, I decided do a smart randomization process using R so that I can be sure it’s how I want it to be.\n\n\nThe dataset\nI won’t go into detail about exactly how I compiled my wordlist, but I’ll share the variables I’ve included. Because I’m working on Utah English, I’ve got the cord-card merger, various pre-lateral mergers (feel-fill, fail-fell, pool-pull, etc.), and a handful of other consonantal things like words like mountain, [t]-epenthesis in words like false (see my ADS presentation with Kyle Vanderniet in January), and a couple small things I’d like to test out. In total, I’ve got 17 lexical classes, grouped into three broad questions: vowels before /r/, vowels before /l/, and “other.”\n\n\nThe goal\nI recently started watching the TV show Numb3rs and this actually came up in the first episode. (It’s interesting and on Netflix, so you should see it for yourself). He asks a group of people to randomly place themselves in the room. Once they did, he pointed out that the way they were standing was not random because they were roughly equidistant from each other. Truly random distributions will produce some clusters. So there’s a difference between truly random and what humans perceive as random.\nTo bring this back into wordlists, if I randomly sort the words in Excel, I will get a (close to) truly random order. But this inevitably creates clusters, or words from the same lexical class near or next to each other, which is undesirable. What I actually want is a pseudo-random order that humans perceive to be as random but actually has quite a bit of order to it.\nSpecifically, here are the criteria that I came up with for an ideal wordlist order:\n\nAdjacent words should not contain vowels from the same lexical class. Better yet, there should be a buffer of at least two words between vowels.\nReally, I don’t even want adjacent words within the same research question (pre-laterals, pre-rhotics, etc.) next to each other. And even if they’re close I don’t want them to be similar vowels.\nAdjacent words starting with the same letter or sounds (no alliteration).\nAdjacent words shouldn’t end with the same morpheme (no rhyming).\n\nI don’t know if these restrictions has any effect on pronunciation, but I don’t want to take any chances, nor do I want people to catch on to what I’m studying.\n\n\nThe solution\nThe first step to all this is to prepare the data. In my spreadsheet, I’ve got columns for the word, what lexical class it belongs to, and what broad research question it targets. But now I need to add columns such as vowel height, vowel backness, syllable structure, and morphological structure. The others I can easily get through formulas in R. The point is that I need columns for all the restrictions I want to control for.\nOnce that is done, I came up with this workflow for the script:\n\nIf there no words yet, pick a random word to start.\nCompile a list of all the words that haven’t been used in the wordlist yet.\nFilter out the ones that shouldn’t be next based on the criteria specified.\nOf the remaining candidates, pick a random one.\nRepeat until all the words are chosen.\n\nWhat I found though was that the conditions were often too restrictive: when there was about 10 or 20 words left, there wouldn’t be any remaining words eligible to come next. So I wrapped this whole process into another loop and ran it until it “converged” and all words were used (it took about 50 times).\n\n\nThe code\nBefore the loop, I declare a few variables. Disclaimer/Apology: writing loops brings be back to my Perl days, so this might not be the most efficient R code.\nlibrary(tidyverse)\n\n# Read the data in.\nwordlist &lt;- original &lt;- read_csv(...) %&gt;%\n\n    # Add a column to keep track if it's been used yet.\n    mutate(used_yet = FALSE,\n           # Extract first and last letters\n           first_letter = str_extract(.$word, \"^.\"),\n           last_three   = str_extract(.$word, \"...$\"))\n\n# While this is true, keep the loop going.\nkeep_going &lt;- TRUE\n\n# Keep track of the number of attempts needed to finally converge.\nattempt &lt;- 1\n\n# Keep the attempted orders in a list, just in case none converge.\ntries &lt;- list(1:100)\nNow comes the loop itself. This main loop theoretically only needs to run once, but if the randomizing doesn’t come up with a complete list, it’ll jump back up to here again and start over from scratch.\nwhile(keep_going) {\nFirst, there are some things I need to do for each attempt, like randomly choosing the first word.\n    # Create an empty list to keep track of the new order of words.\n    newlist &lt;- list(1:nrow(wordlist))\n    \n    # Populate the first row right here. It's just easier to to do it here than to incorporate it into the later loop.\n    next_word &lt;- wordlist %&gt;% sample_n(1)\n\n    # Go back to the original list and mark the \"used_yet\" column as true.\n    # This keeps track of what words already have a place in the new order.\n    wordlist[wordlist$word == next_word$word,]$used_yet &lt;- TRUE\n\n    # This first word is now saved as the first entry in the new wordlist.\n    newlist[[1]] &lt;- next_word\nNow that we’ve got the first word set, start another loop that continues as long as there are words that haven’t been used yet. In other words, this’ll run for as many iterations as you have words total.\n    # The nth iteration of the loop. 2 because we're looking for the second word.\n    i &lt;- 2\n\n    # Start the loop. Loop until all the words are used (= marked as TRUE).\n    while (FALSE %in% wordlist$used_yet) {\nNow once we’re here, we compile a list of potential words.\n        # Get the previous word so I know what I'm working with\n        prev_word &lt;- newlist[[i-1]]\n        \n        # Find a list of potential words\n        potential_words &lt;- wordlist %&gt;%\n            \n            # Only unused words\n            filter(used_yet == FALSE,\n                   \n                   # Can't start with the same first letter\n                   first_letter != prev_word$first_letter)\nNow I actually filter out this list of potential words based on various criteria I’ve come up with. The code is slightly more complex because not every word has properties that I care about (for example, in a word that focuses on the consonants elicited, I’m not too concerned about the vowel quality) so there are NAs. So I need to first see if the previous word in the list is an NA. If it’s not, then I need to select from only the words that do not share that property, or are also NAs. Does that makes sense?This came back to bite me and I only realized it until after the data collection was over. I included the word scale to get at /e/ before /l/, so it was tagged as a pre-lateral word. I also had the word whale to get a (wh)-aspiration. Since I didn’t care about the vowel qualities of words included for their consonants, sure enough scale and whale ended up next to each other. Figures.\nHere, I do this three times. If the previous word has a pre-lateral vowel, the next one can too, just not with the same vowel. Or a word without a pre-lateral vowel is okay too. Same thing with syllabic/morphological structure (the context column). And same thing with the last three letters of the word.\n        # Filter by lateral vowel\n        if (!is.na(prev_word$lateral_vowel)) {\n            potential_words &lt;- potential_words %&gt;%\n                filter(lateral_vowel != prev_word$lateral_vowel | is.na(lateral_vowel))\n        }\n        \n        # Filter by context\n        if (!is.na(prev_word$context)) {\n            potential_words &lt;- potential_words %&gt;%\n                filter(context != prev_word$context | is.na(context))\n        }\n\n        # Filter by ending (I don't want two -ing words in a row)\n        if (!is.na(two_words_ago$last_three)) {\n            potential_words &lt;- potential_words %&gt;%\n                filter(last_three != prev_word$last_three | is.na(last_three)) %&gt;%\n        }\nSome filters I want to be longer distance than just adjacent words. I don’t want words in the same lexical class to be near each other at all: specifically, there should be at least three filler words between any words within the same class. Because of the way my code is written, I can’t run this chunk unless I’m in the third or fourth iteration of my loop (there’s probably a better way to do this, but I’m fine with this), so I’ve got a condition around the whole thing blocking it until we’re sufficiently far into the iterations.\n        # Put three fillers in a row between the same lexical class\n        if (i &gt;= 4) {\n            # Like \"prev_word\", save the info about two and three words ago.\n            three_words_ago &lt;- newlist[[i-3]]\n            two_words_ago   &lt;- newlist[[i-2]]\n\n            # The variable shouldn't match any of these\n            potential_words &lt;- potential_words %&gt;%\n                filter(variable != prev_word$variable,\n                       variable != two_words_ago$variable,\n                       variable != three_words_ago$variable)\n\n        # A shorter version if we're on the third iteration of the loop.\n        } else if (i &gt;= 3) {\n            two_words_ago &lt;- newlist[[i-2]]\n            potential_words &lt;- potential_words %&gt;%\n                filter(variable != prev_word$variable,\n                       variable != two_words_ago$variable)\n\n        # And even shorter if we're on the second iteration.\n        } else {\n            potential_words &lt;- potential_words %&gt;%\n                filter(variable != prev_word$variable)\n        }\nSo that’s the filtering that I’ve done. Now I need to see if there are any words left. If there aren’t any, exit the loop. This list has failed.\n        # If there aren't any left, cut our losses: this list is a bust.\n        if (nrow(potential_words) == 0) { break }\nI had code for a while where it selected the next word alphabetically instead of randomly. This was good for debugging and let me see how it chose the next word. To do this, replace sample_n(1) with arrange(word) %&gt;% head(1).\nAt this point, we only make it this far in the loop if there are any potential words left. This’ll happen most of the time. It’s here that we select a random word.\n        # Get the next word\n        next_word &lt;- potential_words %&gt;%\n            sample_n(1)\nNow that we’ve selected a word, go back to the original dataframe and mark this word as used so that it doesn’t come up as a potential word again. Save this word as the next item in the word list, and increment the counter.\n        # Mark the next word as selected\n        wordlist[wordlist$word == next_word$word,]$used_yet &lt;- TRUE\n        \n        # Add it to the list\n        newlist[[i]] &lt;- next_word\n        \n        # Increment the counter\n        i &lt;- i+1\n\n    } # End the inner while loop. Go on to find the next word or exit if we're done.\nWe’re done with this inner while loop. We exit the loop either because we’re done and all the words are in the new wordlist, or because we broke early because there were no more potential words that fit all the criteria.\nTo finish processing, first we have to use bind_rows() to get this list of one-row dataframes into a single dataframe.\n    newlist &lt;- bind_rows(newlist) %&gt;%\n        # Also, get rid of the \"used_yet\" column since it's TRUE for all words.\n        select(-used_yet)\nThen, we see if we need to continue looping. If we’ve already made 100 attempts and no complete list was found, break out and end: I don’t want to loop for infinity if no order is even possible. If the list contains all the words, that means we’re done: set keep_going to false to stop the outer while loop. Otherwise (meaning, not all the words are included but we haven’t hit 100 attempts), save the list (for future examination), send a message, and increment the attempts counter. The loop starts all over again and a new list is created from scratch.\n    if (attempt &gt; 100) {\n        break\n    } else if (nrow(newlist) == nrow(wordlist)) {\n        keep_going &lt;- FALSE\n    } else {\n        tries[[attempt]] &lt;- newlist\n        message(\"Attempt \", attempt, \"\\t\", nrow(newlist))\n        attempt &lt;- attempt + 1\n    }\n} # end of the main loop. We're done!\nOnce we’re done, newlist should contain the exact same information as wordlist, only in a pseudo-randomized order that you controlled. If it didn’t converge, you can use anti_join to see what words didn’t get selected.\nanti_join(wordlist, newlist)\nAnd that’s it! You’ve got yourself a nice wordlist in a great order.\n\n\nJust one list though?\nNow ideally, every participant would see the same list of words in a different random order. This makes sure that fatigue and other factors don’t have an effect. But I find it best to distribute this wordlist on paper, and it’s kind of a pain to keep track of dozens of different orders. Crucially, you have to keep track of what list people are assigned to because if a speaker has a near-merger, it’s really hard to tell which word is which unless you have their “script” handy as you transcribe.\nPerhaps it would be nice to have speakers read these on a tablet. A fancy app that kept track of randomly generated orders would be cool, but a quick and dirty solution is to have dozens of PDFs ready and labeled, and the speaker would read the code at the top to identify which order they’re saying. In that case, the method I’ve described in this post would certainly apply: you would just need to run it a couple dozen times to get different lists.\nAs a side note, I ended up producing just one list, mostly because it took so long to converge on one possible order. That ended up hurting me because I noticed that speakers paid more attention for the first dozen words or so and said things differently. In the future, I would recommend different orders for each person.\n\n\nConclusion\nGoing through my list, I was impressed with just how random the words seemed. Like the episode of Numb3rs, what I perceived to be more random actually had a lot of structure to it and was deliberately devoid of clusters. Previous lists I’ve used were had close to a truly random order, but this one seems even better. It’s much harder to tell that these words come from a relatively small set lexical classes. I think the key is that similar words aren’t near each other, which is not what you get with a random number generator. I think this guided pseudo-random order is a better way to go for eliciting linguistic data in a wordlist."
  },
  {
    "objectID": "blog/barktools/index.html",
    "href": "blog/barktools/index.html",
    "title": "barktools: Functions to help when working with Barks",
    "section": "",
    "text": "I’m happy to announce that I’ve just released another small R package called barktools. Now that I’ve got one R package out there already, I’ve sort of caught the bug and realized it’s kinda fun to put these small packages out there. This one is just a lightweight little guy that I thought up a few days ago while falling asleep that’ll help me when working with Barks. You can download the package from my GitHub."
  },
  {
    "objectID": "blog/barktools/index.html#load-the-data",
    "href": "blog/barktools/index.html#load-the-data",
    "title": "barktools: Functions to help when working with Barks",
    "section": "Load the data",
    "text": "Load the data\nFor this little vignette, I’ll load some sample vowel data from my own speech. It’s got some outliers, but that’s the nature of automatically-processed data.\n\nlibrary(tidyverse)\nvowels &lt;- read_csv(\"http://joeystanley.com/data/joey.csv\", show_col_types = FALSE)\n\nHere’s a simple plot that shows my vowel space. For now, I’ll just keep it simple.\n\nggplot(vowels, aes(F2, F1)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_minimal() + \n  labs(title = \"Joey's vowels\",\n       subtitle = \"Data and plot are in Hz\")\n\n\n\n\nNow, let’s say I want to plot using Barks. You can use the bark() function to convert the formant frequencies into Barks.\n\nvowels_with_barks &lt;- vowels %&gt;%\n  mutate(F1_bark = bark(F1),\n         F2_bark = bark(F2))\n\nggplot(vowels_with_barks, aes(F2_bark, F1_bark)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  theme_minimal() + \n  labs(title = \"Joey's vowels\",\n       subtitle = \"Data and plot are in Barks\")\n\n\n\n\nThe Bark scale turns the nonlinear Hz data into something a little more linear, so the shape of the vowel space should change somewhat.\nThe problem is most people can’t readily interpret the Barks unit. What is the Hz equivalent of 6 Barks? We can look this up using the hz() function:\n\nhz(6)\n\n[1] 631.1045\n\n\nBut it would be better if we could incorporate more interpretable values into the plot itself. I think the first time I saw this was in Harrington et al’s (2000) paper on how the Queen of England’s speech changes over time:\n\nNotice how the axes are in Barks, but the data is still plotted in Hz. This is a perfect case for using the scale_x_bark() and scale_y_bark() functions. Like the other scale_* functions in ggplot2, this will transform the axes of your plot. In this case, it’ll convert the plotting area to the Bark scale, but the values will be in Hz still.\n\nggplot(vowels, aes(F2, F1)) + \n  geom_point() + \n  scale_x_bark() + \n  scale_y_bark() + \n  theme_minimal() + \n  labs(title = \"Joey's vowels\",\n       subtitle = \"Data is in Hz; plot is in Barks\")\n\n\n\n\nNow, you can see that the shape of the vowel space is identical to the plot above, except the axis labels are more useful: I have a better idea of what 500Hz means. Note that the axes are reversed as well, just like scale_*_reverse.\nAt this point, it might be useful to modify the axes with some additional labels. Since scale_*_bark is just a wrapper around scale_*_continuous, any argument that you would normally include in the latter function will work just fine in the bark function. Specifically, I’ll modify which values get labels with breaks and the gridlines with minor_breaks.\n\nggplot(vowels, aes(F2, F1)) + \n  geom_point() + \n  scale_x_bark(breaks = c(c(500, 1000, 1500, 2000, 3000)),\n               minor_breaks = seq(0, 4000, 100)) +\n  scale_y_bark(breaks = c(c(200, 400, 600, 800, 1000, 1500)),\n               minor_breaks = seq(0, 3000, 100)) + \n  theme_minimal() + \n  labs(title = \"Joey's vowels\",\n       subtitle = \"Data is in Hz; plot is in Barks\")\n\n\n\n\nExactly which values you want to put is up to you, obviously, so play around with it until it looks good."
  },
  {
    "objectID": "blog/barktools/index.html#spectrogram-plots",
    "href": "blog/barktools/index.html#spectrogram-plots",
    "title": "barktools: Functions to help when working with Barks",
    "section": "Spectrogram plots",
    "text": "Spectrogram plots\nThe other type of plot you might want to use scale_y_bark for is something that looks like a spectrogram, that is a time-by-hz plot. You’ll have to transform the data a little bit. You can use the code that I provided in my tutorial with the new pivot_longer function in dplyr. I’ll just pull out my /aʊ/ vowel for this plot:\n\nvowels_long &lt;- vowels %&gt;%\n  filter(vowel == \"AW\") %&gt;%\n  select(contains(\"@\")) %&gt;%\n  rowid_to_column(\"phoneme_id\") %&gt;%\n  pivot_longer(cols = contains(\"@\"), \n               names_to = c(\"formant\", \"percent\"), \n               names_pattern = \"(F\\\\d)@(\\\\d\\\\d)%\", \n               names_ptypes = list(formant = factor(levels = c(\"F1\", \"F2\"))), \n               names_transform = list(percent = as.integer),\n               values_to = \"hz\") %&gt;%\n  unite(traj_id, phoneme_id, formant, remove = FALSE)\n\nHere’s what a spectrogram-like plot might look like\n\nggplot(vowels_long, aes(percent, hz, color = formant, group = traj_id)) + \n  geom_path(alpha = 0.5) +\n  theme_bw()\n\n\n\n\nNow, a lot of the change in /ai/ happens along the F1 dimension, but because of the logarithmic nature of sound, F2 visually takes up most of the vertical space and F1 is sort of squished down at the bottom. We can emphasize F1 by transforming the y-axis into the Bark scale.\n\nggplot(vowels_long, aes(percent, hz, color = formant, group = traj_id)) + \n  geom_path(alpha = 0.5) +\n  scale_y_bark(rev = FALSE) + \n  theme_bw()\n\n\n\n\nNote that this time, I added the argument rev = FALSE to scale_y_bark. By default, the function will flip the axis (like scale_y_reverse), but in this case that behavior is not desired. So, you can suppress that flip by specifying rev = FALSE."
  },
  {
    "objectID": "blog/barktools/index.html#conclusion",
    "href": "blog/barktools/index.html#conclusion",
    "title": "barktools: Functions to help when working with Barks",
    "section": "Conclusion",
    "text": "Conclusion\nAnd that’s it! That’s the whole package. I thought it would be a useful thing for me. Perhaps you’ll find some use for it too."
  },
  {
    "objectID": "blog/secol2017/index.html",
    "href": "blog/secol2017/index.html",
    "title": "SECOL 2017",
    "section": "",
    "text": "I was unable to attend this year, but my colleagues presented two papers I was a part of at the 84th Southeastern Conference on Linguistics (SECOL84) in Charleston, South Carolina.\nThe first presentation was with Bill Kretzschmar and Katie Kuiper and was called “Automated Large-Scale Phonetic Analysis: DASS” wherein we introduce the NSF-funded project—with Drs. Kretzschmar and Peggy Renwick as PIs—that I am involved in. A PDF of the slide show is are available here.\nThe second presentation was with Rachel Olsen, Mike Olsen, and Peggy Renwick and was called “Transcribing the Digital Archive of Southern Speech: Methods and Preliminary Analysis” wherein we talked about the nuts and bolts of how to get a project of this size running. A PDF of the slide show is available here.\nThis work is part of an ongoing project at the Linguistic Atlas Office at the University of Georgia. We have several hundred hours of recordings from the 1970s of speakers all across the South. We have around 40 undergraduate workers transcribing for us with another couple grad students (including myself) doing the less soul-sucking work. Eventually we’ll have all this data freely available online, but in the meantime we’re figuring out how to process such scratchy recordings and doing linguistic analysis on it. It’s been a lot of fun and I’m glad we were able to show others our work."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-2/index.html",
    "href": "blog/making-vowel-plots-in-r-part-2/index.html",
    "title": "Making vowel plots in R (Part 2)",
    "section": "",
    "text": "Note\n\n\n\nThis post was written in 2018 and used code that was up-to-date at the time. In November 2023, I updated the code to reflect some changes in tidyverse.\nThis is Part 2 of a four-part series of blog posts on how to make vowel plots in R. In Part 1, we looked primarily at how to plot individual data points as a scatterplot. This time, I’ll focus entirely on trajectory data, that is, formant measurements per vowel at multiple points along its duration. Today, I’ll cover three things: how to prepare FAVE output for trajectory plots, plotting trajectories in the F1-F2 space, and in the time-Hz space (like what you see in Praat). For both kinds of plots, we’ll see how to show all tokens as well as averages per vowel.\nAs a sociolinguist working on English vowels, I work with FAVE data a lot and I know lots of other people do too. So for this tutorial to be applicable to as many other people as possible, I’ll work directly with FAVE output. This way, you can apply the code here to your own data and hopefully get the same results. If you use Praat to extract your own formant measurements, some of the data wrangling may not directly apply to you.\nFinally, I should mention that in the first tutorial I downplayed the data processing because I didn’t need to do much. Today though, data processing is crucial for making these kinds of plots. I’ll try to explain what I’m doing along the way, but if you haven’t seen this kind of code before, it might be worth it to peruse R for Data Science, which is free online. In fact, even if you have seen this code before, I’d highly recommend reading the book. It’s a good one."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-2/index.html#read-in-the-data",
    "href": "blog/making-vowel-plots-in-r-part-2/index.html#read-in-the-data",
    "title": "Making vowel plots in R (Part 2)",
    "section": "Read in the data",
    "text": "Read in the data\nWe’ll work with the same dataset that I used last time: a recording of me reading about 300 sentences while sitting at my kitchen table.\n\nmy_vowels_raw &lt;- read_csv(\"http://joeystanley.com/data/joey.csv\", show_col_types = FALSE) \n\n\n\n\nI’ve already removed most of the really bad outliers, but there are a few things I still need to do to get this data ready. First, I’ll keep just the vowels with primary stress and remove /ɹ/ with the filter function. Then, I’ll make all the words lowercase with mutate. I’ll also create a new column that just numbers the rows sequentially which will give each vowel it’s own unique identifier. For simplicity, I’m also going to remove some columns that we don’t need for today—or rather, keep just the columns I need—using select. All I want for now are the ID I just created, the vowel, the word, and all the F1@20%, F2@20%, F1@35%, etc. columns. This last group are formant measurements at 20%, 35%, 50%, 65%, and 80% into the vowels’ durations. Since R doesn’t like those non-alphanumeric characters in the column names though, you’ll have to use ticks (`: that thing that shares a key with the tilde on my keyboard) around it: select(vowel, word, t, F1@20%:F2@80%). These five points give us the trajectory data we need. (I’ll toss out the plain F1, F2, and F3 columns because the time that those measurements come from varies from vowel to vowel.)If you read your data in using read.csv() instead of readr::read_csv(), it converts the original column names (F1@20%, etc.) to something like F1.20., with periods instead of the non-alphanumeric characters.\n.Also, you can also use the time (the t column) as the unique id instead of creating new ones, but I like creating sequential numbers better.\n\nmy_vowels &lt;- my_vowels_raw %&gt;%\n  filter(stress == 1, \n         vowel != \"ER\") %&gt;%\n  mutate(word = tolower(word)) %&gt;%\n  rowid_to_column(\"id\") %&gt;%\n  select(id, vowel, word, `F1@20%`:`F2@80%`) %&gt;%\n  print()\n\n# A tibble: 544 × 13\n      id vowel word       `F1@20%` `F2@20%` `F1@35%` `F2@35%` `F1@50%` `F2@50%`\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1 AW    without        483     1518.     594.    1392.     605.    1210 \n 2     2 AA    todd           775.    1337.     740.    1162.     615.    1065.\n 3     3 AE    last           638.    1513.     643.    1445      614.    1005.\n 4     4 EY    places         391     1664.     391     1664.     363.    1810.\n 5     5 EH    guest          468.    1719.     473.    1712.     533     1614.\n 6     6 IY    sleeping       301.    1596.     262.    2216.     254.    2336.\n 7     7 EH    collectors     512     1317      512     1317      530.    1378.\n 8     8 EY    snakes         372.    1856.     469.    1829.     409.    1609.\n 9     9 IY    feeds         1388.    2206.     257.    2035.     253.    1896.\n10    10 IH    trigger        347.    1602.     348.    1607.     353.    1690.\n# ℹ 534 more rows\n# ℹ 4 more variables: `F1@65%` &lt;dbl&gt;, `F2@65%` &lt;dbl&gt;, `F1@80%` &lt;dbl&gt;,\n#   `F2@80%` &lt;dbl&gt;\n\n\nAs a review, here’s what this looks like with a scatterplot of the formants at the midpoints.\n\nggplot(my_vowels, aes(x = `F2@50%`, y = `F1@50%`, color = vowel)) + \n  geom_point() + \n  stat_ellipse(level = 0.67) + \n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\nOkay, so we’ve got some data. Let’s figure out how we can plot these as trajectories."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-2/index.html#data-processing-1",
    "href": "blog/making-vowel-plots-in-r-part-2/index.html#data-processing-1",
    "title": "Making vowel plots in R (Part 2)",
    "section": "Data processing",
    "text": "Data processing\nFor now, let’s just focus on one vowel, my /aɪ/ vowel, since there is some trajectory there.\n\nay &lt;- my_vowels %&gt;%\n  filter(vowel == \"AY\") %&gt;%\n  print()\n\n# A tibble: 52 × 13\n      id vowel word     `F1@20%` `F2@20%` `F1@35%` `F2@35%` `F1@50%` `F2@50%`\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1    15 AY    flight       524.    1109.     535.    1174      511.    1355.\n 2    29 AY    insiders     894.    1662.     730.    1526.     514.    1389 \n 3    31 AY    sites        608.    1498.     442.    1366.     498.    1498.\n 4    36 AY    likely       532.    1120.     576.    1260.     548.    1466.\n 5    40 AY    like         443.     978.     579.    1424.     517     1697.\n 6    58 AY    like         526.    1363.     526.    1487.     484.    1702.\n 7    67 AY    hydrogen     693.    1538.     704.    1568.     492.    1776.\n 8    79 AY    right        447.    1117.     538.    1253.     466.    1555.\n 9    82 AY    michael      214.    1007.     378.     977.     682.    1224.\n10    87 AY    right        432.     970.     566.    1169.     653.    1439.\n# ℹ 42 more rows\n# ℹ 4 more variables: `F1@65%` &lt;dbl&gt;, `F2@65%` &lt;dbl&gt;, `F1@80%` &lt;dbl&gt;,\n#   `F2@80%` &lt;dbl&gt;\n\n\nAnd we’ll plot it for good measure.\n\nggplot(ay, aes(x = `F2@50%`, y = `F1@50%`)) + \n  geom_point() + \n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nBut, midpoints for diphthongs don’t say as much as their entire trajectories do. We have the trajectory data, but it’s spread out across all those columns (F1@20%, etc.). The ggplot function can only plot two columns at a time (the x- and y-axes), but our trajectory data is spread out over ten columns. We need to somehow reshape our data so that all the F1 data is in one column and all the F2 data is in another column.\nWhat we need to do is essentially squish our data to cram 10 columns’ worth of information into just two. Our data currently is in what’s called a “wide” format because we have lots of columns. What we need to do is convert into a “tall” format, which has fewer columns, but more rows. For example, here’s a simplified version of what our data looks like now:\n\n##    vowel F1%20% F1@80% F2@20% F2@80%\n##    &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 vowel1    500    550   1000   1150\n## 2 vowel2    600    650   1500   1550\n## 3 vowel3    700    750   2000   2050\n\nAnd this is what we want it to end up like:\n\n##    vowel percent    F1    F2\n## *  &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 vowel1      20   500  1000\n## 2 vowel1      80   550  1150\n## 3 vowel2      20   600  1500\n## 4 vowel2      80   650  1550\n## 5 vowel3      20   700  2000\n## 6 vowel3      80   750  2050\n\nSo how do we do that? Unfortunately, because we have F1 and F2 measurements, it takes a bit more finagling than one simple function can provide, but I’ll walk you through it.\n\n\n\n\n\n\nNote\n\n\n\nEdit: tidyr has been updated since this tutorial was posted. For more elegant and powerful code that accomplishes the same thing that this section covers, but based on a newer version of tidyr, see this blog post.\n\n\nThey key to this is to use the pivot_longer function from the tidyr library. This function—it’s like black magic, I swear—takes multiple columns and condenses them down into just two. This function has two required arguments:\n\nThe first argument of pivot_longer is cols, which is what columns you would like to condense down. By default, it does all of them, but we want to keep the vowel, word, and t columns. You could type F1@20%, F2@20%, F1@35%, F2@35%, F1@50%, F2@50%, F1@65%, F2@65%, F1@80%, F2@80%, but that’s a lot. Instead, I’ll just use the shortcut starts_with from dplyr. Since all those columns start with \"F\", this works out well.\nThe second argument of gather is names_to, which is the arbitrary name of a new column that will contain the various column names you’ll be condensing down. Since the names of columns we want to gather are F1@50%, F2@50%, etc. I’ll call this new column formant_percent since it will have information about what formant at what percent into the vowel’s duration we’re on.\nThe third argument is values_to, which is the name of a new column that you want to create that will contain all the values in the columns you want to combine together. Since all the cells in the F1@50%, F2@50%, etc. columns currently have formant measurements in Hertz, I like to call this column hz.\n\nSo the code ends up looking like this:\n\nay %&gt;%\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\")\n\n# A tibble: 520 × 5\n      id vowel word   formant_percent    hz\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;\n 1    15 AY    flight F1@20%           524.\n 2    15 AY    flight F2@20%          1109.\n 3    15 AY    flight F1@35%           535.\n 4    15 AY    flight F2@35%          1174 \n 5    15 AY    flight F1@50%           511.\n 6    15 AY    flight F2@50%          1355.\n 7    15 AY    flight F1@65%           444.\n 8    15 AY    flight F2@65%          1550.\n 9    15 AY    flight F1@80%           386.\n10    15 AY    flight F2@80%          1655.\n# ℹ 510 more rows\n\n\nOkay, so even though there are just 52 tokens of /aɪ/, now we have 520 rows in our dataframe. That’s because each vowel is now spread across ten rows, one for each of the ten formant measurements. It might not be clear right now that that’s what it did, but we can sort the data by ID and you can get a better picture:\n\nay %&gt;%\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  arrange(id)\n\n# A tibble: 520 × 5\n      id vowel word   formant_percent    hz\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;\n 1    15 AY    flight F1@20%           524.\n 2    15 AY    flight F2@20%          1109.\n 3    15 AY    flight F1@35%           535.\n 4    15 AY    flight F2@35%          1174 \n 5    15 AY    flight F1@50%           511.\n 6    15 AY    flight F2@50%          1355.\n 7    15 AY    flight F1@65%           444.\n 8    15 AY    flight F2@65%          1550.\n 9    15 AY    flight F1@80%           386.\n10    15 AY    flight F2@80%          1655.\n# ℹ 510 more rows\n\n\nThere we go. Now it’s clearer that the ten formant measurements from this one word are now spread out in ten rows instead of the original ten columns.\nSo, this is closer to what we wanted, but we actually squished it a little too much! We actually want F1 and F2 to be in separate columns, and right now they’re all in one.\n\nThis is a more transparent, but more cumbersome method. Again, see this blog post for instructions on how to reduce these three lines into one.\n\nSo one way to do this is to split up the formant_percent column into two: formant and percent. It’s strange to have two pieces of information (what formant and the timepoint) in one column. We can use the separate_wider_delim function for this. As its first argument, we’ll tell it what column we want to separate (here, it’s formant_percent). Then, we’ll tell is where to split it, which is at the “@” symbol. Then, we’ll add the names argument, which is just a list of new, arbitrary column names that the split data will now be in, which for us is \"formant\" and “percent\".\n\nay %&gt;%\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\"))\n\n# A tibble: 520 × 6\n      id vowel word   formant percent    hz\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1    15 AY    flight F1      20%      524.\n 2    15 AY    flight F2      20%     1109.\n 3    15 AY    flight F1      35%      535.\n 4    15 AY    flight F2      35%     1174 \n 5    15 AY    flight F1      50%      511.\n 6    15 AY    flight F2      50%     1355.\n 7    15 AY    flight F1      65%      444.\n 8    15 AY    flight F2      65%     1550.\n 9    15 AY    flight F1      80%      386.\n10    15 AY    flight F2      80%     1655.\n# ℹ 510 more rows\n\n\nThat worked pretty well! However, we’ll need to trim off the extra “%” symbol at the end of the new percent column, which we can do using str_remove.\n\nay %&gt;%\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\"))\n\n# A tibble: 520 × 6\n      id vowel word   formant percent    hz\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1    15 AY    flight F1      20       524.\n 2    15 AY    flight F2      20      1109.\n 3    15 AY    flight F1      35       535.\n 4    15 AY    flight F2      35      1174 \n 5    15 AY    flight F1      50       511.\n 6    15 AY    flight F2      50      1355.\n 7    15 AY    flight F1      65       444.\n 8    15 AY    flight F2      65      1550.\n 9    15 AY    flight F1      80       386.\n10    15 AY    flight F2      80      1655.\n# ℹ 510 more rows\n\n\nOkay, so now we have one column saying what formant the measurement is for, and another column saying the timepoint. We need to reverse the squish a little bit, and put the formants into two columns. So the opposite of pivot_longer is pivot_wider, and as arguments, you first tell it what column contains the text that will serve as the column names, and then what column contains the numbers you want to be in the cells of those new columns.\n\nay %&gt;%\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  pivot_wider(names_from = formant, values_from = hz)\n\n# A tibble: 260 × 6\n      id vowel word     percent    F1    F2\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1    15 AY    flight   20       524. 1109.\n 2    15 AY    flight   35       535. 1174 \n 3    15 AY    flight   50       511. 1355.\n 4    15 AY    flight   65       444. 1550.\n 5    15 AY    flight   80       386. 1655.\n 6    29 AY    insiders 20       894. 1662.\n 7    29 AY    insiders 35       730. 1526.\n 8    29 AY    insiders 50       514. 1389 \n 9    29 AY    insiders 65       553. 1538 \n10    29 AY    insiders 80       514. 1484.\n# ℹ 250 more rows\n\n\nViolà! Our data is ready. Now each row represents a single time point, and there is one column for F1 and one column for F2.\nDo you remember why we bothered to do all that in the first place? Well, because ggplot needs two columns (F1 and F2) for the x- and y-axes. But our measurements were spread out over 10 columns. So we collapsed them down to two. This is the dataset we want to stick with, so we’ll save it into a new dataframe called ay_tall.\n\nay_tall &lt;- ay %&gt;%\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  pivot_wider(names_from = formant, values_from = hz)\n\nSo now comes the fun part. Our data is set, now we just need to work in ggplot2."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-2/index.html#drawing-lines-in-the-f1-f2-space",
    "href": "blog/making-vowel-plots-in-r-part-2/index.html#drawing-lines-in-the-f1-f2-space",
    "title": "Making vowel plots in R (Part 2)",
    "section": "Drawing lines in the F1-F2 space",
    "text": "Drawing lines in the F1-F2 space\nThe function that we need is geom_path. We’ll use this instead of geom_point, which is what we used for scatterplots. To save you from scrolling, here’s the code for the scatterplot:\n\nggplot(ay, aes(x = `F2@50%`, y = `F1@50%`)) + \n  geom_point() + \n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nThe last two lines transfer over just fine, so we can leave them. But we need to make a few changes to the beginning portion. First, we change the dataset from ay to ay_tall. We’ll then have to change F2@50% and F1@50%—since we no longer have those columns—into just F2 and F1 which are the corresponding columns in ay_tall. We can change geom_point to geom_path and bada-boom!\n\nggplot(ay_tall, aes(x = F2, y = F1)) + \n  geom_path() + \n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nOops. That looks awful. So what happened? As it turns out, geom_path literally just connected the dots in the order that they appear in the dataset. It drew one continuous line because we didn’t tell it to do otherwise. What we need is an additional argument, the group argument, that says to draw one line for every group. So what should the groups be? Well, we want one line per token, so we want a column that has a unique vowel per token. Aha! This is why we created that id column at the beginning! So in the aes function, add group = id and then let’er rip.\n\nggplot(ay_tall, aes(x = F2, y = F1, group = id)) + \n  geom_path() + \n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nOkay, so still not great. It’s just because we have a lot of data. I’ll filter the data down to just the tokens of me saying the word “time”:\n\nlike_tokens &lt;- ay_tall %&gt;%\n  filter(word == \"like\")\nggplot(like_tokens, aes(x = F2, y = F1, group = id)) + \n  geom_path() + \n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nOkay, so less messy. But still not great. For one, we can’t even tell which direction the lines are going. We can add some arrows with the arrow argument in geom_path. I’ll let you look up the help for arrows (hint: ?grid::arrow), but here is some code that has worked for me. It says to put an arrow at the end of the line (as opposed to the beginning), make it a filled triangle, and make it a tenth of an inch in size.\n\nggplot(like_tokens, aes(x = F2, y = F1, group = id)) + \n  geom_path(arrow = arrow(ends = \"last\", \n                          type = \"closed\",\n                          length = unit(0.1, \"inches\"))) +\n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nSo we’re getting there. Let’s see if we can add some color. Maybe I can make the arrows go from one color to another as they progress along the vowel’s trajectory. We can add another aesthetic, color, which will vary with the values in the percent column.\n\nggplot(like_tokens, aes(x = F2, y = F1, group = id, color = percent)) + \n  geom_path(arrow = arrow(ends = \"last\", type = \"closed\", length = unit(0.1, \"inches\"))) +\n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nSince we created that percent column using pivot_longer and separate_wider_delim and stuff, even though they’re numbers it’ll treat them as factors in a categorical variable. You can easily convert it into numeric using as.numeric in mutate. I’ll also change the colors to make it more striking using scale_color_distiller (see more info on this at colorbrewer2.org):\n\nThe problem now is that you’ve got arrowheads at the end of each segment. Four lines per vowel means four arrowheads. I haven’t quite figured out how to get just one arrowhead when you use color like this.\n\n\nlike_tokens &lt;- like_tokens %&gt;% \n  mutate(percent = as.numeric(percent))\nggplot(like_tokens, aes(x = F2, y = F1, group = id, color = percent)) + \n  geom_path(arrow = arrow(ends = \"last\", type = \"closed\", length = unit(0.1, \"inches\"))) +\n  scale_x_reverse() + scale_y_reverse() +\n  scale_color_distiller(palette = \"YlOrBr\") +\n  theme_classic()\n\n\n\n\nSo, we’ve seen how to make trajectory plots with one line per token. As you can in in my data, there’s a bit of noise. This is something I’ve noticed for a lot of the trajectory data I gather. So in the next section, we’ll see how to do essentially the same thing, except we’ll look at averages so we get one line per vowel."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-2/index.html#trajectories-for-all-vowels",
    "href": "blog/making-vowel-plots-in-r-part-2/index.html#trajectories-for-all-vowels",
    "title": "Making vowel plots in R (Part 2)",
    "section": "Trajectories for all vowels",
    "text": "Trajectories for all vowels\nSo maybe you’re not interested in individual tokens because your plots look like spaghetti, and would rather look at all the tokens for that vowel averaged together. No problem. We’ll create a new dataset that will just contain the averages for each vowel at each time point. This will involve a bit more wrangling, but the payoff is worth it.\n\nData wrangling\nSo let’s think about what kind of dataset we want. Starting with my_vowel, which is essentially the raw FAVE output, we have ten columns for each vowel. We want to reduce this down to just 14 rows (one for each vowel), with the average formants at each time point. To accomplish this, we can use summarize. This is a handy function that will help you, well, summarize your data. For example, let’s say we want to get the average F1 at the 20% point for the entire dataset. We can create a new column called mean_F1@20% and set it equal to the mean of the values in the F1.20. column:\n\nmy_vowels %&gt;%\n  summarize(`mean_F1@20%` = mean(`F1@20%`))\n\n# A tibble: 1 × 1\n  `mean_F1@20%`\n          &lt;dbl&gt;\n1          470.\n\n\nSimilarly, we can do this for all the columns:\n\nmy_vowels %&gt;%\n  summarize(`mean_F1@20%` = mean(`F1@20%`),\n  `mean_F2@20%` = mean(`F2@20%`),\n  `mean_F1@35%` = mean(`F1@35%`),\n  `mean_F2@35%` = mean(`F2@35%`),\n  `mean_F1@50%` = mean(`F1@50%`),\n  `mean_F2@50%` = mean(`F2@50%`),\n  `mean_F1@65%` = mean(`F1@65%`),\n  `mean_F2@65%` = mean(`F2@65%`),\n  `mean_F1@80%` = mean(`F1@80%`),\n  `mean_F2@80%` = mean(`F2@80%`))\n\n# A tibble: 1 × 10\n  `mean_F1@20%` `mean_F2@20%` `mean_F1@35%` `mean_F2@35%` `mean_F1@50%`\n          &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1          470.         1505.          468.         1492.          469.\n# ℹ 5 more variables: `mean_F2@50%` &lt;dbl&gt;, `mean_F1@65%` &lt;dbl&gt;,\n#   `mean_F2@65%` &lt;dbl&gt;, `mean_F1@80%` &lt;dbl&gt;, `mean_F2@80%` &lt;dbl&gt;\n\n\nUgh, that’s a lot of typing though, and it’s so repetitive. Isn’t there a shortcut? Yes! There’s a function called across, which will perform a function on all the columns you specify. First, you select the columns you want to summarize, which I’ll do with the shortcut starts_with(\"F\"). The next argument in across is the name of the function you want to perform on all of these. We want the average, so we’ll just type mean. You can add additional arguments to across, which get passed to the function we’re calling (mean), so for good measure I like to add na.rm = TRUE just to make the code a little more robust at handling NAs in our data.\n\nmy_vowels %&gt;%\n  summarize(across(starts_with(\"F\"), mean, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarize()`.\nℹ In argument: `across(starts_with(\"F\"), mean, na.rm = TRUE)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 1 × 10\n  `F1@20%` `F2@20%` `F1@35%` `F2@35%` `F1@50%` `F2@50%` `F1@65%` `F2@65%`\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     470.    1505.     468.    1492.     469.    1510.     453.    1547.\n# ℹ 2 more variables: `F1@80%` &lt;dbl&gt;, `F2@80%` &lt;dbl&gt;\n\n\nSo in just one line of code we can do what took us ten before. Thanks, across!\nBut wait. We don’t really want to know the average formant values for the entire dataset. What we really want is the average per vowel. How can we do the same thing per vowel? Well, luckily we can just add the .by argument (yes, with the dot at the beginning) to specify a column and it’ll essentially divide the dataset into chunks and the summarize function will apply to each chunk.\n\nmy_vowels %&gt;%\n  summarize(across(starts_with(\"F\"), mean, na.rm = TRUE), .by = vowel)\n\n# A tibble: 14 × 11\n   vowel `F1@20%` `F2@20%` `F1@35%` `F2@35%` `F1@50%` `F2@50%` `F1@65%` `F2@65%`\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 AW        532.    1340.     598.    1249.     602.    1171.     534.    1144.\n 2 AA        568.    1235.     610.    1142.     640.    1143.     657.    1168.\n 3 AE        557.    1575.     609.    1576.     644.    1506.     644.    1488.\n 4 EY        391.    1782.     391.    1841.     373.    1911.     345.    1973.\n 5 EH        483.    1605.     483.    1600.     511.    1583.     514.    1577.\n 6 IY        440.    1918.     325.    1982.     291.    2062.     290.    2086.\n 7 IH        392.    1658.     382.    1662.     400.    1713.     389.    1708.\n 8 AY        528.    1224.     579.    1255.     570.    1431.     495.    1624.\n 9 UH        456.    1316.     411.    1254.     397.    1256.     364.    1307.\n10 AO        589.    1136.     618.    1070.     605.    1052.     622.    1126.\n11 UW        368.    1595.     337.    1557.     324.    1481.     314.    1423.\n12 AH        491.    1291.     497.    1281.     526.    1268.     511.    1288.\n13 OW        439.    1207.     431.    1067.     418.     999.     401.    1035.\n14 OY        416.     996.     446.     900.     444.    1019.     425.    1298.\n# ℹ 2 more variables: `F1@80%` &lt;dbl&gt;, `F2@80%` &lt;dbl&gt;\n\n\nYou can actually add as many columns as you want to .by. This is good for getting averages per vowel split up by following segment, for example. For example, try doing by = c(vowel, fol_seg).\nNote that now we have a column we didn’t have before, vowel, which shows the group variable. Now we have the dataset we want: the average formant measurements at each time point for all 14 vowels. Let’s save this as a new dataframe.\n\nmy_vowel_means &lt;- my_vowels %&gt;%\n  summarize(across(starts_with(\"F\"), mean, na.rm = TRUE), .by = vowel)\n\nOkay, so we have this dataset. It’s essentially the parallel to my_vowels, only instead of individual tokens it has the averages. But, remember that we need to turn the data into a tall format for it to plot the way we want? So, we have to take this my_vowel_means data and turn it tall. Fortunately, you can copy and paste the code we used before:\n\nmy_vowel_means_tall &lt;- my_vowel_means %&gt;%\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  pivot_wider(names_from = formant, values_from = hz) %&gt;%\n  print()\n\n# A tibble: 70 × 4\n   vowel percent    F1    F2\n   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 AW    20       532. 1340.\n 2 AW    35       598. 1249.\n 3 AW    50       602. 1171.\n 4 AW    65       534. 1144.\n 5 AW    80       444. 1183.\n 6 AA    20       568. 1235.\n 7 AA    35       610. 1142.\n 8 AA    50       640. 1143.\n 9 AA    65       657. 1168.\n10 AA    80       636. 1228.\n# ℹ 60 more rows\n\n\nOkay cool. We’re now ready to plot.\n\n\nPlotting\nBecause we’ve been consistent with our code and variable names and stuff, the plotting code is also going to be very similar to what we saw before. All we need to do is change two things. First, we’ll change the data from like_tokens that we had earlier to to our new my_vowel_means_tall. Then, our group variable is now vowel instead of id because we want it to draw a separate line for each vowel.\n\nggplot(my_vowel_means_tall, aes(x = F2, y = F1, group = vowel)) + \n  geom_path(arrow = arrow(ends = \"last\", type = \"closed\", length = unit(0.1, \"inches\"))) +\n  scale_x_reverse() + \n  scale_y_reverse() +\n  theme_classic()\n\n\n\n\nBecause we have so many vowels, maybe we can add a little bit of color. Let’s have the color of the lines vary by the vowel as well:\n\nggplot(my_vowel_means_tall, aes(x = F2, y = F1, group = vowel, color = vowel)) + \n  geom_path(arrow = arrow(ends = \"last\", type = \"closed\", length = unit(0.1, \"inches\"))) +\n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nOkay, so now we have a trajectory plot. Now, if you look closely, some of this data might not be so good. For example, look at /i/, the blue line to the far left. It’s quite unlikely that my vowel actually starts that low. Perhaps there’s some bad data.\nAs it turns out, the problem was when we took the average. Because there are a few outliers in this data still, and because vowel formant outliers tend to be towards the low front portion of the vowel space, it’s pulling a lot of the measurements in that direction. The mean is quite sensitive to outliers. What we can try instead is to take the median, which is less sensitive.\nSo, the only change that we need to do is in across where we change the mean to median. In fact, we don’t even need to create a new object if you don’t want: we can pipe everything all at once, straight into ggplot! So in this code, I take the beginning dataset, create a summary of it, and then pipe it into ggplot. Note that when we pipe something into ggplot, you don’t need to specify the data anymore (the first argument), so I leave that blank.\n\nmy_vowels %&gt;%\n  # Summarize by vowel\n  summarize(across(starts_with(\"F\"), median, na.rm = TRUE), .by = vowel) %&gt;%\n  \n  # Turn it into tall data\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  pivot_wider(names_from = formant, values_from = hz) %&gt;%\n  \n  # Plot it!\n  ggplot(aes(F2, F1, color = vowel, group = vowel)) +\n  geom_path(arrow = arrow(ends = \"last\", type = \"closed\", length = unit(0.1, \"inches\"))) +\n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic()\n\n\n\n\nThe result is a much cleaner picture: /i/ and /e/ sort of hook to the left while /u/ and /o/ start centralized and move to the back. My diphthongs follow their expected trajectories by starting centralized and sort of scooping down and up again. While cot and caught are somewhat close at their onsets, they have very different trajectories, clearly differentiating the phonemes. Finally, /ɪ/ is quite monophthongal, showing very little change in trajectory at all.\nRight now, things are a little hard to read because there are 14 colors and a lot of them are very similar to one another. Also, the legend has the vowels in alphabetical order, which isn’t too helpful. In the previous tutorial, we went over how to change that, so I won’t comment on it much here. The only difference is I’ll set the factor levels using mutate in the dplyr package rather than using base R:\n\nmy_vowels %&gt;%\n  # Relevel the vowels (random order to spread out similar colors)\n  mutate(vowel = factor(vowel, levels = c(\"IY\", \"EH\", \"AO\", \"UW\", \n                                          \"AW\", \"IH\", \"AE\", \"OW\", \n                                          \"AH\", \"OY\", \"EY\", \"AA\", \"UH\", \"AY\"))) %&gt;%\n  \n  # Summarize by vowel\n  summarize(across(starts_with(\"F\"), median, na.rm = TRUE), .by = vowel) %&gt;%\n  \n  # Turn it into tall data\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  pivot_wider(names_from = formant, values_from = hz) %&gt;%\n  \n  # Plot it!\n  ggplot(aes(F2, F1, color = vowel, group = vowel)) +\n  geom_path(arrow = arrow(ends = \"last\", type = \"closed\", length = unit(0.1, \"inches\"))) +\n  scale_x_reverse() + scale_y_reverse() +\n  # Put the vowels in a meaningful order in the legend\n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \"AA\", \n  \"AO\", \"OW\", \"UH\", \"UW\", \n  \"AH\", \"AY\", \"AW\", \"OY\")) + \n  theme_classic()\n\n\n\n\nThis is still suboptimal. It would be best to just have the name of the vowel on the plot itself. Fortunately, we saw how to do that in the previous tutorial. We’ll have to create a different dataset and use it in geom_label. Instead of one mega string of piped functions, I’ll save the main dataset into a new dataframe called my_vowel_medians_tall. I’ll then use that but just select the onset (the 20% mark) to create labels_at_onset, a dataframe specifically for plotting the labels.\n\nmy_vowel_medians_tall &lt;- my_vowels %&gt;%\n  # Relevel the vowels\n  mutate(vowel = factor(vowel, levels = c(\"IY\", \"EH\", \"AO\", \"UW\", \n                                          \"AW\", \"IH\", \"AE\", \"OW\", \n                                          \"AH\", \"OY\", \"EY\", \"AA\", \"UH\", \"AY\"))) %&gt;%\n  \n # Summarize by vowel\n  summarize(across(starts_with(\"F\"), median, na.rm = TRUE), .by = vowel) %&gt;%\n  \n  # Turn it into tall data\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  pivot_wider(names_from = formant, values_from = hz)\n\nlabels_at_onset &lt;- my_vowel_medians_tall %&gt;%\n  filter(percent == 20)\n\nNow, when we go to plot it, we add the geom_label function and use the labels_at_onset as the dataframe. The F1, F2, and color columns still work, so all we need to add is an aesthetic for the label. I also took out scale_color_discrete since we’re removing the legend, which is done with theme(legend.position = \"none\").\n\nggplot(my_vowel_medians_tall, aes(F2, F1, color = vowel, group = vowel)) +\n  geom_path(arrow = arrow(ends = \"last\", type = \"closed\", length = unit(0.1, \"inches\"))) +\n  geom_label(data = labels_at_onset, aes(label = vowel)) + \n  scale_x_reverse() + scale_y_reverse() +\n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n\n\nSo that’s the end of this portion of the tutorial. We’ve seen how to wrangle your data to get it to work for drawing paths. We’ve seen how to draw all observations or how to summarize them and plot just one line per vowel. Pretty handy."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-2/index.html#plotting-all-data",
    "href": "blog/making-vowel-plots-in-r-part-2/index.html#plotting-all-data",
    "title": "Making vowel plots in R (Part 2)",
    "section": "Plotting all data",
    "text": "Plotting all data\nJust like we did above, we’ll start with plotting all tokens of /aɪ/, which is going to be a bit messy. Then we’ll see how to summarize the data and plot just the medians for each vowel.\nSo last time, we took my_vowels and compressed the 10 columns into 1, but that was too far so we used pivot_wider to spread them out into F1 and F2 columns. Well, as it turns out, to make this Praat-like plot, we want that super tall, compressed version.\nAll of this code you’ve seen before, so it should be relatively clear what is happening. We’ll start with the first dataset, my_vowel, only keep the tokens of “AY”, then we’ll smush all ten columns into one, separate out the formant and percent, and then sort it by the ID. We’ll save this as a new dataframe, ay_very_tall, because it’s a “very tall” version of the data.\n\nay_very_tall &lt;- my_vowels %&gt;%\n  filter(vowel == \"AY\") %&gt;% \n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  arrange(id) %&gt;%\n  print()\n\n# A tibble: 520 × 6\n      id vowel word   formant percent    hz\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1    15 AY    flight F1      20       524.\n 2    15 AY    flight F2      20      1109.\n 3    15 AY    flight F1      35       535.\n 4    15 AY    flight F2      35      1174 \n 5    15 AY    flight F1      50       511.\n 6    15 AY    flight F2      50      1355.\n 7    15 AY    flight F1      65       444.\n 8    15 AY    flight F2      65      1550.\n 9    15 AY    flight F1      80       386.\n10    15 AY    flight F2      80      1655.\n# ℹ 510 more rows\n\n\nOnce we’ve got this, plotting is pretty straightforward. For the x-axis, we want normalized time, which is in the percent column. For the y-axis, we want frequency in Hz, so we’ll use the hz column. We want to use geom_path again, and for maximal clarity, we’ll make the two formants different colors.\n\nggplot(ay_very_tall, aes(x = percent, y = hz, group = id, color = formant)) +\n  geom_line() + \n  theme_classic()\n\n\n\n\nAh, crud. What happened? As it turns out, we’re plotting one line for each token because the group variable is based on the id column, which has a unique value per token. That’s not actually what we want though, is it? When we did the F1-F2 plot, we wanted a single line per token, sure, but in this plot, we actually want two lines per token, the F1 and the F2. In order for ggplot to draw separate lines, you’re going to need a column that has a unique value for every line you want to draw. So we want a different line for each formant for each token. In other words, we need to create what I like to think of as a “formant id” column. (And yes, that means just a little bit more data wrangling.)\nThe way I like to do this is to use the unite function, which simply concatenates the values in multiple columns for each row. Since each token has its own id already, the id column, we can combine this with the formant in the formant column. To use unite, you first type the name of the new column you want to create (I did formant_id), then whatever columns you want to combine. By default, they will concatenate without any intervening material, but I like underscores, so I add sep = \"_\" to put an underscore between them. Also, by default, it’ll remove the contributing columns. We don’t really need the token id column anymore, but I like to keep the formant column so I can color the lines by formant, so we’ll add remove = FALSE to unite. Tag all this on to the end of the list of functions and you get the following block of code.\n\nay_very_tall &lt;- my_vowels %&gt;%\n  filter(vowel == \"AY\") %&gt;% \n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  arrange(id) %&gt;%\n  unite(formant_id, formant, id, sep = \"_\", remove = FALSE) %&gt;%\n  print()\n\n# A tibble: 520 × 7\n   formant_id    id vowel word   formant percent    hz\n   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 F1_15         15 AY    flight F1      20       524.\n 2 F2_15         15 AY    flight F2      20      1109.\n 3 F1_15         15 AY    flight F1      35       535.\n 4 F2_15         15 AY    flight F2      35      1174 \n 5 F1_15         15 AY    flight F1      50       511.\n 6 F2_15         15 AY    flight F2      50      1355.\n 7 F1_15         15 AY    flight F1      65       444.\n 8 F2_15         15 AY    flight F2      65      1550.\n 9 F1_15         15 AY    flight F1      80       386.\n10 F2_15         15 AY    flight F2      80      1655.\n# ℹ 510 more rows\n\n\nNow we can use the same ggplot2 code, but use the formant_id instead of id, and we get what we want:\n\nggplot(ay_very_tall, aes(x = percent, y = hz, group = formant_id, color = formant)) +\n  geom_line() + \n  theme_classic()\n\n\n\n\nAgain, messy data, but you can see the resemblance with how Praat shows the data. The general trend is kinda there though: the vowel starts with a high F1, gets slightly higher, and then comes down to form a higher vowel. Meanwhile, F2 starts slow and moves high, creating a back-to-front trajectory in the vowel space.\nIn the next section, we’ll summarize the data again by plotting just the median for each vowel and we can see how to plot all the vowels at once."
  },
  {
    "objectID": "blog/making-vowel-plots-in-r-part-2/index.html#plotting-summarized-data",
    "href": "blog/making-vowel-plots-in-r-part-2/index.html#plotting-summarized-data",
    "title": "Making vowel plots in R (Part 2)",
    "section": "Plotting summarized data",
    "text": "Plotting summarized data\nJust like last time, we’ll have to do some data wrangling to get what we want, but fortunately there’s nothing new in this section: it’s just applying code that we’ve seen before.\n\nmy_vowel_medians_very_tall &lt;- my_vowels %&gt;%\n  \n  # Take the median of each vowel at each timepoint\n  summarize(across(starts_with(\"F\"), median, na.rm = TRUE), .by = vowel) %&gt;%\n  \n  # Convert it into a \"very tall\" format\n  pivot_longer(cols = starts_with(\"F\"), names_to = \"formant_percent\", values_to = \"hz\") %&gt;%\n  separate_wider_delim(formant_percent, delim = \"@\", names = c(\"formant\", \"percent\")) %&gt;%\n  mutate(percent = str_remove(percent, \"%\")) %&gt;%\n  \n  # Create the formant id column\n  unite(formant_id, formant, vowel, sep = \"_\", remove = FALSE) %&gt;%\n  print()\n\n# A tibble: 140 × 5\n   formant_id vowel formant percent    hz\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n 1 F1_AW      AW    F1      20       514.\n 2 F2_AW      AW    F2      20      1322.\n 3 F1_AW      AW    F1      35       622.\n 4 F2_AW      AW    F2      35      1224.\n 5 F1_AW      AW    F1      50       605.\n 6 F2_AW      AW    F2      50      1179.\n 7 F1_AW      AW    F1      65       533.\n 8 F2_AW      AW    F2      65      1155 \n 9 F1_AW      AW    F1      80       461.\n10 F2_AW      AW    F2      80      1181.\n# ℹ 130 more rows\n\n\nAnd now we plot it. I’ll go ahead and manually change the order of the vowels too, like I did before, with scale_color_discrete.\n\nggplot(my_vowel_medians_very_tall, aes(x = percent, y = hz, color = vowel, group = formant_id)) +\n    geom_line() + \n    scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \"AA\", \n                                    \"AO\", \"OW\", \"UH\", \"UW\", \n                                    \"AH\", \"AY\", \"AW\", \"OY\")) + \n    theme_classic()\n\n\n\n\nThis looks a lot cleaner than the previous plot. But we’re still left with the same problem of just too many darn vowels. For this one what I like to do is to facet the plot. Essentially, have each vowel as its own tile, forming a bit of a mosaic. To do that, we use the facet_wrap function, and then after a tilde, the name of the column you want to facet the plot by.\n\nggplot(my_vowel_medians_very_tall, aes(x = percent, y = hz, color = vowel, group = formant_id)) +\n  geom_line() + \n  facet_wrap(~vowel) + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \"AA\", \n                                  \"AO\", \"OW\", \"UH\", \"UW\", \n                                  \"AH\", \"AY\", \"AW\", \"OY\")) + \n  theme_classic()\n\n\n\n\nHere, color and legend are superfluous and you can remove them if you want, but the color at least makes it look pretty I guess. This is a handy way to quickly compare the formants for all the vowels at once.\nIn fact, if you want to create a new column specifically meant for faceting, it’s relatively easy to and might make a more meaningful plot. Here, I create a new column called class, which tells whether the vowel is tense, lax, or a diphthong. I do this using the case_when function, which is essentially a way to create a stack of if-else-if statements. So I say, okay, if the vowel is one of these five, in the new class column, put the value as “tense”. If it’s one of these six vowels, call it “lax”, and if it’s one of those three call it a “diphthong.” The only change in the plot code is in the facet_wrap, where I take out vowel and put in this new class column.\n\nmy_vowel_medians_very_tall %&gt;%\n  mutate(class = case_when(vowel %in% c(\"IY\", \"EY\", \"AE\", \"OW\", \"UW\") ~ \"tense\",\n                           vowel %in% c(\"IH\", \"EH\", \"AA\", \"AO\", \"UH\", \"AH\") ~ \"lax\",\n                           vowel %in% c(\"AY\", \"AW\", \"OY\") ~ \"diphthong\")) %&gt;%\n  ggplot(aes(x = percent, y = hz, color = vowel)) +\n  geom_line(aes(group = formant_id)) + \n  facet_wrap(~class) + \n  scale_color_discrete(breaks = c(\"IY\", \"IH\", \"EY\", \"EH\", \"AE\", \"AA\", \n                                  \"AO\", \"OW\", \"UH\", \"UW\", \n                                  \"AH\", \"AY\", \"AW\", \"OY\")) + \n  theme_classic()\n\n\n\n\nThe result is a pretty cool plot that lets you easily compare the different classes of vowels. /aɪ/ and /aʊ/ have roughly the same F1 but have very different F2 trajectories. The lax vowels are relatively monophthongal while the tense vowels are more peripheral and dynamic. Also, my /æ/ and /u/ start off with the same F2, which is pretty cool."
  },
  {
    "objectID": "blog/jealousy-list-3/index.html",
    "href": "blog/jealousy-list-3/index.html",
    "title": "Jealousy List 3",
    "section": "",
    "text": "This is the third iteration of my Jealousy List, which is a list of articles so good I wish I had been the one to write them. My first two lists were posted about a year ago (see the list of lists here) and this one is long overdue, so I apologize for some of the posts being a little less recent. Regardless, here are a list of posts I’ve found in the past few weeks and months that I found exceptional in some way, entertaining, informative, or just plain cool.\n\nSaskia Freytag. “Workshop: Dimension reduction with R”.\n\n\nSo I wrote a tutorial on dimension reductions in #rstats. It has actually turned out to be fairly comprehensive. It uses a fun example dataset on cereals (🎉 - not looking for at iris) I would love some feedback: https://t.co/VsiHX2KYdT\n\n— Saskia Freytag (@trashystats) August 16, 2019\n\n\nYou may have heard of PCA (Principle Components Analysis) as a way to reduce a bunch of variables down to a more manageable number. As it turns out, this is just one way to do a dimension reduction on your data. Freytag’s workshop does a really nice job at explaining some of the different dimension reduction techniques that are out there, including helpful plots for the visual learners out there. It also goes into detail about the pros and cons of each method, and gives some sample R code showing you how to run the analysis yourself. I wish there were more workshops like there floating around! Plus, it uses data from a bunch of cereals, and I’m pretty sure I’ve used that dataset before in my workshops…\n\nAustin Wehrwein. “Burden of roof: Revisiting housing costs with tidycensus”.\n\n\nIn this episode I use the A+ tidycensus #rstats package to examine housing costs along with income data AND stretch wordplay as far as it will go. Burden of roof: revisiting housing costs with tidycensus. https://t.co/gDVlOb9N9H pic.twitter.com/i3gxzz6C8G\n\n— Austin Wehrwein (@awhstin) August 2, 2019\n\n\nThis is a short blog post, but I really like it because is succinctly shows how to quickly produce a really complelling story with some data and a nice visual. It uses the tidycensus package by Kyle Walker to extract some information about median income and housing prices per US county, and then creates a stunning map to display the data. I also learned that the county I live in now, Clarke County, Georgia, was among the top 25 worst counties in the country in this regard. I guess that’s where all my money is going!\n\nGarrick Aden-Buie. “Custom Discrete Color Scales for ggplot2”.\n\n\nI wrote up a short blog post on creating custom ggplot2 color scales. I focused on discrete color scales to demo a setup that makes binary colors easy, but I hope the post is helpful if you're working on a #ggplot2 theme for your org or brand. #rstats https://t.co/jQDxE61K3W\n\n— Garrick Aden-Buie (@grrrck) August 16, 2019\n\n\nI’ve become a bit of a color snob, so I appreciate a good post on colors in data visualization. This one is less about the colors themselves, and more about how to more easily implement your own custom color scheme in ggplot2. Aden-Buie even goes so far as to provide helpful tips for when you compile all these custom commands into an R package. I’ll definitely be using this whenever I get my package off the ground.\n\nRafael Irizarry. “Dynamite Plots must Die”. From Simply Statistics.\n\n\nOpen letter to journal editors: dynamite plots must die. Dynamite plots, also known as bar and line graphs, hide important information. Editors should require authors to show readers the data and avoid these plots. https://t.co/0GNKEIUCJL pic.twitter.com/OS9ytEFRZN\n\n— Rafael Irizarry (@rafalab) February 22, 2019\n\n\nData visualization must have been on my mind for a while now, because five months ago I bookmarked this blog post so that it’d make it on my next Jealousy List. This is a nice critique about Dynamite Plots, or basically bar plots with those little error bars at the top. Basically, they obscure the underlying distribution and can be replaced by a very small table. Fortunately, the complaint comes with a few recommendations for alternative visuals.\n\nJulia Silge. “Introducing Tidylo”.\nThere will probably always be at least one Julia Silge post on my Jealousy Lists. This one introduces a new R package, tidylo, which calculates weighted log odds using within the framework of the tidyverse. The post itself is, as always, a fun read and there are some great visuals. This’ll make it really easy to choose a baby name characteristic of like the 1920s for my next kid or something.\n\n\nSo that’s it for my long-overdue Jealousy List: statistical procedures, succinct tutorials, color, data visualization, and more statistics. Again, a decent representation of what I’ve been reading recently."
  },
  {
    "objectID": "blog/ads-meeting/index.html",
    "href": "blog/ads-meeting/index.html",
    "title": "ADS Meeting!",
    "section": "",
    "text": "I’m thrilled to announce I’ve been accepted to present a paper at the 2017 annual conference of the American Dialect Society!\nWhen I was an undergrad I was a part of a research team that presented at the LSA Annual Meeting in Boston in 2013. (I actually cut my honeymoon short so I could attend that meeting!) I was a starry-eyed budding little linguist with no real idea of what was going on. However, I do remember spending a large chunk of my time at that conference in the ADS presentations. In fact I still talk about some of the presentations I saw there.For a fuller account of my journey into linguistics, see 10 Years of Linguistics.\nTwo years later, the LSA and its sister societies met in Portland. Being a short drive from where my wife grew up, we decided to fly out so she could see her family and so I could attend the conference. Again, I found myself almost exclusively attending the ADS meetings. (Part of that may have been that it was up on like the 23rd floor and the room had a fantastic view!) I recognized people from the Boston meeting (Tyler Kendall, Sali Tagliamonte, etc.) and was also able to put some faces to names I had read (Charles Boberg, David Bowie, etc.). As it turns out, there were several presentations on language in the West that I ended up citing in some of my own work.\nNow, two years later, I’m extremely happy to be presenting at the 2017 meeting. Now that I have a clear idea about what my research interests are, and I know many more scholars’ works besides recognizing their name, I hope to reach out and talk with many of these people. Hopefully I’ll come across less of a fan and more as a colleague.\nI get to present a portion of my data that I collected in Washington just a few months ago. In a nutshell I’m showing that in word lists people pronounce pull and pole the same but Mary and merry different, but in a minimal pair task, pull and pole are separate while Mary and merry are the same.\nI’m about to give two completely unrelated presentations soon, so my focus hasn’t been on this Pacific Northwest, but once that wave has passed I’ll be able to devote more time and energy into this. It’s a very exciting time for me and I really look forward to the conference."
  },
  {
    "objectID": "blog/ads2018/index.html",
    "href": "blog/ads2018/index.html",
    "title": "ADS2018",
    "section": "",
    "text": "Thanks for attending my presentations. At the 2018 annual meeting of the American Dialect Society in Salt Lake City, Utah, I was fortunate to present on two aspects of my research."
  },
  {
    "objectID": "blog/ads2018/index.html#thursdays-presentation-on-the-gsv",
    "href": "blog/ads2018/index.html#thursdays-presentation-on-the-gsv",
    "title": "ADS2018",
    "section": "Thursday’s presentation on the “GSV”",
    "text": "Thursday’s presentation on the “GSV”\n\n\n\n\n\n\nTip\n\n\n\nDownload the slideshow here!\n\n\nThursday, I represented Peggy Renwick, Bill Kretzschmar, Rachel Olsen, and Mike Olsen and introduced a website called The Gazetteer of Southern Vowels. This is a online tool that makes it easy to visualize linguistic atlas data (specifically, the Digital Archive of Southern Speech, or DASS) that is currently being processed at the University of Georgia. The site has several features:\n\nSide-by-side plots make it easy to compare two subsets of our data, whether it be by demographic factors, language-internal factors, or methodological differences.\nA “point-pattern analysis” page shows an underlaid grid on the plot as an alternative way of visualizing the vowel space.\nAt the top of each page are options to subset the data however you like. Speakers can be selected by typical demographic factors. You can filter out stop words or examine specific words. You can subset by vowel, stress, and following consonant. Different transcription systems, filtering algorithms, and normalization procedures are available.\nThe plots themselves are highly customizable. Users can display any combination of points, ellipses, averages, and words. For each of these, the size and opacity can be controlled. This makes it easy to visualize the same data in lots of different ways.\n\nWe hope that you enjoy the Gazetteer of Southern Vowels and find it useful for visualizing linguistic atlas data."
  },
  {
    "objectID": "blog/ads2018/index.html#sundays-presentation-on-consonants-in-utah",
    "href": "blog/ads2018/index.html#sundays-presentation-on-consonants-in-utah",
    "title": "ADS2018",
    "section": "Sunday’s presentation on consonants in Utah",
    "text": "Sunday’s presentation on consonants in Utah\n\n\n\n\n\n\nTip\n\n\n\nDownload the slideshow here!\n\n\nOn Sunday afternoon, Kyle Vanderniet and I presented on consonantal variation in Utah English. We looked at three variables:\n\nWe found that words like mountain, cotton, and Latin have three pronunciations in Utah. The most common is what most other North American Engilsh speakers say: moun[ʔn̩]. Some women in our sample frequently used a second form, moun[ʔɨn], which has become almost stereotypical in Utah and has a lot of stigma. Finally, a third form, moun[tʰɨn], appears to be a hyperarticulated form in response to the stigma associated with the glottal stop. This was relatively frequent in our sample: about 25% of tokens had it, which is much more than similar audio from other states. Men tended to use this more, especially younger men.\nThen we looked at [t]-epenthesis in words like false, also, and else and found that while this isn’t particularly common overall, some women had it a fair amount in their speech.\nFinally, we looked at [k]-epenthesis after velar nasals. Despite being very frequent in other Utah English studies (like Di Paolo and Johnson’s study just before ours), this was rarely attested in our sample, so we have to figure out why.\n\nOverall, we feel that consonants in Utah deserve further study because of the high amount of variation."
  },
  {
    "objectID": "blog/sosy5/index.html",
    "href": "blog/sosy5/index.html",
    "title": "SoSy",
    "section": "",
    "text": "Today, I’m in Champaign, Illinois at the 5th Sociolinguistics Symposium (SoSy) at the University of Illinois Urbana-Champaign. I’m with a student of mine, Katya Kravchenko, and we’re here presenting her research project, “Surzhyk: Attitudes and Usage among Ukrainian People.” You can view the slides here.\nThroughout my sociolinguistics course last semester, Katya regularly kept us updated on fascinating things going on with language attitudes and ideologies among Ukrainians since the Russian invasion of Ukraine in February 2022. I think pretty much every Ukrainian speaks Russian, but not all speak Ukrainian. Even President Zelenskyy isn’t a native Ukrainian speaker and reportedly hired a tutor to help him achieve full command of the language.\nAttitudes towards Russian have changed dramatically in the past year though. Before, TV shows would put Ukrainians subtitles if someone speaks Russian. Now, they remove the Russian audio entirely and dub it over with Ukrainian. (This what happened on the Ukrainian version of the show, “The Bachelor”.) Sometimes, there’s a disclaimer at the start of the show saying apologetically that it was filmed before February 2022 and that they apologize for the Russian. There’s a grassroots effort (I wish I had the website, but I can’t find it now) to replace Russian borrowings into Ukrainian with pure Ukrainian words. This is done either by coining neologisms, or by going back to and older form of Ukrainian and finding words that, for whatever reason, didn’t make their way into modern Ukrainian. Basically, many Ukrainians are vowing to never speak Russian again.\nFor native bilinguals, it’s not too much of a problem. However, for those who aren’t fluent in Ukrainian this can be a tricky thing!\nIn comes Surzhyk. Surzhyk is a mix of Ukrainian and Russian, akin to Spanglish. There’s not a lot of linguistic research on it, but it’s rather stigmatized. In the past year though, Katya is finding that people’s view of Surzhyk is becoming more positive. She interviewed 15 Ukrainians who were living in Ukraine in February 2022 and since fled to Utah. One part of the interview asked them about their usage patterns. Like any Fergesonian Low language, it’s seen as too casual for formal settings. But, when it comes to attitudes, it’s seen as a bridge to help Russian speakers learn Ukrainian. Speaking Surzhyk instead of Russian signals a lot about someone’s position in the war. A year and a month ago, speaking one language—whether it be Ukrainian or Russian—was better than mixing the two. Today, speaking Surzhyk is seen as better than full Russian.\nThis is 99% Katya’s research, but it has been fun to hop on board and work with her on this. It’s not about vowels, statistics, or American English, and I don’t speak a word of Russian, Ukrainian, or Surzhyk. But it has been a fun stretch for me."
  },
  {
    "objectID": "blog/making-a-website-is-fun/index.html",
    "href": "blog/making-a-website-is-fun/index.html",
    "title": "Making a website is fun!",
    "section": "",
    "text": "In the past month or so I’ve been putting a lot of time and effort into increasing my professional web presence. In about a year I’ll be applying for academic positions, and it would sure be nice to be more visible to my potential employers. The sheer fact that you’re reading this means you’ve seen some of the fruits of my labor."
  },
  {
    "objectID": "blog/making-a-website-is-fun/index.html#why-now",
    "href": "blog/making-a-website-is-fun/index.html#why-now",
    "title": "Making a website is fun!",
    "section": "Why now?",
    "text": "Why now?\nIt took me a while before I wanted to find out what area of linguistics I wanted to go into. I’ve been interested in a lot of things at one time or another: typology, documentation, indigenous languages of South America, language change, simulation, morphology, network analysis, forms of address, among other things. I’ve even gone to conferences presenting some of this research. But I knew at the time that whatever it was that I was presenting on wasn’t going to be what I wanted to be known for. So I didn’t bother networking with other people, and I hardly took people’s advice because I would brush this off and say it was just a glorified term paper.\nBut I’ve found my niche. I’m interested in sociolinguistics, dialectology, phonetics, phonology, and using computer and statistics to help me out. This is something I’d like to be known for. Right now I’m working on English in the Pacific Northwest, and I’m familiar with a lot of the work that’s been done in that area, so I know who to talk to at conferences because I’ve read a lot of their work."
  },
  {
    "objectID": "blog/making-a-website-is-fun/index.html#github",
    "href": "blog/making-a-website-is-fun/index.html#github",
    "title": "Making a website is fun!",
    "section": "Github",
    "text": "Github\nOver the past five years or so in my undergrad and graduate education, I’ve acquired some computer skills. I minored in linguistic computing, so I learned Perl and C# as a part of the required coursework. More importantly though, I learned that learning to use computers is pretty darn useful for my research. So I’ve learned a few other skills along the way to help me out with my larger linguistics questions.\nJust this month I presented a paper on Quechua morphology. I mentioned in it that I wrote a computer script to help me out with generating the correct forms of the paradigm. One of the participants in the Q&A session asked if the code was available on Github and I said it wasn’t. But why shouldn’t it be? That acted as a catalyst into getting a github profile and uploading some code. I also found out I could host a webpage (this webpage!) on there too. Well sweet."
  },
  {
    "objectID": "blog/making-a-website-is-fun/index.html#creating-the-webpage",
    "href": "blog/making-a-website-is-fun/index.html#creating-the-webpage",
    "title": "Making a website is fun!",
    "section": "Creating the webpage",
    "text": "Creating the webpage\nHaving never done web design before, I had a lot to learn. Turns out you’ve gotta host the webpage somewhere. By that I mean that all the files and formatting and content and stuff in a webpage has to be stored on some computer somewhere. It can’t be mine, because I’ve set it up as a server and stuff and that’s way beyond me expertise—plus I’m pretty sure I don’t want to have my laptop as a web server. So luckily, Github, will host mine for free. Okay good.\nThe next task was to figure out how. I had recently heard about a website called The Programming Historian, which has a lot of slick, easy-to-follow tutorials on how to do useful computer stuff for research. Well, one of their pages is called “Building a static website with Jekyll and GitHub Pages”. Awesome. Now, to be clear, it wasn’t the easiest tutorial. You have to download all sorts of stuff to your computer using the command line and manage a bunch of files and stuff. But that’s fine. I got it. I was able to create a simple webpage.\nWell, I’m never really satisfied with the default design of things, and I like having unlimited flexibility in how things look. So, I went over to Lynda.com, which I have access to through UGA, and took a more detailed course on how to build a webpage using Jekyll. And it was great. I learned a lot and figured out what a lot of stuff means.Edit: Looks like the “Jekyll Web Design” course is no longer available. Makes sense; it was from 2016!\nBut, I’m still not satisfied with the way it looks. I learned how the website works in that course, but not a lot of formatting. So I’m currently taking another Lynda.com , which I think will help me a lot. As it turns out, the concepts are very similar to the job I did as an undergrad, where I essentially created eBooks for a program called WordCruncher.Looks like this one is missing too. It was called CSS Core Concepts. I wish I knew who the instructor was.\nMy goal in all this is to be able to make a beautiful webpage that is uniquely mine. I want all the flexibility I could ever want in how it works and looks. By so doing, I’m learning a lot of new skills like CSS, but that’s perfectly okay with me. So, today the webpage is still a hack off of the tutorial I went through, but hopefully over the course of the next few weeks and months it’ll slowly transform into my own.Now that I’ve switched to Quarto, I’ve lost much of that uniqueness and flexibility.\nI guess the end goal of this is to wow potential employers still. But, let’s be honest, I’m sure enjoying the journey."
  },
  {
    "objectID": "blog/tweeting_LSA2017/index.html",
    "href": "blog/tweeting_LSA2017/index.html",
    "title": "Tweeting LSA2017",
    "section": "",
    "text": "In addition to the awesome experiences I had overall at the LSA2017 conference (which you can read about here), I made an effort to be active on Twitter during the conference.\nI’ve followed conferences in the past (such as LSA and NWAV last year) when I wasn’t able to attend them, and really enjoy them. I livetweeted the Linguistics Conference at the University of Georgia (LCUGA) in October, which was my first experience as a livetweeter, though I didn’t do much other than introduce who was presenting next. So this year, not only did I follow the twitter feed, but I also contributed as much as I could myself. Here are some of my more popular tweets:\n\n\nFor anyone at #LSA2017, you should come to the first session of #ADS2017 (\"Vowels, vowels, vowels\") and hear me talk about Washington State!\n\n— Joey Stanley (@joey_stan) January 5, 2017\n\n\nThis one got a surprising amount of traffic (over 800 people saw it on Twitter) for being self promotion, but it’s because the LSA account retweeted it. I don’t think they retweet everyone’s self-advertising tweets though, so I wonder why mine made the cut… Either way, I didn’t mind the advertising!\n\n\nThomas & Kendall: “You can't get a full sociolinguistic picture of a community by looking at only one kind of variable.” #ADS2017 #LSA2017\n\n— Joey Stanley (@joey_stan) January 6, 2017\n\n\nThis one was one of many tweets during Erik Thomas and Tyler Kendall’s presentation, a direct quote from one of their last slides. It’s a really good quote overall and a lot of people seemed to like it.\n\n\nAntieau: Double modals. One Utahn used “might usually would”. Nice! #ADS2017 #LSA2017\n\n— Joey Stanley (@joey_stan) January 7, 2017\n\n\nThis was one of many things that Lamont Antieau found in the Linguistic Atlas of the Middle Rockies. Having lived in Utah, most of his results were surprising to me, but this one was especially so. Apparently others thought so too.\n\n\n@BrentPWoo: \"and/or\" as a fully lexicalized coordinator with the union set of constraints of on \"and\" and \"or.\" #lsa2017\n\n— Joey Stanley (@joey_stan) January 7, 2017\n\n\nBrent Woo from the University of Washington is doing some really interesting research. We met when he presented at the Linguistics Conference at the University of Georgia in October. He must have some dedicated twitter followers though because anything I tweet at him gets a lot of traffic. I mean, his findings are pretty cool though.\n\n\nPreston: “I grew up in a paint store so I have female-like familiarity with color, and it hasn’t costed me my masculinity.”#ADS2017 #LSA2017\n\n— Joey Stanley (@joey_stan) January 7, 2017\n\n\nThis was a random side comment Dennis Preston made after explaining the color scheme in is graphs. Instead of red, green, and blue or something, he used lavender, forest green, chartreuse, and a couple others. After explaining which colors represented which variable, he said that line. Even though it had a typo (hasn’t costed me my masculinity), a lot of people liked it. I mean, Dennis Preston seems like a pretty funny guy, so this was great.\n\n\nAfter #LSA2017 I feel simultaneously intimidated, inspired, and exhausted. A clear sign that this was a fantastic conference.\n\n— Joey Stanley (@joey_stan) January 8, 2017\n\n\nAt the end of conferences, there’s a lot of “I’m so sad the conference is over” tweets. As I’ve mentioned above, I put as much as I could into the conference, and I did feel exhausted. I was intimidated because seeing what great stuff other grad students are doing I suddenly feel less employable. But I was inspired to do more and better research. This tweet succinctly summed up what my conference experience, and it looks like others felt the same way too.\n\n\nThis was my first time & it took more multitasking than expected. I have a newfound respect for tweeters and I appreciate 'em even more now. https://t.co/1HgUFmHzvV\n\n— Joey Stanley (@joey_stan) January 8, 2017\n\n\nThis was part of a small thread that was going. Basically, people were thanking all the livetweeters out there, and I got a mention. I thought I’d thank the other livetweeters as well, now that I know how hard it is.\nAll this tweeting paid off though because I think people are noticing me on twitter now, and I think I’m seen as “one of the live-tweeters”. Not a bad reputation to have. I’m about to get my hundredth follower, now that I’ve gotten about 10 more since this conference. (I got about 10 just by following NWAV and liking tweets, and another half dozen when I tweeted very basic things at the LCUGA conference.) My goal is for my followers to outnumber the people I follow, but I don’t know if that’ll happen anytime soon. I’m not very active on twitter outside of conferences, other than shameless self-promotion. Maybe I should get better at that.\nIf you’re interested in getting more into twitter, I’d recommend this guide:\n\n\n@joey_stan have you seen @GretchenAMcC's excellent guide? https://t.co/03AjgiGtuq\n\n— Rachael Tatman (@rctatman) January 9, 2017\n\n\nI enjoy being active on twitter for lots of reasons, most of them completely selfish, but starting to get a small following is pretty exciting and it’s a fun group to be a part of."
  },
  {
    "objectID": "blog/a-tutorial-on-extracting-formants-in-praat/index.html",
    "href": "blog/a-tutorial-on-extracting-formants-in-praat/index.html",
    "title": "A Tutorial on Extracting Formants in Praat",
    "section": "",
    "text": "Note\n\n\n\nI gave a workshop that covered the contents of this workshop. You can find the handout here, which is slightly modified from this blog post.\nJust this week I’ve had three people ask for a Praat script that extracts formant measurements. I’ve been meaning to create some Praat scripting tutorials so this was a good excuse to get something going. Instead of providing you a Praat script, I’m going to show how to write your own. Instead of giving you a fish to feed you for a night, I’ll teach you how to fish.\nNote, this post was written in small chunks over Christmas break with my in-laws in town and I haven’t had time to proofread it carefully. I explain what to do, but I barely skim the surface when it comes to why you need to do it that way (for example, I gloss over basic computer coding concepts like for loops and variables). Hopefully it’ll work for you."
  },
  {
    "objectID": "blog/a-tutorial-on-extracting-formants-in-praat/index.html#sample-data",
    "href": "blog/a-tutorial-on-extracting-formants-in-praat/index.html#sample-data",
    "title": "A Tutorial on Extracting Formants in Praat",
    "section": "Sample data",
    "text": "Sample data\nFor this tutorial, I’ll work with a recording of myself reading a couple dozen words with the /u/ vowel that I created for something a few months ago. I did the word-level transcription by hand (including boundaries) and then I sent it off to DARLA for forced alignment. Here’s what one word looks like:\n\nThe important part to note is that I have the word-level transcription in the second tier, and a phoneme-level transcription—in ARPABET—in the first tier. I’m going to assume you’ve used DARLA or FAVE to process your files so they should look like mine does."
  },
  {
    "objectID": "blog/my-cot-caught-distribution/index.html",
    "href": "blog/my-cot-caught-distribution/index.html",
    "title": "My cot-caught distribution",
    "section": "",
    "text": "In appendix C of my dissertation, I listed 838 words that contain low vowels, classified as /ɑ/ or /ɔ/ according to my idiolect. I think it may be a useful resource for those who have the cot-caught merger, so I thought I’d pull it out and make it more available.\nThe words that are included here are all those that came up in the course of the 54 sociolinguistic interviews I did in Washington State as part of my dissertation work. This means that there are some quirky ones included like Sinterklaas and Jawas. By nature of the naturalistic data, certain common words may be missing.\nPlease note that, again, this reflects my idiolect, and may not be reflective of others’ speech, other varieties of English, or historical distributions. For more information about the phonetic realization my two vowels and some phonological peculiarities, please see my description of my idiolect.\nIf you find this page useful, please cite it as Appendix C of my dissertation:"
  },
  {
    "objectID": "blog/my-cot-caught-distribution/index.html#lot-cot-ɑ-bot-etc.",
    "href": "blog/my-cot-caught-distribution/index.html#lot-cot-ɑ-bot-etc.",
    "title": "My cot-caught distribution",
    "section": "lot, cot, /ɑ/, bot, etc.",
    "text": "lot, cot, /ɑ/, bot, etc.\nThe following 550ish words are all those that were classified as part of the lot lexical set, which includes many words in the palm set (though not palm itself because of that pesky /l/!). Personal names and other identifying words have been removed.\naccelerometer, accommodate, accommodation, accomplish, accomplished, adopt, adopted, adopting, agronomous, ah, aloha, Aotearoa, apostrophe, approximately, aquatic, assada, astronomical, atomic, atrocities,\nbattery-operated, biography, biological, blah, blobs, block, blockage, blocked, blocking, blocks, blossomed, body, bomb, bomber, bombing, bombings, bombs, bombshell, bon, bond, bonfire, bonfires, Bonnie, Bonnie’s, bother, bothered, bothering, bothers, bottle, bottles, bottom, bottoms, box, boxes, boxing,\nChicago, chocks, chop, chopped, chopping, chops, chronically, cilantro, clock, clock’s, clocked, clocks, closet, closeted, cocker, cockney, cocktails, cod, cog, cogs, cognitive, colonoscopy, com, combat, combination, combos, comedies, comedy, comical, comment, comments, commerce, commodity, common, commonly, comp, compact, compensate, compensating, competent, competition, competitions, complement, complex, complicate, complicated, complications, compound, comstock, con, concentrate, concentrating, concept, concepts, concert, concerts, concrete, condo, conduct, conference, conferences, confidence, confident, confirmation, conflict, conglomerate, congregates, congress, Conner, Connie, conquer, cons, conscience, conscious, consequence, consequences, constable, constant, constantly, constitution, contact, contacted, content, contest, contests, continental, contraband, contract, contracted, contractor, contractors, contracts, contrast, controversy, conversation, converse, convert, cooperating, cop, copper, cops, copy, copying, Cossack, costume, cot, cots, counter-clockwise, Cozumel, crop, crops,\ndeposit, dichotomy, dock, doctor, doctor’s, doctorate, doctors, doctrine, documented, documents, dodge, dodgeball, dodged, Don, Donald, dot, drama, drop, drop-out, dropoff, dropouts, dropped, dropping, drops,\nebonics, economic, economics, economy, electronic, electronics, ensemble, esophagus, Exxon,\nfather, father-in-law, father-in-law’s, father’s, fiance, fiance’s, Firefox, flock, flocks, flops, fondest, fondue, forgot, forgotten, fox,\ngaloshes, garage, garages, geography, geometry, god, god’s, goddess, godfather, godparents, gosh, gospel, gossip, got, gotcha, gotta, gotten, grandfather, grandma, grandma’s, grandpa, grandpa’s, great-grandfather’s, Guam,\nha, hobbies, hobby, hockey, homogenized, Honda, honest, honestly, honesty, honor, honorable, hopped, hopper, hops, hospice, hospital, hospitals, hostage, hot, hotter, hottest, hypothesis,\nimpossible, impoverished, improper, Iran, ironic, ironically,\nJava, Jawa, Jawas, job, job-wise, jobs, jock, jockey, jockeying, jockeys, jocks, Joplin,\nKawasaki, knob, knock, knocked, knocking, knocks,\nla, Lafayette, lava, Lhasa, llama, llamas, lobby, lobster, lock, locked, locking, locks, lodge, logic, lot, lots, lotta, lotteries,\nma, Mazama, Mazamas, McDonald, McDonald’s, McDonalds, microprocessor, microprocessors, mill-dominated, mock, model, modeling, models, moderate, modern, modest, modify, modular, mom, mom’s, moms, monetary, Monica, monocle, monopoly, monotone, monster’s, Monterrey, Montgomery, Montiville, monument, mop, Morocco, motto,\nnah, Nazi, Nazis, neon, nodding, non-athletes, non-profit, non-profits, non-stop, novel, novelist,\no’clock, obligated, obsolete, obstacles, obvious, obviously, occupies, October, odd, odd-shaped, oddly, odds, Ohana, omelette, omelettes, op, opera, operate, operating, operation, operations, operator, operators, opportunities, opportunity, opposite, option, optional, options, orthodontist, ostracized, Ostrander, ostriches, otters, oxy, oxy-fuel, oxygen,\npa, pasta, pecan, pecans, periodontist, phenomenal, philanthropic, philosophies, philosophy, phonologically, phosphorus, photographer, photography, Photoshop, plopped, Pocahontas, pocket, pockets, pod, podcast, pond, ponds, Pontiac, pop, popped, pops, popular, population, posh, positing, positive, possible, possibly, pot, potholes, potluck, potlucks, Potter, potty, pottying, poverty, predominantly, probably, problem, problem-solvers, problems, process, processed, processes, processing, proclamation, product, products, progress, project, projects, prom, prominent, prop, proper, properly, properties, property, props, prosecuted, proselyte, prospects, prostate, psychologically,\nquads,\nrecognizance, reconnaissance, regatta, remodel, remodeled, remodeling, remodels, repository, reprocess, respond, responded, response, responses, responsible, robin, robin’s, rock, rock’s, Rockefeller, rocker, rockers, rocket, rockets, rocking, rocks, rocky, rod, rods, rotted, rotten,\nSaigon, Scotland, self-conscious, shock, shocked, shop, shopped, shopping, shops, shot, shotguns, shots, Sinterklaas, ska’s, sloppy, snobbies, snobby, snot, sobbed, soccer, soccer’s, sock, socket, socks, sponsor, sponsoring, spot, spots, spotted, spotter, spotters, squash, squat, squatter, squatters, stock, stocking, stockings, stomping, stop, stopped, stopping, stops,\ntaco, telethon, teriyaki, throttle, toddler, top, topic, topics, topless, Toppenish, toppled, tops, trigonometry, tropical, trot, Tsugawa’s, tsunami,\nUganda, unblock, uncommon, Vietnam,\nwhatnot, Wisconsin,\nYamaha,\nzombie"
  },
  {
    "objectID": "blog/my-cot-caught-distribution/index.html#thought-caught-ɔ-bought-etc.",
    "href": "blog/my-cot-caught-distribution/index.html#thought-caught-ɔ-bought-etc.",
    "title": "My cot-caught distribution",
    "section": "thought, caught, /ɔ/, bought, etc.",
    "text": "thought, caught, /ɔ/, bought, etc.\nMeanwhile, the following 280ish words are all those that were classified as part of the thought lexical set, which includes most (or probably all) words in the cloth set. Again, personal names and other identifying words have been removed.\nacross, along, alongside, applesauce, Auburn, auction, auctions, audio, audio’s, audits, aught, August, Austin, Austria, Austrian, author’s, authorized, autism, auto, auto-related, autobiography, automatic, automatically, automobiles, autopsy, awe, awesome, awful, awkward, awkwardness,\nbelong, belonged, belongings, beyond, blog, blogger, bloggers, blogging, blogs, blond, bonko, bonkos, boss, bosses, Boston, bought, bra, bras, broad, broadcast, broadcasting, broadway, brothels, brought,\nc’mon, calm, caught, cause, caused, causes, causing, cautious, chainsaw, chalked, chocolate, chocolates, clause, clog, clogged, clogging, cloth, coffee, Cong, cost, Costco, costs, cross, cross-wired, crossed, crosses, crossing, crosswise, crossword, cutoff,\ndaughter, daughter-in-law, daughter-in-laws, daughter’s, daughters, daughters-in-law, dawn, dawned, dishwater, dog, dog’s, doggie, dogging, doggone, dogs, dong, donkeys, draw, drawed, drawing, drawn, draws,\nelongated, exhaust,\nfog, fought, frogs, froth,\ngloss, gone, goner, gong, goth, goths, granddaughter, granddaughters,\nhaunted, haunts, hawed, hawk, hog, Hogwarts,\njaw, jigsaw, jog, jogged, jogging, Johnston, Johnston’s,\nLas Vegas, laundry, law, lawn, lawnmower, lawns, laws, lifelong, log, logged, logger, loggers, logging, logjammed, logs, long, Long-Bell, Long-Bell’s, long-span, long-standing, long-term, longer, longest, longjohns, longshore, longshoreman, longshoreman’s, Longview, Longview-ite, Longview’s, loss, losses, lost,\nmama, Montana, moss, mossy, naughty,\noff-duty, offer, offered, offerings, offers, offhand, office, officer, officers, offices, often, Ogden, online, on, onset, onto,\npalm, pause, paused, pausing, paw, pawn, paws, pong, profit, profitable, prolonging, promise, promised, promising, prompt, prophecy, prophet,\nquantities,\nraw, restaurant, restaurants,\nsauce, saucers, sauces, sausage, Sauvie, saw, sawing, sawmill, saws, Schwann’s, scoff, slaughtering, sloshes, soft, softball, software, somber, sombering, song, songs, sophomore, spawning, squaw, stalker, stalking, straw, strawberries, strawberry, strong, stronger, strongest, swans, swath,\ntalk, talkative, talked, talker, talkie, talking, talks, taught, thaw, thought, thoughts, tomboy, Tonga, tossed, traumatized,\nunderwater, upon, Utah,\nvaudeville,\nwaffle, waffles, walk, walked, walking, walks, walkway, wand, wandering, wash, washboardy, Washburn, washed, washer, Washington, watch, watched, watches, watching, water, watering, waters, wrong"
  },
  {
    "objectID": "blog/youre_a_statistician_harry/index.html",
    "href": "blog/youre_a_statistician_harry/index.html",
    "title": "You’re a Statistician, Harry!",
    "section": "",
    "text": "The job hunt was not successful this year. I applied to about two dozen positions, got interviewed for five of them (yay!) but ultimately got zero offers (boo…). I’m disappointed, sure, but it’s probably for the best anyway: it took longer to write my dissertation than I anticipated, so it probably wouldn’t have been feasible to finish it and graduate by August. Plus, I have funding for one more year. But, the funny thing is I’m now in this weird position where the bulk of my dissertation has been written, but I have about another year left as a student. What can I during this time? I considered a lot of options, but I think I’ve settled on something fun: I’m going to try and get an M.S. in Statistics!"
  },
  {
    "objectID": "blog/youre_a_statistician_harry/index.html#how-is-this-possible",
    "href": "blog/youre_a_statistician_harry/index.html#how-is-this-possible",
    "title": "You’re a Statistician, Harry!",
    "section": "How is this possible?",
    "text": "How is this possible?\nAs it turns out, UGA offers a “secondary” Master’s degree in statistics. It designed so that UGA students who are seeking degrees in other departments can walk away with a Master’s in statistics as well. I’d take the required courses, fill out some paperwork, write a thesis, and boom—I’ve got an MS. I wouldn’t be part of the statistics department so I’ll have to continue getting funded through my home department (linguistics), but I would get a full-fledged degree from them by the time I graduate.\nI’ve known about this option for a while now, and the main thing holding me back was my lack of a strong mathematical background. I took calculus in high school and quite enjoyed it, but that was more than 10 years ago, and I’d actually need three semesters of calculus to be able to do well in some of the core stats courses. So I decided it was a nice thought but was ultimately not going to happen. When the realization came that I’d probably be around for another year though, I looked at the requirements, talked with the graduate coordinator, and put together a schedule. It’ll be a busy year, but I think I can make it work.\nThe math problem still hasn’t gone away, so my goal this summer is to fly through differential, integral, and multivariate calculus on my own before the semester starts in August. Fortunately, it looks like I can learn everything I need through Khan Academy, which is awesome. I’m already auditing Mathematical Statistics during the summer so when take it for real in the fall I’ll know the material well enough to pass. So far, it has been coming back pretty quickly, so I feel pretty good about it all."
  },
  {
    "objectID": "blog/youre_a_statistician_harry/index.html#why-bother",
    "href": "blog/youre_a_statistician_harry/index.html#why-bother",
    "title": "You’re a Statistician, Harry!",
    "section": "Why bother?",
    "text": "Why bother?\nWhen I was weighing my options for what I could do during my last year, it was all based on how I could improve my job prospects for next year. My first and the most obvious option was to just hunker down and crank out a bunch of publications to lengthen that CV. This is a good plan, but ultimately I decided I probably wouldn’t be able to get them done in time for potential employers to see them anyway. I have a couple projects in various stages of the publication pipeline right now, so I’ll continue working on those, but that won’t be what I spend 100% of my time on.\nWell so then I looked at perhaps strengthening the teaching part of my job applications. I’ve been funded through research assistantships this whole time, which has been great! But that means I’ve only had the opportunity to teach twice and I think universities like to see more teaching experience than that. So I thought about getting the graduate certificate in University Teaching, which would involve some actual courses in pedagogy. I figure if I couldn’t impress them with experience, maybe I could with training and certification. I’ll admit, it didn’t sound that fun, but some of the courses did look somewhat interesting. However, I found out that I’m ineligible for the certificate anyway because I need to have taught four sections while at UGA to qualify, which I have not. So, I scratched that idea.I think I would have taken ones like adult learning, how to do proper assessment, and skills for being able to teach an online course.\nSo then I considered getting the graduate certificate in GIS. It’s not totally out of the blue: as a dialectologist, it would be nice to have some actual geography training and it might put me ahead of other potential applicants for positions that wanted a hard-core sociolinguist. The certificate would have been feasible too: I’d take the Intro to GIS course over the summer, and then it would be just two courses in the Fall and two courses in the Spring. In fact, that was what I planned on, and I’m currently taking the Intro to GIS course online right now (and loving it!). But, as I looked through the course offerings, it turns out that the really fun ones that I wanted to take  weren’t going to be offered this year. That means I’d have to take things like working with aerial photography or forestry-related courses or something to satisfy those electives, which wouldn’t be particularly fun or easy.I really wanted to do things like spatial statistics, programming in GIS, and cartography.\nAll the while, this statistics possibility has always intrigued me. I have always had a knack for quantitative methods in linguistics and feel like I have a better-than-normal grasp of the statistics that linguists use. My undergraduate minor in linguistic computing exposed me to coding, and I’ve learned a lot of R programming since coming to UGA. A lot of the jobs I applied for wanted someone with skills in quantitative linguistics, and while I think I have the skills they want, I had a hard time proving it since my publications so far haven’t been particularly quantitative. I figure an entire degree in statistics might do the trick this time around and would definitely make me stand out. Plus, it’s a nice skill to have to fall back on: it opens doors to college-level teaching opportunities in statistics and to industry jobs as well. So I finally reached out to the statistics graduate coordinator, explained the situation, and he said it would definitely be a possibility."
  },
  {
    "objectID": "blog/youre_a_statistician_harry/index.html#so-whats-the-plan",
    "href": "blog/youre_a_statistician_harry/index.html#so-whats-the-plan",
    "title": "You’re a Statistician, Harry!",
    "section": "So what’s the plan?",
    "text": "So what’s the plan?\nRight away, I had to consider what track I wanted to be on with this degree. The stats department offers a thesis and a non-thesis option for their MS. The non-thesis track has a comprehensive exam at the end and requires two extra elective courses. That sounds great because there are lots of courses I really do want to take. The problem is I just don’t have the time.\nMy other option was to do the thesis track. There are slightly fewer classes required, which is good for me, but that of course means I’ll have to do a thesis. So I guess I’m writing a master’s thesis in statistics! I don’t know what I’ll write on yet, but it’ll definitely pertain to statistics used in (socio)linguistics, possibly involving some simulations and stuff, but we’ll see. I’m kind of excited about the idea of doing that actually. Plus, it’ll be really weird I think to write a master’s thesis after doing a Ph.D. dissertation.\nSo I’ve got a busy year ahead of me. After learning as much calculus as I can this summer, I’ll take three courses in the fall, three in the spring, and do some consulting work over the summer while I write my thesis. It should be a blast."
  },
  {
    "objectID": "blog/simulating_werewolf/index.html",
    "href": "blog/simulating_werewolf/index.html",
    "title": "Simulating Werewolf",
    "section": "",
    "text": "I really enjoy the party game called Werewolf. When I was an undergrad, I played it many, many times but unfortunately, I haven’t had a chance to play it for several years. After successfully simulating an easier game like Chutes and Ladders a few weeks ago, I thought I’d try moving on to something more difficult. Here are the results of a bunch of simulations of simple Werewolf games."
  },
  {
    "objectID": "blog/simulating_werewolf/index.html#what-is-werewolf",
    "href": "blog/simulating_werewolf/index.html#what-is-werewolf",
    "title": "Simulating Werewolf",
    "section": "What is Werewolf?",
    "text": "What is Werewolf?\nWerewolf is a party game, ideally played with about 10–15 people sitting in a circle, plus one narrator. Usually with the help of regular playing cards, the narrator distributes roles to each person, which they keep secret. The simplest game has two roles: werewolves and townspeople. For the werewolves, the goal of the game is to eliminate as many townspeople as possible. The townspeople’s goal is to eliminate all the werewolves.\nThe game proceeds in alternating phases. First, in the “nighttime” phase, all players start by closing their eyes. The narrator then “wakes” the werewolves and they collectively, but silently, indicate to the narrator who they “attack” that night. If you have additional roles in use, they, one at a time, wake up and do other actions like pick someone to save from a werewolf attack or some other action.\nWhen all the nighttime events have finished, the game moves on to the “daytime” phase. Everyone can open their eyes and the narrator informs everyone the results of last night’s events. In a simple game, it just means everyone finds out who the werewolves attacked. That player “dies”, reveals their role to everyone, and are eliminated from the game. Everyone then holds a “town hall” meeting and vote on who they should eliminate, under the guise of “hey everyone, there are werewolves among us, let’s kill the person we think is most suspicious.” The werewolves of course participate in this town hall event while hiding their identities. When a person is voted out, they also “die”, reveal their role, and are eliminated.\nThis nighttime-daytime cycle repeats until one of the game over conditions are met. The werewolves win if they equal or outnumber the non-werewolves. Everyone else wins if the werewolves are eliminated. It’s a fun game."
  },
  {
    "objectID": "blog/simulating_werewolf/index.html#simulating-werewolf",
    "href": "blog/simulating_werewolf/index.html#simulating-werewolf",
    "title": "Simulating Werewolf",
    "section": "Simulating Werewolf",
    "text": "Simulating Werewolf\nI’ve tried half a dozen times to write a computer program to do Werewolf. When you’ve got 15 players, each with their own unique roles and abilities, working out the logic of interacting events can get tricky. I’ve always wanted to write an iPhone app or something that would be the perfect tool for the narrator to help figure out what all happened. As it turns out, for being such a simple game conceptually, I’ve found it quite difficult to program (with my limited programming abilities).I most often was the narrator when I played, so I think about helping that person the most.\nI had a lot of fun simulating Chutes and Ladders, so I thought I’d try simulating Werewolf. In all the previous versions, I’ve tried the narrator’s tool approach to programming it, but it ended up being simpler to program a simulation rather than a tool for a live game. The benefit of this completely automated game is that I can wrap it up into a loop and run it many times.\nI won’t describe the code in this post because it’s a little complicated. Perhaps overly complicated for the simple version of the game that I have now, but I’m anticipating more types of roles in future versions, so I’m leaving room to add those in later. But you’re welcome to look at the code yourself on my Github.I’m trying to increase my Github presence, so there it is"
  },
  {
    "objectID": "blog/simulating_werewolf/index.html#the-simplest-game-werewolves-and-townspeople",
    "href": "blog/simulating_werewolf/index.html#the-simplest-game-werewolves-and-townspeople",
    "title": "Simulating Werewolf",
    "section": "The simplest game: werewolves and townspeople",
    "text": "The simplest game: werewolves and townspeople\nThe main thing I wanted to determine with these simulations was what the ideal ratio of werewolves to townspeople is in a game. Normally when I play in real life, I shoot for somewhere between 1 for every 3–4 players, depending on what other roles are being used and how many people are playing.\nSo, I started the simulation with the simplest possible game and included just two roles: werewolves and townspeople. The parameters of the simulation that I could modify were how many players are playing and how many of those players were assigned a werewolf role. The largest game I’ve participated in was 22 people, but since this is a computer, I thought I’d try much larger groups. So I simulated games as small as three players to monster 40-person groups. For the number of werewolves, they ranged from 1 to 20.\nAs I looked through the simulations, there was kind of a lot variation as far as how long the game lasted, and in some cases, what side won. So I ran more and more games per combination of players and decided that with about 50 games I could get a reasonable estimate of how often each side won. I can get about 9 games per second with the current code, so it takes about 35 minutes to run all of them. I could run it overnight I suppose to get more simulations, but I figure 19,000 total games was sufficient.\nSo, here are the results of those simulations! This first plot shows the results of 19,000 games with just werewolves and townspeople:\n\nThere are lots of interesting things to notice here. First, for groups less than about 27 people—which is pretty much any in-person game you’d ever play—just a small number of werewolves are needed. Surprisingly, in a group of 10 people, if you have just one werewolf, that player’s got a 50-50 shot at winning. Pretty impressive. That ratio of approximately 1 in 10 continues until about 27 people. At that point—for some reason unknown to me—it changes drastically and you need about 1 in 3 players to be werewolves for it to be an even game.\nSo, let’s say you’ve got a group of four people (other than the narrator) that wants to play. If just one person is a werewolf, it’s a fair game. But, if twenty-two of your closest friends come to join, just two of them should be werewolves to keep the odds the same. But then, if another twelve people join, you’d need to make eight of them werewolves to keep the odds the same. What the heck??"
  },
  {
    "objectID": "blog/simulating_werewolf/index.html#add-the-angel",
    "href": "blog/simulating_werewolf/index.html#add-the-angel",
    "title": "Simulating Werewolf",
    "section": "Add the Angel",
    "text": "Add the Angel\nOkay, so it’s not a lot of fun when the werewolves to have such good odds in small groups. So, for this reason, additional roles are added to the game to help the townspeople’s side. The most basic one is the Angel. After the werewolves have selected their victim, the Angel then “wakes up” and chooses someone to protect that night. That person is saved from werewolf attacks. The Angel is on the townspeople’s team when it comes to winning the game, so ideally, the odds that the townspeople win should go up when the Angel is included in the game.\nHere are the results:\n\nFor the most part, the results are very similar. It’s hard to tell when the two plots are displayed separated from each other here, but if you could quickly toggle between them, most of the white squares shift over to the left about one unit. That means that if one of the townspeople is an Angel, you’d need one fewer total people to make it a fair game. In other words, the Angel is like having two townspeople. (Of course if the Angel gets attacked by the werewolves early on, then you’re toast.) Furthermore, a lot of the dark green squares are a little less dark, meaning the Angel does help the townspeople win a little bit more often even when the odds are against them.\nBut the effect is quite small. I thought the Angel would be a little more helpful, but I guess not."
  },
  {
    "objectID": "blog/simulating_werewolf/index.html#add-the-witch",
    "href": "blog/simulating_werewolf/index.html#add-the-witch",
    "title": "Simulating Werewolf",
    "section": "Add the Witch",
    "text": "Add the Witch\nThe last role I’ve programmed so far is the Witch. The first of the Witch’s two powers is a saving spell. After the werewolves have made their attack, and if the Angel hasn’t already saved them, the Witch is told who is about to die. If they want, the Witch may save that person—but they can only do so once per game. Additionally, they also have a kill that they can use once per game, but I haven’t programmed that in yet. The Witch is on the townspeople’s side, so they should aim to use their kill on a werewolf.\nSo, in a real game, the Witch uses their save wisely. They usually don’t use it the first round and will most likely use it when they have a good reason to save that person. I can’t program all the gut feelings and stuff that go into the game, so I simply made it so that the Witch uses their power 33% of the times they have the opportunity to. So, it doesn’t always get used each game.\nAnyway, here are the results of a game that includes some number of werewolves, one Angel, one Witch, and the rest as townspeople:\n\nSo, very close to the same as the previous plot. The Witch’s save really doesn’t change the odds all that much. There are ways that I could program it better: like they would automatically use it if they know they’re going to be killed, or the odds of using it increase the further along in the game they are. Maybe if I add this in, their effect will be stronger."
  },
  {
    "objectID": "blog/simulating_werewolf/index.html#conclusion-for-now",
    "href": "blog/simulating_werewolf/index.html#conclusion-for-now",
    "title": "Simulating Werewolf",
    "section": "Conclusion (for now)",
    "text": "Conclusion (for now)\nSo overall, it seems like the werewolves are pretty dang powerful. I read somewhere though that simulations don’t do an adequate job at capturing real-life game behavior. Trusting gut feelings and learning to read others’ behavior and body language is where the fun happens. The vote as it is now, is just chosen at random. In real life, the werewolves rarely vote off their own. Plus, the Witch gains some information each night when they’re told who’s about to get attacked because they know that person is not a werewolf so they’d probably not vote that person off down the road if that person survives. I’d to program some that in, but it gets real complex really quick.\nI’ll continue working on this in my spare time. With every additional role, it gets quite a bit more complicated. This is partly because new roles do additional things that I haven’t coded for yet, things besides just saving and killing. Also, as more roles are added, the number of combinations and obscure hypothetical increases: what happens if someone is targeted by the werewolves, saved by the Angel, and then killed by the Witch?"
  },
  {
    "objectID": "blog/kohler-tapes/index.html",
    "href": "blog/kohler-tapes/index.html",
    "title": "Kohler Tapes",
    "section": "",
    "text": "So, I just acquired a goldmine of data that I can use for linguistic analysis. Sitting in my office are 452 cassette tapes, each containing at least 30 minutes of recorded interviews with an older folks from Heber City, Utah. And that’s about half of the collection: the other half is with a historian in Midway, Utah. So, I’m looking at roughly 400–500 hours of audio. Not sure how I’m going to process it all, but I wanted to kick off the beginning of this long-term project with a blog post describing the history of the tapes, why I’m interested in them, and speculations about the future."
  },
  {
    "objectID": "blog/kohler-tapes/index.html#background",
    "href": "blog/kohler-tapes/index.html#background",
    "title": "Kohler Tapes",
    "section": "Background",
    "text": "Background\nI first heard about the tapes a little over three years ago. In January 2018, the LSA annual meeting was in Salt Lake City. Wanting to take advantage of the trip out there, I applied for and received a grant from the University of Georgia to collect audio in Heber City, aiming for multiple generations within a family to track language change over time. I decided on Heber partly because it was a region of Utah that had never been the subject of acoustic (let alone linguistic) analysis, as far as I know. My parents were living there at the time too, so they could hook me up with some potential contacts.\nSo on the morning of the first day of my fieldwork, the first thing I did was go to the Heber Valley Visitor’s Center as a way to potentially find some contacts. Literally the first person I talked to told me about a man who had a huge collection of tapes. One person led me to another, and I was talking to an elderly man named Norm Kohler in his nursing home.Side note, it’s amazing that I heard about this goldmine literally through the first person I talked to while doing fieldwork? Who knew that there’d be such an amazing collection of audio sitting in someone’s basement nearby? In fact, there could be lots of collections like these, just collecting dust in people’s basements. All it takes is to find the right person!\nNorm was a beloved middle school teacher in Heber City in the 1980s and 1990s. As a history project, he had each of his students get a cassette tape and interview a grandparent. I don’t know what the interview questions were, but I think they mostly concerned life in Heber Valley. He kept all the tapes his students turned in and, over the course of two decades, he ended up with over 1200 interviews! Norm intended to compile them and put together an oral history of the town, but unfortunately was unable to do so. So, just weeks before I met him, he decided it was best to return the tapes to the family members’ of his students and the people they interviewed. So he put an ad in the paper and hundreds of people claimed their tapes and were able to hear their ancestors’ voices, perhaps for the first time.\nHowever, not all the tapes were claimed. I was told a few hundred remained. So, after Norm passed away a few months later, his family held on to them for a while before finally donating them to the Midway Historical Society. (Midway is the town next door to Heber.) Several complications made it difficult for me to get access to the tapes, including outdated contact information on Midway’s website, the society going on an extended hiatus, me living in Georgia, and then covid. But I did my best to reach out to anyone who might know about where the tapes were being stored.\nFinally, on Thursday this week, I was contacted by the historian in custody of the tapes. She asked if I was still interested in them, and I most definitely am! So we had a nice chat about what my goals were for them and what the goals were for the Historical Society, and we think there’s mutual interest in getting them digitized and transcribed. So, the next day, yesterday, she happened to be in Provo so she dropped off about half of the tapes—452 of them!—at my office!\nSo after three years of following tenuous leads, I finally have the tapes!"
  },
  {
    "objectID": "blog/kohler-tapes/index.html#why-am-i-so-interested",
    "href": "blog/kohler-tapes/index.html#why-am-i-so-interested",
    "title": "Kohler Tapes",
    "section": "Why am I so interested?",
    "text": "Why am I so interested?\nI am a linguist, so why should I care about these tapes? Well, the obvious reason is that it’s a lot of audio. For my dissertation I analyzed about 40 hours of interviews and that was already a lot of data. This is at least 10 times the amount of audio. In fact, it’s about the size of the Digital Archive of Southern Speech, a subset of the Linguistic Atlas of the Gulf States, that I spent four years in grad school analyzing. So having access to this much audio is absolutely incredible.\nBut it’s not just the amount of audio. There are dozens of oral history projects even in Utah. This particular set is attractive for several reasons:\n\nThe nature of the homework assignment ensured good metadata. A few tapes have already been digitized and they all start off introducing the interviewer (the middle-schooler) and the interviewee, with information like the date of interview, their age, and where they grew up.\nBecause these were all students in the same smallish town in Utah the sample will be relatively homogeneous geographically. While it doesn’t ensure that the interviewees (the grandparents) were from Heber or Heber Valley generally, my guess is that a significant number of them are.\nTypically, interviews happen with a historian or someone that the interviewer is unfamiliar with. In sociolinguistics, it’s generally accepted that the degree of familiarity with the interviewer can have an influence on a person’s speech. In all cases with these tapes, the interviewer is a teenager and a grandchild of the interviewee. So that lowers the formality of the situation and will likely mean that the interviewees’ speech will be more casual.\nHeber Valley has been the focus of very little acoustic research. There may be occasional interviews as parts of the Linguistic Atlas Project or the Dictionary of Regional American English, but no study, as far as I know, has focused on Heber. Instead, most research looks at people from Utah Valley and Salt Lake Valley. This collection of interviews will offer a new spot on the map of Utah dialectology and a nice point of comparison between more urban and more rural areas of the state.\nI have virtually no metadata about the interviewees right now, but if their grandchildren were about 14 years old in the 1980s and 1990s, then the speakers in these tapes were born perhaps sometime between 1900 and 1940. There has been some research on the development of Utah English, mostly by David Bowie, but he acknowledges that it was based on public sermons given by upper-class white men. This collection offers a unique look into how other Utahns born around that time talked. And since I have some comparable data from contemporary Heber City residents, I can begin to look at language change in real time.\n\nThere were about 4500 people living in Heber in the 1980s and 1990s, which means this sample is a significant chunk of the community!So there are lots of reasons for why I’m really interested in this collection of tapes. And that’s on top of the oral history the Midway Historical Society wants to create based on them."
  },
  {
    "objectID": "blog/kohler-tapes/index.html#looking-ahead",
    "href": "blog/kohler-tapes/index.html#looking-ahead",
    "title": "Kohler Tapes",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nLuckily, I’ve had some experience working on a project of this size. For four years at the University of Georgia, I was a part of the team that processed the Digital Archive of Southern Speech, which is a 367-hour subset of the Linguistic Atlas of the Gulf States. So I’ve sat in on transcriber training sessions, seen what kinds of obstacles get in the way of processing, managed thousands of files, and analyzed spreadsheets with a couple million acoustic measurements in them. However, that was only as a graduate student. I’m sure there’s a lot that goes on behind the scenes as a PI that I didn’t see.\nTranscribers—Some back-of-the-envelope calculations suggest that I’ll need a sizable grant to get this all processed. Again, I don’t have definite numbers for anything, I know my 452 tapes are a little over half of them, so let’s say there are 700 tapes total. They’re all at least 30 minutes long and I know many went longer, so if I average say 40 minutes per tape, that’s 28,000 minutes or roughly 467 hours. I think the the transcribers for DASS averaged about 13 hours per 50 minutes of audio or so, but this audio is newer and I presume Utah transcribers will be more familiar with Utah speakers I think, so I’ll estimate 10 hours of work per tape. That’s 4670 hours of transcription. At $15 per hour, I’m looking at about $70,000 in student wages. Obviously, I can’t get that much coin internally so it sounds like this is only going to happen with an external grant.\nGrad student workers—That’s of course assuming that the only wages I’ll need to pay for are transcribers. This might be getting into “If you give a mouse a cookie” territory, but it would be nice to have some grad students helping out with the project. At UGA, we had at least four and as many as six grad students involved in the project at a time. There was a lot of overlap between our duties, but very roughly speaking, one managed the transcribers, one managed the spot-checks, one managed the acoustic analysis, and one did miscellaneous duties. We were all involved in analysis, and a few others popped in for a semester or two to do additional analysis or perform other duties. To lighten my load, it would be handy to have perhaps three grad students manage the transcribers, check their work, and do the acoustic analysis. I’m fuzzy on what costs are associated with RA-ships at BYU, but I do know it’ll add significantly to the total cost of the project.\nTime—How long will transcriptions take? I’ve done transcriptions and they’re soul-sucking work. Even when I was highly motivated to process my own dissertation data, that I collected myself, and under a bit of a time crunch, I could barely put in more than about two hours a day. I surely don’t expect undergraduate transcribers to do more than 10 hours a week. When motivated by money, I’ve seen some at UGA do more, but those students were exceptional. I’ll estimate five hours of work per transcriber per week. So under the assumption of 4670 hours of work total, that’s 934 transcriber-weeks. If a semester is fifteen weeks, that’s 62 transcriber-semesters. If I set a goal of getting all the work done in two years (six semesters if you include summers), it would take ten or eleven transcribers to do it in two years. Of course, these are all very rough estimates, but managing several tens of thousands of dollars and almost a dozen workers for two years is not something I expected to do right away!\nDigitizing—Regardless of the cost, number of workers, and time involved, the first step of the process will be digitization. Fortunately, it sounds like the Office of Digital Humanities can take care of that for me! Wow! So my short term goal is to get a batch—maybe 30 or 50 tapes—done first. While they work on digitizing the next batch, I can get started on listening to the first few minutes of the completed tapes and extracting whatever metadata I can from them. Eventually, all the tapes will be digitized and I can have a more concrete idea of how much audio (and consequently, people, time, hours, and money) I’m looking at.\nMetadata—After digitizing all of them, my next step will be to finish collecting the metadata. It’ll be nice to have a clear picture of birth years, genders, and birthplaces for all 700 or so people. The most likely scenario is that I won’t get an external grant because they’re extremely competitive, so I’ll have to prioritize which ones to transcribe first. The Historical Society would like to start with some of the prominent members of the community and descendants of the town’s founders. I’d like to find a balance of genders and birth years too, so we’ll probably settle on a subset that satisfies both of our needs. How big? I’m thinking between 35 and 70 (5% to 10% of the tapes). That’s a more reasonably-sized project that I could possibly get funded internally. It could provide me at least a beginning look at the speech community which would help seed an external grant.\nFollow-up project?—In case I just need more data to analyze (ha!) wouldn’t it be cool to track down some of the tapes that were given away? Presumably, if an ad in the paper is what it took for the families to get them, then an ad might be a good place to start to find them. We’d digitize the tapes right there for people, give them a copy and return the tape to them of course, but then also add that to the collection for the oral history. I think it would be especially cool to interview those people themselves! That way we can get some contemporary data to compare the tapes to, as well as track change within the family. That’ll have to wait until I get NSF grant number two!\nPublications—What’s the end goal? Well, I’ll obviously start cranking out some papers as soon as a reasonable amount of data has been processed. There is a lot going on in Utah English. Many of the stereotyped features are dying out, so these people may provide good acoustic data for what would otherwise be hard to study phonetically today. But there are also lots of other features that I believe are recent innovations, so if they’re infrequent or missing from these speakers, it’ll help establish the timing of when they did develop. Even before I had the tapes, I’ve been thinking a full analysis of this collection deserves a book-length treatment. It likely won’t get done before I’m up for tenure, but maybe it’ll go towards my application for full professor."
  },
  {
    "objectID": "blog/kohler-tapes/index.html#conclusion",
    "href": "blog/kohler-tapes/index.html#conclusion",
    "title": "Kohler Tapes",
    "section": "Conclusion",
    "text": "Conclusion\nThe history of the Kohler Tapes is pretty cool, and I’m lucky to be a part of the creation of an oral history of Heber City. It’s so satisfying teaming up with a historical society and finding ways to help the community I’m studying too. Linguistically, they’re interesting to me for lots of reasons, but I think everyone benefits from seeing these tapes get processed. As far as how I’m going to go about processing all of them, I really have no idea what I’m doing so there will be a lot of learning involved. But I’m excited to be involved and to have a clear research trajectory for the next decade or so!"
  },
  {
    "objectID": "data/index.html",
    "href": "data/index.html",
    "title": "Datasets",
    "section": "",
    "text": "Below you will find some datasets I use for various workshops. They are all either my own data (so like, based on my voice or behavior) or are publicly available.\n\n\nA dataset of containing a bunch of information about 80 different kinds of breakfast cereal. I use this in some of my ggplot2 workshops. I got the dataset from the the one that Chris Crawford at Kaggle.com has made available but I’ve removed some of the columns to make it more manageable for a workshop. In his words, “If you like to eat cereal, do yourself a favor and avoid this dataset at all costs. After seeing these data it will never be the same for me to eat Fruity Pebbles again.”\n\n\n\nThis dataset from me reading about 300 sentences at home in my kitchen. I had it automatically transcribed, force-aligned, and formant-extracted using DARLA and I took zero effort to clean it up so it’s a little messy. Because DARLA utilizes FAVE for formant extraction, all the typical FAVE columns are there. This dataset is the one I use in a lot of my tutorials.\n\n\n\nA dataset of each menu item at McDonald’s. I use this in some of my ggplot2 workshops. The full dataset, with many more columns than what I have provided here, is available on Kaggle.com, which was ultimately scraped from the McDonald’s website. I’ve trimmed it down a little bit for the purposes of the workshop.\n\n\n\nThis is data about the atheletes, events, from the Olympic games. The dataset was downloaded from Kaggle.com, which was made available thanks to user Randi Griffin. I use this in my tidyverse workshops to illustrate joining different datasetes.\n\n\n\nThis is a collection of recordings from two projects I’ve done. The ones labeled Carol, Daniel, Kathleen, Doug, and Margaret are roughly one-minute clips from interviews I did in Washington and contain first-hand accounts of Mount St. Helens. Stephanie, Jordan, Erica, Julia, Sabrina, Rodney, and Corey are one-sentence recordings collected via Amazon Mechanical Turk from Western American English speakers. There is also one clip of my own voice in there. Each recording comes with a sentence, word, and phoneme-level transcription. These speakers have given consent for me to distribute brief recordings for educational purposes. All names are pseudonyms.\n\n\n\nThis is data about when I’ve woken up every morning for about two years. Just information I downloaded from my iPhone’s Health app. I use this in some Tidyverse workshops for simple illustrations of reading in an excel file and doing some data manipulation and joining.\n\n\n\nA small dataset with basic information about each episode of Stranger Things. I use this in my most recent ggplot2 workshops. I went to IMDb’s data download page to actually get it, and did a little bit of prep work to make it ready for the workshop.\n\n\n\nA small dataset simply listing the top 25 most common baby girl names in 2017 in the US and how many babies were given those names. Created with the help of the babynames package which contains data from the Social Security database. This is used in the ggplot2 workshop. Note that this is saved as a tab-delimited file to help practice reading in different file types.\n\n\n\nTwo very small datasets of sample acoustic measurements. They contain identical information, but one is “tall” and the other is “wide.” They’re used in my first Tidyverse workshop to illustrate reshaping dataframes."
  },
  {
    "objectID": "data/index.html#cereal-.csv-or-.txt.",
    "href": "data/index.html#cereal-.csv-or-.txt.",
    "title": "Datasets",
    "section": "",
    "text": "A dataset of containing a bunch of information about 80 different kinds of breakfast cereal. I use this in some of my ggplot2 workshops. I got the dataset from the the one that Chris Crawford at Kaggle.com has made available but I’ve removed some of the columns to make it more manageable for a workshop. In his words, “If you like to eat cereal, do yourself a favor and avoid this dataset at all costs. After seeing these data it will never be the same for me to eat Fruity Pebbles again.”"
  },
  {
    "objectID": "data/index.html#joeys-vowels",
    "href": "data/index.html#joeys-vowels",
    "title": "Datasets",
    "section": "",
    "text": "This dataset from me reading about 300 sentences at home in my kitchen. I had it automatically transcribed, force-aligned, and formant-extracted using DARLA and I took zero effort to clean it up so it’s a little messy. Because DARLA utilizes FAVE for formant extraction, all the typical FAVE columns are there. This dataset is the one I use in a lot of my tutorials."
  },
  {
    "objectID": "data/index.html#mcdonalds-menu-items",
    "href": "data/index.html#mcdonalds-menu-items",
    "title": "Datasets",
    "section": "",
    "text": "A dataset of each menu item at McDonald’s. I use this in some of my ggplot2 workshops. The full dataset, with many more columns than what I have provided here, is available on Kaggle.com, which was ultimately scraped from the McDonald’s website. I’ve trimmed it down a little bit for the purposes of the workshop."
  },
  {
    "objectID": "data/index.html#olympic-games-athletes-events-years",
    "href": "data/index.html#olympic-games-athletes-events-years",
    "title": "Datasets",
    "section": "",
    "text": "This is data about the atheletes, events, from the Olympic games. The dataset was downloaded from Kaggle.com, which was made available thanks to user Randi Griffin. I use this in my tidyverse workshops to illustrate joining different datasetes."
  },
  {
    "objectID": "data/index.html#sample-audio",
    "href": "data/index.html#sample-audio",
    "title": "Datasets",
    "section": "",
    "text": "This is a collection of recordings from two projects I’ve done. The ones labeled Carol, Daniel, Kathleen, Doug, and Margaret are roughly one-minute clips from interviews I did in Washington and contain first-hand accounts of Mount St. Helens. Stephanie, Jordan, Erica, Julia, Sabrina, Rodney, and Corey are one-sentence recordings collected via Amazon Mechanical Turk from Western American English speakers. There is also one clip of my own voice in there. Each recording comes with a sentence, word, and phoneme-level transcription. These speakers have given consent for me to distribute brief recordings for educational purposes. All names are pseudonyms."
  },
  {
    "objectID": "data/index.html#snoozing",
    "href": "data/index.html#snoozing",
    "title": "Datasets",
    "section": "",
    "text": "This is data about when I’ve woken up every morning for about two years. Just information I downloaded from my iPhone’s Health app. I use this in some Tidyverse workshops for simple illustrations of reading in an excel file and doing some data manipulation and joining."
  },
  {
    "objectID": "data/index.html#stranger-things",
    "href": "data/index.html#stranger-things",
    "title": "Datasets",
    "section": "",
    "text": "A small dataset with basic information about each episode of Stranger Things. I use this in my most recent ggplot2 workshops. I went to IMDb’s data download page to actually get it, and did a little bit of prep work to make it ready for the workshop."
  },
  {
    "objectID": "data/index.html#top-25-girl-names-in-2017",
    "href": "data/index.html#top-25-girl-names-in-2017",
    "title": "Datasets",
    "section": "",
    "text": "A small dataset simply listing the top 25 most common baby girl names in 2017 in the US and how many babies were given those names. Created with the help of the babynames package which contains data from the Social Security database. This is used in the ggplot2 workshop. Note that this is saved as a tab-delimited file to help practice reading in different file types."
  },
  {
    "objectID": "data/index.html#vowels-tall-and-wide",
    "href": "data/index.html#vowels-tall-and-wide",
    "title": "Datasets",
    "section": "",
    "text": "Two very small datasets of sample acoustic measurements. They contain identical information, but one is “tall” and the other is “wide.” They’re used in my first Tidyverse workshop to illustrate reshaping dataframes."
  },
  {
    "objectID": "pages/setup/index.html",
    "href": "pages/setup/index.html",
    "title": "Setup Instructions",
    "section": "",
    "text": "Thanks for your interest in one of my workshops. Please follow the steps below to get R and RStudio installed to your computer before the workshop starts. If you’re having difficulty, please come a little early and I can help you out."
  },
  {
    "objectID": "pages/setup/index.html#step-1-get-r-installed",
    "href": "pages/setup/index.html#step-1-get-r-installed",
    "title": "Setup Instructions",
    "section": "Step 1: Get R installed",
    "text": "Step 1: Get R installed\nTo download R, go to https://cran.r-project.org/mirrors.html. This will take you to a list of CRAN mirrors. All these lists of sites are identical, they’re just hosted on various servers across the world to handle the traffic. Just pick one near your current location and click on it.\n\n\nFrom there, download the package appropriate for your computer.\n\nMac users will be taken to a screen where they’ll give you various versions to choose from. At the time of writing, the latest package is 3.6.1, so go ahead and download that one and install it like any other piece of software.\n\nWindows users will have a link that says “install R for the first time” which will take them to the download page. You can then install R like normal.\n \nI’m not entirely sure how to do it on a Linux, but I figure if you’re using a Linux, you know what you’re doing :)"
  },
  {
    "objectID": "pages/setup/index.html#step-2-get-rstudio-installed",
    "href": "pages/setup/index.html#step-2-get-rstudio-installed",
    "title": "Setup Instructions",
    "section": "Step 2: Get RStudio installed",
    "text": "Step 2: Get RStudio installed\nTo download RStudio, go to https://www.rstudio.com and click “Download” under RStudio.\n\nThere are several intense versions of RStudio and we only need the free Desktop version, which is the one furthest to the left. Click the download button and then click on the link appropriate for your operating system."
  },
  {
    "objectID": "pages/setup/index.html#step-3-install-some-packages",
    "href": "pages/setup/index.html#step-3-install-some-packages",
    "title": "Setup Instructions",
    "section": "Step 3: Install some packages",
    "text": "Step 3: Install some packages\nOpen RStudio. In the left-hand side, type the following commands and hit enter:\ninstall.packages(\"tidyverse\")\n\nThis will take a minute or so. This command installs a couple add-on packages that I frequently use in my workshops, including ggplot2, dplyr, tidyr and several others.\nAnd that’s it! See you at the workshop!\n\n\n« Back to the workshops page"
  },
  {
    "objectID": "pages/gsv.html",
    "href": "pages/gsv.html",
    "title": "GSV",
    "section": "",
    "text": "If you are not redirected to the Gazetteer of Southern Vowels, click here."
  },
  {
    "objectID": "pages/r-workshops.html",
    "href": "pages/r-workshops.html",
    "title": "R Workshops",
    "section": "",
    "text": "In grad school I offered workshops on a variety of topics relating to R between Fall 2017 and Spring 2020. They were held Fridays at 3:30 in the DigiLab at the UGA Main Library. Videos for some of these are available here. Below you will find materials for these workshops. I hope they are thorough enough that you can make use of them as stand-alone documents.\nThe data used in these workshops can be found at my datasets page.\nIf you are planning on attending a workshop in the near future, please see the setup instructions to help you get R and RStudio installed on your computer."
  },
  {
    "objectID": "pages/r-workshops.html#intro-to-r-part-1-and-part-2",
    "href": "pages/r-workshops.html#intro-to-r-part-1-and-part-2",
    "title": "R Workshops",
    "section": "Intro to R (Part 1 and Part 2)",
    "text": "Intro to R (Part 1 and Part 2)\nIn this first workshop I cover the basics of R itself. I talk about the differences between R and RStudio and I help folks get both installed and running on their computers. We create a simple “Hello, World!” script using R. Part 2 covers the basics of the R language. It is also be a very simple introduction to some core computer coding concepts like declaring variables and variable types.\n\n\n\n\n\n\nTip\n\n\n\nAdditional Materials: You may also be interested in a PDF of the slides I used in Part 1 or a PDF version of Part 2’s handout. An older version that combines elements of both parts can be found here as a PDF or RMarkdown file."
  },
  {
    "objectID": "pages/r-workshops.html#visualizations-with-ggplot2",
    "href": "pages/r-workshops.html#visualizations-with-ggplot2",
    "title": "R Workshops",
    "section": "Visualizations with ggplot2 ",
    "text": "Visualizations with ggplot2 \nggplot2 is a widely used package that allows for high-quality visualizations. These workshops take you from installation to pretty advanced topics.\n\nPart 1: Intro to ggplot2\nIn Part 1 of this workshop I cover the basic syntax and how to make some simple types of plots.\n\n\nPart 2: Extending your ggplot2 skills\nOften you’ll want to customize your plots in some way. So, in Part 2, we cover how to mess with properties of the plots like the axes, colors, and legends to make the plot work better for you.\n\n\nSupplement to Part 2\nApparently I had a lot to say about how to extend your ggplot2 skills, so I ended up creating a supplement with lots of additional detail on how to modify your plots. This handout will vary from time to time as I add to it when I learn new things or remove sections to incorporate them into future workshops.\n\n\nCustom Themes\nBased on a popular blog post I wrote, this workshop wraps all customization methods together and shows how to create your own themes.\n\n\nSupplement: A detailed look at ggplot2::theme\nAs I was preparing for the custom themes workshop, I got a little carried away illustrating all the components of the theme function. I decided to simplify that portion of the workshop and create this separate handout that just focuses on theme. It is not yet finished, but it may be of some help to people (including myself!).\n\n\n\n\n\n\nTip\n\n\n\nAdditional Materials: You may also be interested in the 2018 versions of some of these workshops that I gave (Part 1 Rmarkdown and PDF and Part 2 RMarkdown and PDF). An older version from 2017 that combines elements of Parts 1 and 2 can be found here as a PDF or RMarkdown file."
  },
  {
    "objectID": "pages/r-workshops.html#introduction-to-the-tidyverse-part-1-and-part-2",
    "href": "pages/r-workshops.html#introduction-to-the-tidyverse-part-1-and-part-2",
    "title": "R Workshops",
    "section": "Introduction to the Tidyverse (Part 1 and Part 2)",
    "text": "Introduction to the Tidyverse (Part 1 and Part 2)\nThe tidyverse is a suite of packages that includes dplyr and tidyr which help you wrangle your data. In this two-part workshop, we learn some of the common functions in the tidyverse and compare them to base R, showing that there are multiple ways to accomplish the same task in R. Part 2 in particular looks at how to reshape and transform your data, merging it with other dataset, and other super useful and powerful tools.\n\n\n\n\n\n\nTip\n\n\n\nAdditional Materials: You may also be interested in PDFs for Part 1 and Part 2. An older version that combines elements of both parts can be found here as a PDF or RMarkdown file."
  },
  {
    "objectID": "pages/r-workshops.html#an-intro-to-rmarkdown",
    "href": "pages/r-workshops.html#an-intro-to-rmarkdown",
    "title": "R Workshops",
    "section": "An Intro to RMarkdown",
    "text": "An Intro to RMarkdown\nR Markdown is a way to create different types of documents using R (pdfs, word files, html files). In this one-day crash course, I show how to make R Markdown files and the kinds of things they would be useful for.\n\n\n\n\n\n\nTip\n\n\n\nAdditional Materials: If you’d rather use a PDF of the workshop materials, there’s one available here."
  },
  {
    "objectID": "pages/r-workshops.html#introduction-to-shiny",
    "href": "pages/r-workshops.html#introduction-to-shiny",
    "title": "R Workshops",
    "section": "Introduction to Shiny",
    "text": "Introduction to Shiny\nShiny is an R package that allows you to make your own interactive web pages. An entire semester could be devoted to Shiny and there’s a bit of a learning curve, especially if you haven’t used HTML before. This two-day workshop covers just the essentials.\n\n\n\n\n\n\nNote\n\n\n\nThe materials for this workshop include interactive shiny elements, which means I can’t host them on my own website. So instead, they’re hosted on Shiny’s free server space. But this comes with a major drawback: they’re only available for 25 user-hours a month. So, if the link above does not work for you, try again in a week or so. Sorry for the inconvenience."
  },
  {
    "objectID": "pages/dataviz.html#part-1-intro-to-ggplot2",
    "href": "pages/dataviz.html#part-1-intro-to-ggplot2",
    "title": "Data Visualization Workshops",
    "section": "Part 1: Intro to ggplot2",
    "text": "Part 1: Intro to ggplot2\nAugust 21, 2019—In Part 1 of this workshop I cover the basic syntax and how to make some simple types of plots."
  },
  {
    "objectID": "pages/dataviz.html#part-2-extending-your-ggplot2-skills",
    "href": "pages/dataviz.html#part-2-extending-your-ggplot2-skills",
    "title": "Data Visualization Workshops",
    "section": "Part 2: Extending your ggplot2 skills",
    "text": "Part 2: Extending your ggplot2 skills\nAugust 28, 2019—Often you’ll want to customize your plots in some way. So, in this workshop we cover how to mess with properties of the plots like the axes, colors, and legends to make the plot work better for you.\n\nSupplement to Part 2\nApparently I had a lot to say about how to extend your ggplot2 skills, so I ended up creating a supplement with lots of additional detail on how to modify your plots."
  },
  {
    "objectID": "pages/dataviz.html#custom-themes",
    "href": "pages/dataviz.html#custom-themes",
    "title": "Data Visualization Workshops",
    "section": "Custom Themes",
    "text": "Custom Themes\nSeptember 4, 2019–Based on a popular blog post I wrote, this workshop wraps all customization methods together and shows how to create your own themes.\n\nSupplement to Part 3: A detailed look at ggplot2::theme\nAs I was preparing for the custom themes workshop, I got a little carried away illustrating all the components of the theme function. I decided to simplify that portion of the workshop and create this separate handout that just focuses on theme. It is not yet finished, but it may be of some help to people (including myself!)."
  },
  {
    "objectID": "pages/dataviz.html#fidelity-integrity-and-sophistication-edward-tuftes-principles-of-data-visualization",
    "href": "pages/dataviz.html#fidelity-integrity-and-sophistication-edward-tuftes-principles-of-data-visualization",
    "title": "Data Visualization Workshops",
    "section": "Fidelity, integrity, and sophistication: Edward Tufte’s principles of data visualization",
    "text": "Fidelity, integrity, and sophistication: Edward Tufte’s principles of data visualization\nOctober 16, 2019—In this workshop, I introduce a few key concepts from Edward Tufte’s book, The Visual Display of Quantitative Information such as graphical integrity, proportional ink, data-ink ratio, removing redundant material, and general graphical sophistication. Basically, the workshop could be thought of as, “How not to make awful plots.”"
  },
  {
    "objectID": "pages/dataviz.html#send-the-right-message-the-dos-and-donts-of-color-in-data-visualization",
    "href": "pages/dataviz.html#send-the-right-message-the-dos-and-donts-of-color-in-data-visualization",
    "title": "Data Visualization Workshops",
    "section": "Send the right message: The dos and don’ts of COLOR in data visualization",
    "text": "Send the right message: The dos and don’ts of COLOR in data visualization\nOctober 23, 2019—Meagan Duever, GIS Librarian at UGA will help me lead this workshop. We talk about general principles of color in data visualization, introduce a whole bunch of nice color palettes, and demonstrate how to customize your colors in ArcMap, QGIS, Excel, and R."
  },
  {
    "objectID": "pages/dataviz.html#older-versions",
    "href": "pages/dataviz.html#older-versions",
    "title": "Data Visualization Workshops",
    "section": "Older versions",
    "text": "Older versions\nYou may also be interested in the 2018 versions of some of these workshops that I gave (Part 1 Rmarkdown and PDF and Part 2 RMarkdown and PDF). An older version from 2017 that combines elements of Parts 1 and 2 can be found here as a PDF or RMarkdown file."
  },
  {
    "objectID": "todo.html#other",
    "href": "todo.html#other",
    "title": "Joey Stanley",
    "section": "Other",
    "text": "Other\nTurn Rmd handouts to incorporated quarto files: * /downloads/190821-intro_to_ggplot2.html * /downloads/190828-ggplot2_intermediate.html * /downloads/190828-ggplot2_supplement.html * /downloads/190904-ggplot2_custom_themes.html * /downloads/190904-ggplot2_theme.html * /downloads/190911-intro_to_Praat.html * /downloads/190918-praat_scripting.html * /downloads/191002-formant_extraction.html * /downloads/191001-formant_extraction_supplement.html * All R workshops\nSee what files are left in the downloads folder and figure out what to do with them."
  },
  {
    "objectID": "todo.html#cv",
    "href": "todo.html#cv",
    "title": "Joey Stanley",
    "section": "CV",
    "text": "CV\n\nFind “A workshop on preparing conference abstracts.”” Given by invitation by the Linguistics Club at the University of Georgia. Athens, GA. March 5, 2020. \n\nTrack down the first Brand Yourself workshop slides: \n\nFind dates for my college funding/grants"
  },
  {
    "objectID": "blog/jbdowse/index.html",
    "href": "blog/jbdowse/index.html",
    "title": "Visualizing Jonathan Dowse’s Vowels",
    "section": "",
    "text": "I’ve seen interactive IPA charts where a single person produces all the sounds. But a couple years ago, I was teaching a Phonetics & Phonology course, and I stumbled upon Jonathan Dowse’s IPA extended chart with audio. In this post, I take his vowels and map their formants.\nNote that I tweeted about this on Frburary 7, 2022. I don’t know what the future of X holds, so I thought I’d make this a more perminant home for this plot."
  },
  {
    "objectID": "blog/jbdowse/index.html#dowses-ipa-charts",
    "href": "blog/jbdowse/index.html#dowses-ipa-charts",
    "title": "Visualizing Jonathan Dowse’s Vowels",
    "section": "Dowse’s IPA charts",
    "text": "Dowse’s IPA charts\nDowse’s IPA chart is the most detailed one I’ve ever seen. The consonants include many more places of articulation not normally found on the IPA chart, like linguolabial, alveolo-palatal, rounded velar, low uvular, and aryepiglottal. For each one, he uses all the manners that are physically possible, including ones like aspirated stop, affricate, taps, and trills. And for each manner and place, he has voiced and voiceless variants. Not only is the chart itself interesting to look through with all the diacritics and stuff, but he’s got recordings of each consonant in different contexts: [C], [Ca], [aC], and [aCa]. Pretty cool. Further down, he has a whole nother table with “rarer” manners, including lateral fricatives, lateral flaps, fricative trills, implosive, and different kinds of ejectives. It’s pretty intersting to listen to them. He also has a whole table of clicks at eight places of articulation, six manners, and velar and uvular variants of each.\nLook, I don’t know enough about phonetics to say whether these are all accurately produced, but it’s impressive that Dowse, who does not appear to have much formal training in lingusitics, can produce all these sounds.\nToday’s post is not about the consonants though; it’s about the vowels. His vowel chart is equally extensive. He contrasts five front-to-back distinctions and seven height distinctions, with rounded and unrounded versions of each one. He also has an entirely separate chart showing nasalized versions of all of these.\n\n\n\nJonathan Dowse’s vowel chart\n\n\nI do know enough about phonetics to be able to look at these vowels. I was curious about how these 70 vowel qualities mapped to the acoustic space. I wanted to see whether these distinctions were all equidistant and whether their distribution in the acoustic space matched this rectangular tabular layout in the chart."
  },
  {
    "objectID": "blog/jbdowse/index.html#formant-extraction",
    "href": "blog/jbdowse/index.html#formant-extraction",
    "title": "Visualizing Jonathan Dowse’s Vowels",
    "section": "Formant extraction",
    "text": "Formant extraction\nThe first step was to download the audio, which I did by just clicking on each one and downloading them one at a time. I then processed them using FastTrack. This produces in a spreadsheet for each vowel produced, with measurements and bandwidths for the first three formants (plus some other measurements) every few milliseconds. Here’s an example from the high front vowel.\n\nlibrary(tidyverse)\nlibrary(santoku)\n\nread_csv(\"./csvs/high_front.csv\")\n\n# A tibble: 193 × 12\n     time    f1    b1    f2    b2    f3    b3   f1p   f2p   f3p    f0 intensity\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 0.0252  243   35.3 2289.  168. 3354.  230.  219. 2264. 3323.     0      53.7\n 2 0.0272  246.  34.9 2202.  323. 3357.  380.  219. 2265. 3322.     0      51.5\n 3 0.0292  247   44.1 1990.  553. 3282.  640.  219. 2266. 3322.     0      49.4\n 4 0.0312  235.  72.1 1889.  598. 3150   773   219. 2266. 3322.     0      48  \n 5 0.0332  209.  99.9 2035.  678. 3254.  883   219. 2268. 3321.     0      49  \n 6 0.0352  192.  96.2 2209.  526. 3338.  574.  219. 2269. 3320.     0      50.1\n 7 0.0372  181   83.5 2276.  387. 3336.  396.  219. 2271. 3319.     0      52.3\n 8 0.0392  173   69.6 2306.  310. 3333.  298.  219. 2273. 3318.     0      54.5\n 9 0.0412  172.  58.6 2324.  242  3338.  244.  219. 2275. 3317      0      56.6\n10 0.0432  183.  56.1 2337.  175. 3347.  228.  219. 2278. 3316.     0      58.6\n# ℹ 183 more rows\n\n\nThis is more information than I need, especially since Dowse tries to say the vowels as monophthongally as he can. But we can take a look at the trajectories in just a sec. \nSo, I want to plot the midpoints of all vowels at once. Since each is stored in a separate spreadsheet, I’ll use Sys.glob to get the paths to all those spreadsheets and map the read_csv function onto all of those paths. Since the vowel quality is stored in the filename itself (i.e., “high_front.csv”), I’ll strip away the path and the extension to leave just that filename and use it as the name of the vowel itself. Finally, I’ll take all those spreadsheets and combine them into one big one with bind_rows and unnest.\n\nvowels_raw &lt;- tibble(vowel = Sys.glob(\"./csvs/*.csv\")) %&gt;%\n  mutate(data = map(vowel, read_csv, show_col_types = FALSE),\n         vowel = str_remove_all(vowel, \"./csvs/\"),\n         vowel = str_remove_all(vowel, \".csv\")) %&gt;%\n  bind_rows() %&gt;%\n  unnest(data) %&gt;%\n  print()\n\n# A tibble: 14,772 × 13\n   vowel        time    f1    b1    f2    b2    f3    b3   f1p   f2p   f3p    f0\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 high-mid_… 0.0252  478.  90    918. 122.  2683.  324   423   863. 2696.  107 \n 2 high-mid_… 0.0272  476.  77.7  917. 100.  2689.  303.  423.  863. 2696.  107.\n 3 high-mid_… 0.0292  470.  71    914.  82.7 2695.  291.  424.  863. 2697.  107.\n 4 high-mid_… 0.0312  460.  72.9  908.  77.5 2692.  308.  424.  863. 2698.  107.\n 5 high-mid_… 0.0332  451.  78.9  901.  86.2 2680.  337.  425   864. 2700.  107.\n 6 high-mid_… 0.0352  450   79    895.  96.9 2675.  336.  426.  864. 2701.  106.\n 7 high-mid_… 0.0372  453.  74.6  891.  97.4 2685.  308.  427.  865. 2703.  106.\n 8 high-mid_… 0.0392  452.  73.7  887.  92.3 2703.  284.  428.  865. 2705.  106.\n 9 high-mid_… 0.0412  448.  81.5  880   94.6 2723.  280.  429.  866  2708.  106.\n10 high-mid_… 0.0432  446.  89.6  873. 104.  2740.  284.  431.  867. 2710.  106 \n# ℹ 14,762 more rows\n# ℹ 1 more variable: intensity &lt;dbl&gt;\n\n\nThis results in a spreadsheet with 14,772 rows, each representing a set of formant measurements at a particular point in time across all 70 recordings. Kind of a lot of data, considering it’s only 70 vowels, but that’s the kind of resolution FastTrack can give you.\nOkay, so for the purposes of the plot, I need to create a spreadsheet that is just the metadata about the vowel itself. In Step 1, I take that monster dataframe and just keep the name of the vowel (i.e. “high-mid_back-unrounded”) and only keep unique values. I then split that name up into its three parts (height, backness, and rounded) using separate. In Step 2, I then modify each one of those a little bit. I turn rounded into a boolean instead of a string. Then I turn height and backness into factors and set what order they should be in. Finally, I create a marked column to indicate whether a front vowel is rounded or a back vowel is unrounded—I felt like this might be handy to create a visual of the unmarked vowels. For Step 3, I add the IPA symbols to it. There’s no shortcut: I just had to put the symbols in one at a time based on what the chart showed.\n\nvowels_meta &lt;- vowels_raw %&gt;%\n  \n  # Step 1: split the vowel name up\n  distinct(vowel) %&gt;%\n  separate(vowel, c(\"height\", \"backness\", \"rounded\"), sep = \"_\", fill = \"right\", remove = FALSE) %&gt;%\n  \n  # Step 2: modify those attributes\n  mutate(rounded  = case_when(rounded == \"rounded\" ~ TRUE,\n                             is.na(rounded) ~ FALSE),\n         height   = factor(height,   levels = c(\"high\", \"near-high\", \"high-mid\", \"mid\", \"low-mid\", \"near-low\", \"low\")),\n         backness = factor(backness, levels = c(\"front\", \"near-front\", \"central\", \"near-back\", \"back\")),\n         marked = case_when(backness %in% c(\"near-back\", \"back\") & !rounded ~ TRUE,\n                            backness %in% c(\"front\", \"near-front\", \"central\") & rounded ~  TRUE,\n                            TRUE ~ FALSE)) %&gt;%\n  \n  # Step 3: add IPA\n  arrange(height, backness, rounded) %&gt;%\n    mutate(ipa = c(\"i\", \"y\", \"ï\", \"ÿ\", \"ɨ\", \"u̶\", \"ɯ̈\", \"ü\", \"ɯ\", \"u\",\n                   \"i̞\", \"y̙̞\", \"ɪ\", \"ʏ\", \"ɪ̈\", \"ʊ̈\", \"ɯ̽\", \"ʊ\", \"ɯ̞\", \"u̞\",\n                   \"e\", \"ø\", \"ë\", \"ø̈\", \"ɘ\", \"ɵ\", \"ɤ̈\", \"ö\", \"ɤ\", \"o\",\n                   \"e̞\", \"ø̞\", \"ë̞\", \"ø̞̈\", \"ə\", \"ɵ̘\", \"ɤ̞̈\", \"ö̞\", \"ɤ̞\", \"o̞\",\n                   \"ɛ\", \"œ\", \"ɛ̈\", \"œ̈\", \"ɜ\", \"ɞ\", \"ʌ̈\", \"ɔ̈\", \"ʌ\", \"ɔ\",\n                   \"æ\", \"œ̞\", \"æ̈\", \"ɶ̽\", \"ɐ\", \"ɞ̞\", \"ɑ̽\", \"ɒ̽\", \"ʌ̞\", \"ɔ̞\",\n                   \"a\", \"ɶ\", \"ä\", \"ɶ̈\", \"ɐ̞\", \"ɐ̞̹\", \"ɑ̈\", \"ɒ̈\", \"ɑ\", \"ɒ\")) %&gt;%\n\n  print()\n\n# A tibble: 70 × 6\n   vowel                   height backness   rounded marked ipa  \n   &lt;chr&gt;                   &lt;fct&gt;  &lt;fct&gt;      &lt;lgl&gt;   &lt;lgl&gt;  &lt;chr&gt;\n 1 high_front              high   front      FALSE   FALSE  i    \n 2 high_front_rounded      high   front      TRUE    TRUE   y    \n 3 high_near-front         high   near-front FALSE   FALSE  ï    \n 4 high_near-front_rounded high   near-front TRUE    TRUE   ÿ    \n 5 high_central            high   central    FALSE   FALSE  ɨ    \n 6 high_central_rounded    high   central    TRUE    TRUE   u̶    \n 7 high_near-back          high   near-back  FALSE   TRUE   ɯ̈    \n 8 high_near-back_rounded  high   near-back  TRUE    FALSE  ü    \n 9 high_back               high   back       FALSE   TRUE   ɯ    \n10 high_back_rounded       high   back       TRUE    FALSE  u    \n# ℹ 60 more rows\n\n\nOkay, so now I have a dataframe that has the name of the vowel from the filename, and then some metadata about that vowel.\nNow let’s go back and process that acoustic data. I’ll just keep the formants.\n\nvowels &lt;- vowels_raw %&gt;%\n    select(vowel, time, f1, f2, f3) %&gt;%\n    mutate(time_diff = time - min(time),\n           percent = time_diff / max(time_diff),\n           time_cut = kiru(percent,\n                           breaks = seq(0, 1, 0.1),\n                           labels = 1:10),\n           .by = vowel) %&gt;%\n    summarize(across(c(f1, f2, f3), median), .by = c(vowel, time_cut)) %&gt;%\n    filter(time_cut %in% 4:8) %&gt;%\n    summarize(across(c(f1, f2, f3), median), .by = vowel) %&gt;%\n    left_join(vowels_meta, by = \"vowel\") %&gt;%\n    # left_join(ipa_symbols, by = c(\"height\", \"backness\", \"rounded\")) %&gt;%\n    print()\n\n# A tibble: 70 × 9\n   vowel                     f1    f2    f3 height backness rounded marked ipa  \n   &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;lgl&gt;   &lt;lgl&gt;  &lt;chr&gt;\n 1 high-mid_back           426.  854. 2838. high-… back     FALSE   TRUE   ɤ    \n 2 high-mid_back_rounded   406.  523. 2594. high-… back     TRUE    FALSE  o    \n 3 high-mid_central        414. 1670. 2316. high-… central  FALSE   FALSE  ɘ    \n 4 high-mid_central_roun…  413.  977. 2337. high-… central  TRUE    TRUE   ɵ    \n 5 high-mid_front          415  2232. 2896. high-… front    FALSE   FALSE  e    \n 6 high-mid_front_rounded  341. 2050. 2596. high-… front    TRUE    TRUE   ø    \n 7 high-mid_near-back      472. 1192. 2552. high-… near-ba… FALSE   TRUE   ɤ̈    \n 8 high-mid_near-back_ro…  412.  731. 2516. high-… near-ba… TRUE    FALSE  ö    \n 9 high-mid_near-front     414. 1940  2480. high-… near-fr… FALSE   FALSE  ë    \n10 high-mid_near-front_r…  397. 1686. 2219. high-… near-fr… TRUE    TRUE   ø̈    \n# ℹ 60 more rows\n\n\n\nvowels %&gt;%\n    distinct() %&gt;%\n    unite(line_id, height, rounded, remove = FALSE) %&gt;%\n    # filter(!marked) %&gt;%\n    ggplot(aes(f2, f1, color = height, shape = backness)) + \n    # geom_point(size = 4) + \n    geom_line(aes(group = line_id, linetype = rounded)) + \n    geom_label(aes(label = ipa, fill = rounded), size = 5) +\n    scale_fill_manual(values = c(\"white\", \"gray90\")) +\n    scale_x_reverse() + \n    scale_y_reverse() + \n    ggthemes::scale_color_ptol() + \n    labs(title = \"Acoustic measurements from jbdowse.com/ipa/\",\n         x = \"F2\", y = \"F1\",\n         caption = 'Along the front-to-back dimension, the vowels are \"front\", \"near-front\", \"central\", \"near-back\", and \"back.\"\\nLines connect vowels of the same height and rounding. Dotted lines connect rounded vowels.',) + \n    theme_minimal()\n\n\n\n\n\nvowels %&gt;%\n    distinct() %&gt;%\n    filter(!marked) %&gt;%\n    ggplot(aes(f2, f1, color = height, shape = backness)) + \n    # geom_point(size = 4) + \n    geom_line(aes(group = height)) + \n    geom_label(aes(label = ipa), size = 5) +\n    scale_x_reverse() + \n    scale_y_reverse() + \n    ggthemes::scale_color_ptol() + \n    theme_minimal()\n\n\n\n\nLook at trajectoires."
  },
  {
    "objectID": "blog/jbdowse/index.html#data-processing",
    "href": "blog/jbdowse/index.html#data-processing",
    "title": "Visualizing Jonathan Dowse’s Vowels",
    "section": "Data Processing",
    "text": "Data Processing\nThe first step was to download the audio, which I did by just clicking on each one and downloading them one at a time. I then processed them using FastTrack. This produces in a spreadsheet for each vowel produced, with measurements and bandwidths for the first three formants (plus some other measurements) every few milliseconds. Here’s an example from the high front vowel.\n\nlibrary(tidyverse)\nlibrary(santoku)\n\nhigh_front &lt;- read_csv(\"./csvs/high_front.csv\")\n\nThis is more information than I need, especially since Dowse tries to say the vowels as monophthongally as he can. But we can take a look at the trajectories in just a sec. \nSo, I want to plot the midpoints of all vowels at once. Since each is stored in a separate spreadsheet, I’ll use Sys.glob to get the paths to all those spreadsheets and map the read_csv function onto all of those paths. Since the vowel quality is stored in the filename itself (i.e., “high_front.csv”), I’ll strip away the path and the extension to leave just that filename and use it as the name of the vowel itself. Finally, I’ll take all those spreadsheets and combine them into one big one with bind_rows and unnest.\n\nvowels_raw &lt;- tibble(vowel = Sys.glob(\"./csvs/*.csv\")) %&gt;%\n  mutate(data = map(vowel, read_csv, show_col_types = FALSE),\n         vowel = str_remove_all(vowel, \"./csvs/\"),\n         vowel = str_remove_all(vowel, \".csv\")) %&gt;%\n  bind_rows() %&gt;%\n  unnest(data) %&gt;%\n  print()\n\n# A tibble: 14,772 × 13\n   vowel        time    f1    b1    f2    b2    f3    b3   f1p   f2p   f3p    f0\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 high-mid_… 0.0252  478.  90    918. 122.  2683.  324   423   863. 2696.  107 \n 2 high-mid_… 0.0272  476.  77.7  917. 100.  2689.  303.  423.  863. 2696.  107.\n 3 high-mid_… 0.0292  470.  71    914.  82.7 2695.  291.  424.  863. 2697.  107.\n 4 high-mid_… 0.0312  460.  72.9  908.  77.5 2692.  308.  424.  863. 2698.  107.\n 5 high-mid_… 0.0332  451.  78.9  901.  86.2 2680.  337.  425   864. 2700.  107.\n 6 high-mid_… 0.0352  450   79    895.  96.9 2675.  336.  426.  864. 2701.  106.\n 7 high-mid_… 0.0372  453.  74.6  891.  97.4 2685.  308.  427.  865. 2703.  106.\n 8 high-mid_… 0.0392  452.  73.7  887.  92.3 2703.  284.  428.  865. 2705.  106.\n 9 high-mid_… 0.0412  448.  81.5  880   94.6 2723.  280.  429.  866  2708.  106.\n10 high-mid_… 0.0432  446.  89.6  873. 104.  2740.  284.  431.  867. 2710.  106 \n# ℹ 14,762 more rows\n# ℹ 1 more variable: intensity &lt;dbl&gt;\n\n\nThis results in a spreadsheet with 14,772 rows, each representing a set of formant measurements at a particular point in time across all 70 recordings. Kind of a lot of data, considering it’s only 70 vowels, but that’s the kind of resolution FastTrack can give you.\nOkay, so for the purposes of the plot, I need to create a spreadsheet that is just the metadata about the vowel itself. In Step 1, I take that monster dataframe and just keep the name of the vowel (i.e. “high-mid_back-unrounded”) and only keep unique values. I then split that name up into its three parts (height, backness, and rounded) using separate. In Step 2, I then modify each one of those a little bit. I turn rounded into a boolean instead of a string. Then I turn height and backness into factors and set what order they should be in. Finally, I create a marked column to indicate whether a front vowel is rounded or a back vowel is unrounded—I felt like this might be handy to create a visual of the unmarked vowels. For Step 3, I add the IPA symbols to it. There’s no shortcut: I just had to put the symbols in one at a time based on what the chart showed.\n\nvowels_meta &lt;- vowels_raw %&gt;%\n  \n  # Step 1: split the vowel name up\n  distinct(vowel) %&gt;%\n  separate(vowel, c(\"height\", \"backness\", \"rounded\"), sep = \"_\", fill = \"right\", remove = FALSE) %&gt;%\n  \n  # Step 2: modify those attributes\n  mutate(rounded  = case_when(rounded == \"rounded\" ~ TRUE,\n                             is.na(rounded) ~ FALSE),\n         height   = factor(height,   levels = c(\"high\", \"near-high\", \"high-mid\", \"mid\", \"low-mid\", \"near-low\", \"low\")),\n         backness = factor(backness, levels = c(\"front\", \"near-front\", \"central\", \"near-back\", \"back\")),\n         marked = case_when(backness %in% c(\"near-back\", \"back\") & !rounded ~ TRUE,\n                            backness %in% c(\"front\", \"near-front\", \"central\") & rounded ~  TRUE,\n                            TRUE ~ FALSE)) %&gt;%\n  \n  # Step 3: add IPA\n  arrange(height, backness, rounded) %&gt;%\n    mutate(ipa = c(\"i\", \"y\", \"ï\", \"ÿ\", \"ɨ\", \"u̶\", \"ɯ̈\", \"ü\", \"ɯ\", \"u\",\n                   \"i̞\", \"y̙̞\", \"ɪ\", \"ʏ\", \"ɪ̈\", \"ʊ̈\", \"ɯ̽\", \"ʊ\", \"ɯ̞\", \"u̞\",\n                   \"e\", \"ø\", \"ë\", \"ø̈\", \"ɘ\", \"ɵ\", \"ɤ̈\", \"ö\", \"ɤ\", \"o\",\n                   \"e̞\", \"ø̞\", \"ë̞\", \"ø̞̈\", \"ə\", \"ɵ̘\", \"ɤ̞̈\", \"ö̞\", \"ɤ̞\", \"o̞\",\n                   \"ɛ\", \"œ\", \"ɛ̈\", \"œ̈\", \"ɜ\", \"ɞ\", \"ʌ̈\", \"ɔ̈\", \"ʌ\", \"ɔ\",\n                   \"æ\", \"œ̞\", \"æ̈\", \"ɶ̽\", \"ɐ\", \"ɞ̞\", \"ɑ̽\", \"ɒ̽\", \"ʌ̞\", \"ɔ̞\",\n                   \"a\", \"ɶ\", \"ä\", \"ɶ̈\", \"ɐ̞\", \"ɐ̞̹\", \"ɑ̈\", \"ɒ̈\", \"ɑ\", \"ɒ\")) %&gt;%\n\n  print()\n\n# A tibble: 70 × 6\n   vowel                   height backness   rounded marked ipa  \n   &lt;chr&gt;                   &lt;fct&gt;  &lt;fct&gt;      &lt;lgl&gt;   &lt;lgl&gt;  &lt;chr&gt;\n 1 high_front              high   front      FALSE   FALSE  i    \n 2 high_front_rounded      high   front      TRUE    TRUE   y    \n 3 high_near-front         high   near-front FALSE   FALSE  ï    \n 4 high_near-front_rounded high   near-front TRUE    TRUE   ÿ    \n 5 high_central            high   central    FALSE   FALSE  ɨ    \n 6 high_central_rounded    high   central    TRUE    TRUE   u̶    \n 7 high_near-back          high   near-back  FALSE   TRUE   ɯ̈    \n 8 high_near-back_rounded  high   near-back  TRUE    FALSE  ü    \n 9 high_back               high   back       FALSE   TRUE   ɯ    \n10 high_back_rounded       high   back       TRUE    FALSE  u    \n# ℹ 60 more rows\n\n\nOkay, so now I have a dataframe that has the name of the vowel from the filename, and then some metadata about that vowel.\nNow let’s go back and process that acoustic data. I start by taking the raw data and just keeping the formants. Having like 170 timepoints for each vowel is fine, but when visualizing such data, the odds of getting a wonky one are higher and it’ll ruin the whole plot. Here’s [i]. You can see that most of the formants are pretty stable. But towards the beginning and end, things get weird and they distract from the good data.\n\nggplot(high_front, aes(f2, f1, color = time)) + \n  geom_path() + \n  scale_x_reverse() + \n  scale_y_reverse() +\n  theme_minimal()\n\n\n\n\nWhat I’ll do then is take all this high resolution temporal data and make it lower resolution. What I found works is to bin the times into about 10 bins and then take the median within each one. I’ll do this by first normalizing the time so that the onset starts at t = 0. That new version of time is now time_diff. I’ll then normalize the time by converting it into percent duration, so that the onset is at 0 and the offset is at 1, with the midpoint at 0.5. That is now in percent. This makes it easier to then slice the data into 10 parts using the santoku::kiru function. For each of those 10 chunks, I can then get the median F1, F2, and F3 measurements.\n\nhigh_front_summarized &lt;- high_front %&gt;%\n  mutate(time_diff = time - min(time),\n         percent = time_diff / max(time_diff),\n         time_cut = kiru(percent,\n                         breaks = seq(0, 1, 0.1),\n                         labels = 1:10)) %&gt;%\n  summarize(across(c(f1, f2, f3), median), .by = c(time_cut)) %&gt;%\n  print()\n\n# A tibble: 10 × 4\n   time_cut    f1    f2    f3\n   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1         231. 2343. 3324.\n 2 2         225. 2383. 3275.\n 3 3         216. 2371. 3239.\n 4 4         214. 2385  3227.\n 5 5         216. 2389. 3230.\n 6 6         216. 2371  3204.\n 7 7         217. 2376. 3215.\n 8 8         217. 2371. 3218.\n 9 9         220. 2364  3241.\n10 10        222. 2344. 3191.\n\n\nI can plot this new lower-resolution version of the data, and you can see it’s much tighter because the extreme outliers were lost. I’ll add the original data in gray to provide some context.\n\nggplot(high_front_summarized, aes(f2, f1)) + \n  geom_path(data = high_front, color = \"gray80\") +\n  geom_path() + \n  scale_x_reverse() + \n  scale_y_reverse() +\n  theme_minimal()\n\n\n\n\nOkay great. So, that worked for one vowel. Let’s do that for all vowels. The code is the same, except I’m grouping things by vowel. This results in trajs dataframe (for “trajectories”). We’ll look at that in just a second.\n\ntrajs &lt;- vowels_raw %&gt;%\n  select(vowel, time, f1, f2, f3) %&gt;%\n  mutate(time_diff = time - min(time),\n         percent = time_diff / max(time_diff),\n         time_cut = kiru(percent,\n                         breaks = seq(0, 1, 0.1),\n                         labels = 1:10),\n         .by = vowel) %&gt;%\n  summarize(across(c(f1, f2, f3), median), .by = c(vowel, time_cut)) %&gt;%\n  print()\n\n# A tibble: 700 × 5\n   vowel         time_cut    f1    f2    f3\n   &lt;chr&gt;         &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 high-mid_back 1         448.  872. 2745.\n 2 high-mid_back 2         431.  835. 2747 \n 3 high-mid_back 3         422.  873. 2803.\n 4 high-mid_back 4         422.  874  2834.\n 5 high-mid_back 5         426.  842. 2789.\n 6 high-mid_back 6         416.  844. 2838.\n 7 high-mid_back 7         427.  854. 2914.\n 8 high-mid_back 8         430.  903. 2874.\n 9 high-mid_back 9         404.  966. 3011.\n10 high-mid_back 10        967. 1639. 3047 \n# ℹ 690 more rows\n\n\nFor now, let’s look at the midpoints. You may have noticed in the high front plot above that the middle 50% or so of the vowel was indeed quite monophthongal with very little formant change. I’ll assume that’s the case for all the vowels. So I’ll take the middle few bins and take the median measurment for each one. That’ll give me a new midpoints dataset.\n\nmidpoints &lt;- trajs %&gt;%\n  \n  # Get the middle few bins and find the median\n  filter(time_cut %in% 3:7) %&gt;%\n  summarize(across(c(f1, f2, f3), median), .by = vowel) %&gt;%\n  \n  # Add the vowel metadata back in.\n  left_join(vowels_meta, by = \"vowel\") %&gt;%\n  print()\n\n# A tibble: 70 × 9\n   vowel                     f1    f2    f3 height backness rounded marked ipa  \n   &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;lgl&gt;   &lt;lgl&gt;  &lt;chr&gt;\n 1 high-mid_back           422.  854. 2834. high-… back     FALSE   TRUE   ɤ    \n 2 high-mid_back_rounded   408.  522. 2583. high-… back     TRUE    FALSE  o    \n 3 high-mid_central        414. 1670. 2316. high-… central  FALSE   FALSE  ɘ    \n 4 high-mid_central_roun…  420.  979. 2322. high-… central  TRUE    TRUE   ɵ    \n 5 high-mid_front          416. 2237. 2911. high-… front    FALSE   FALSE  e    \n 6 high-mid_front_rounded  346. 2050. 2596. high-… front    TRUE    TRUE   ø    \n 7 high-mid_near-back      478. 1189. 2550. high-… near-ba… FALSE   TRUE   ɤ̈    \n 8 high-mid_near-back_ro…  413.  728. 2516. high-… near-ba… TRUE    FALSE  ö    \n 9 high-mid_near-front     420. 1940  2476. high-… near-fr… FALSE   FALSE  ë    \n10 high-mid_near-front_r…  403. 1680. 2219. high-… near-fr… TRUE    TRUE   ø̈    \n# ℹ 60 more rows\n\n\nOkay, we now have a spreadsheet with reasonably good midpoint measurements for each vowel."
  },
  {
    "objectID": "blog/jbdowse/index.html#plotting-midpoints",
    "href": "blog/jbdowse/index.html#plotting-midpoints",
    "title": "Visualizing Jonathan Dowse’s Vowels",
    "section": "Plotting midpoints",
    "text": "Plotting midpoints\nIt’s now time to plot it! Here’s just a raw look at the data.\n\nggplot(midpoints, aes(f2, f1)) + \n    geom_text(aes(label = ipa), size = 5) +\n    scale_x_reverse() + \n    scale_y_reverse() + \n    ggthemes::scale_color_ptol() + \n    labs(x = \"F2\", y = \"F1\") + \n    theme_minimal()\n\n\n\n\nOkay, so interesting already because we can see that the overall shape is a trapezoid still and not a square. We can see that the lower back portion of the vowel space is a bit denser than, say, the high front. And there’s a bit of a gap in the mid-to-high central portion.\nLet’s zhuzh this plot up a bit. I’ll color the vowels by height. Within each height, I’ll connect rounded vowels with a dotted line and unrounded vowels with a solid line. To do that, I’ll create a new column that has a unique value for vowel height before I pass it into ggplot. I’ll color rounded vowels in gray. Finally, I’ll add some annotations.\n\nmidpoints %&gt;%\n    unite(line_id, height, rounded, remove = FALSE) %&gt;%\n    ggplot(aes(f2, f1, color = height, shape = backness)) + \n    geom_line(aes(group = line_id, linetype = rounded)) + \n    geom_label(aes(label = ipa, fill = rounded), size = 5) +\n    scale_fill_manual(values = c(\"white\", \"gray90\")) +\n    scale_x_reverse() + \n    scale_y_reverse() + \n    ggthemes::scale_color_ptol() + \n    labs(title = \"Acoustic measurements from jbdowse.com/ipa/\",\n         x = \"F2\", y = \"F1\",\n         caption = 'Along the front-to-back dimension, the vowels are \"front\", \"near-front\", \"central\", \"near-back\", and \"back.\"\\nLines connect vowels of the same height and rounding. Dotted lines connect rounded vowels.',) + \n    theme_minimal()\n\n\n\n\nOkay, now we’re starting to see some things! So, it looks like rounded vowels are pretty consistently further back than their unrounded counterparts. In some cases, drastically so (see [ɯ] compared to [u]). F2 is pretty level across most of the higher vowels. Among the low vowels, the further back they were the higher they were. Here we can better see the clustering in the low back portion of the vowel space. This also gives some nice context for the Moulton (1968:464), who says that the fieldworkers for the Linguistic Atlas of New England were “hopelessly and humanly incompetent at transcribing phonetically the low and back vowels they heard from their informants” (cited in Johnson 2010:32). Given a spot in the low back portion of the vowel space, there are lots of ways to transcribe it that would come pretty darn close.\nLet’s pause and just make sure we’re on the same page when it comes to mapping acoustics to perception. I’m not saying that Dowse was wrong. I don’t make this plot just to point and laugh and say, “wow, he sure did a terrible job!” I haven’t sat sound and like measured out my perception or anything, but the vowels sound more or less equidistant from each other to me. So, what this really shows is that there’s a pretty stark difference between what is perceptually equidistant and what is acoustically equidistant. Perhaps what this is showing is that we can actually hear small distances between vowels in the low back space more than in the high front space. Or perhaps Dowse was a little too ambitious at creating an artificially inflated number of low back distinctions and that we should stick with the trapezoidal shape that the IPA chart has. I don’t know. But I’m sure there’s some interesting paper from the 70s or something that has been written about this.\nLet’s clean the vowel chart up a little bit by removing the rounded front vowels and the unrounded back vowels.\n\nmidpoints %&gt;%\n    unite(line_id, height, rounded, remove = FALSE) %&gt;%\n    filter(!marked) %&gt;%\n    ggplot(aes(f2, f1, color = height, shape = backness)) + \n    geom_line(aes(group = line_id, linetype = rounded)) + \n    geom_label(aes(label = ipa), size = 5) +\n    scale_x_reverse() + \n    scale_y_reverse() + \n    ggthemes::scale_color_ptol() + \n    theme_minimal()\n\n\n\n\nOkay, so this is a little bit sparser I don’t know if there’s any new insight here, other than the middle of the vowel space really opens up quite a bit."
  },
  {
    "objectID": "blog/jbdowse/index.html#plotting-trajectories",
    "href": "blog/jbdowse/index.html#plotting-trajectories",
    "title": "Visualizing Jonathan Dowse’s Vowels",
    "section": "Plotting trajectories",
    "text": "Plotting trajectories\nSince we have trajectory data, let’s plot some of those. I’ll have to reshape the data so that all the formant data shows up into a single column. I’ll use pivot_longer to do that, which you can read more about how it’s helpful for such vowel data here. If we just look at the high front data, we can see what that looks like.\n\nhigh_front %&gt;%\n  pivot_longer(cols = c(f1, f2, f3), names_to = \"formant\", values_to = \"hz\") %&gt;% \n  ggplot(aes(time, hz, color = formant, group = formant)) + \n  geom_point() + \n  geom_path() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nThat wonky data we saw in the F1-F2 plot makes sense now. Let’s look at the summarized data.\n\nhigh_front_summarized %&gt;%\n  pivot_longer(cols = c(f1, f2, f3), names_to = \"formant\", values_to = \"hz\") %&gt;% \n  ggplot(aes(time_cut, hz, color = formant, group = formant)) + \n  geom_point() + \n  geom_path() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nOkay, so that’s cleaner.\nLet’s plot all the vowels then in this spectrogram-like plot.\n\ntrajs %&gt;%\n  pivot_longer(cols = c(f1, f2, f3), names_to = \"formant\", values_to = \"hz\") %&gt;% \n  unite(traj_id, vowel, formant, remove = FALSE) %&gt;%\n  ggplot(aes(time_cut, hz, color = formant, group = traj_id)) + \n  geom_point() + \n  geom_path() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n\n\n\n\nNothing too surprising here. We’ve got a lot of lines that are relatively stable. Towards the ends, most formants shift a little bit. Not sure why. A few others have some movement in other places. But, you’ve got to appreciate Dowse’s ability to hold a monophthong.\nWe can view this in a traditional F1-F2 plot. Here I’ve filtered out the edges because they had a lot of really wonky measurements, so this shows between 20% into the duration of the vowel and 70% into the duration of the vowel.\n\ntrajs %&gt;%\n  left_join(vowels_meta, by = \"vowel\") %&gt;%\n  filter(time_cut %in% c(2:7)) %&gt;%\n  ggplot(aes(f2, f1, color = height)) + \n  geom_line(aes(group = vowel), arrow = joeyr::joey_arrow()) + \n  scale_x_reverse() +\n  scale_y_reverse() +\n  ggthemes::scale_color_ptol() +\n  labs(title = \"Trajectories from jbdowse.com/ipa/\",\n       x = \"F2\", y = \"F1\") +\n  theme_minimal()\n\n\n\n\nOverall, you can see that Dowse does a good job at holding a monophthong. The higher vowels are generally pretty monophthongal. The lower the vowel, the more back-gliding it is. Low vowels appear to be less stable in height."
  },
  {
    "objectID": "blog/jbdowse/index.html#conclusion",
    "href": "blog/jbdowse/index.html#conclusion",
    "title": "Visualizing Jonathan Dowse’s Vowels",
    "section": "Conclusion",
    "text": "Conclusion\nWhen I found Dowse’s vowel chart, I wanted to see what the acoustics were. I think it’s pretty enlightening to see how acoustic differences perceptual distances map onto perceptual distances and vice versa."
  }
]