[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "On this page you’ll find links to all sorts of stuff that I have found useful, including tutorials, books, and general reading on R and Praat, statistics, software, corpora, design, and other stuff.\nNote that [as of September 2022] I haven’t really updated this page since about 2019, so it may not include the latest resources and some links may be dead."
  },
  {
    "objectID": "resources.html#my-handouts-tutorials-and-workshops",
    "href": "resources.html#my-handouts-tutorials-and-workshops",
    "title": "Resources",
    "section": "My handouts, tutorials, and workshops",
    "text": "My handouts, tutorials, and workshops\n\nR Workshops\nThis is a series of workshops on how to use R which includes a variety of topics. I have included PDFs and additional information on each installment of this series.\n\n\nFormant extraction tutorial\nThis tutorial walks you through writing a praat script that extracts formant measurements from vowels. If you’ve never worked with Praat scripting but want to work with vowels, this might be a good starting point.\n\n\nVowel plots in R tutorials (Part 1 and Part 2)\nThis is a multi-part tutorial on how to make sort of the typical vowel plots in R. Part 1 shows plotting single-point measurements as scatter plots and serves as a mild introduction to ggplot2. Part 2 shows how to plot trajectories, both in the F1-F2 space and in a Praat-like time-Hz space, and is a bit of an introduction to tidyverse as well.\n\n\nMeasuring vowel overlap in R (Part 1 and Part 2)\nThis is a two-part tutorial on calculating Pillai scores and Bhattacharyya’s Affinity in R. The first covers what I consider the bare necessities, culminating custom R functions for each. The second is a bit more in-depth as it looks at ways to make the functions more robust, but it also shows some simple visualizations you can make with the output.\n\n\nMake yourself googleable\nI’m no expert, but I have given a workshop on how grad students can increase their online presence and make themselves more googleable, based in large part to ImpactStory’s fantastic 30-day challenge, which you can read here.\n\n\nAcademic Poster Workshop\nIn response to the need for a “How to Make an Academic Poster” workshop, I put one together last minute. Poster-making is more of an art than a science and this is a very opinionated view on the dos and don’ts of making an academic poster.\n\n\nExcel Workshop\nI once gave a workshop on Excel and ended producing a long handout, that goes from the very basics to relatively tricky techniques. The link above will take you to a blog post that summarizes the workshop, and you can also find the handout itself.\n\n ## R Resources\nHere is a list of resources I’ve found for R. I’ve gone through some of them and others are on my to-do list. These are in no particular order.\n\n\nGeneral R Coding\n\nThe website for Tidyverse is a great go-to place for learning how to use dplyr, tidyr, and many other packages.\nR for Data Science by Garrett Grolemund & Hadley Wickham is a fantastic overview of tidyverse functions.\nAdvanced R by Hadley Wickham with the solutions by Malte Grosser, Henning Bumann, Peter Hurford & Robert Krzyzanowski.\nR Packages by Hadley Wickham. Also try Shannon Pileggi’s tutorial called Your first R package in 1 hour to see some of these tools in action.\nHands-On Programming with R by Garrett Grolemund & Hadley Wickham for writing functions and simulations. Haven’t read it, but it looks good.\nr-statistics.co by Selva Prabhakaran which has great tutorials on R itself, ggplot2, and advanced statistical modeling.\nTidymodels is like the Tidyverse suite of packages, but it’s meant for better handling of many statistical models. Also see it’s GitHub page.\nLearn to purrr by Rebecca Barter is the tutorial on purrr that I wish I had.\nModern R with the Tidyverse by Bruno Rodriguez is a work in progress (as of June 2022), but it’s another free eBook that shows R and the Tidyverse.\nEasystats “is a collection of R packages, which aims to provide a unifying and consistent framework to tame, discipline, and harness the scary R statistics and their pesky models.”\nOscar Baruffa’s monstrous Big Book of R is your one-stop resource for open-source R books on pretty much any topic. There are hundreds of books!\n\n\n\nWorking with Text\n\nText Mining with R by Julia Silge & David Robinson. Haven’t read it, but it looks great.\nHandling Strings with R by Gaston Sanchez.\nIf you use the CMU Pronouncing Dictionary, you should look at the phon package. It makes the whole thing searchable and easy to find rhymes. Personally, this’ll make it a lot easier to find potential words for a word list.\nThe ggtext package by Claus O. Wilke makes it a lot easier to work with text if you want to add a little bit of rich text to your plots.\n\n\n\n\nRMarkdown, Bookdown, and Blogdown\nNote: Now that Quarto is available, some of this material may be out of date.\n\nElegant, flexible, and fast dynamic report generation with R by Yihui Xie is a great resource for RMarkdown.\nR Markdown: The Definitive Guide Yihui Xie, J. J. Allaire, and Garrett Grolemund is the comprehensive guide to R Markdown and Bookdown.\n15 Tips on Making Better Use of R Markdown by Yihue Xie offers some very useful and practical tips for getting the most out of RMarkdown. (These are slides from a presentation in 2019.)\nbookdown: Authoring Books and Technical Documents with R Markdown by Yihui Xie. See an introduction to Bookdown by RStudio here.\nIf your love for Zotero is what’s preventing you from using RMarkdown, never fear! Zotero hacks: unlimited synced storage and its smooth use with rmarkdown by Ilya Kashnitsky is the perfect guide to getting those two integrated.\nThis is an excellent blog post by Rebecca Barter about how to start a blog and what kinds of things to do on it. Becoming an R blogger.\n\n\n\nGIS and Spatial Stuff\n\n\nAn Introduction to Spatial Analysis in R by Chris Brown.\nAnother book with the same title, An Introduction to Spatial Data Analysis and Statistics: A Course in R by Antonio Paez has a chapter 9 on point pattern analysis.\nSpatial Data Science by Edzer Pebesma and Roger Bivand.\nSpatial and Spatioteporal Data Analysis in R, a workshop Edzer Pebesma, Roger Bivand, and Angela Li at the useR! 2019 conference on Jul 9, 2019.\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow.\nR for Geospatial Processing by Nicolas Roelandt.\nGIS and Mapping in R: An Introduction to the sf package by Oliver Gimemez.\nI’ve needed to do a bivariate cloropleth before, so Timo Grossenbacher’s blog post was helpful because it illustrates what this is and how you can do it in R.\nI get all my shape files from the National Historical Geographic Information System (NHGIS) website.\nAnd because I haven’t quite gotten the hang of it yet in R, I do all my mapmaking using the QGIS, the open-source, Mac-friendly, and free alternative to ArcGIS. Shout-out to @mjduever of UGA Libraries for teaching me everything I know about GIS.\n\n\n\nWorking with Census Data\n\nKyle Walker’s online book Analyzing US Census Data: Methods, Maps, and Models in R.\nA Guide to Working with US Census Data in R by Ari Lamstein and Logan Powell is a nice, brief guide to census data and some places to go if you want to work with it in R.\nThe tidycensus package by Kyle Walker looks really slick and makes it easy to work with census data within the Tidyverse framework. This blog post, Burden of roof: revisiting housing costs with tidycensus, by Austin Wehrwein is a walkthrough of a real-world application with tidycensus.\n\n\n\nMiscelleny\n\ngt or, the “Grammar of Tables,” the is basically the ggplot2 but for tables.\ntidymodels is collection of packages harmoneous with the tidyverse, that mkes it really easy to run models on your data.\nSelf-explanatory tweets:\n\n\n\nAs 2019 comes to a close, I want to thank all of the lovely people in the #rstats world who have made my year a professional success. For each person in this thread, I'm going to tweet one thing they've done that I particularly appreciate.\n\n— David Keyes (@dgkeyes) December 31, 2019"
  },
  {
    "objectID": "resources.html#data-visualization",
    "href": "resources.html#data-visualization",
    "title": "Resources",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nCourses\n\nHere’s an entire open-access course on Data Visualization by Andrew Heiss, based in R and ggplot2.\n\n\n\nBooks\n\nggplot2 by Hadley Wickham is a comprehensive resource for learning all the ins and outs of ggplot2. Version 3 is due in 2020, but you can look through what’s been written so far here.\nA ggplot2 grammar guide by Gina Reynolds is a great online resource for figuring out ggplot2 works!\nData Visualization: A Practical Introduction by Kieran Healy. I haven’t had the time to look through it, but this books looks quite good. It covers data prep, basic plots, visualizing statistical models, maps, and a whole bunch of other stuff.\nFundamentals of Data Visualization by Claus O. Wilke is “meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional.”\nInteractive web-based data visualization with R, plotly, and shiny by Carson Sievert is another free online book on data visualization in in R. This has a good focus on interactivity since it involves plotly and Shiny.\nMastering Shiny by Hadley Wickham is under development and will be released late 2020. I’m looking forward to this comprehensive book on Winston Chang’s shiny package a lot actually, but in the meantime though you and I can peruse the online version for free.\n\n\n\nColors\nI’ve given a workshop on colors in data visualization, which you can view here. In it, I list the following resources, plus a whole bunch of other ones.\n\nUsing colors in data visualization\n\nYour Friendly Guide to Colors in Data Visualisation by Lisa Charlotte Rost is a great overview of using colors in data visualization with lots of links to other sites and resources.\nWhat to consider when choosing colors for data visualization by Lisa Charlotte Rost has great brief tips for color in data visualization. Be sure to see the links at the bottom for more resources!\nWhen you do create your own palettes, be sure to run it through this Color Blindness Simulator to make sure that everyone can see them. Nick Tierney’s blog post also walks you through a way to check this in R.\nStephen Few has a nice guide for using colors and has his own palette you can use.\nMasataka Okabe and Kei Ito have a guide called Color Universal Design that is pretty well-known.\nFabio Crameri, Grace E. Shephard & Philip J. Heron’s article in Nature called The misuse of colour in science communication may help you when choosing a color palette.\n\n\n\nPrepackaged color palettes\n\nA monster compilation of color palettes in R can be found at Emil Hvitfeldt’s Github.\nThe scico package has a bunch of colorblind-safe, perceptually uniform, ggplot2-friendly color palettes for use in visuals. Very cool.\nThe color brewer website, while best for maps, offers great color palettes that are colorblind and sometimes also printer-safe. The have native integration with ggplot2 with the scale_[color|fill]_ [brewer|distiller] functions.\nPaul Tol has come up with some additional color themes, which you can access with scale_color_ptol in the ggthemes package.\nThere is no shortage of color palettes. Here are a handful of ones I’ve seen and liked for one reason or another:\n\nnationalparkcolors: An R package by Katie Jolly with color palettes based on vintage-looking national parks posters.\nearthtones: An R package by Will Cornwell where you give it GPS coordinates and it’ll go to that location in Google Maps and create a color palette based on satellite images. Pretty cool.\nRSkittleBrewer: An R package by Alyssa Frazee that includes color palettes based on Skittles!\npokepalettes.com: A simple webpage that takes a Pokemon name and generates a color palette.\nwesanderson is based on this Tumbler post that has color palettes based on Wes Anderson movies.\n@CINEMAPALETTES on Twitter has color palettes based on movie stills.\ndutchmasters: Instead of coming up with your own colors, why not use ones created by Dutch painters? This is an R package by Edwin Thoen.\nPrettyCols by Nicola Rennie.\n\nColors.css: A nicer color palette for the web look like nice, customizable colors that work great for websites.\n\n\n\nCreating your own color palettes\n\nIf you want to make your own discrete color scale in R, definitely check out Garrick Aden-Buie’s tutorial, Custom Discrete Color Scales for ggplot2.\nCheck out the simplecolors package, by Jake Riley, to find hex codes for consistently-named colors.\nDefinitely check out Adobe’s Color app for some inspiration on color palettes.\nAlso, check out Coolers for more inspiration on color palettes.\nAnd if you have a start and end point, this Colorpicker app can get colors in between those points.\nI’ve needed to do a bivariate cloropleth before, so Timo Grossenbacher’s blog post was helpful because it illustrates what this is and how you can do it in R.\n\n\n\n\nAnimation\n\nThomas Lin Pedersen’s gganimate package has now made it possible to make really cool animations in R. Sometimes you want to add a bit of pizzazz to your presentation, but other times animation really is the best way to visualize something. Either way, this package will help you out a lot.\n\n\n\nRayshader\n\nDefinitely check out Tyler Morgan-Wall’s rayshader package. It makes it pretty simple to make absolutely stunning 3D images of your data in R. You can make 3D maps if you have spatial data, but you can also turn any boring ggplot2 plot into a 3D work of art. Seriously, go try it out.\nLego World Map - Rayshader Walkthrough by Arthur Welle is an awesome walkthrough on rayshader and maps made out of virtual Legos. It’s a lot of fun.\n\n\n\nMaking better plots\n\nEdward Tufte is a statistician known for his series of four books that focus on best practices in the presentation of data: The Visual Display of Quantitative Information, Envisioning Information, Visual Explanations, and Beautiful Evidence. I read them over several months on the bus and they are very cool. As a practical application of them, this page by Lukasz Piwek shows how to implement many of these visualizations in R. You can also use ggthemes to get some of this implementation.\nJoey Cherdarchuk of Darkhorse Analytics has put together some really succinct presentations on how to simplify things you might put in a paper like maps, charts, tables, and reducing the data to ink ratio.\nClaus Wilke’s Practical ggplot2 is a “repository [that] houses a set of step-by-step examples demonstrating how to get the most out of ggplot2, including how to choose and customize scales, how to theme plots, and when and how to use extension packages.”\nMalcom Barrett’s Designing ggplots: Making clear figures that communicate is a great walk-through, with code, on how to really make your plots look professional, with emphasis on telling a story.\nThe Glamour of Graphics, a talk at RStudio::Conf 2020 by William Chase that discusses how to make nice-looking plots.\nA ggplot2 Tutorial for Beautiful Plotting in R by Cédric Scherer.\n\n\n\nMiscellany\n\nThe R Graph Gallery has hundreds of plots, with code, illustrating what the plots are typically used for and different variants of the same plot. Very cool.\nMy friend Andres Karjus has given several workshops on wide range of data visualization topics, collectively called aRt of the figure: explore and visualize your data using R. You should definitely explore his github and check out his materials.\nThis blog post by Jesse Sadler is a great tutorial on how to use R to visualize network data.\nPlotting special characters or unique fonts can be tricky. Yixuan Qiu’s tutorial showtext: Using Fonts More Easily in R Graphs can help you with that.\nGeorge Bailey’s excelent workshop materials for visualizing vowel formant data can be found here.\nNot sure what kind of data visualization you should use, try From Data to Viz to help you find the most appropriate plot for your data."
  },
  {
    "objectID": "resources.html#statistics-resources",
    "href": "resources.html#statistics-resources",
    "title": "Resources",
    "section": "Statistics Resources",
    "text": "Statistics Resources\n\nGeneral Statistics Knowledge\n\nThe American Statistical Association, which is essentially the statistics equivalent in scope and prestige as the the Linguistic Society of America, put out a statement on p-values in 2016. In March of 2019, they followed up with a monster 43-article special issue, Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05, wherein they recommend that the expression “statistically significant” be abandoned. This has potential to be a pivot point in the field of statistics. Why should a linguist care? Well, the first article in that issue says “If you use statistics in research, business, or policymaking but are not a statistician, these articles were indeed written with YOU in mind.” If you use statistics in your research, it might be worth reading through at least the first article of this issue.\nThe book Modern Dive: An Introduction to Statistical and Data Sciences via R by Chester Ismay and Albert Y. Kim is a free eBook available that teachest the basics of R and statistics. See Andrew Heiss’s post about this book for more information.\nSame Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. This went viral in some circles and shows that you can get the exact same summary statistics with wildly different distributions. Very cool.\nHere’s a BuzzFeed article by Stephanie M. Lee about a researcher who made the news because of his unbelieveable amount of p-hacking and using “statistics” to lie about his data.\nHave you learned about tests like t-tests, ANOVA, chi-squared tests? Did you know they’re all just reguression under the hood? Check out this explanation by Jonas Kristoffer Lindeløv called Common statistical tests are linear models. It’s mathy and based in R.\n\n\n\nLinear mixed-effects models\n\nBodo Winter’s mixed-effects modeling tutorials are the best resource I’ve found on using these in linguistics research. It’s a two-part tutorial, so be sure to look through both of them.\nMixed-Effects Regression Models in Linguistics, edited by Dirk Speelman, Kris Heylen, & Dirk Geeraerts and published by Springer is an entire book on mixed-effects models, specifically for linguists.\nMichael Clark’s post called Shrinkage in Mixed Effects Models has some beautiful illustrations that demonstrate shirnkage. In fact, he has written a much larger document explaining what mixed-effects models and how to run them in R.\nMixed Modeling as a Foreign Language, a blog post by Andrew McDonald, first off is a good explanation of what mixed modeling is all about. But more importantly, it makes the point that “if you only partly understand the words you are using, you will humiliate yourself eventually.” In other words, it’s important to know what you’re doing when you use statistics, and if you don’t, maybe you should reconsider before you do something wrong.\nReference Collection to push back against “Common Statistical Myths” is a crowdsourced compilation (managed by Andrew Althouse) of articles that may be used to argue against some common statistical myths or no-nos.\nLisa M. DeBruine & Dale J. Barr’s paper “Understanding Mixed-Effects Models Through Data Simulation”, in Advances in Methods and Practices in Psychological Science serves as a nice tutorial to mixed-effects modeling.\nStefano Coretta’s brief blog post, On Random Effects helps explain what a random effect even is.\nNot sure how to actually run a linear mixed effects model? Try this PDF of (Standard Operating Procedures For Using Mixed-Effects Models)[http://decision-lab.org/wp-content/uploads/2020/07/SOP_Mixed_Models_D2P2_v1_0_0.pdf].\n\n\n\nGAM(M)s\nMy dissertation makes heavy use of generalized additive mixed-effects models (GAMMs). Here are some resources that I used to help learn about these.\n\nGeneralised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction by Márton Sóskuthy.\nHow to analyze linguistic change using mixed models, Growth Curve Analysis and Generalized Additive Modeling by Bodo Winter and Martijn Wieling is a tutorial on using GAMs—with one M—and Growth Curve Analysis.\nAnalyzing dynamic phonetic data using generalized additive mixed modeling: A tutorial focusing on articulatory differences between L1 and L2 speakers of English is another tutorial by Martijn Wieling in the Journal of Phonetics.\nIn fact, Martijn Wieling has the slides for a graduate course in statistical methods, including GAMMs, avilable on his website.\nStudying Pronunciation Changes with gamms by Josef Fruehwald.\nOverview GAMM analysis of time series data by Jacolien van Rij. I haven’t had time to go through this one yet, but it’s on my todo list. Actually all of her tutorials look great.\nGAMs in R by Noam Ross is a free interactive course on GAMs in R.\nIntroduction to Generalized Additive Models with R and mgcv by Gavin Simpson.\n\nIf you don’t like the visuals in mgcv, try Gavin Simpson’s R package, gratia with some ggplot2 alternatives.\ntidymv: Tidy Model Visualisation is an R package by Stefano Coretta that lets you visualize GAMMs using tidyverse-friendly code.\n\nSince model objects can get huge, you might want to try Joyce Cahoon’s butcher package to reduce the size of those giant objects.\n\n\n\nOther Models\nI know there are other types of models out there but I haven’t had the opportunity to use them. Here are some resources I’ve found that might be good for me down the road.\n\n15 Types of Regression You Should Know is a post on the blog Listen Data that is a nice overview of different kinds of regression and how to implement them in R.\nIntroduction to Generalized Linear Models by Heather Turner\nCourse materials for the generalized nonlinear models (GNM) half-day course at the useR! 2019 conference by Heather Turner. Here’s her full-day version from Zurich R Course series.\n\n\n\nBayesian Statistics\nI have not yet learned about Bayesian stats, but here are some resources I’ve come across that I may use later.\n\nBayes Rules! An Introduction to Bayesian Modeling with R by Alicia A. Johnson, Miles Ott, Mine Dogucu.\nRichard McElreath’s Statistical Rethinking: A Bayesian Course Using R and Stan is an entire course.\nStefano Coretta, Joseph V. Casillas, and Timo Roettger’s learning materials for their Learn Bayesian Analysis for the Speech Sciences workshop.\n\n\n\nStatistics for Linguists\n\nSky Onnossen has made available the materials from his Workshop for Statistics for Linguists at the University of Manitoba in December 2019.\nBodo Winter’s mixed-effects modeling tutorials are the best resource I’ve found on using linear mixed-effects models in linguistics research.\nGeneralised Additive Mixed Models for Dynamic Analysis in Linguistics: A Practical Introduction by Márton Sóskuthy is the best resource I’ve found on using generalized additive mixed-effects models in linguistics research.\nSantiago Barreda’s Bayesian multilevel models for repeated-measures data: A conceptual and practical introduction in R is an entire course on Bayesian stats geared towards linguists.\nMorgan Sonderegger’s book Regression modeling for linguistic data is a working draft of intermediate book on statistical analysis for language scientists.\nHave you used Varbrul or at least read a paper that has? You’ll know that there’s some terminology that is unique to that method. Josef Fruehwald’s video helps translate Varbrul to more contemporary terms.\n\n\n\nMiscelleny\n\nThis workshop, Dimension reduction with R, by Saskia Freytag shows different methods for dimension reduction, weighs their pros and cons, and includes examples and visuals of their applications. Pretty useful.\nIf you use statistical modeling in your research, the report package is a useful tool to convert your model into human-readable prose.\nHere’s an open source course on data science by Danielle Navarro.\nHere’s Michael Franke’s Introduction to Data Analysis.\nThis blog post by Alex Cookson does a cool job at explaining PCA while also including some super cool visuals.\nThis blog post by Joshua Loftus visualizes least squares as springs. Makes a lot of sense to me!\nIf you’ve come up with an outlier detection algorithm, try following Sevvandi Kandanaarachchi’s Testing an Outlier Detection Method to see if it works.\nEasystats “is a collection of R packages, which aims to provide a unifying and consistent framework to tame, discipline, and harness the scary R statistics and their pesky models.”"
  },
  {
    "objectID": "resources.html#praat-resources",
    "href": "resources.html#praat-resources",
    "title": "Resources",
    "section": "Praat Resources",
    "text": "Praat Resources\n\nWill Styler’s Praat tutorial is probably the most thorough I’ve seen. The PDF can be found here but don’t forget to look at the page it comes from which has more information about it.\nPhonetics on Speed: Praat Scripting Tutorial by Jörg Mayer is what I find myself coming back to again and again.\nSpeCT - The Speech Corpus Toolkit for Praat is a collection of well-documented Praat scripts written by Mietta Lennes. I often find my way to this page when I need help for a specific task in Praat and incorporate some of the code in these scripts into my own.\nUniversity of Washington Phonetics Lab has a bunch of tutorials and scripts.\nMichelle Cohn has written and posted a bunch of very useful Praat scripts that you can download and use.\nA YouTube channel called ListenLab by Watt Winn that has a bunch of video tutorials on how to do stuff in Praat.\nAnother YouTube channel called Intro to Speech Acoustics that may be useful to students of acoustics, phonetics, etc.\nAnd I’ve written a tutorial on writing a script for basic automatic formant extraction.\n\n ## Working with audio\nThere are three main steps for processing audio: transcription, forced alignment, and formant extraction.\n\nAutomatic Transcription\nThere is software available that you can use to transcribe in like Praat, Transcriber, and ELAN. But here are some tools I’ve seen that do automatic transcription.\n\nCLOx is a new automatic transcriber available from the University of Washington. It’s a web-based service that uses Microsoft Bing’s Speech Recognition system to transcribe your audio. It’s estimated that a sociolinguistic interview can be transcribed in a fifth the time as a manual transcription. The great news is that it’s available for several languages!\nDARLA is actually a whole collection of tools available through a web interface from Dartmouth University. It can transcribe, align, and extract formants from your (English) audio files all in one go. For automatic transcription, you can use their own in-house system by using the “Completely Automated” method. They admit the transcriptions won’t be perfect, but they provide a handy tool for manual correcting.\nOH-Portal is by the Institute of Phonetics and Speech Processing. It works on several languages, and on clean lab data, it’s a little faster to run this and correct the transcription than it is to do a transcription from scratch. Runs entirely through the web browser, so you don’t have to download anything.\n\n\n\nForced Aligners\nI’ve got a lot of audio that I need to process, so a crucial part of all that is force aligning the text to the audio. Smart people have come up with free software to do this. Here’s a list of the ones I’ve seen.\n\nDARLA, avilable from Dartmouth University, is the one I’ve used the most. It can transcribe, align, and extract formants from your (English) audio files all in one go. Previously, its forced aligner is built using Prosody-Lab but now uses the Montreal Forced Aligner (see below).\nThe Montreal Forced Aligner is a relatively new one that I heard about for the first time at the 2017 LSA conference. It is fundamentally different than other ones in that it uses a software called Kaldi. It’s easy to set up and install and I’ve used it on my own data. The benefit of this over DARLA is that it’s on your own computer so you don’t have to wait for files to upload. And you can process files in bulk. Be sure to check out Michael McAuliffe’s blog on updates.\nFAVE is probably the most well-known forced aligner. It’s open source and you can download it on your own computer from Joe Fruehwald’s Github page. Or if you’d prefer, you can UPenn’s their web interface instead.\nProsodylab-Aligner is, according to their website, “a set of Python and shell scripts for performing automated alignment of text to audio of speech using Hidden Markov Models.” This is a software available through McGill University that actually allows you to train your own acoustic model (e.g. on a non-English audio corpus). I haven’t used it yet, but if I ever need to process non-English audio, this’ll be my go-to.\nSPPAS is a software package with several functions including forced alignment in several languages. Of the aligners you can download to your computer, this might be one of the easier ones to use.\nWebMAUS is another web interface with multiple functions including a forced aligner for several languages.\nGentle advertises itself as a “robust yet lenient forced aligner built on Kaldi.” It’s easy to download and use and produces what appear to be very good word-level alignments of a provided transcript. It even ignored the interviewer’s voice in the file I tried. The output is a .csv file, so I’m not sure how to turn that into a TextGrid, and if you need phoneme-level acoustic measurements, a word-level transcription isn’t going to work.\n\n\n\nFormant Extractors\n\nSantiago Barreda’s Fast Track is my current go-to tool for automated formant extraction. It’s a Praat plug-in, but it works really well with the accompanying R package, FastTrackR. Give them both a try!\nFAVE-Extract is the standard that tons of people use.\nPolyglotDB works well with large, force-aligned corpora.\nIf you want to do write a script yourself, I’ve written a tutorial on writing a script for basic automatic formant extraction."
  },
  {
    "objectID": "resources.html#phonetics-resources",
    "href": "resources.html#phonetics-resources",
    "title": "Resources",
    "section": "Phonetics Resources",
    "text": "Phonetics Resources\n\nThe rtMRI IPA chart has MRI videos of all the sounds on the IPA chart.\nJonathan Dowse’s IPA Charts with Audio includes basically any possible combination of co-articulatations, regardless of whether they’re actually attested in human language.\nIPA Phonetics is an iPhone app has what they call an “elaborated” IPA chart with lots of extra places and manners of articulation, complete with audio clips of all the sounds. You can play a game where it’ll play a sound and you can guess what you heard. It’s just fun to see things like a voiced uvular fricative (ɢʁ) or a dentolabial fricative [θ̼] on an IPA chart. Credits to University of Victoria linguistics and John Esling’s “Phonetic Notation” (chapter 18 of the Handbook of Phonetic Sciences, 2nd ed.).\nPink Trombone is an interesting site that has a interactive simulator of the vocal tract. You can click around and make different vowels and consonants. Pretty fun resource for teaching how speech works."
  },
  {
    "objectID": "resources.html#typography-web-design-and-css",
    "href": "resources.html#typography-web-design-and-css",
    "title": "Resources",
    "section": "Typography, Web Design, and CSS",
    "text": "Typography, Web Design, and CSS\nI enjoy reading and attempting to implement good typography into my website. Here are some resources that I have found helpful for that.\n\nBeautiful Websites\nI designed this website more or less from scratch, so I can appreciate the work others put into their own academic sites. Here are some examples of beautiful websites that I have found that I really like.\n\nKieran Healy has one of the beautiful academic websites I’ve ever seen. I created this category on this page just so I could include his page on here. Wow.\nPractical Typography by Matthew Butterick is was my gateway into typography. My font selection and many other little details on my site (slides, posters, CV, etc.) were influenced by this book.\n\n\n\nCSS\n\nIf you enjoy the work of Edward Tufte and would like to incorporate some of his design principles into your website, you’ll be interested in Tufte CSS by Dave Liepmann. If you’re interested in your RMarkdown files rendering in a Tufte-style (like this), there are ways to do that too, which you can read in chapter 3 of bookdown by Yihui Xie or chapter 6 of R Markdown, by Yihui Xie, J. J. Allaire, and Garrett Grolemund (cf. this)."
  },
  {
    "objectID": "resources.html#academic-life",
    "href": "resources.html#academic-life",
    "title": "Resources",
    "section": "Academic Life",
    "text": "Academic Life\nOccasionally, I’ll see posts with really good and insightful tips on how to be an academic. For the ones I saw I Twitter, I’ve put the first post here: click on them to go directly to that tweet where you can read the rest.\n\nHow to make effective slides by Kieran Healy.\nAdvice to a young scholar by Kensy Kooperrider.\nHere’s a page with a Q&A and lots of links to pages about what it’s like going into grad school in linguistics, written by Joshua Raclaw.\nTwitter for Scientists by Daniel S. Quintana has all insider tips and recommendations for how to use Twitter as an academic.\nA list of self-explanatory tweets:\n\n\n\nHey academics-coming-up! Congratulations on sending out that article! However, that probably also means, a few months later, you got your article rejected. Not even a Revise and Resubmit. Worry not. It happens to all of us, most of the time. Here's a thread on what I do.\n\n— Jeff Guhin (@jeffguhin) November 12, 2019\n\n\n\n\nI finally went through all my bookmarked tweets to compile a list of resources I want my grad students to have and wanted to (1) thank everyone who posted these resources, and (2) pay it forward and share the compiled list with all of you!\n\n— Kaitlin Fogg (@kaitlin_fogg) November 8, 2019\n\n\n\n\nAfter reading approximately 30 applications over the past few days that explicitly requests a diversity statement. I got some notes on what to do and what not to do. The \"DON'T\" list is long but please bear with me. But first, lets define a diversity statement (1/x) pic.twitter.com/qx1e8EyIGJ\n\n— Dr. Samniqueka Halsey (@Samniqueka_H) December 30, 2019\n\n\n\n\nUse less text. One of the most important tips for creating engaging scientific presentations is reducing text as much as possible. The audience is not there to read but to listen to you 1/7@AcademicTwitter #AcademicChatter pic.twitter.com/ybR7cSRor2\n\n— Timo Roettger (@TimoRoettger) March 1, 2020\n\n\n\n\nHow to revise:As an editor and author I have seen many revised papers return to journals. Given effort, most go well (ie step toward acceptance). Some go pear-shaped. I’ve slowly improved and have an approach known by my group as the ’Breakspear method”. Here is its essence\n\n— Michael Breakspear (@DrBreaky) June 19, 2020\n\n\n\n\nHere’s what you’ll need to prepare if you want to pitch yr academic book project to a publisher this year:1. A working title for the book. Don’t worry, you can change it later.2. A project description or overview. Summarize your main argument, how you prove it, why it matters\n\n— Laura Portwood-Stacer, Jeopardy Champ (she/her) (@lportwoodstacer) January 2, 2021\n\n\n\n\nA review of 2020 reviews & a 🧵of jumbled thoughts:Ad-hoc Review requests received: 109Requests accepted: 37Action Editor ms for J1: 35Action Editor ms for J2: 86Thoughts on the current state of review:1/\n\n— Koraly Pérez-Edgar 🇵🇷 (@Dr_Koraly) January 3, 2021\n\n\n\n\nHere's some of the best advice I got when I became a manager last year! It's simple, but considering most people receive no management training whatsoever these days, it's better than nothing. Thread!\n\n— ella dawson (@brosandprose) December 6, 2019\n\n\n\n\nIt is that time of the year where many aspirants will be applying for grad school and tenure track positions. I just wanted to share some advice that I wish I had known when I was going through these things. [continued below]\n\n— 𝙷𝚒𝚖𝚊 𝙻𝚊𝚔𝚔𝚊𝚛𝚊𝚓𝚞 (@hima_lakkaraju) November 24, 2019"
  },
  {
    "objectID": "resources.html#miscellaneous",
    "href": "resources.html#miscellaneous",
    "title": "Resources",
    "section": "Miscellaneous",
    "text": "Miscellaneous\nJust random stuff that doesn’t fit elsewhere.\n\nThe great American word mapper is an interactive tool put together by Diansheng Guo, Jack Grieve, and Andrea Nini that lets you see regional trends in how words are used on Twitter.\nCollecting, organizing, and citing scientific literature: an intro to Zotero is a great tutorial on how to use Zotero by Mark Dingemanse. Zotero is a fantastic tool for, well, collecting, organizing, and citing scientific literature and I’m not exaggerating when I say that I could not be in academics without it.\nVulgar: A Language Generator is a site that automatically creates a new conlang, based on parameters that you specify. The free web version allows you to add whatever vowels and consonants you’d like to include, and it’ll create a full language: a language name; IPA chart for vowels and consonants; phonotactics; phonological rules; and paradigms for nominal morphology, definite and indefinite articles, personal pronouns, and verb conjugations; derivational morphology; and a lexicon of over 200 words. For $19 you can download the software and get a lexicon of 2000 words, derivational words, random semantic overlaps with natural languages, and the ability to customize orthography, syllable structure, and phonological rules. In addition to just being kinda fun, this is a super useful resource for creating homework assignments for students.\nThe EMU-webApp “is a fully fledged browser-based labeling and correction tool that offers a multitude of labeling and visualization features.” I haven’t given this enough time to learn to use it properly, but it seems very helpful.\nJonhannes Haushofer’s CV of Failures. Other people have written this more elegantly than I could, but sometimes it’s nice to see that other academics fail too. You’re not going to get into all the conferences you apply for, your papers are sometimes going to be rejected, and you’re definitely not getting all the funding you apply for. I find it therapeutic to put together a CV of failures like his researcher did and to keep it updated and formatted just as would a regular CV. Don’t let impostor syndrome get in the way by thinking others haven’t failed too.\nKieran Healey’s The Plain Person’s Guide to Plain Text Social Science is an entire book on an aspect of productivity that I’ve only thought about occasionally: what kind of software should you do your work? Before you get too entrenched in your workflow, it’s good to consider what your options are.\nThisWordDoesNotExist.com is a fun site created by Thomas Dimson."
  },
  {
    "objectID": "pages/r-workshops.html",
    "href": "pages/r-workshops.html",
    "title": "R Workshops",
    "section": "",
    "text": "In grad school I offered workshops on a variety of topics relating to R between Fall 2017 and Spring 2020. They were held Fridays at 3:30 in the DigiLab at the UGA Main Library. Videos for some of these are available here. Below you will find materials for these workshops. I hope they are thorough enough that you can make use of them as stand-alone documents.\nThe data used in these workshops can be found at my datasets page.\nIf you are planning on attending a workshop in the near future, please see the setup instructions to help you get R and RStudio installed on your computer.\nNote: If you attended these workshops, please consider giving me some feedback with this anonymous survey so I can know how to improve.\n\n\n\nIn this first workshop I cover the basics of R itself. I talk about the differences between R and RStudio and I help folks get both installed and running on their computers. We create a simple “Hello, World!” script using R. Part 2 covers the basics of the R language. It is also be a very simple introduction to some core computer coding concepts like declaring variables and variable types.\nAdditional Materials: You may also be interested in a PDF of the slides I used in Part 1 or a PDF version of Part 2’s handout. An older version that combines elements of both parts can be found here as a PDF or RMarkdown file.\n\n\n\n\nggplot2 is a widely used package that allows for high-quality visualizations. These workshops take you from installation to pretty advanced topics.\n\n\nIn Part 1 of this workshop I cover the basic syntax and how to make some simple types of plots.\n\n\n\nOften you’ll want to customize your plots in some way. So, in Part 2, we cover how to mess with properties of the plots like the axes, colors, and legends to make the plot work better for you.\n\n\n\nApparently I had a lot to say about how to extend your ggplot2 skills, so I ended up creating a supplement with lots of additional detail on how to modify your plots. This handout will vary from time to time as I add to it when I learn new things or remove sections to incorporate them into future workshops.\n\n\n\nBased on a popular blog post I wrote, this workshop wraps all customization methods together and shows how to create your own themes.\n\n\n\nAs I was preparing for the custom themes workshop, I got a little carried away illustrating all the components of the theme function. I decided to simplify that portion of the workshop and create this separate handout that just focuses on theme. It is not yet finished, but it may be of some help to people (including myself!).\nAdditional Materials: You may also be interested in the 2018 versions of some of these workshops that I gave (Part 1 Rmarkdown and PDF and Part 2 RMarkdown and PDF). An older version from 2017 that combines elements of Parts 1 and 2 can be found here as a PDF or RMarkdown file.\n\n\n\n\n\nThe tidyverse is a suite of packages that includes dplyr and tidyr which help you wrangle your data. In this two-part workshop, we learn some of the common functions in the tidyverse and compare them to base R, showing that there are multiple ways to accomplish the same task in R. Part 2 in particular looks at how to reshape and transform your data, merging it with other dataset, and other super useful and powerful tools.\nAdditional Materials: You may also be interested in PDFs for Part 1 and Part 2. An older version that combines elements of both parts can be found here as a PDF or RMarkdown file.\n\n\n\n\nR Markdown is a way to create different types of documents using R (pdfs, word files, html files). In this one-day crash course, I show how to make R Markdown files and the kinds of things they would be useful for.\nAdditional Materials: If you’d rather use a PDF of the workshop materials, there’s one available here.\n\n\n\n\nShiny is an R package that allows you to make your own interactive web pages. An entire semester could be devoted to Shiny and there’s a bit of a learning curve, especially if you haven’t used HTML before. This two-day workshop covers just the essentials.\nNote that the materials for this workshop include interactive shiny elements, which means I can’t host them on my own website. So instead, they’re hosted on Shiny’s free server space. But this comes with a major drawback: they’re only available for 25 user-hours a month. So, if the link above does not work for you, try again in a week or so. Sorry for the inconvenience."
  },
  {
    "objectID": "pages/r-workshops.html#intro-to-r-part-1-and-part-2",
    "href": "pages/r-workshops.html#intro-to-r-part-1-and-part-2",
    "title": "R Workshops",
    "section": "",
    "text": "In this first workshop I cover the basics of R itself. I talk about the differences between R and RStudio and I help folks get both installed and running on their computers. We create a simple “Hello, World!” script using R. Part 2 covers the basics of the R language. It is also be a very simple introduction to some core computer coding concepts like declaring variables and variable types.\nAdditional Materials: You may also be interested in a PDF of the slides I used in Part 1 or a PDF version of Part 2’s handout. An older version that combines elements of both parts can be found here as a PDF or RMarkdown file."
  },
  {
    "objectID": "pages/r-workshops.html#visualizations-with-ggplot2",
    "href": "pages/r-workshops.html#visualizations-with-ggplot2",
    "title": "R Workshops",
    "section": "",
    "text": "ggplot2 is a widely used package that allows for high-quality visualizations. These workshops take you from installation to pretty advanced topics.\n\n\nIn Part 1 of this workshop I cover the basic syntax and how to make some simple types of plots.\n\n\n\nOften you’ll want to customize your plots in some way. So, in Part 2, we cover how to mess with properties of the plots like the axes, colors, and legends to make the plot work better for you.\n\n\n\nApparently I had a lot to say about how to extend your ggplot2 skills, so I ended up creating a supplement with lots of additional detail on how to modify your plots. This handout will vary from time to time as I add to it when I learn new things or remove sections to incorporate them into future workshops.\n\n\n\nBased on a popular blog post I wrote, this workshop wraps all customization methods together and shows how to create your own themes.\n\n\n\nAs I was preparing for the custom themes workshop, I got a little carried away illustrating all the components of the theme function. I decided to simplify that portion of the workshop and create this separate handout that just focuses on theme. It is not yet finished, but it may be of some help to people (including myself!).\nAdditional Materials: You may also be interested in the 2018 versions of some of these workshops that I gave (Part 1 Rmarkdown and PDF and Part 2 RMarkdown and PDF). An older version from 2017 that combines elements of Parts 1 and 2 can be found here as a PDF or RMarkdown file."
  },
  {
    "objectID": "pages/r-workshops.html#introduction-to-the-tidyverse-part-1-and-part-2",
    "href": "pages/r-workshops.html#introduction-to-the-tidyverse-part-1-and-part-2",
    "title": "R Workshops",
    "section": "",
    "text": "The tidyverse is a suite of packages that includes dplyr and tidyr which help you wrangle your data. In this two-part workshop, we learn some of the common functions in the tidyverse and compare them to base R, showing that there are multiple ways to accomplish the same task in R. Part 2 in particular looks at how to reshape and transform your data, merging it with other dataset, and other super useful and powerful tools.\nAdditional Materials: You may also be interested in PDFs for Part 1 and Part 2. An older version that combines elements of both parts can be found here as a PDF or RMarkdown file."
  },
  {
    "objectID": "pages/r-workshops.html#an-intro-to-rmarkdown",
    "href": "pages/r-workshops.html#an-intro-to-rmarkdown",
    "title": "R Workshops",
    "section": "",
    "text": "R Markdown is a way to create different types of documents using R (pdfs, word files, html files). In this one-day crash course, I show how to make R Markdown files and the kinds of things they would be useful for.\nAdditional Materials: If you’d rather use a PDF of the workshop materials, there’s one available here."
  },
  {
    "objectID": "pages/r-workshops.html#introduction-to-shiny",
    "href": "pages/r-workshops.html#introduction-to-shiny",
    "title": "R Workshops",
    "section": "",
    "text": "Shiny is an R package that allows you to make your own interactive web pages. An entire semester could be devoted to Shiny and there’s a bit of a learning curve, especially if you haven’t used HTML before. This two-day workshop covers just the essentials.\nNote that the materials for this workshop include interactive shiny elements, which means I can’t host them on my own website. So instead, they’re hosted on Shiny’s free server space. But this comes with a major drawback: they’re only available for 25 user-hours a month. So, if the link above does not work for you, try again in a week or so. Sorry for the inconvenience."
  },
  {
    "objectID": "pages/gsv.html",
    "href": "pages/gsv.html",
    "title": "GSV",
    "section": "",
    "text": "If you are not redirected to the Gazetteer of Southern Vowels, click here."
  },
  {
    "objectID": "pages/praat-workshops.html",
    "href": "pages/praat-workshops.html",
    "title": "Praat Workshops",
    "section": "",
    "text": "Lisa Lipani and I will be hosting a series of four workshops on Praat and Praat scripting during Fall 2019. This page will house the handouts and all materials for those workshops.\nNote: All workshops will be held at 3:35pm at the DigiLab (300 Main Library)\n\n\n\nSeptember 11, 2019—In this workshop, I’ll introduce the software Praat, and will how how to install the software, record audio, save audio, extract some acoustic measurements by hand, and do a manual transcription. It will also briefly discuss the important steps you need to take to prep your data before using Praat. (For former students of mine, this workshop will overlap with this homework assignment.)\n\n\n\n\nSeptember 18, 2019—With some basic understanding of the software, Lisa will lead the workshop on the fundamentals of Praat scripting: object management, looping through a TextGrid, and how to get your data in and out of a Praat script.\n\n\n\n\nOctober 2, 2019—In this workshop, you’ll learn, from start to finish, how to write a Praat script that will automatically extract formant measurements and a other acoustic measurements from your audio and transcription. There will be quite a bit of overlap with this blog post, only you will get more detail with an in-person guide."
  },
  {
    "objectID": "pages/praat-workshops.html#praat-basics-introduction-to-the-software",
    "href": "pages/praat-workshops.html#praat-basics-introduction-to-the-software",
    "title": "Praat Workshops",
    "section": "",
    "text": "September 11, 2019—In this workshop, I’ll introduce the software Praat, and will how how to install the software, record audio, save audio, extract some acoustic measurements by hand, and do a manual transcription. It will also briefly discuss the important steps you need to take to prep your data before using Praat. (For former students of mine, this workshop will overlap with this homework assignment.)"
  },
  {
    "objectID": "pages/praat-workshops.html#praat-scripting-basics-loops-io-and-textgrids",
    "href": "pages/praat-workshops.html#praat-scripting-basics-loops-io-and-textgrids",
    "title": "Praat Workshops",
    "section": "",
    "text": "September 18, 2019—With some basic understanding of the software, Lisa will lead the workshop on the fundamentals of Praat scripting: object management, looping through a TextGrid, and how to get your data in and out of a Praat script."
  },
  {
    "objectID": "pages/praat-workshops.html#automatic-formant-extraction-in-praat-and-supplement",
    "href": "pages/praat-workshops.html#automatic-formant-extraction-in-praat-and-supplement",
    "title": "Praat Workshops",
    "section": "",
    "text": "October 2, 2019—In this workshop, you’ll learn, from start to finish, how to write a Praat script that will automatically extract formant measurements and a other acoustic measurements from your audio and transcription. There will be quite a bit of overlap with this blog post, only you will get more detail with an in-person guide."
  },
  {
    "objectID": "blog/kohler-tapes/index.html",
    "href": "blog/kohler-tapes/index.html",
    "title": "Kohler Tapes",
    "section": "",
    "text": "So, I just acquired a goldmine of data that I can use for linguistic analysis. Sitting in my office are 452 cassette tapes, each containing at least 30 minutes of recorded interviews with an older folks from Heber City, Utah. And that’s about half of the collection: the other half is with a historian in Midway, Utah. So, I’m looking at roughly 400–500 hours of audio. Not sure how I’m going to process it all, but I wanted to kick off the beginning of this long-term project with a blog post describing the history of the tapes, why I’m interested in them, and speculations about the future."
  },
  {
    "objectID": "blog/kohler-tapes/index.html#background",
    "href": "blog/kohler-tapes/index.html#background",
    "title": "Kohler Tapes",
    "section": "Background",
    "text": "Background\nI first heard about the tapes a little over three years ago. In January 2018, the LSA annual meeting was in Salt Lake City. Wanting to take advantage of the trip out there, I applied for and received a grant from the University of Georgia to collect audio in Heber City, aiming for multiple generations within a family to track language change over time. I decided on Heber partly because it was a region of Utah that had never been the subject of acoustic (let alone linguistic) analysis, as far as I know. My parents were living there at the time too, so they could hook me up with some potential contacts.\nSo on the morning of the first day of my fieldwork, the first thing I did was go to the Heber Valley Visitor’s Center as a way to potentially find some contacts. Literally the first person I talked to told me about a man who had a huge collection of tapes. One person led me to another, and I was talking to an elderly man named Norm Kohler in his nursing home.Side note, it’s amazing that I heard about this goldmine literally through the first person I talked to while doing fieldwork? Who knew that there’d be such an amazing collection of audio sitting in someone’s basement nearby? In fact, there could be lots of collections like these, just collecting dust in people’s basements. All it takes is to find the right person!\nNorm was a beloved middle school teacher in Heber City in the 1980s and 1990s. As a history project, he had each of his students get a cassette tape and interview a grandparent. I don’t know what the interview questions were, but I think they mostly concerned life in Heber Valley. He kept all the tapes his students turned in and, over the course of two decades, he ended up with over 1200 interviews! Norm intended to compile them and put together an oral history of the town, but unfortunately was unable to do so. So, just weeks before I met him, he decided it was best to return the tapes to the family members’ of his students and the people they interviewed. So he put an ad in the paper and hundreds of people claimed their tapes and were able to hear their ancestors’ voices, perhaps for the first time.\nHowever, not all the tapes were claimed. I was told a few hundred remained. So, after Norm passed away a few months later, his family held on to them for a while before finally donating them to the Midway Historical Society. (Midway is the town next door to Heber.) Several complications made it difficult for me to get access to the tapes, including outdated contact information on Midway’s website, the society going on an extended hiatus, me living in Georgia, and then covid. But I did my best to reach out to anyone who might know about where the tapes were being stored.\nFinally, on Thursday this week, I was contacted by the historian in custody of the tapes. She asked if I was still interested in them, and I most definitely am! So we had a nice chat about what my goals were for them and what the goals were for the Historical Society, and we think there’s mutual interest in getting them digitized and transcribed. So, the next day, yesterday, she happened to be in Provo so she dropped off about half of the tapes—452 of them!—at my office!\nSo after three years of following tenuous leads, I finally have the tapes!"
  },
  {
    "objectID": "blog/kohler-tapes/index.html#why-am-i-so-interested",
    "href": "blog/kohler-tapes/index.html#why-am-i-so-interested",
    "title": "Kohler Tapes",
    "section": "Why am I so interested?",
    "text": "Why am I so interested?\nI am a linguist, so why should I care about these tapes? Well, the obvious reason is that it’s a lot of audio. For my dissertation I analyzed about 40 hours of interviews and that was already a lot of data. This is at least 10 times the amount of audio. In fact, it’s about the size of the Digital Archive of Southern Speech, a subset of the Linguistic Atlas of the Gulf States, that I spent four years in grad school analyzing. So having access to this much audio is absolutely incredible.\nBut it’s not just the amount of audio. There are dozens of oral history projects even in Utah. This particular set is attractive for several reasons:\n\nThe nature of the homework assignment ensured good metadata. A few tapes have already been digitized and they all start off introducing the interviewer (the middle-schooler) and the interviewee, with information like the date of interview, their age, and where they grew up.\nBecause these were all students in the same smallish town in Utah the sample will be relatively homogeneous geographically. While it doesn’t ensure that the interviewees (the grandparents) were from Heber or Heber Valley generally, my guess is that a significant number of them are.\nTypically, interviews happen with a historian or someone that the interviewer is unfamiliar with. In sociolinguistics, it’s generally accepted that the degree of familiarity with the interviewer can have an influence on a person’s speech. In all cases with these tapes, the interviewer is a teenager and a grandchild of the interviewee. So that lowers the formality of the situation and will likely mean that the interviewees’ speech will be more casual.\nHeber Valley has been the focus of very little acoustic research. There may be occasional interviews as parts of the Linguistic Atlas Project or the Dictionary of Regional American English, but no study, as far as I know, has focused on Heber. Instead, most research looks at people from Utah Valley and Salt Lake Valley. This collection of interviews will offer a new spot on the map of Utah dialectology and a nice point of comparison between more urban and more rural areas of the state.\nI have virtually no metadata about the interviewees right now, but if their grandchildren were about 14 years old in the 1980s and 1990s, then the speakers in these tapes were born perhaps sometime between 1900 and 1940. There has been some research on the development of Utah English, mostly by David Bowie, but he acknowledges that it was based on public sermons given by upper-class white men. This collection offers a unique look into how other Utahns born around that time talked. And since I have some comparable data from contemporary Heber City residents, I can begin to look at language change in real time.\n\nThere were about 4500 people living in Heber in the 1980s and 1990s, which means this sample is a significant chunk of the community!So there are lots of reasons for why I’m really interested in this collection of tapes. And that’s on top of the oral history the Midway Historical Society wants to create based on them."
  },
  {
    "objectID": "blog/kohler-tapes/index.html#looking-ahead",
    "href": "blog/kohler-tapes/index.html#looking-ahead",
    "title": "Kohler Tapes",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nLuckily, I’ve had some experience working on a project of this size. For four years at the University of Georgia, I was a part of the team that processed the Digital Archive of Southern Speech, which is a 367-hour subset of the Linguistic Atlas of the Gulf States. So I’ve sat in on transcriber training sessions, seen what kinds of obstacles get in the way of processing, managed thousands of files, and analyzed spreadsheets with a couple million acoustic measurements in them. However, that was only as a graduate student. I’m sure there’s a lot that goes on behind the scenes as a PI that I didn’t see.\nTranscribers—Some back-of-the-envelope calculations suggest that I’ll need a sizable grant to get this all processed. Again, I don’t have definite numbers for anything, I know my 452 tapes are a little over half of them, so let’s say there are 700 tapes total. They’re all at least 30 minutes long and I know many went longer, so if I average say 40 minutes per tape, that’s 28,000 minutes or roughly 467 hours. I think the the transcribers for DASS averaged about 13 hours per 50 minutes of audio or so, but this audio is newer and I presume Utah transcribers will be more familiar with Utah speakers I think, so I’ll estimate 10 hours of work per tape. That’s 4670 hours of transcription. At $15 per hour, I’m looking at about $70,000 in student wages. Obviously, I can’t get that much coin internally so it sounds like this is only going to happen with an external grant.\nGrad student workers—That’s of course assuming that the only wages I’ll need to pay for are transcribers. This might be getting into “If you give a mouse a cookie” territory, but it would be nice to have some grad students helping out with the project. At UGA, we had at least four and as many as six grad students involved in the project at a time. There was a lot of overlap between our duties, but very roughly speaking, one managed the transcribers, one managed the spot-checks, one managed the acoustic analysis, and one did miscellaneous duties. We were all involved in analysis, and a few others popped in for a semester or two to do additional analysis or perform other duties. To lighten my load, it would be handy to have perhaps three grad students manage the transcribers, check their work, and do the acoustic analysis. I’m fuzzy on what costs are associated with RA-ships at BYU, but I do know it’ll add significantly to the total cost of the project.\nTime—How long will transcriptions take? I’ve done transcriptions and they’re soul-sucking work. Even when I was highly motivated to process my own dissertation data, that I collected myself, and under a bit of a time crunch, I could barely put in more than about two hours a day. I surely don’t expect undergraduate transcribers to do more than 10 hours a week. When motivated by money, I’ve seen some at UGA do more, but those students were exceptional. I’ll estimate five hours of work per transcriber per week. So under the assumption of 4670 hours of work total, that’s 934 transcriber-weeks. If a semester is fifteen weeks, that’s 62 transcriber-semesters. If I set a goal of getting all the work done in two years (six semesters if you include summers), it would take ten or eleven transcribers to do it in two years. Of course, these are all very rough estimates, but managing several tens of thousands of dollars and almost a dozen workers for two years is not something I expected to do right away!\nDigitizing—Regardless of the cost, number of workers, and time involved, the first step of the process will be digitization. Fortunately, it sounds like the Office of Digital Humanities can take care of that for me! Wow! So my short term goal is to get a batch—maybe 30 or 50 tapes—done first. While they work on digitizing the next batch, I can get started on listening to the first few minutes of the completed tapes and extracting whatever metadata I can from them. Eventually, all the tapes will be digitized and I can have a more concrete idea of how much audio (and consequently, people, time, hours, and money) I’m looking at.\nMetadata—After digitizing all of them, my next step will be to finish collecting the metadata. It’ll be nice to have a clear picture of birth years, genders, and birthplaces for all 700 or so people. The most likely scenario is that I won’t get an external grant because they’re extremely competitive, so I’ll have to prioritize which ones to transcribe first. The Historical Society would like to start with some of the prominent members of the community and descendants of the town’s founders. I’d like to find a balance of genders and birth years too, so we’ll probably settle on a subset that satisfies both of our needs. How big? I’m thinking between 35 and 70 (5% to 10% of the tapes). That’s a more reasonably-sized project that I could possibly get funded internally. It could provide me at least a beginning look at the speech community which would help seed an external grant.\nFollow-up project?—In case I just need more data to analyze (ha!) wouldn’t it be cool to track down some of the tapes that were given away? Presumably, if an ad in the paper is what it took for the families to get them, then an ad might be a good place to start to find them. We’d digitize the tapes right there for people, give them a copy and return the tape to them of course, but then also add that to the collection for the oral history. I think it would be especially cool to interview those people themselves! That way we can get some contemporary data to compare the tapes to, as well as track change within the family. That’ll have to wait until I get NSF grant number two!\nPublications—What’s the end goal? Well, I’ll obviously start cranking out some papers as soon as a reasonable amount of data has been processed. There is a lot going on in Utah English. Many of the stereotyped features are dying out, so these people may provide good acoustic data for what would otherwise be hard to study phonetically today. But there are also lots of other features that I believe are recent innovations, so if they’re infrequent or missing from these speakers, it’ll help establish the timing of when they did develop. Even before I had the tapes, I’ve been thinking a full analysis of this collection deserves a book-length treatment. It likely won’t get done before I’m up for tenure, but maybe it’ll go towards my application for full professor."
  },
  {
    "objectID": "blog/kohler-tapes/index.html#conclusion",
    "href": "blog/kohler-tapes/index.html#conclusion",
    "title": "Kohler Tapes",
    "section": "Conclusion",
    "text": "Conclusion\nThe history of the Kohler Tapes is pretty cool, and I’m lucky to be a part of the creation of an oral history of Heber City. It’s so satisfying teaming up with a historical society and finding ways to help the community I’m studying too. Linguistically, they’re interesting to me for lots of reasons, but I think everyone benefits from seeing these tapes get processed. As far as how I’m going to go about processing all of them, I really have no idea what I’m doing so there will be a lot of learning involved. But I’m excited to be involved and to have a clear research trajectory for the next decade or so!"
  },
  {
    "objectID": "blog/lcuga6/index.html",
    "href": "blog/lcuga6/index.html",
    "title": "LCUGA6",
    "section": "",
    "text": "I presented at the 6th annual Linguistics Conference at UGA today! My presentation, which was called “Real Time Vowel Shifts in Georgia English” compared Georgians born around the 1890s to those born in the 1990s—100 years of change! The gist: nearly every vowel has changed, and it seems like the trajectory of that change is in the direction of the Elsewhere Shift, rather than just a simple recession of Southern features.\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nThe data came from two very different sources. The older speakers, were born between 1887 and 1903, were recorded doing Linguistic Atlas interviews and are part of the Digital Archive of Southern Speech. The younger speakers, who were born between 1994 and 1997) were UGA undergraduates that I recorded as they read sentences in a lab. Two very different datasets, but they’re all Georgian natives, so it starts to give us a glimpse into how things have changed around here.\nHere’s a summary of the changes I found.\n\nprice was monophthongal for older speakers and definitely a diphthong in the younger speakers.\nThe Euclidean distance between fleece and kit and between face and dress was grew over 100 years, suggesting the younger speakers’ vowels are not as “Southern” sounding.\nWhile the older speakers’ lot and thought were close, the younger speakers’ were almost entirely merged.\nThe front lax vowels, trap, dress, and kit were front and monophthongal in the older speakers while the younger speakers realized them more centralized and more diphthongal. trap underwent the most change, then dress, and then kit.\ngoose was centralized even in the older speakers, but the younger speakers took it further by using an even more fronted onset.\n\nThis animation basically sums up the entire talk. Plus I just had fun making animations! Here’s the change for the women.\n\n\n\n\n\nAnd here’s the change for the men.\n\n\n\n\n\nOverall, it appears that it’s not just that the younger speakers lack features associated with the South, it’s that they’re also using realizations characteristic of other regions like California and Canada. It makes sense, since Atlanta is a large metropolitan area and is kind of known as a non-Southern city in the middle of the South. But this might be an indication that the Elsewhere Shift has made its way into the South."
  },
  {
    "objectID": "blog/using-mturk/index.html",
    "href": "blog/using-mturk/index.html",
    "title": "Using MTurk",
    "section": "",
    "text": "A few weeks ago, I wroteabout a grant I was awarded where I’ll use Amazon Mechanical Turk (“MTurk”) to collect data from people all across the West. Today, I did a soft launch of the request and already got recordings from five people!\nAfter weeks of carefully wrangling my MTurk request, my Qualtrics survey, and my IRB forms, I finally got it all set up. I’ve had a handful of projects get approved by the IRB, but this one was a little different since it was through MTurk, so I was a little unsure how to go about some things. Luckily, our IRB office was having open houses all through the semester, which were very helpful.\nI decided to do a soft release first. $2500 is a lot of money to just throw into a task all at once and I wanted to make sure things were working out right. So I put in enough for five people do to the task. Within the hour I was getting data sent to me! It was crazy!\nI got all five in one day with no problem. I’m glad I did the soft release though because there were a couple small snafus that I had to fix. For example, I underestimated how much time it would take people to finish the task, so I’ll raise the amount they’re compensated: I can afford fewer workers that way, but at least I pay them an honest amount.\nI’ll spend the next few days making absolutely certain that the task I want them to do is what’s right for this project. But at some point, I’ll pull the trigger and let’er rip. From that point on, all I need to do is approve people’s work (to make sure they get paid) and then just enjoy the hours and hours of recordings showing up in my inbox. What a way to collect data!"
  },
  {
    "objectID": "blog/using-mturk/index.html#may-22",
    "href": "blog/using-mturk/index.html#may-22",
    "title": "Using MTurk",
    "section": "May 22",
    "text": "May 22\nSo this happened:\n\n\nThank you, MTurker, for pointing out that my consent form says the software I ask you download \"will be harmful to your computer.\" #Typo\n\n— Joey Stanley (@joey_stan) May 22, 2017"
  },
  {
    "objectID": "blog/using-mturk/index.html#june-9",
    "href": "blog/using-mturk/index.html#june-9",
    "title": "Using MTurk",
    "section": "June 9",
    "text": "June 9\nOkay, so several weeks have passed, and the data collection phase is drawing to a close. In just a couple of weeks, I was able to get data from almost 200 people. I had some major time constraints on how I could use my money, so I had to find ways to use it quicker. I ended up creating an entirely new task, similar to the first one, with a whole new batch of sentences and words for people to read. A large portion of my participants returned to do the second part, meaning I have around 30 minutes of audio from almost 100 people.\nThis is an incredible dataset I’ve collected. I don’t know how much audio I have total yet, but it’s well over 50 hours. That’s pretty good for just three weeks.\nHowever, I will be the first to say that it was a rough three weeks. It seems like every hour I was getting data emailed to me, and several times a day I had to sit and catalogue the recordings and speaker metadata, while managing the MTurk tasks. Most of the time, it was relatively straightforward, but some participants needed a little extra attention because of technical difficulties, glitches in the system, or complaints here and there. Luckily, I did this when I wasn’t in classes, because otherwise it would have been impossible."
  },
  {
    "objectID": "blog/using-mturk/index.html#june-20",
    "href": "blog/using-mturk/index.html#june-20",
    "title": "Using MTurk",
    "section": "June 20",
    "text": "June 20\nAt last, my data collection has drawn to a close. I ended up with about 212 speakers and 84 hours of data. Not bad. Now comes the daunting task of processing all of this. For every person, if I just want to do a small task that only takes a minute, it’ll take over 3 hours to do it for all speakers! This will take a very long time for me to get through, but from the 2% that I’ve looked at so far, it’s going to be very fruitful corpus."
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "",
    "text": "In English sociolinguistics, you’ll often see vowel phonemes represented by a single word in small caps. For example, trap represents /æ/. However, in a lot of American dialectology papers, you’ll see authors use the label bat instead. In this post, I explain why I think these competing labels are used… and why I prefer trap over bat.\nSee also: “Thoughts on allophonic extensions to Wells’ lexical sets.”"
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html#wells-lexical-sets",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html#wells-lexical-sets",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "Wells Lexical Sets",
    "text": "Wells Lexical Sets\nAs it turns out the trap label came first. In fact, trap is just one in a set of 24 labels, one for each English vowel. The creator of this lexical set is John C. Wells, who established them in his 1982 three-volume series, Accents of English. In the preface of each volume, Wells explains a notation system that has since been called the “Wells Lexical Sets.” Because it is brief, I’ll quote it in its entirety (bold and small caps in original):\n\nWords written in capitals\n\n\nThroughout the work, use is made of the concept of standard lexical sets. These enable one to refer concisely to large groups of words which tend to share the same vowel, and to the vowel which they share. They are based on the vowel correspondences which apply between British Received Pronunciation and (a variety of) General American, and make use of keywords intended to be unmistakable no matter what accent one says them in. Thus ‘the kit words’ refers to ‘ship, bridge, milk…’; ‘the kit vowel’ refers to the vowel these words have (in most accents, /ɪ/); both may just be referred to as kit.\n\nWells then provides this table:\n\nTable 1: Wells’ original lexical sets. From Wells (1982:xviii–xix).\n\n\n\n\n\n\n\n\n\nRP\nGenAm\n\n\n\n\n\n\n\nɪ\nɪ\n1.\nkit\nship, sick, bridge, milk, myth, busy…\n\n\ne\nɛ\n2.\ndress\nstep, neck, edge, shelf, friend, ready…\n\n\næ\næ\n3.\ntrap\ntap, back, badge, scalp, hand, cancel…\n\n\nɒ\nɑ\n4.\nlot\nstop, sock, dodge, romp, possible, quality…\n\n\nʌ\nʌ\n5.\nstrut\ncup, suck, budge, pulse, trunk, blood…\n\n\nʊ\nʊ\n6.\nfoot\nput, bush, full, good, look, wolf…\n\n\nɑː\næ\n7.\nbath\nstaff, brass, ask, dance, sample, calf…\n\n\nɒ\nɔ\n8.\ncloth\ncough, broth, cross, long, Boston…\n\n\nɜː\nɜr\n9.\nnurse\nhurt, lurk, urge, burst, jerk, term…\n\n\niː\nu\n10.\nfleece\ncreep, speak, leave, feel, key, people…\n\n\neɪ\neɪ\n11.\nface\ntape, cake, raid, veil, steak, day…\n\n\nɑː\nɑ\n12.\npalm\npsalm, father, bra, spa, lager…\n\n\nɔː\nɔ\n13.\nthought\ntaught, sauce, hawk, jaw, broad…\n\n\nəʊ\no\n14.\ngoat\nsoap, joke, home, know, so, roll…\n\n\nuː\nu\n15.\ngoose\nloop, shoot, tomb, mute, huge, view…\n\n\naɪ\naɪ\n16.\nprice\nripe, write, arrive, high, try, buy…\n\n\nɔɪ\nɔɪ\n17.\nchoice\nadroit, noise, join, toy, royal…\n\n\naʊ\naʊ\n18.\nmouth\nout, house, loud, count, crowd, cow…\n\n\nɪə\nɪ(r\n19.\nnear\nbeer, sincere, fear, beard, serum…\n\n\nɛə\nɛ(r\n20\nsquare\ncare, fair, pear, where, scarce, vary…\n\n\nɑː\nɑ(r\n21\nstart\nfar, sharp, bark, carve, farm, heart…\n\n\nɔː\nɔ(r\n22\nnorth\nfor, war, short, scorch, born warm…\n\n\nɔː\no(r\n23\nforce\nfour, wore, sport, porch, borne, story…\n\n\nʊə\nʊ(r\n24.\ncure\npoor, tourist, pure, plural, jury…\n\n\n\nLater on in the book (p. 122–124), Wells compares Received Pronunciation and General American English and goes into more detail about the principle behind the lexical sets:\n\nWhen we compare the pronunciation of particular words in the two accents, we find that in many respects there is a good match: for example, almost all words that have /iː/ in RP have the corresponding /i/ in GenAm, and vice versa: thus creep, sleeve, key, people and hundreds of other words. Likewise /aɪ/, transcribed identically for the two accents, and used in both cases for ripe, arrive, high, try and many other words…\n\n\nInvestigation shows that… we can successfully match the vowels in RP and GenAm forms of particular words for the vast bulk of the vocabulary…\n\n\nThis matching furnishes us with the framework of standard lexical sets which we use not only for comparing RP and GenAm but also for describing the lexical incidence of vowels in all the many accents we consider in this work. It turns out that for vowels in strong (stressed or stressable) syllables there are twenty-four matching pairs of RP and GenAm vowels. We identify each pair, and each standard lexical set of words whose stressed syllable exhibits the correspondence in question, by a keyword, which we shall always write in small capitals. Thus the correspondence between RP /iː/ and GenAm /i/ is the basis for the standard lexical set fleece…\n\n\nIn the rest of this work standard lexical set keywords will also be used to refer to (i) any or all of the words belonging to the standard lexical set in question; and (ii) the vowel sound used for the standard lexical set in question in the accent under consideration. Rather than using expressions such as ’short i/ for example, we shall speak of the kit vowel or simply of kit.\n\nIf you’re unfamiliar with these labels, I encourage you to look at Wells’ book. He explains each of these lexical sets in greater detail on pages 122–168 of Volume I."
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html#an-alternative-lexical-set-the-b_t-frame",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html#an-alternative-lexical-set-the-b_t-frame",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "An Alternative Lexical Set: the b_t frame",
    "text": "An Alternative Lexical Set: the b_t frame\nThe Wells sets are very useful, but for some reason, they have not become adopted universally. Several researchers have opted to use an alternative set of labels that take advantage of a large minimal set in English, the b_t frame.\nI did some digging in old American Speech and volumes of the Publications of the American Dialect Society to see when these labels were first used. The earliest instance I could find goes all the way back to  Sumner Ives’ 1954 study called The Phonology of the Uncle Remus Stories, which was the 22nd volume in the PADS series. On page 6, the author states that the following words are to refer to English vowels: beet, bit, bait, bet, bat, not, bought, boat, put, boot, but, curt, bite, bout, boy, and above. This set is remarkably close to what some researchers use today!Sumner Ives. 1954. The Phonology of the Uncle Remus Stories. Publication of the American Dialect Society 22(1): 3–59. https://doi.org/10.1215/-22-1-3\n In contemporary sociolinguistics, I believe the b_t frame was popularized by Erik Thomas & Malcah Yaeger-Dror’s 2009 edited volume, African American English Speakers and Their Participation in Local Sound Changes: A Comparative Study. In the introduction, starting on page 8 and spilling into page 9, they say that theyErik Thomas & Malcah Yaeger-Dror, eds. 2009. African American English Speakers and Their Participation in Local Sound Changes: A Comparative Study. Publication of the American Dialect Society 94. Available here.\n\n…found that it would be helpful to formulate a convention to unify the text and simplify the reader’s task; with that thought in mind, we have suggested that authors use neither a phonological / / nor a variable ( ) presentation, both of which differ in conventions from author to author. We have chosen instead to refer to a given vowel class using keywords, following the principle behind Wells (1982). To further simplify, we turned to Ladefoged’s (2005) choice of keyword paradigm, which uses words that are as untrammeled by their consonantal environment as possible. To obtain these keywords, he chose an h_d frame, to have his speakers “say heed again.\n\n\nTo minimize the need for varying the “carrier” environment, in each case, the vowel being focused on here will be a b_t paradigm.\n\n\nTable 2: The lexical set based on the b_t frame. This is a subset of the table by Thomas & Yaeger-Dror (2009:6).\n\n\nIPA\nKeyword\n\n\n\n\n/i/\nbeet\n\n\n/ɪ/\nbit\n\n\n/e/\nbait\n\n\n/ɛ/\nbet\n\n\n/æ/\nbat\n\n\n/ɑ/\nbot\n\n\n/ɔ/\nbought\n\n\n/o/\nboat\n\n\n/ʌ/\nbut\n\n\n/ʊ/\nbook\n\n\n/u/\nboot\n\n\n/aɪ/\nbite\n\n\n/aʊ/\nbout\n\n\n/ɔɪ/\nboy\n\n\n/ɚ/\nbird\n\n\n\n\nThey end with this statement:\n\nWe hope that this convention will permit the reader to follow all the authors without difficult transitioning between chapters.\n\n It appears that their goal for continuity has beyond their volume because the set was used in later volumes of the Publications of the American Dialect Society. For instance, here are the remarks by the editors of Speech in the Western States: Volume 1: The Coastal States (Fridland et al. 2016):Valerie Fridland, Tyler Kendall, Betsy E. Evans, & Alicia Beckford Wassink, eds. 2016. Speech in the Western States, Vol 1., The Coastal States. Publication of the American Dialect Society 101. Available here.\nThe description in Speech in the Speech in the Western States: Volume 2: The Mountain West (Fridland et al. 2017) is almost identical.\n\nFor the purpose of clarity and continuity, authors use the conventions of the International Phonetic Alphabet throughout the chapters, though, in many cases, keywords in the b_t frame are used to highlight particular word classes and subclasses, following other recent PADS volumes (Thomas & Yaeger-Dror 2009). These frames are built upon those made for comparative study of English dialects by Wells (1982) but have been adapted to allow representation of the particular vowel changes and conditioning environments of interest to the present study of the U.S. West.\n\n\nTable 3: The lexical set used in the Speech in the Western States volumes. From Fridland et al. 2016:3 and Fridland et al. 2016:5; Table 1.1 in both. The columns have been rearranged for consistency within this blog post.\n\n\nIPA\nWells Keyword\nb_t Keyword\n\n\n\n\nɪ\nkit\nbit\n\n\nɛ\ndress\nbet\n\n\næ\ntrap\nbat\n\n\nɑ ~ a\nlot\nbot\n\n\nɔ ~ a\ncloth/thought\nbought\n\n\nʌ\nstrut\nbut\n\n\nʊ\nfoot\nbook\n\n\nɚ\nnurse\nburt\n\n\ni\nfleece\nbeet\n\n\ne\nface\nbait\n\n\no\ngoat\nboat\n\n\nu\ngoose\nboot\n\n\naɪ\nprice\nbite\n\n\nɔɪ\nchoice\nboy\n\n\naʊ\nmouth\nbout\n\n\nɪɹ ~ iɹ\nnear\nbeer\n\n\nɚɹ\nsquare\nbare\n\n\nɑɹ\nstart\nbar\n\n\nɔr / or\nnorth/force\nbore\n\n\nʊɹ\ncure\nburr\n\n\nəɹ\nletter\n\n\n\nə\ncomma\n\n\n\n\nThe only difference is that bird was changed to burt.\nOutside of the PADS volumes, this frame was also used in McCarthy (2011), which explicitly states that these labels were used because of Thomas & Yaeger-Dror (2009).\nI don’t know the reason why the b_t frame was designed when the Wells lexical sets were already established. Perhaps the draw of the nearly complete minimal set to contrast all English vowels was useful. Maybe it’s because the words in the b_t frame are shorter, which makes for less cluttered visualizations and written prose. Thomas & Yaeger-Dror did say that the use of the consonants /b/ and /t/ in the keywords helped reduce the effects of surrounding consonants on the vowels themselves. Ultimately though, I’m not sure."
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html#my-thoughts-on-the-b_t-frame",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html#my-thoughts-on-the-b_t-frame",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "My thoughts on the b_t frame",
    "text": "My thoughts on the b_t frame\nHot take: I don’t think this the b_t is any more useful than IPA. Hear me out:\nImagine you’re at a conference talking about the low back merger and you yourself have the merger. Since you don’t naturally differentiate the words bot and bought, you struggle to explain the differences that may exist in your study. Even if you do distinguish the sounds, many people in your audience may not, so they’ll have a hard time understanding. This was one of the reasons Wells came up with his lexical sets: neither you nor your audience would have much difficulty understanding the words lot or thought since, as far as I know, /lɔt/ and /θɑt/ are not English words.\nEven if you’re not talking about a merger, if you are someone with particularly shifted vowels, when you say an isolated, ambiguous token like [bɛ̞t] or [bæ̙t], it may not be immediately clear to listeners of other dialects which vowel you’re talking about.\nThe words in the b_t frame may be “untrammeled by their consonantal environment,” but I don’t know if the lack of transition formants make for the most effective label in speech or writing. Keywords are labels to refer to large lexical sets, so while they may not make for ideal tokens when collecting phonetic data, they need to still serve the purpose of unambiguously identifying a vowel phoneme.\nIn fact, Wells specifically designed his original set so that it specifically would not use the b_t frame:\n\nThe keywords have been chosen in such a way that clarity is maximized: whatever accent of English they are spoken in, they can hardly be mistaken for other words. Although fleece is not the commonest of words, it cannot be mistaken for a word with some other vowel; whereas beat, say, if we had chosen it instead, would have been subject to the drawback that one man’s pronunciation of beat may sound like another’s pronunciation of bait or bit. (Wells 1982:123)\n\nI question the usefulness of the b_t frame. It’s convenient that a common enough English word can be created by filling in almost any vowel into the template, but I don’t know if this large minimal set makes for the most unambiguous lexical set. When he introduces his keywords, Wells says that they are “intended to be unmistakable no matter what accent one says them in.” This property is not retained in the b_t set of keywords. In fact, creating a set based on minimal pairs defeats the very purpose of a lexical set.\nTo put it another way, just because we call something the cot-caught merger doesn’t mean we should refer to the entire lexical sets as cot and caught. In fact, I think we should actively avoid referring to them as cot and caught for the very reason that they do form a minimal pair.\nIt’s my impression that researchers on non-American varieties of English use Wells’ original lexical sets without any problems. Using a different set is potentially confusing and may alienate ourselves from studies on other World Englishes.\nTo be clear, I am in no way criticizing the researchers who came up with or use the b_t frame. If you’re familiar with what I do you’ll know that their work is highly relevant to my own studies, and I cite a lot of studies that use the b_t frame. Their work is excellent and I model my own work after their theirs. I just think the labels could be clearer."
  },
  {
    "objectID": "blog/why-do-people-use-bat-instead-of-trap/index.html#conclusion",
    "href": "blog/why-do-people-use-bat-instead-of-trap/index.html#conclusion",
    "title": "Why do people use BAT instead of TRAP?",
    "section": "Conclusion",
    "text": "Conclusion\nThis reminds me of how some people use the Labovian transcription system instead of standard IPA. See Josef Fruehwald’s blog post on those. There are two competing systems of lexical sets being used in American dialectology: the Wells lexical set and the b_t frame. To answer my titular question of why people use bat instead of trap… I don’t really know. I think it may largely depend on what university the work is coming out of. But, I think Wells’ original set may be a little better.\nPS: Regardless of which system you use, I think we should make sure we use small caps instead of ALL CAPS or even Capitalized Small Caps. It’s truer to Wells’ original notation and I think they just look a lot better typographically.\nPPS: To my knowledge, Wells never intended for the lexical set labels (or even the example words in the original explanation) to be ideal tokens for eliciting the vowels they represent. So, while the b_t labels might be “untrammeled by their consonantal environment,” the Wells labels are not. So, there’s probably no need to eliciting the words fleece, kit, face, dress, etc. when getting tokens of these vowels.\n\nUpdate: Click here for further musings, ramblings, and recommendations for non-canonical extensions to Wells’ lexical sets when referring to allophones."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html",
    "title": "New publication in the latest PADS volume",
    "section": "",
    "text": "This week I finally got to lay my hands on a physical copy of my latest publication! It’s called “The Absence of a Religiolect among Latter-day Saints in Southwest Washington” and it’s in the latest Publication of the American Dialect Society, Speech in the Western States Volume III: Understudied Dialects by Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall. The physical copy was delivered to my office about two weeks ago, but my wife and daughter had just tested positive for covid-19 (they’re fine—very mild symptoms) so I was only just now able to see it now that my two-week quarantine is over."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html#a-brief-bit-of-background",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html#a-brief-bit-of-background",
    "title": "New publication in the latest PADS volume",
    "section": "A brief bit of background",
    "text": "A brief bit of background\nI’m so excited to make it into this PADS volume! Just to give a brief timeline, I decided that I wanted to study English in the West around January 2015, after attending the American Dialect Society annual meeting in Portland and seeing the great talks on language in the West there. I did as much background and reading as I could over the next year or so and put together a grant proposal to go do fieldwork in southwest Washington. So when the first volume came out in January 2016, I was thrilled to see the latest research and to know that my research idea was a hot topic in American dialectology. That first volume, which covered California, Oregon, and Washington was extremely important in helping me shape my own research.\nIn 2016, my research in the West began in earnest. I got my grant to go do fieldwork and spend June and July in Cowlitz County. I had processed enough of the wordlist data in time to submit a paper to the American Dialect Society annual meeting. I was excited because it was my first big conference and was my introduction to the field as a dialectologist. Around that time Speech in the Western States Volume II came out, which focused on the Mountain West and included chapters on Arizona, New Mexico, Nevada, Utah, Colorado, and Montana. I was bummed I wasn’t able to make it into that volume, but I really didn’t have anything relevant to that area at the time and my research was in the beginning stages still.Being excited about this meeting was the first thing I blogged about!\nSo at that point, I figured I had missed the train. I hopped on the West bandwagon just a year or so too late—soon enough to really benefit from the current research, but not soon enough to be a part of that first conversation. Nevertheless, my research continued.\nI had finished processing my Washington data and was working on my dissertation when I heard about a third volume of Speech in the Western States, this time focusing not on any geographic area within the West but rather on understudied communities. While I didn’t have any data from ethnic minorities from Washington, it did occur to me that I had a nice balance of members of the Church of Jesus Christ of Latter-day Saints to non-members. So I put together an abstract that compares the two groups, sent it to the editors, and the rest is history.\nSo I am just thrilled to be a part of this last volume on Speech in the Western States. The first two volumes were so important as my research was developing and so it just made it that much sweeter to be a part of the last volume."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html#a-brief-summary-of-the-chapter",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html#a-brief-summary-of-the-chapter",
    "title": "New publication in the latest PADS volume",
    "section": "A brief summary of the chapter",
    "text": "A brief summary of the chapter\nThe question I had was this: do Latter-day Saints in southwest Washington sound different than non–Latter-day Saints? In other words, do they have a religiolect? I thought they might since they do in Utah and in Southern Alberta. Specifically, they tend to have more (or more exaggerated) Utah English features and tend to lag behind regional language changes. I thought I’d test for that same thing in Washington since I had a nice, balanced sample of Latter-day Saints to non–Latter-day Saints.\nSo, using fancy statistics and looking at vowels known to be variable in Utah and Washington, I conclude that there was very little difference between the two groups. So, a null result. The question then is this, why not? Why don’t Washingtonian Latter-day Saints have a religiolect while Albertan and Utahn Latter-day Saints do? I got into a lot of detail on Latter-day Saint culture and history and eventually conclude that they’re just too small of a minority, aren’t locally salient enough, and aren’t as entrenched in Latter-day Saint culture for a religiolect to have developed."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html#lots-of-qualitative-analysis",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html#lots-of-qualitative-analysis",
    "title": "New publication in the latest PADS volume",
    "section": "Lots of qualitative analysis",
    "text": "Lots of qualitative analysis\nOverall the paper is dense. The whole chapter is 28 PDF pages, and a quarter of that is just references! I tend to be a little citation-heavy in all my writing to be honest, but over 100 references in this otherwise relatively short paper might have been a little much… But I had a lot of things to bring in: Utah English, Washington English, Latter-day Saints, religiolect, and GAMMs.\nYou may notice that the paper is and is a bit very top-heavy as well. If you just take the 6,962 words of actual body text, 2,967 of them (43%) is background and lit review. I go into a lot of detail on Utah English and Washington English, not to mention detail on Latter-day Saint culture and history. I then have 2,098 words (30%) of methods, which was also necessary since GAMMs can take a while to explain. I even had to leave out a lot of detail and refer readers to my dissertation! So you don’t even get to the linguistic analysis until three-quarters of the way into the paper!\nThe actual results are only 982 words (14%), but I think I made some good visualizations that explain things better than words do. Besides, it’s a null-result paper, so there’s not a lot to say other than, “nope and this isn’t significant either.” Finally, I finish with another 915 words (13%) of discussion and conclusion, which again go into detail about Latter-day Saint culture.In fact, a stylized version of one of my plots made its way to the cover of the book!\nAnd, on top of all this this is three pages of notes, some of which are pretty long, an appendix with more visuals, and online supplementary material. So, a lot of background for what turns out to be relatively little actual phonetic analysis, but I think all that background was important to really appreciate the overall message of the paper. I think it strikes a nice balance of qualitative and quantitative work."
  },
  {
    "objectID": "blog/new-publication-in-the-latest-pads-volume/index.html#my-favorite-parts",
    "href": "blog/new-publication-in-the-latest-pads-volume/index.html#my-favorite-parts",
    "title": "New publication in the latest PADS volume",
    "section": "My favorite parts",
    "text": "My favorite parts\nOne thing I’m particularly pleased with in that paper is the map on page 96. This one map shows a lot of information all at once, pulled together from a few different sources. First, it shows state and county boundaries, which come from publicly available sources. Those counties are then colored by number of Latter-day Saints per 1000 residents, which was data I had to pull from the US Religion Census. Finally, there are the outlines of the “Mormon Culture Region,” which was defined in Meinig (1965). For that, I had to scan in the original map and basically trace the original boundaries onto the digital map. I’m quite pleased with how it turned out, and the publishing editors did a fantastic job at sprucing it up so that it matches the look and feel of American Speech articles.\n\nAnother thing that this paper introduces is a method of outlier detection that I’ve been working on for a few years. I’ll give more detail on a future blog post, and perhaps a publication all on its own, but for now, you can read the details in page 102.\nFinally, I quite like deep dives that I took in some of the footnotes. In particular, Note 2 gives a brief history of the Church of Jesus Christ of Latter-day Saints in Cowlitz County. This was data that was also pulled from several sources. I had to ask my mother-in-law if she knew any old-timers who might be able to fill in some of the gaps, which was very helpful. But my friend Jonathan Hepworth, a history PhD student at the University of Georgia, was the most help. He was able to track down some primary sources that helped fill in some of the details (like the date of creation of the Longview Stake)—and he did so in like an hour! I’m amazed at the resources that historians have access to.\nAnyway, so that’s it for this paper."
  },
  {
    "objectID": "blog/excel_workshop/index.html",
    "href": "blog/excel_workshop/index.html",
    "title": "Excel Workshop",
    "section": "",
    "text": "Today I had the opportunity to give a workshop in the DigiLab in UGA’s main library. It was a packed with librarians and grad students from across campus. In just over an hour, I started with the absolute basics and showed more and more tricks that I think would help people with their research projects.\nThis was the first time I’ve ever given a presentation without powerpoint slides. As I was preparing though, it seemed silly to include detailed descriptions and screenshots when I could just switch over the Excel and show it live. I ended up putting together a handout instead, which had all of the information on it instead. The presentation (and handout) went through the following topics."
  },
  {
    "objectID": "blog/excel_workshop/index.html#the-basics",
    "href": "blog/excel_workshop/index.html#the-basics",
    "title": "Excel Workshop",
    "section": "The Basics",
    "text": "The Basics\nI started off by explaining what the differences were between Microsoft Office 2016, 365, and Online. Essentially, Office 2016 is stand-alone software that you buy once and keep forever but doesn’t upgrade. Office 365 is a subscription service where you have the software as long as you subscribe to it, but it upgrades all the time. Office Online is a free cloud-based version. Through UGA, we can get Office 365 for free, and a heavily reduced priced Office 2016.\nI then opened up Excel and covered the absolute basics. Data entry. Moving around the spreadsheet. Cell formatting. Borders. Text formats. I’m pretty sure everyone there knew this basic stuff, but I thought I’d cover it anyway because, you never know, there might be someone who’s never seen it before.\nAfter that, into topics that make it easier to play around with your data. Search and replace, with some extras like matching the entire cell. I also covered sorting and filtering and showed that you can filter multiple columns to get a really specific subsets of your data."
  },
  {
    "objectID": "blog/excel_workshop/index.html#pivot-tables",
    "href": "blog/excel_workshop/index.html#pivot-tables",
    "title": "Excel Workshop",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nThis is where I wanted to spend the most time. Pivot tables are things that a lot of people had heard of (and from the show of hands, about half the people in the room), but for some reason—and I don’t say this to be all high and mighty—I have never met anyone who knows how to use them. I learned them in my Intro to Linguistic Computing class with Monte Shelley at BYU and have used them a ton since then, but I guess people just haven’t had the chance to learn about them.\nPivot tables are dang useful. They can summarize your data in tons of fancy ways. At the very basic level, you can at least see all the unique values in a particular column in your dataset, which is good for checking typos or for copying and pasting into lookup tables (see below). But when you add columns, you can then see how many row of your data frame there are that match the row and column. In the presentation we looked at come census data and saw how many people were from each city within that county. By adding columns, we could see how many men and women there were in each city. We could then add additional columns (like fabricated favorite color data) so that we could see how many men and women liked each color within that city.\nThere are different ways of viewing the data as well. Instead of the raw count, we looked at how to view the proportion of men to women there were in each town. We switched to a numerical data type (fabricated weight and height data), and were able to see the average weight and height for men and women in each city, as well as the tallest, shortest, heaviest, and lightest man and women in each town. I heard some audible whoa’s from people as I showed some of this stuff, which was great to hear."
  },
  {
    "objectID": "blog/excel_workshop/index.html#functions",
    "href": "blog/excel_workshop/index.html#functions",
    "title": "Excel Workshop",
    "section": "Functions",
    "text": "Functions\nI then looked at some Excel functions. We started with some basic math, but quickly went into some functions. I showed how some can just stand on their own, like pi(), today(), and now(). We looked at how to reference other cells in functions and how they update automatically. I showed how to create a sequential list of numbers using functions that reference each other. There are some functions like year(), month(), weekday(), and datedif() that work on dates and others like concat(), upper(), lower(), left(), and right() are for manipulating strings. Then there are some that make reference to ranges, like sum() and average(). I showed how the concat() function can be useful to string together last name and first name to create a “Last, First” column. I know there are much more complicated and useful functions than the ones I covered, but I didn’t want to intimidate anyone.\nWe then looked at some conditionals and how they work. As an example, I created a new column in the census data that essentially collapsed the birth state down to two: Washington or not Washington."
  },
  {
    "objectID": "blog/excel_workshop/index.html#lookup-tables",
    "href": "blog/excel_workshop/index.html#lookup-tables",
    "title": "Excel Workshop",
    "section": "Lookup Tables",
    "text": "Lookup Tables\nThe lookup() function is one that I use all the time, and I wanted to make sure I covered it in the workshop. When preparing for the workshop, I looked at some other site and they all mention that lookup() is essential. What this function can do is basically link together multiple spreadsheets so it starts to act like a database. You set up a table that acts like a dictionary: alphabetic, unique values in one column, with paired information in another. You can then “lookup” some value in this table, and the function will return the information associated with it.\nWhy is this useful? I use it for two main purposes: a converter, and a collapser. I use it as a converter for things like turning ARPABET representations of vowels into IPA. In one column is the ARPABET pair of letters, and the other column are the IPA symbols. It’s perfect for that. I use it also to collapse data down to fewer categories. We did this in the workshop by collapsing the 50 states down to 4–5 regions. We used this lookup table to add a “region” column to the census data, and then made a pivot table with it. Pretty cool.\nFor the last part of this section, I showed how to handle the places where lookup() fails, like blank cells or cells not in the dictionary. For blanks, it returns an error, and you can overcome that by wrapping the lookup() function in if(isblank()). But for those pesky typos, lookup() returns the closest value, which I only discovered recently and was not happy with it. I didn’t have to demonstrate, but in the handout I show that if you do something like =IF(COUNTIF(A1:A5,C2)&gt;0, LOOKUP(C2, A1:A5, B1:B5), \"ERROR\") it’ll work great."
  },
  {
    "objectID": "blog/excel_workshop/index.html#visualizations",
    "href": "blog/excel_workshop/index.html#visualizations",
    "title": "Excel Workshop",
    "section": "Visualizations",
    "text": "Visualizations\nNext was how to do quick and dirty visualizations in Excel. I explained briefly (probably too briefly) that not all visualizations work for all kind of data, which I feel is important for people to know. I then showed how to make a bar chart, pie chart, line graph, and scatterplot. I of course used pivot tables to help summarize the data for visualization. I did say though that I don’t use Excel’s visualizations for anything because I find them ugly, not customizable enough, and not robust enough to handle what I want to do."
  },
  {
    "objectID": "blog/excel_workshop/index.html#bonus-tips-and-tricks",
    "href": "blog/excel_workshop/index.html#bonus-tips-and-tricks",
    "title": "Excel Workshop",
    "section": "Bonus Tips and Tricks",
    "text": "Bonus Tips and Tricks\nIn the last four minutes, I tried to cover some bonus little tips and tricks that I’ve picked up along the way. There are little things like anchoring and freezing/splitting the table. I did show conditional formatting because I use that all the time (my tables look a little psychedelic actually). In the handout I cover how to convert text-to-columns, which can be useful when importing data from somewhere else or for just splitting things up like “Last, First” into two columns. I covered paste special and how you can transpose, paste multiply, and overwriting functions.\nIt was a real whirlwind of a presentation but I think some people got a lot out of it. I don’t know if too many people walked away with any new skills per se, but at least people were exposed to what kinds of things Excel can do, and were given the resources (i.e. this handout) to learn how to do it themselves. I enjoyed giving the presentation, and even though I use R for most of my work nowadays, knowing the ins and outs of Excel sure is useful."
  },
  {
    "objectID": "blog/excel_workshop/index.html#downloads",
    "href": "blog/excel_workshop/index.html#downloads",
    "title": "Excel Workshop",
    "section": "Downloads",
    "text": "Downloads\nAgain, you can download this handout here. Feel free to also download the three datasets I used: the vowels for one speaker and the vowels subset of larger dataset, which both come from the Linguistic Atlas of the Gulf States, and the Cowlitz County 1930 census data, which I gathered myself. Please also visit the accompanying blog post on the DigiLab website."
  },
  {
    "objectID": "blog/ads-meeting/index.html",
    "href": "blog/ads-meeting/index.html",
    "title": "ADS Meeting!",
    "section": "",
    "text": "I’m thrilled to announce I’ve been accepted to present a paper at the 2017 annual conference of the American Dialect Society!\nWhen I was an undergrad I was a part of a research team that presented at the LSA Annual Meeting in Boston in 2013. (I actually cut my honeymoon short so I could attend that meeting!) I was a starry-eyed budding little linguist with no real idea of what was going on. However, I do remember spending a large chunk of my time at that conference in the ADS presentations. In fact I still talk about some of the presentations I saw there.For a fuller account of my journey into linguistics, see 10 Years of Linguistics.\nTwo years later, the LSA and its sister societies met in Portland. Being a short drive from where my wife grew up, we decided to fly out so she could see her family and so I could attend the conference. Again, I found myself almost exclusively attending the ADS meetings. (Part of that may have been that it was up on like the 23rd floor and the room had a fantastic view!) I recognized people from the Boston meeting (Tyler Kendall, Sali Tagliamonte, etc.) and was also able to put some faces to names I had read (Charles Boberg, David Bowie, etc.). As it turns out, there were several presentations on language in the West that I ended up citing in some of my own work.\nNow, two years later, I’m extremely happy to be presenting at the 2017 meeting. Now that I have a clear idea about what my research interests are, and I know many more scholars’ works besides recognizing their name, I hope to reach out and talk with many of these people. Hopefully I’ll come across less of a fan and more as a colleague.\nI get to present a portion of my data that I collected in Washington just a few months ago. In a nutshell I’m showing that in word lists people pronounce pull and pole the same but Mary and merry different, but in a minimal pair task, pull and pole are separate while Mary and merry are the same.\nI’m about to give two completely unrelated presentations soon, so my focus hasn’t been on this Pacific Northwest, but once that wave has passed I’ll be able to devote more time and energy into this. It’s a very exciting time for me and I really look forward to the conference."
  },
  {
    "objectID": "blog/dissertation/index.html",
    "href": "blog/dissertation/index.html",
    "title": "Dissertation",
    "section": "",
    "text": "I’m happy to report that I successfully defended my dissertation today! The defense was held in the DigiLab (300 Main Library). The study itself is called “Vowel Dynamics of the Elsewhere Shift: A sociophonetic analysis of English in Cowlitz County, Washington.”\n\n\n\nMe with my committee: Chad Howe, Peggy Renwick, and Bill Kretzschmar (Skyping in).\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can download my dissertation here!\n\n\nThe version linked above is a revision that I’ve made after correcting some small typos. Click here to view the official, submitted version."
  },
  {
    "objectID": "blog/secol2017/index.html",
    "href": "blog/secol2017/index.html",
    "title": "SECOL 2017",
    "section": "",
    "text": "I was unable to attend this year, but my colleagues presented two papers I was a part of at the 84th Southeastern Conference on Linguistics (SECOL84) in Charleston, South Carolina.\nThe first presentation was with Bill Kretzschmar and Katie Kuiper and was called “Automated Large-Scale Phonetic Analysis: DASS” wherein we introduce the NSF-funded project—with Drs. Kretzschmar and Peggy Renwick as PIs—that I am involved in. A PDF of the slide show is are available here.\nThe second presentation was with Rachel Olsen, Mike Olsen, and Peggy Renwick and was called “Transcribing the Digital Archive of Southern Speech: Methods and Preliminary Analysis” wherein we talked about the nuts and bolts of how to get a project of this size running. A PDF of the slide show is available here.\nThis work is part of an ongoing project at the Linguistic Atlas Office at the University of Georgia. We have several hundred hours of recordings from the 1970s of speakers all across the South. We have around 40 undergraduate workers transcribing for us with another couple grad students (including myself) doing the less soul-sucking work. Eventually we’ll have all this data freely available online, but in the meantime we’re figuring out how to process such scratchy recordings and doing linguistic analysis on it. It’s been a lot of fun and I’m glad we were able to show others our work."
  },
  {
    "objectID": "blog/jealousy-list-2/index.html",
    "href": "blog/jealousy-list-2/index.html",
    "title": "Jealousy List 2",
    "section": "",
    "text": "This is the second post in my occasional series of Jealousy Lists. I’m subscribed to about 50 blogs, most of them Data Science–related, and I’ve see a lot of really cool stuff coming out recently. It makes me really want to take my R skills to the next level. Anyway, these are some cool posts that I read recently:\n\nMichael Höhle. “Judging Freehand Circle Drawing Competitions”.\nHave you ever noticed it’s really hard to draw a perfect circle? Apparently, there are people that are really good at it. This post shows how you might determine how perfect the a handdrawn circle is: “We took elements of computer vision, image analysis and total least squares to segment a chalk-drawn circle on a blackboard and provided measures of it’s circularness.” Spoiler alert: the guy’s circle was pretty dang near perfect.\nYihui Xie. “Impact: Depth or Breadth?”\nIn this brief post, Yihui discusses whether we should strive for breadth or depth in our research. He says, “I prefer a small number of people (could even be only one person) feeling extremely excited over a large number of people only slightly nodding.” The logic is that that one person may shout from the rooftops for you, and your impact will spread from there. Also, the one person may just be you. I’ve heard this about app development (make something that you want to use) and it was interesting to see it applied to research too.\nJulia Silge. “Training, Evaluating, and Interpreting Topic Models”.\nI’m a big fan of Julia Silge’s work, and though I’ve never needed to topic modeling in my own research, her blogs (and book) always make me want to start. In this post, she takes a bunch of texts from the Hacker News Corpus and shows how you might determine the best number of topics to choose when doing topic modeling. She then does the analysis itself and shows how the words fit into topics.\nThomas Lin Pedersen. “What Are We Plotting, What Are We Animating”\nAnimations are getting more and more popular, and Pedersen’s gganimiate package is a great tool for you to create them in R. This post looks at what happens when you try to animate things that are a big different from each other. One type of animation will do so clunkily (like if you want to use Powerpoint to animate transitions between slides…). This post takes a dive at what happens under the hood in ggplot2 and gganimate to help you understand your data and the functions you use.\nLaura Ellis. “Create stylish tables in R using formattable”.\nBy default, tables in R are nothing more than text. In this post, Laura Ellis demonstrates the formattable package in R and shows how you can make things look a lot prettier. You can add colors, alignment, font, and all sorts of other stuff.\n\nSo that’s my jealousy list for now. Image recognition, research, topic modeling, animations, and tables. I’m glad other people put so much interesting stuff online."
  },
  {
    "objectID": "blog/nwav46/index.html",
    "href": "blog/nwav46/index.html",
    "title": "NWAV46",
    "section": "",
    "text": "At the 46th New Way of Analyzing Variation conference in Madison, Wisconsin, I presented a poster called Changes in the Timber Industry as a Catalyst for Linguistic Change.\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nIn a nutshell, I found a couple interesting things going on in a small town in Washington:\nLinguistic changes—I focused on two variables: bag-raising and goat-diphthongization. Specifically, older people raised bag and younger people had a more diphthongal goat. The generational divide was around 1970 and the difference between older and younger people was sudden.\nCensus data—Based on topics of conversation in the interviews, I took a look at census data and found some correlation with these linguistic changes. The mills laid a bunch of people off, people were earning less money, a bunch of people left town, and more of those who stayed worked outside the community. Correlation does not equal causation, but it’s paints a pretty compelling picture.\nCatastrophic events—One of the interesting findings was using regression with breakpoint as a way to model catastrophic change. I’m refining this methodology right now, but it does seem to work for modeling language change in time.\n\n\n(Photo credits I believe go to Maciej Baranowski)"
  },
  {
    "objectID": "blog/dh2019/index.html",
    "href": "blog/dh2019/index.html",
    "title": "DH 2019",
    "section": "",
    "text": "At the Digital Humanities 2019 conference in Utrecht, the Netherlands, I presented with Bill Kretzschmar on ways to visualize a lot of phonetic data."
  },
  {
    "objectID": "blog/dh2019/index.html#the-gazetteer-of-southern-vowels",
    "href": "blog/dh2019/index.html#the-gazetteer-of-southern-vowels",
    "title": "DH 2019",
    "section": "The Gazetteer of Southern Vowels",
    "text": "The Gazetteer of Southern Vowels\nThe first half of the presentation was essentially me showcasing the Gazetteer of Southern Vowels (or GSV), a website I created in Shiny to help visualize 1.3 million acoustic measurements from the Digital Archive of Southern Speech.The full web address is http://lap3.libs.uga.edu/ u/jstanley/vowelcharts/, but I’ve got a redirect at joeystanley.com/gsv that’s easier to type. In the talk I spend most of the time in the “Vowel Plot Comparison” tab (below) and show how you can interact with the data.\n\nFirst, you can subset the data by demographic factors. The Speaker Selection tab has menu items for speakers’ sex, age, ethnicity, home state, social class, and a couple other variables. When you select one or more of these, the plot automatically updates to reflect that subset.\n\nYou can also subset the data by linguistic factors. In the Words tab, you’ll see that a list of stopwords is displayed and that those are excluded by default. You can add to or remove words from that stoplist, or switch it so display only those words (or some other set of words like numbers or colors).\n\nIn the Vowels tab, you’ve got a whole bunch of options. First, you can choose what vowel is being displayed, what kind of stress it can have, and what its phonetic environment is(based on following segment only). There are some methodological choices too, like ways of filtering and normalizing the data. You can also choose what transcription system is being used.\n\nThen, there are ways for you to customize the plot. The Plot Style tab as four main options (points, ellipses, means, and words), that act independently with their own controls for size and opacity. So if you want means and ellipses but no dots, you can do that. If you want to display the words themselves, but in a small font and transparent, be my guest.\n\nThe Plot Customization tab lets you change things like the zoom, axes, aspect ratio, and (some control over) colors. I’m hoping to add more options to this tab in the future. With these two tabs, I feel like you can make a lot of very different plots, all based on the same data, which is pretty cool.\n\nFinally, the Download Options is the newest tab. You could always take a screenshot, but you’re limited to how your web browser displays the image and your computer’s screensize. In this tab, you can set the height, width, quality, and format, so you can make publication-quality images. In fact, the plots in this blog post were all created using this download button, so you can recreate them yourself!\n\nSo that’s it! My goal in creating these options was to allow users to create any type of plot using any conceivable subset of DASS, and I think the GSV does a pretty good job at that. Here’s a quick gallery of six different plots:"
  },
  {
    "objectID": "blog/dh2019/index.html#point-pattern-analysis",
    "href": "blog/dh2019/index.html#point-pattern-analysis",
    "title": "DH 2019",
    "section": "Point Pattern Analysis",
    "text": "Point Pattern Analysis\nIn the second half of the presentation, Bill Kretzschmar took over and discussed using point pattern analysis in the visualization of vowel data. He has found that when you overlay a grid on the F1-F2 space (just as geographers do with geospatial data), you can see the central tendency of vowels by which “cells” in this new grid are the densest. They roughly follow the 80-20 rule, with a few cells being heavy concentrated, some having some tokens, and many with very few. Here’s just one image of a Georgia man’s fleece vowel.\nIncidentally, because the grid itself is new element, there are additional controls in the Plot Customization tab, like how many cells and the opacity or size of the labels. And since there’s only one color being used, I’ve got controls over whether the shading is discrete or continuous, if it’s discrete then how many levels, and you can even put a custom color in hex notation. Also, I’m still working on getting the ranges in the legend to display integers only, since 54.6 data points in a cell is somewhat nonsensical.\n\nBill finds that if you plot them in order of density, the resulting curve is an asymptotic hyperbolic curve, or just A-curve for short. And, as it turns out, this distribution is fractal in nature, so regardless of how much you subset the data, you’ll find the same distribution. The GSV makes it easy to see this distributions interactively.\nAt the very end, we hinted at some additional visualizations we’d like to develop to make it easier to view trajectory data, taking advantage of a third-dimension in the plot itself. Hopefully, we’ll have more to say about that in the future."
  },
  {
    "objectID": "blog/dh2019/index.html#conclusion",
    "href": "blog/dh2019/index.html#conclusion",
    "title": "DH 2019",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s our presentation! We’ve got a lot of data and we needed lots of plots to make sense of it all. Instead of saving plot after plot, we decided an interactive Shiny app might be a better option. Most importantly, I think we’ve learned a little more about how language works because of the size of the data and the interactivity of this tool."
  },
  {
    "objectID": "blog/mary-merry-marry/index.html",
    "href": "blog/mary-merry-marry/index.html",
    "title": "A big list of Mary-merry-marry words",
    "section": "",
    "text": "Most Americans, including me, have this thing called the Mary-merry-marry merger. We pronounce all three words—and the vowels in similarly patterning words—the same. However, some Americans retain at least a two-way distinction and most, if not all, varieties of English outside of North America distinguish between all three.\nAs is typical for people with a merger, it’s not easy for me to separate words into their historic distributions. But sometimes I need to for teaching or preparing wordlists. So, as I prepared to cover the merger (or rather, the lack thereof) in my Varieties of English course this semester, I wanted to show the students a list of words that group with Mary, merry, and marry. But I couldn’t find a decent list anywhere. So I asked Twitter and I was pleasantly surprised to get lots of help from my non-merging followers!\nThank you to all those who sent me lists of words that belong to each class! And to those who pointed me to searchable dictionaries that distinguish between the three! I won’t mention names here, but I hope they are okay with me turning their collective input into a blog post.\nSo, the purpose of this post is to provide the most comprehensive list I could come up with of Mary, merry, and marry words—the list I was hoping to find a few days ago—just in case any other American needs it.\nNote: I’m told that outside the US, there is little variation in which class each word belongs to. However, in areas of North America that do make some distinction, there can be some variability. So I guess take this list with a grain of salt."
  },
  {
    "objectID": "blog/mary-merry-marry/index.html#merry",
    "href": "blog/mary-merry-marry/index.html#merry",
    "title": "A big list of Mary-merry-marry words",
    "section": "merry",
    "text": "merry\nThe first set, merry, is actually the dress lexical set. John Wells points out that merry is typically spelled with &lt;e&gt;. It also seems like a lot of French words have this vowel.\nHere are some words that were verified by some folks who do not have the merger.\n\nberry, beryl, burial, bur{y|ied}, cherry, derring-do, Derry, error, derriere, ferret, ferrous, ferry, Gerry, inherit(ed), Jerry, Kerry, merit(s), Merovingian, Merriam, Merrion, Merrow, merry, Perrier, Perry, perry, seropositive, sherry, skerry, steril{e|ize}, terrier, terribl{e|y}, terror, terrif{y|ied}, Terry, verisimilitude, very, wherry \n\nHere’s a more complete list from the Britphone dictionary, based on the search pattern “ˈɛ ɹ” with a few additions from The Routledge Dictionary of Pronunciation for Current English, based on the search pattern “ɛr|”.\n\nAmerica{s|n|ns}, atmospheric, beret, burial, cerebral, ceremony, chemotherapy, cherish, Cheryl, clerical, Derek, deterren{t|ce}, equerry, Eri{c|k}, Ericsson, experiment(s), generic, Gerald, Herald, Hereford, Herefordshire, heritage, heroin, heron, inherent, inheritance, Jeremy, knobkerrie, merrily, necessarily, numeric(al), peril, perish, primarily, prosperity, referral(s), serif, severity, sheriff, stereo, stereotype, terrace, territor{y|ies}, terrorism, terrorist(s), therapist, therapy, verif{y|ied}"
  },
  {
    "objectID": "blog/mary-merry-marry/index.html#marry",
    "href": "blog/mary-merry-marry/index.html#marry",
    "title": "A big list of Mary-merry-marry words",
    "section": "marry",
    "text": "marry\nThe second of the three, marry, is actually the trap lexical set. John Wells points out that the when an a is followed by two r’s, it’s a decent indicator of marry.\nHere are some words that were verified by some folks who do not have the merger.\n\narable, arid, apparent, Aragon, Areopagus, arid, arrant, arrow, baritone, baron, barren, barricade, barrow, Barry, Caradon, caravan(s), caret, carob, carol, Carol{e|ine|yn}, carr{y|s|ied|ing|ier}, charabanc, chariot, charity, charitable, Clarence, clarify, clarion, clarity, comparison(s), circularity, Darrell, Darren, Darrow, Faraday, Farrell, farrier, farrow, Garamond, Gareth, Gary, guarantee, harakiri, harried, harrow, Harry, Iscariot, Jared, larrikin, Karen, Larry, larynx, marabou, Marazion, Marian, Marilyn, Marion, marital, maritime, marronage, marrow, marr{y|ied|s|ing}, narrative, narrow(ly), para(-bolic, -chute, -graph, -llel, -noid, -normal, -lyze* etc), *parr{y|ied}, parody, parrot, Raritan, saraband, Saracen, scarify, Sharon, taradiddle, tarry, varicose, yarrow, Zara\n\nAnd here’s a longer list from the Britphone dictionary, based on the search pattern “ˈæ ɹ” with a few additions from The Routledge Dictionary of Pronunciation for Current English, based on the search pattern “ar|”.\n\napparel, Arab, Arabic, barracks, barrel, barrier(s), caribou, carriage, Carrie, carrier(s), Carroll, carrot(s), character{s|ize}, charit{y|ies|able}, comparative(ly), Daryl, disparage, embarrass{ed|ing|ment}, garage(s), gharry, glengarry, harass(ment), Haringey, Harold, Harriet, Harris, Harrison, Harrogate, karri, Larry, marathon, marriage(s), Marriott, Maryland, Marylebone, miscarriage(s), paradigm, paradise, paradox, Paraguay, Paris, parish, popularity, similarit{y|ies}, solidarity, tariff, tarot, transparen{t|cy}"
  },
  {
    "objectID": "blog/mary-merry-marry/index.html#mary",
    "href": "blog/mary-merry-marry/index.html#mary",
    "title": "A big list of Mary-merry-marry words",
    "section": "Mary",
    "text": "Mary\nFinally, there’s mary, which is actually the square lexical set. John Wells points out that the when an a is followed by just one r, it’s a decent indicator of mary (though not always).\nHere are some words that were verified by some folks who do not have the merger.\n\nAaron, aerate, aerial, airing, area, baring, bear{ing|er}, Bering, carer(s), Carey, caring, chairing, Charing, Clary, dare{e|ing}, dairy, fairy, far{e|ing}, flare-up, flaring, glaring, (nom de) guerre, hairy, hare, haring, lair, lairy, mare, Mary, mayoral, Nair, nary, pair(ing), Pharaoh, precarious, rare, raring, Sara(h), scar{y|ing}, shar{er|ing}, sparing, squaring, staring, swearing, tear(ing), Vair, variable(s), varian{t|ce}, var{y|ying|ied|ies}, wary\n\nAnd here are some other ones based on the Britphone dictionary, based on the search pattern “ˈɛə ɹ”.\n\naerosol, aerospace, air{y|ing}, apparent(ly), aquarium, Averham, Barham, Bulgaria(n), canary, comparing, contrary, eire, Faeroes, hilarious, humanitarian, Hungarian, invariably, librarian, malaria, Ontario, parent{s|ing}, pharaoh(s), prairie, preparing, secretariat, sierra, variegated, various, vegetarian, wearing, whereabouts\n\n\nWord-final and pre-consonantal tokens\nIt occured to me that the search patterns that I used only included words that, in UK English, are pronounced with /ɹ/ (i.e. the /ɹ/ is intervocalic). So you’ll notice that caring is on the list of Mary words but care is not. So I searched for tokens in Britphone with “ˈɛə” that are not followed by an /ɹ/ (so, preconsonantal and word-final) to produce the following list.\nThis list includes words that I thought were from different lexical sets. For example, care and hair are on this list, which are presumably part of Mary since caring and hairy are too, but then the word square is also in this list, which is, by definition, part of merry, since merry is just square. But, when I asked for clarification on Twitter, I was told that these are all square—and therefore Mary.\n\n\nOkay, so what about \"pair\", \"pear\", and \"pare\"? Is that a minimal triplet for the non-MMM-mergers out there? Britphone doesn't differentiate the classes word-finally, so \"care\" and \"square\" are both ˈɛə even though I'm pretty sure they're respectively DRESS and SQAURE. https://t.co/3esqw2l4FO\n\n— Joey Stanley (@joey_stan) September 22, 2020\n\n\nAnyway, so here’s the fuller list:\n\naffair(s), air, aircraft, airfare, airline(s), airlines, airplane, airport(s), airways, aware, awareness, Ayrshire, bare(ly), bear(s), beware, Blair, blare, care{s|d}, careful(ly), chair(s), Clair(e), clare, compare(d), dare, declare(d), despair, downstairs, fair(ly), fare(s), flair, flare, glare, hair{s|ed}ed), haircut, hairdresser(s), hare, heir, heirloom, hilaire, impaired, lair, mare, mayor, pair(s), pare, pear, Pierre, prayer(s), prepare(d), questionnaire, rare(ly), repair(s), scarce(ly), scare(d), share{s|d}, shareholder(s), shareware, snare, spare, square{s|d}, stair(s), staircase, stare(d), swear, tear(s), their(s), there, therefore, unaware, unfair, upstairs, ware, warehouse, wear(s), where, wherewhithal\n\n\n\nSupplemental Mary words\nFinally, when I searched the The Routledge Dictionary of Pronunciation for Current English, I found these 695 words containing the search pattern “ɛ:r”. I didn’t bother incorporating them into the above list because many of them are very infrequent words. But I wanted to include them for the sake of completeness.\n\nAaron, abecedarian, Abertillery, actuarial, actuarially, adversarial, aerial, aerialist, aeriality, aerially, aeriated, aerie, aeriform, aero, aerobatic, aerobe, aerobiologist, aerobiology, aerodrome, aerodynamic, aerodynamically, aerodynamicist, aerodyne, aero-engine, Aeroflot, aerofoil, aerogram, aerogramme, aerolite, aerological, aeromagnetic, aeronaut, aeronautic, aeronautical, aeronautically, aeroplane, aerosol, aerospace, aerostat, aerostatic, aerostatically, aerotow, aerotrain, aery, affair, agrarian, Ahasuerus, airer, airily, airiness, airing, airy, Althusserean, Althusserian, Antares, antimalarial, antiquarian, antiquarianism, antisabbatarian, antitrinitarian, aorist, apiarian, Apollinaris, aquaria, Aquarian, aquarium, Aquarius, araucaria, area, areal, areaway, areometer, Ares, Arian, Arianism, ariel, Aries, Arius, armamentaria, armamentarium, armillaria, aroid, arum, Aryan, authoritarian, authoritarianism, Azeri, Balearic, ballbearing, Ballesteros, barbarian, baric, barite, barium, Bavaria, Bavarian, bearability, bearable, bearably, bearer, bearing, bearish, bearishness, Behrens, Behring, Belisarius, Berengaria, Bering, billionairess, bolero, Buenos Aires, Bulgaria, Bulgarian, burglarious, burglariously, bursarial, caballero, caesarean, caesarian, calcareous, calcareousness, calceolaria, caldaria, caldarium, caldera, Canaries, canary, Cancerian, carabiniere, carabinieri, carer, Carew, Carey, Caria, caries, caring, carious, Carpentaria, Carreras, Cary, cassowary, centenarian, cercaria, cercariae, certiorari, cesarean, cesarian, charily, chariness, Charing Cross, Charon, chary, cheeseparing, childbearing, ciguatera, cineraria, cinerarium, clairaudience, clairaudient, Clara, clary, columbaria, columbarium, commissarial, commissariat, communitarian, condottiere, condottieri, contrarily, contrariness, contrariwise, contrary, cordillera, costmary, covariance, cruzeiro, cupbearer, daguerreotype, daguerrotype, dairy, dairying, dairymaid, dairyman, dairymen, darer, Dari, Darien, daring, daringly, Darius, de-aerate, declarable, declarant, declaredly, declarer, Demerara, denarii, denarius, despairingly, de Valera, dinero, disciplinarian, doctrinairism, doctrinarian, dolphinarium, Dun Laoghaire, egalitarian, egalitarianism, √âire, equalitarian, equalitarianism, establishmentarian, establishmentarianism, Europarliamentarian, eyrie, eyry, faerie, Faeroe Islands, Faeroes, Faeroese, faery, fairing, fairish, fairy, fairyland, Fareham, faro, Faro, Faroe, Faroese, filaria, filariae, filarial, filariasis, Fleet Air Arm, forastero, forbearance, forbearingly, frigidaria, frigidarium, fruitarian, fusaria, fusarium, futilitarian, garish, garishly, garishness, gharial, Gibraltarian, glaireous, glairiness, glairy, glaringly, glaringness, glary, glossarial, godparent, grammarian, Gran Canaria, grandparent, gregarious, gregariously, gregariousness, Guarneri, Guarnerius, Guerrero, Guti√©rrez, habanera, hairily, hairiness, hairy, Halmahera, Hanoverian, hardwearing, harem, harum-scarum, heiress, herbaria, herbarium, Herero, Herrera, hilarious, hilariously, hilariousness, honoraria, honorarium, houseparent, humanitarian, humanitarianism, Hungarian, Indo-Aryan, inegalitarian, infralapsarian, insectaria, insectarium, invariability, invariable, invariableness, invariably, invariance, invariant, Inveraray, Karaite, lairage, lairy, lares, latitudinarian, latitudinarianism, Lehrer, leprosaria, leprosarium, libertarian, libertarianism, librarian, librarianship, Lilliburlero, llanero, Lothario, lumpenproletariat, lupus vulgaris, mace-bearer, Mainwaring, majoritarian, malaria, malarial, malarian, malarious, Marian, mariolatry, Mariology, Marist, Mary, Mary Celeste, Maryland, Mary Magdalene, Maryport, Maseru, mayoral, mayoralty, mayoress, miliaria, militaria, millenarian, millenarianism, millenarianist, millionairess, miserere, multifarious, multifariously, multifariousness, multivariate, nareal, nares, narial, nary, necessarian, necessarianism, necessitarian, necessitarianism, nectarean, nectareous, nefarious, nefariously, Nehru, nightmarish, nightmarishness, nonagenarian, notarial, notarially, Nyerere, obituarial, oceanaria, oceanarium, octogenarian, octonarian, octonarii, octonarius, Old Sarum, O’Meara, omnifarious, Ontario, ovarian, ovariectomy, ovariotomy, pairing, pallbearer, pampero, pareira, parent, parentage, parenthood, parentless, parer, Parian, paring, parliamentarian, Perak, pereira, Pharaoh, Pharaonic, Pharoah, pharos, Pierian, Pinero, planarian, planetaria, planetarium, platitudinarian, plein-airist, potrero, prairie, precarious, precariously, precariousness, predestinarian, prelapsarian, preparedness, preparer, primavera, proletarian, proletarianisation, proletarianise, proletarianism, proletarianization, proletarianize, proletariat, pulmonaria, quadragenarian, quinquagenarian, quodlibetarian, radiolaria, radiolarian, ranchero, rara avis, raree-show, rarefaction, rarefactive, rarefication, rarefy, rarify, raring, rarity, Rarotonga, Rarotongan, Rastafarian, Rastafarianism, repairable, repairer, retiarii, retiarius, riparian, Ripuarian, Rivera, Riviera, Romero, rosaria, rosarian, rosarium, Rotarian, sabbatarian, sabbatarianism, sacramentarian, sacraria, sacrarium, Sagittarian, Sagittarius, salariat, Salieri, Samaria, samarium, sanataria, sanatarium, sanitaria, sanitarian, sanitarium, Sara, Sarah, sarify, Saros, Sarum, Sauveterrian, scarer, scarification, scarificator, scarifier, scarify, scarily, scariness, scarious, scaroid, scarus, scary, scenario, seafarer, seafaring, secretarial, secretariat, sectarian, sectarianise, sectarianism, sectarianize, sederunt, seminarian, senarii, senarius, septenarii, septenarius, septuagenarian, sexagenarian, shareable, share-out, sharer, Sharon, sharon fruit, snarer, solaria, solarium, sombrero, sparer, sparerib, sparing, sparingly, sparingness, square-eyed, squarer, Squarial, squarish, starer, step-parent, Stradivarius, sublapsarian, sub-librarian, sudaria, sudarium, Sumerian, supralapsarian, swearer, swordbearer, talaria, talebearer, talebearing, tearable, tearaway, tearer, tear-off, temerarious, tepidaria, tepidarium, termitaria, termitarium, terraria, terrarium, therabouts, thereabout, thereafter, thereanent, thereat, therein, thereinafter, thereinbefore, thereinto, thereof, thereon, thereout, thereunder, thereunto, thereupon, time-sharing, Tipperary, Tocharian, Tokharian, topiarian, torch-bearer, torero, totalitarian, totalitarianism, tractarian, Tractarian, Tractarianism, transparence, transparency, transparent, transparently, transparentness, Trinitarian, Trinitarianism, Trocadero, turbellarian, ubiquitarian, ubiquitarianism, unbearable, unbearableness, unbearably, unbury, uncaring, uncaringly, uniformitarian, uniformitarianism, Unitarian, Unitarianism, unpreparedly, unpreparedness, unrepairable, unsectarian, unsparing, unsparingly, unsparingness, untearable, unvaried, unvarying, unvaryingly, unvaryingness, unwarily, unwariness, unwary, unwearable, urticaria, utilitarian, utilitarianism, vagarious, Valera, valerian, valetudinarian, valetudinarianism, vaquero, varia, variability, variable, variably, variance, variant, variate, variation, variational, variationally, variationist, varices, varicolored, varicoloured, varied, variedly, variegate, variegation, varifocal, variform, variolate, variole, variolite, variolitic, varioloid, variolous, variometer, variorum, various, variously, variousness, varix, varus, vary, varyingly, vegetarian, vegetarianism, veterinarian, vicarial, vicariate, vicarious, vicariously, vicariousness, vivaria, vivarium, vulgarian, Wareham, Wareing, warily, wariness, Waring, wary, wayfarer, wayfaring, wearability, wearable, wear-and-tear, wearer, wearing, wearingly, welfarism, welfarist, whereabouts, whereabouts, where’er, whereupon, worksharing"
  },
  {
    "objectID": "blog/mary-merry-marry/index.html#conclusion",
    "href": "blog/mary-merry-marry/index.html#conclusion",
    "title": "A big list of Mary-merry-marry words",
    "section": "Conclusion",
    "text": "Conclusion\nI hope this list is useful to other folks who have the merger but need to distinguish them for whatever reason. I’m starting to remember which class words belong to a little bit, though this is just learned knowledge rather than intuition. Also, if anyone has different intuitions than what this page shows, please let me know!"
  },
  {
    "objectID": "blog/brand-yourself/index.html",
    "href": "blog/brand-yourself/index.html",
    "title": "Brand Yourself",
    "section": "",
    "text": "Note\n\n\n\nGo here to see a more recent version of this talk.\nToday Emily McGinn of the Digital Humanities Lab at UGA and I did a workshop called “Brand Yourself: A professionalization workshop for grad students” [Edit: and again by invitation April 13, 2017]. We gave a presentation on different ways grad students can boost their online presence through building a personal webpage, utilizing social media, and finding your field’s conversation. We then let the attendees a chance to work on their own to create a new online profile, using what they learned."
  },
  {
    "objectID": "blog/brand-yourself/index.html#social-media",
    "href": "blog/brand-yourself/index.html#social-media",
    "title": "Brand Yourself",
    "section": "Social Media",
    "text": "Social Media\nAcademia.edu is a social networking site for academics. Users can create profiles, upload their papers, and follow particular research topics. They can also follow others that have done the same. It’s a great resource for finding papers that may be behind a paywall, although it has gotten a lot of criticism for this. Papers you upload can be found by Google Scholar, which is a nice perk. The website will keep track of your analytics, and there’s nothing more thrilling than getting an email saying someone has found your profile!\n\n\n\n\n\nThe site got some criticism for offering authors the chance to promote their work for a fee. There’s also a chance at any time the site could get shut down because publishers aren’t happy about it, but with 30 million users, I don’t know if that’s going to happen any time soon.\nI’m less familiar with ResearchGate, but in my cursory look, there’s a lot of overlap with academia.edu as far as its features. A big difference I noticed is that it seems like it’s more focused on creating networks based on people you cite and your co-authors while academia.edu is more focused on following your field and your interests. One thing I don’t like about ResearchGate is that the number of emails it sends you is borderline spam. It invites me to follow other grad students at my university, but, no offense to the sciences, I’m not particularly concerned with what a microbiologist on the other side of campus is doing.\nI would imagine most researchers use Google Scholar regularly, but did you know you can create a profile for others to see? You can tell a researcher has done that when you see their name underlined in a search:\n\n\n\n\n\nIn this screenshot (live link here), you can see that Walt Wolfram, Natalie Schilling, Sali Tagliamonte have created their profiles, but Shanna Poplack and Penny Eckert have not. I’d like to see what else the last two researchers have written, but I can’t simply click on their names like I can with the first three. When you do click on their links, you can see the full profile including what else they have written and how many times each has been cited.\n\n\n\n\n\nIt does take a bit of work to get a full profile going, because Google’s data can be a bit messy, so you’ll have to add stuff in by hand. But I think the payoff is worth the effort.\nThere are a handful of other websites out there that can help you build an online presence. Impact Story is one that can keep track of how much of an impact you have on people by keeping track of when people cite, mention, read you and your work. For $10 a month, it might not be worth it for a grad student, but for a professor applying for tenure this might be.\nLinkedIn is one I should mention, but I don’t find it terribly useful for academics. It might be worth it to set up a low-maintenance page that gives a good view of you in a nutshell, just in case people look."
  },
  {
    "objectID": "blog/brand-yourself/index.html#building-a-personal-webpage",
    "href": "blog/brand-yourself/index.html#building-a-personal-webpage",
    "title": "Brand Yourself",
    "section": "Building a Personal Webpage",
    "text": "Building a Personal Webpage\nKeeping track of all these profiles can be tedious. Do you need to update seven different profiles every time you present at a conference? Is it worth it to invest the time in these sites that don’t communicate with each other? One solution is to keep your top (or only!) three or four papers on the social media sites, but include links to a central page that has your full profile. For this reason, it’s nice to have a personal webpage.\nThe problem with personal webpages it that they come with a cost, either in money or skills (and sometimes both). You can set up a webpage through Word Press, Wix.com, or Square Space, which take little technical skill to get a professional page set up. These can be free, but you can get some extra features for $10 a month or more. To me, that’s a pretty penny to pay for a relatively simple webpage.\nAnother option, which is what I did for the previous version of this website, is to host the page on Github. It’s free, but it takes a bit of skill. I’ve had to learn to use Jekyll, Markdown, and CSS, but through some help on ProgrammingHistorian.com and Lynda.com, I was able to get this site up. The benefit of going this route is I have unlimited flexibility in how the site looks, and I really, really like that.\nEither way, it’s probably worth it to set up a personal domain name. For as little as $1 a month, you can buy your own domain name (like www.joeystanley.com), which looks much more professional than www.blogsplot.com/joeystanley or www.github.com/joeystanley."
  },
  {
    "objectID": "blog/brand-yourself/index.html#finding-your-conversation",
    "href": "blog/brand-yourself/index.html#finding-your-conversation",
    "title": "Brand Yourself",
    "section": "Finding Your Conversation",
    "text": "Finding Your Conversation\nThe last thing we talked about in our workshop is to find where the big names in your field are having their online conversations. This sounds a little weird at first, but every field has some secret space where people are collaborating and sharing ideas informally as well as posting calls for papers, invitations for publications, and job openings. The problem is that where is space is is different for every field.\nIn some fields, these are a listserv. As far as I know, network analysis and Slavic languages each have a well-known listserv where all the conversation happens. If you’re not on that listserv, you’re out of the loop. Digital Humanities has a space on Slack where over 800 researchers get together and talk. For some fields, it might just be at coffee breaks during certain conferences. You may have to ask around established academics in your field to find that space.\nOne thing I will mention is that a lot of action happens on Twitter. I’ve covered this in more depth in an earlier blog post, but basically a lot of good stuff can come out of following the right people and seeing just the right tweets."
  },
  {
    "objectID": "blog/brand-yourself/index.html#conclusion",
    "href": "blog/brand-yourself/index.html#conclusion",
    "title": "Brand Yourself",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, I thought the workshop went very well. Most of the attendees did end up setting up some sort of profile: some did an academia.edu profile, some google scholar, and a few were ambitious and set up a github page. At the very least, I got this very webpage set up as a result of preparing for this workshop, and I learned a lot about all these other pages. It was a great feeling to see a dozen students directly benefiting from our presentation.\n\nYou can download the old version slideshow we used for this presentation here.\nI am indebted to the Impact Challenge blog series, with the accompanying 200+ page pdf, from which I learned a lot about all this. I would highly recommend that you download it and take a look. Not only does it include much more than what I’ve mentioned here, including step-by-step how-to guides to getting these profiles set up, but also many more topics to get yourself more visible. Thanks, Impact Challenge."
  },
  {
    "objectID": "blog/brother-joseph/index.html",
    "href": "blog/brother-joseph/index.html",
    "title": "Brother Joseph",
    "section": "",
    "text": "I had the fun opportunity to be a guest in a podcast today! Faith Promoting Rumors is a new podcast that my brother and dad started that explores Mormon myths and culture. Having published on an interesting linguistic quirk about Mormon culture—the alternation between calling someone either as “Brother Jones” or as “Bob”—I was asked to talk about my research and about this convention in Mormon culture generally."
  },
  {
    "objectID": "blog/brother-joseph/index.html#background",
    "href": "blog/brother-joseph/index.html#background",
    "title": "Brother Joseph",
    "section": "Background",
    "text": "Background\nThere is a robust practice of using titles among Mormons. Kids and teenagers are expected to refer to all adults using the appropriate title (usually Brother or Sister, though in some cases Elder, Bishop, or President—see below), plus their last name. Adults reciprocate by calling minors by their first name, establishing a clear superiority between kids and adults, though this can be flouted for comedic, sarcastic, or reverential effects.\nBut between adults the rules are less straightforward. Sometimes adults use first name for each other and other times they use titles. Familiarity is the strongest factor but age and status within the congregation play a role as well. I had a hunch that things like situation, audience, family make up, and place in the social network had to do with it too. This is essentially the same as how some languages have their T-V distinction in pronouns. This is a classic sociolinguistic variable since it doesn’t appear to be a conscious decision by speakers and it’s an extremely common linguistic feature.\nSo what started off as a mild curiosity during Sunday meetings ended up being a term paper in both my sociolinguistics and social network analysis courses, two conference presentations, a spot in the 2016 Penn Working Papers in Linguistics, and my first qualifying paper—I got a lot of mileage out of that study. I was just getting into quantitative methods, so it’s a little statistics-heavy, but I did find some interesting results."
  },
  {
    "objectID": "blog/brother-joseph/index.html#summary-of-the-study",
    "href": "blog/brother-joseph/index.html#summary-of-the-study",
    "title": "Brother Joseph",
    "section": "Summary of the Study",
    "text": "Summary of the Study\nEssentially, what I did to answer this question was give a survey out to members of my own congregation and ask them to indicate what they would call other members of the congregation in four different situations. I also asked them to tell me how well they knew each person.\nAs I mention in the podcast, some of the main findings are pretty intuitive. The better someone knows another person, the more likely they are to use their first name. There was more first name among people of the same sex, especially women. Holding all other variables constant, more peripheral members of the social network of the congregation generally use more titles and get called by titles more than the core members. There were some differences between the sexes and the situation: men use more titles for present company while women use more first name in face-to-face situations.\nOne thing I tested was whether southerners use more titles. It’s a pretty common stereotype that southerners are more polite and call people “Miss Betty” more often. It turns out this carries over into Mormon circles as well: southerners generally used titles slightly more than northerners or Utahns.\nSurprisingly, age wasn’t a factor. I thought there would be a clear effect of increased usage of titles for older people, but this didn’t pan out. What did appear to be the case was that people roughly within a couple years of each other use more titles for each other, but this was more an effect of familiarity than anything else: people are friends with others their age so they use fewer titles.\nOne of the more interesting findings was that people who have children were called by their titles more than those that didn’t. I’ve heard anecdotes where unmarried people get titles far less often than married people their same age, and it seems like having kids moves a person up another step in the “adult” category. One of my conclusions was that it seems like a Mormon is truly considered an adult until they are married and have children. Interestingly, the number of kids didn’t matter, just whether someone had kids. This is explainable by the family-centered religion and culture that Mormons are a part of, and it seems to made manifest in how people address each other—at least in my Georgia congregation."
  },
  {
    "objectID": "blog/brother-joseph/index.html#titles-first-name",
    "href": "blog/brother-joseph/index.html#titles-first-name",
    "title": "Brother Joseph",
    "section": "Titles + First Name?",
    "text": "Titles + First Name?\nThe title of the episode, “Brother Joseph”, alludes to the practice that early church leaders had in calling people by a title and their first name. I don’t know exactly when the change from first to last name happened, but it appears to be sometime in the mid-to-late 1800s. It also might be that certain individuals had this special use of the title: Brother Brigham [Young], Brother Joseph [Smith], Brother John [Taylor], Brother George [Cannon], and Sister Eliza [Snow] were some of the top hits in the mid 1800s. Though Brother [John] Taylor and Brother [George] Cannon were also common in the corpus I looked through. There are a lot of unknowns about forms of address in the early days of Mormonism, but we try to look into it a little bit."
  },
  {
    "objectID": "blog/brother-joseph/index.html#other-titles",
    "href": "blog/brother-joseph/index.html#other-titles",
    "title": "Brother Joseph",
    "section": "Other Titles",
    "text": "Other Titles\nAs we mention in the podcast, there are other titles too that you might hear occasionally. Elder is reserved for male full-time missionaries, whether they be the young guys you might see on the streets or for men in global leadership positions. Women who serve missions retain their generic title of Sister. What’s interesting about the women though is that in some languages this is a unique title: in Portuguese for example the title is Síster rather than the generic Irmã (‘sister’) that other women have.\nPresident is for leaders of specifically organized groups of men. This can apply to the president of the church, the leader of the greater local area (what we call “stakes”), or the group of 14–15-year old boys. This is a case when calling a 15-year-old boy “President Jones” is attested in certain circumstances. This title can also be used for the president’s assistants or “counselors”, though this applies more to the larger groups and is much less common at the local level.\nBishop is for the presiding authority of a congregation. This title, as well as those for full-time missionaries, have a unique position syntactically: they can stand on their own. In other words, it’s perfectly acceptable to approach to missionaries and say “Hey, Sisters!” or to approach a bishop and say “Hey, Bishop” instead of “Hey, Bishop Jones.” President can be used sometimes in this way though it’s less common."
  },
  {
    "objectID": "blog/brother-joseph/index.html#conclusions",
    "href": "blog/brother-joseph/index.html#conclusions",
    "title": "Brother Joseph",
    "section": "Conclusions",
    "text": "Conclusions\nThis is an interesting part of Mormonism, and in the podcast we discuss some of the cultural implications of it. Linguistically though I still think there’s a lot more to be said and I’d be curious to see other research on this."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html",
    "href": "blog/testing-vot-durations/index.html",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "",
    "text": "So I’m teaching phonetics and phonology this semester and we’re using Ladefoged & Johnson’s A Course in Phonetics textbook. As I was preparing to teach about stops, I thought it might be a good idea as a homework assignment for students to gather their own data to see if some of these ideas panned out. Here’s my quick study."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html#hypotheses",
    "href": "blog/testing-vot-durations/index.html#hypotheses",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "Hypotheses",
    "text": "Hypotheses\nThe four hypotheses I wanted to test come from Chapter 3 from my 6th edition of A Course in Phonetics:\n\nWord-initially, /p, t, k/ have longer aspiration than /b, d, g/.\nAfter onset /s/, /p, t, k/ have about as much aspiration as word-initial /b, d, g/.\nWord-finally, voiced obstruents have an overall longer duration (closure + burst + aspiration) than voiceless obstruents.\nVowels preceding voiced obstruents are longer than those preceded by voiceless obstruents."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html#methods",
    "href": "blog/testing-vot-durations/index.html#methods",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "Methods",
    "text": "Methods\nEach student was asked to record a friend reading the following words: tack, soap, days, pad, steep, sit, code, tab, bees, scope, dice, goes, bus, seep, cab, spit, gas, peg. I chose these words to maximize onset and coda obstruents in as few words as possible. Vowel quality is assumed to have no effect.\nFor stops, student measured durations of aspiration and closure; for fricatives it was duration of the fricative itself. This was done in Praat. They have worked with Praat once before in this course, and were taught how to identify boundaries for these, but were otherwise relatively untrained linguistics undergraduates. I provided them with a template spreadsheet to fill out.\nI ended up with measurements from 432 words: 18 unique words each from 24 students. I then combined the spreadsheets and wrote up the R code."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html#results",
    "href": "blog/testing-vot-durations/index.html#results",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "Results",
    "text": "Results\n\nWord-initial stop aspiration\nThe first hypothesis is one that is commonly taught in intro to linguistics courses: word-initial voiceless stops have aspiration and word-initial voiced stops have little, if any. In my data, this turned out to be the case based on the words pad, peg, tab, tack, cab, code, bees, bus, days, dice, gas, and goes.\n\nA mixed-effects regression model that predicts this aspiration with the underlying voicing of the stop as a fixed effect and student and word as random effects suggests that this difference is significant.\nsummary(lmer(aspiration ~ underlying_voicing + (1|student) + (1|word), data=wi_stops))\n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n student  (Intercept) 1.039e-03 0.03224 \n word     (Intercept) 9.550e-06 0.00309 \n Residual             1.084e-03 0.03293 \nNumber of obs: 285, groups:  student, 24; word, 12\n\nFixed effects:\n                            Estimate Std. Error t value\n(Intercept)                 0.047309   0.007241   6.534\nunderlying_voicingvoiceless 0.037331   0.004291   8.699\nConclusion: Yep. Based on this data, the model predicts that voiced stops get around 47ms of aspiration while voiceless stops get about 85ms. Cool.\n\n\nStops following /s/\nWe learn that voiceless stops following /s/ in the same syllable are not aspirated in English. In our sample, this also proved to be correct based on the same words as above with the addition of spit, steep, and scope.\n\nI ran a mixed-effects regression model like the one described above but with position (=environment) as the main effect and voiced word-initial stops as the reference level.\nsummary(lmer(aspiration ~ position + (1|student) + (1|word), data=onset_stops))\n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n student  (Intercept) 0.001153 0.03395 \n word     (Intercept) 0.000000 0.00000 \n Residual             0.001150 0.03391 \nNumber of obs: 355, groups:  student, 24; word, 15\n\nFixed effects:\n                                Estimate Std. Error t value\n(Intercept)                     0.047309   0.007485   6.321\npositionvoiceless word-initial  0.037395   0.004019   9.304\npositionfollowing /s/          -0.007779   0.004948  -1.572\nWe get the same coefficients for word-initial stops. Only this time, we can see how stops followed by /s/ fit in. The model shows that the duration of aspiration was not significantly different from word-initial voiced stops. There’s some indication that the stops following /s/ have even less aspiration, but this didn’t reach significance.\n\n\nDuration of word-final stops\nWhat I didn’t know before preparing for this class was that the overall duration of word final stops (closure + burst + aspiration) is longer for voiceless obstruents than it is for voiced obstruents. Based on all 18 words (scope, seep, soap, steep, sit, spit, tack, cab, tab, code, pad, peg, bus, dice, gas, bees, days, goes), this was true.\n\nThe difference is small but significant: in a mixed-effects regression model that predicts duration with voicing and manner of articulation as fixed effects and student and word as random effects, the difference reached statistical significance.\nsummary(lmer(duration ~ underlying_voicing + manner + (1|student) + (1|word), data=wf))\n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n student  (Intercept) 3.002e-03 0.054790\n word     (Intercept) 3.745e-06 0.001935\n Residual             7.550e-03 0.086893\nNumber of obs: 427, groups:  student, 24; word, 18\n\nFixed effects:\n                          Estimate Std. Error t value\n(Intercept)               0.194404   0.012836  15.145\nunderlying_voicingvoiced -0.035321   0.008540  -4.136\nmannerfricative           0.096638   0.008976  10.766\n\nThis model shows that the voiceless stops were about 194ms long, and voiced stops were 159ms (35ms shorter). Furthermore, and I didn’t expect this until I saw the plot, fricatives were generally about 97ms longer than stops. I don’t do a lot of nitty-gritty phonetics work like this too often, especially on consonants, so this was news to me.\n\n\nVowel length\nFinally, it is pretty well known that vowels before voiced obstruents are said to be longer than vowels before voiceless obstruents. As expected, the data showed this to be the case.\n\nOne last regression model, similar to what was done previously, showed not only that this voicing difference is significant but that the manner of articulation mattered as well.\nsummary(lmer(vowel ~ underlying_voicing + manner + (1|student) + (1|word), data=wf))\n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n student  (Intercept) 0.001937 0.04401 \n word     (Intercept) 0.001114 0.03337 \n Residual             0.002274 0.04769 \nNumber of obs: 431, groups:  student, 24; word, 18\n\nFixed effects:\n                         Estimate Std. Error t value\n(Intercept)               0.18905    0.01513  12.495\nunderlying_voicingvoiced  0.09595    0.01654   5.799\nmannerfricative           0.05349    0.01744   3.067\nHere, we see that vowels followed by voiceless stops are predicted to have a duration of 189ms. If the following segment is voiced, the model predicts it to actually be 285ms (an increase of 95ms). What stands out is that vowels followed by /g/ were quite a bit shorter. Due to an oversight in my data, there was just one /g/-final word, peg, and it was listed last on the stimulus. No doubt this had an effect. In fact, a more rigorous study that would include more tokens and randomization might find the difference to be even greater, assuming /g/ falls more in like with /b/ and /d/.\nI learned something new here as well: fricatives lengthen the vowel even more. The model predicts that vowels preceding fricatives will be 53ms longer than stops with the same voicing. I didn’t know that."
  },
  {
    "objectID": "blog/testing-vot-durations/index.html#conclusion",
    "href": "blog/testing-vot-durations/index.html#conclusion",
    "title": "Testing VOT Durations in A Course in Phonetics",
    "section": "Conclusion",
    "text": "Conclusion\nThis study was short, unsystematic, and full of methodological issues. Despite its flaws, it shows evidence to support what Ladefoged and Johnson say in A Course in Phonetics. I also learned a few things: in addition to themselves being longer than stops, fricatives cause their preceding vowels to lengthen as well.\nSpecial thanks goes to my Fall 2017 LING 3060 class at the University of Georgia, who bothered their friends with this silly assignment and painstakingly measured durations in software they barely know how to use."
  },
  {
    "objectID": "blog/ar-raising/index.html",
    "href": "blog/ar-raising/index.html",
    "title": "/ɑr/-Raising",
    "section": "",
    "text": "I’ve noticed for a while in my own speech that the vowel in star is higher and longer than start. I have American Raising, which, simplifying a bit, is where /aɪ/ is raised before voiceless consonants. So I just expected this to be another manifestation of that. I had some time so I thought I’d test this empirically. So here’s a breif study on my own speech to figure out what’s going on.See Davis & Berkson 2021 for more detail on American Raising. I particularly liked Moreton’s chapter because it shows how stress, syllable structure, and morphologica structure all matter."
  },
  {
    "objectID": "blog/ar-raising/index.html#hypothesis",
    "href": "blog/ar-raising/index.html#hypothesis",
    "title": "/ɑr/-Raising",
    "section": "Hypothesis",
    "text": "Hypothesis\n/ɑr/ raises before voiceless segments."
  },
  {
    "objectID": "blog/ar-raising/index.html#methods",
    "href": "blog/ar-raising/index.html#methods",
    "title": "/ɑr/-Raising",
    "section": "Methods",
    "text": "Methods\nIn COCA’s list of the most frequent 5000 words, there are 88 that have the /ɑr/ sequence in stressed position. Here they are in order of frequency.\nlarge, car, art, party, heart, article, artist, argue, hard, card, bar, yard, garden, partner, sorry, argument, department, apartment, farm, tomorrow, start, army, farmer, largely, hardly, smart, sharp, regarding, dark, far, mark, marketing, parking, darkness, armed, chart, remarkable, market, garlic, partly, partnership, regardless, target, carbon, borrow, margin, architect, park, architecture, march, harsh, particle, guitar, guard, alarm, starter, carve, starting, departure, sharply, hardware, garbage, cart, barn, carpet, star, jar, shark, charter, charge, partially, harm, artifact, partial, marketplace, part, harmony, remark, regard, arm, apart, marble, charm, marker, spark, harvest, depart, cargo\nThe words were randomized and I recorded myself reading them using Praat in a quiet place with a good mic. I segmented out the /ar/ sequence by hand (incidentally, I sounded like a seal: “ar ar ar ar”). Using a Praat script, I extracted formants at 30% of the way into the /ar/ sequence (what seemed to be about the midpoint of the vowel). Filtered out a few bad measurements. Analyzed in R. For this analysis, word-final /ar/ is being treated as pre-voiced."
  },
  {
    "objectID": "blog/ar-raising/index.html#results",
    "href": "blog/ar-raising/index.html#results",
    "title": "/ɑr/-Raising",
    "section": "Results",
    "text": "Results\nI’ll split my analysis up into monosyllabic words and polysyllabic words because the results were cleaner.\n\nMonosyllabic words\nThe effect that place of articulation had on duration was clear. A simple boxplot shows the stark contrast.\n\nWith the exception of arm being short and mark being long, there’s a clear difference. In fact, the longest ones were mostly the word-final /ɑr/ words, but for this analysis I’ll group word-final and pre-voiced into one category.\nMoving on to vowel quality, it’s clear that the voicing of the following segment has an effect. In fact, there’s virtually no overlap between the two vowel classes.\n\nI ran a MANOVA test that took into account F1, F2, and duration, and the results show that the two groups are distinct.\nsummary(manova(cbind(F1, F2, dur) ~ fol_voice, data=ar))\n\n          Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nfol_voice  1 0.51552   28.375      3     80 1.348e-12 ***\nResiduals 82                                                                                        \n The p-value is small, which indicates the two are different, but I’ve noticed they tend to be on vowel data even for what seems like merged classes. More importantly, the Pillai score is pretty high: on a scale from 0 (=complete overlap) to 1 (completely separate), it’s about in the middle.Using the formula that me and Betsy Sneller came up with in our 2022 paper, we’d expect a Pillai score less than 0.0618 with this much data if the two classes were underlyingly merged. The fact that it’s so high with a sample size of 88 is pretty indicative of separation. In case the visual wasn’t clear enough.\nOut of curiosity, I did a k-means clustering analysis. I made it blind to phonetic environment so it finds the best two clusters based on F1 and F2 alone. As expected, the clusters essentially captured the voicing distinction, but there were a couple exceptions: march is voiced but clustered with the voiceless words, and both yard and guard are voiced but clustered with the voiceless words.\nConclusion so far: with the exception of a few words, it’s pretty clear that /ɑr/ is raised, fronted, and shorter before voiceless sounds.\n\n\nPolysyllabic words\nWhen there’s more than one syllable, things get complicated because of their distribution within the word. Most of the two-syllable words have word-initial stress, but there are a few misfits (alarm, depart, guitar, regard, remark). With three syllables stress is either on the first syllable (architect, argument, article, artifact, harmony, marketing, marketplace, partially, particle, partnership) or the second (apartment, department, departure, regarding, regardless, tomorrow). There were two four-syllable words, with the /ɑr/ segment on different syllables (architecture, remarkable). Most of these can be split up by the voicing of the following segment too, which spreads the data pretty thin.\n\nGenerally, the the raised variant occurs with voiceless segments, but this data is a little bit messier. What’s interesting is to look at the exceptions.\nThere were several pre-voiced segments that appeared to be raised: sorry, regardless, target (that one’s hard to see), regarding and maybe argue, argument, garden, and regard. I can’t help but notice that a lot of those words have the sequence /gɑr/. In fact, the only other /gɑr/ words were garlic and garbage, which were the most fronted voiced words even though they weren’t raised.\nIt’s interesting that guard was one of the few exceptions on the k-means clustering analysis on the monosyllabic words and was raised and fronted compared to the others. A part of me wants to think that /ɑr/ is raised after /g/, which would be cool.\nIt’s also worth noting that most of the voiceless words that were outside the ellipse were after bilabials: depart, market, remark, remarkable, etc. These are actually near the voiced segments after bilabials like borrow and tomorrow. This even explains why march was one of the exceptions detected by the k-means clustering analysis. Perhaps bilabials have something to do with backing."
  },
  {
    "objectID": "blog/ar-raising/index.html#discussion",
    "href": "blog/ar-raising/index.html#discussion",
    "title": "/ɑr/-Raising",
    "section": "Discussion",
    "text": "Discussion\nThere is a phonetic explanation for the behavior of both the velars and the bilabials. Let’s talk about transition formants for a second. To start, the low vowel /ɑ/ has a high F1 and a low F2. Velars cause F1 to lower and F2 to raise (the latter as a part of the velar pinch), which is more like a higher, fronter vowel. Meanwhile, bilabials cause all formants to lower (because of the slight lengthening of the vocal tract), meaning raising and backing.\nWhat’s happening is that some of these transition formants are raising the nucleus of the /gɑr/ sequence. The reason why we don’t see raising in /kɑr/ sequences such as car, carbon, cargo, carve, etc. is because of the extra padding of the aspiration. Transition formants do appear in aspiration and the fact that this VOT increases the time between the velar stop and the vowel gives my articulators time to transition to a full /ɑ/.\nI don’t know of a good formal way to test this right now, but we can look at measurements at other points along the vowels’ durations and see if they’re predicted by the place of articulation. Moving closer to the vowel onset, the /gɑr/ words are even higher and fronter. And in fact, at 10% into the /ɑr/ sequence, the distribution is more easily predicted by the previous place of articulation.\n\nHere, we see that the /gɑr/ sequences (highlighted in green now) are all very high and very front, together with /jɑr/ and /ʃɑr/ which have similar transition formant patterns. Similarly, the /ɑr/ words after bilabials (now in yellow) are somewhat backer, though this isn’t as stark. In fact the difference in voicing of the following stop is much smaller, and might not even be significant at all anymore.\nIf we measure later in time, the influence of the /r/ and its following consonant is increasing and the distinctions between the vowels also decreases.\n\nHere we see that at halfway through the /ɑr/ sequence, the /gɑr/ words are still among the higher and fronter within the blue cloud. But at 70%, they are pretty much randomly dispersed among the other pre-voiced segments. The bilabials make it clear that the following place of articulation predicts this vowel plot: words like mark, park, and spark are higher and fronter due to the velar pinch.\nGoing back to that 30% point, which is roughly the middle of the vowel itself, here it is again, but with the highlights.\n\nHere, the position of each word within the voiced/voiceless cloud is already being influenced by the surrounding consonants. Word that end in velars like mark, park, and spark are already fronting. Nearly all the /gɑr/ words are followed by an alveolar sound, which cause some raising and fronting but not as extreme as velars. So it makes sense that they would stay high but not as high as the pre-velars."
  },
  {
    "objectID": "blog/ar-raising/index.html#conclusion",
    "href": "blog/ar-raising/index.html#conclusion",
    "title": "/ɑr/-Raising",
    "section": "Conclusion",
    "text": "Conclusion\nIn my speech, I definitely have evidence to reject the null hypothesis and to conclude that /ar/ is raised before voiceless segments. I think that part is pretty clear.\nI’ll admit, I came into writing this blog in hopes of showing that my /gɑr/ sequence is also raised. But after doing more tests, visualizations, and especially after taking into account the pattern of the bilabials, I just don’t have enough evidence to suggest it’s anything more than influence of surrounding consonants.\nThis makes me think hard about how to interpret vowel plots in the future. Every combination of surrounding consonants has its own trajectory, and it’s crazy that we can find patterns anywhere. This means ideally we should take into account more consonantal influences when interpreting vowel data. But at the same time that often spreads out data out way too thin since there will be only so a few tokens for any combination.\nI’ve seen studies where they’ll put following place of articulation as a random effect in a mixed-effects model. In my opinion, that doesn’t seem methodologically sound because there’s a finite number of possible options, we generally know their effects, and any duplication study would mostly have the same places of articulation. Now, I’m reconsidering this, and I wonder if it might be a good idea to put a combination of previous and following consonant as a random factor (since they interact). While we probably know the effects of these interactions, any replication of the study might not have the same words and therefore might not have the exact same combinations of previous and following consonants. It’s quite common to go the extreme route and just put each word as a random effect since “every word has its own history.”\nIt’s also important to consider trajectory information more. As I showed above, just looking at one measurement can lead to erroneous claims, and we can get a more fuller picture by looking at the trajectory information. There are statistical methods out there that we can use for trajectories (SS-ANOVA, GAMMs, etc.) but I haven’t quite got the hang of them yet.\nWell, I learned a few things by doing this quick study: surrounding consonants affect vowels and trajectories are important. Seems like old news, but it’s nice to have learned this for myself."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Website Version 3\n\n\n\n\n\n\n\nMeta\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2023\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV50\n\n\n\n\n\n\n\nConferences\n\n\nMethods\n\n\nPhonetics\n\n\nPresentations\n\n\nResearch\n\n\nSimulations\n\n\nStatistics\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNew publication in the Penn Working Papers in Linguistics\n\n\n\n\n\n\n\nMethods\n\n\nResearch\n\n\nPublications\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nADS and LSA 2022\n\n\n\n\n\n\n\nAnimations\n\n\nConferences\n\n\nData Viz\n\n\nMTurk\n\n\nPhonetics\n\n\nPresentations\n\n\nR\n\n\nResearch\n\n\nSouth\n\n\nStudents\n\n\nUtah\n\n\nVowel Overlap\n\n\nWest\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2022\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nASA181\n\n\n\n\n\n\n\nConferences\n\n\nDissertation\n\n\nMethods\n\n\nPacific Northwest\n\n\nPhonetics\n\n\nPresentations\n\n\nR\n\n\nResearch\n\n\nSimulations\n\n\nStatistics\n\n\nVowel Overlap\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV49\n\n\n\n\n\n\n\nConferences\n\n\nMethods\n\n\nPresentations\n\n\nResearch\n\n\nSimulations\n\n\nSouth\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n365 Papers (Update)\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nKohler Tapes (Update)\n\n\n\n\n\n\n\nResearch\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nKohler Tapes\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n10 Years of Linguistics\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2021\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNew publication in the latest PADS volume\n\n\n\n\n\n\n\nPacific Northwest\n\n\nPublications\n\n\nResearch\n\n\nWest\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nA big list of Mary-merry-marry words\n\n\n\n\n\n\n\nLexical Sets\n\n\nSide Projects\n\n\nTeaching\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nI got a job at BYU!\n\n\n\n\n\n\n\nPersonal\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nFull house at my first LaTeX workshop!\n\n\n\n\n\n\n\nGithub\n\n\nLaTeX\n\n\nPresentations\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nUGA Linguistics Colloquium 2020\n\n\n\n\n\n\n\nAnimations\n\n\nDissertation\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLSA and ADS 2020\n\n\n\n\n\n\n\nAnimations\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nResearch\n\n\nSouth\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2020\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nDissertation\n\n\n\n\n\n\n\nDissertation\n\n\nPacific Northwest\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nWhy do people use BAT instead of TRAP?\n\n\n\n\n\n\n\nLexical Sets\n\n\nMethods\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLCUGA6\n\n\n\n\n\n\n\nAnimations\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nPresentations\n\n\nResearch\n\n\nSouth\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nThank You\n\n\n\n\n\n\n\nData Viz\n\n\nMeta\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nJealousy List 3\n\n\n\n\n\n\n\nJealousy Lists\n\n\nData Viz\n\n\nR\n\n\nSkills\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nDH 2019\n\n\n\n\n\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nPresentations\n\n\nResearch\n\n\nSouth\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nYou’re a Statistician, Harry!\n\n\n\n\n\n\n\nGIS\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nSimulating Chutes and Ladders\n\n\n\n\n\n\n\nAnimations\n\n\nGithub\n\n\nSide Projects\n\n\nSimulations\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLSA and ADS 2019\n\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2019\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV47\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nJealousy List 2\n\n\n\n\n\n\n\nJealousy Lists\n\n\nR\n\n\nSkills\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLCUGA5\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nPhonetics\n\n\nPresentations\n\n\nResearch\n\n\nSouth\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nBrand Yourself 2\n\n\n\n\n\n\n\nCSS\n\n\nGithub\n\n\nHow-to Guides\n\n\nMeta\n\n\nPresentations\n\n\nTwitter\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nJealousy List 1\n\n\n\n\n\n\n\nJealousy Lists\n\n\nR\n\n\nSkills\n\n\nStatistics\n\n\nGIS\n\n\nData Viz\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nTranscribing a Sociolinguistic Corpus\n\n\n\n\n\n\n\nDissertation\n\n\nMethods\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n365papers\n\n\n\n\n\n\n\nResearch\n\n\nSide Projects\n\n\nTwitter\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nADS2018\n\n\n\n\n\n\n\nConferences\n\n\nDissertation\n\n\nLinguistic Atlas\n\n\nPhonetics\n\n\nPresentations\n\n\nResearch\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2018\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nNWAV46\n\n\n\n\n\n\n\nConferences\n\n\nDissertation\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLCUGA4\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\nUtah\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\n/ɑr/-Raising\n\n\n\n\n\n\n\nSide Projects\n\n\nPhonetics\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nTesting VOT Durations in A Course in Phonetics\n\n\n\n\n\n\n\nSide Projects\n\n\nTeaching\n\n\nStatistics\n\n\nPhonetics\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nGeneral Update\n\n\n\n\n\n\n\nWest\n\n\nUtah\n\n\nMTurk\n\n\nResearch\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nPacific Northwest\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nUsing MTurk\n\n\n\n\n\n\n\nResearch\n\n\nWest\n\n\nPacific Northwest\n\n\nMTurk\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLaboratory Research\n\n\n\n\n\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nAdmission to Candidacy\n\n\n\n\n\n\n\nResearch\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLots of Transcribing\n\n\n\n\n\n\n\nPacific Northwest\n\n\nResearch\n\n\nDissertation\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nA Survey of the Western American English using MTurk\n\n\n\n\n\n\n\nPacific Northwest\n\n\nResearch\n\n\nWest\n\n\nMTurk\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nBrother Joseph\n\n\n\n\n\n\n\nSide Projects\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nMount St. Helens and Vowels\n\n\n\n\n\n\n\nPacific Northwest\n\n\nPresentations\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nSECOL 2017\n\n\n\n\n\n\n\nConferences\n\n\nLinguistic Atlas\n\n\nPresentations\n\n\nResearch\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nUpdated mvnorm.etest() function\n\n\n\n\n\n\n\nStatistics\n\n\nR\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nWebsite Version 2\n\n\n\n\n\n\n\nCSS\n\n\nSkills\n\n\nMeta\n\n\nGithub\n\n\nHow-to Guides\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nExcel Workshop\n\n\n\n\n\n\n\nHow-to Guides\n\n\nPresentations\n\n\nSkills\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nTweeting LSA2017\n\n\n\n\n\n\n\nConferences\n\n\nTwitter\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nLSA2017\n\n\n\n\n\n\n\nPacific Northwest\n\n\nConferences\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2017\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Guarani Dictionary\n\n\n\n\n\n\n\nCSS\n\n\nGuarani\n\n\nSide Projects\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nBrand Yourself\n\n\n\n\n\n\n\nCSS\n\n\nGithub\n\n\nHow-to Guides\n\n\nMeta\n\n\nPresentations\n\n\nTwitter\n\n\n\n\nToday Emily McGinn of the Digital Humanities Lab at UGA and I did a professionalization workshop for grad students. We gave a presentation on different ways grad students can boost their online presence through building a personal webpage, utilizing social media, and finding your field’s conversation. We then let the attendees a chance to work on their own to create a new online profile, using what they learned.\n\n\n\n\n\n\nNov 11, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nDiVar\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nHow I Implemented the Links in this Site\n\n\n\n\n\n\n\nCSS\n\n\nHow-to Guides\n\n\nMeta\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nThe Importance of Twitter\n\n\n\n\n\n\n\nHow-to Guides\n\n\nMethods\n\n\nResearch\n\n\nTwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nMaking a website is fun!\n\n\n\n\n\n\n\nCSS\n\n\nGithub\n\n\nHow-to Guides\n\n\nMeta\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nReviewer Feedback\n\n\n\n\n\n\n\nConferences\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nJMP\n\n\n\n\n\n\n\nData Viz\n\n\nPresentations\n\n\nSkills\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nThe Linguistic Atlas of the Pacific Northwest\n\n\n\n\n\n\n\nLinguistic Atlas\n\n\nPacific Northwest\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\n  \n\n\n\n\nADS Meeting!\n\n\n\n\n\n\n\nConferences\n\n\nPacific Northwest\n\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2016\n\n\nJoey Stanley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/colloquium-2020/index.html",
    "href": "blog/colloquium-2020/index.html",
    "title": "UGA Linguistics Colloquium 2020",
    "section": "",
    "text": "For the fourth time in six years, I presented some of my research at the UGA Linguistics Colloquium. I talked about some findings from my dissertation, though I focused on just the low vowels trap, lot, and thought."
  },
  {
    "objectID": "blog/colloquium-2020/index.html#these-are-relatively-old-changes",
    "href": "blog/colloquium-2020/index.html#these-are-relatively-old-changes",
    "title": "UGA Linguistics Colloquium 2020",
    "section": "These are relatively old changes",
    "text": "These are relatively old changes\nThe gist of the talk is that trap has been gradually lowering over the course of four generations in Cowlitz County Washington, with men consistently lagging behind the women and with the first half of the trajectory doing most of the lowering. Meanwhile, lot and thought have been in a near-merged state since at least the 1930s, with no apparent conditioning by sex or generation. Considering that Cowlitz County was settled by English speakers since only the 1850s, these are relatively old changes for this area.\nThe interesting part is that while the position of trap is consistently lower for the women, the shape of the trajectory is the same within a generation. That is, when it comes to the vowel’s dynamics, the men are keeping up with the women. It’s just that the global position of that vowel is less advanced.\nFor me, this opens up a lot of questions about vowel trajectories. I’m curious about what kinds of social conditioning can be found in the trajectory of a vowel, rather than its relative position in the F1-F2 space. In fact, I’ve got some experimental work in motion to answer just that…"
  },
  {
    "objectID": "blog/colloquium-2020/index.html#interpreting-difference-smooths",
    "href": "blog/colloquium-2020/index.html#interpreting-difference-smooths",
    "title": "UGA Linguistics Colloquium 2020",
    "section": "Interpreting Difference Smooths",
    "text": "Interpreting Difference Smooths\nOne thing I included in this presentation is an animation to help learn how to interpret difference smooths.I’ve been meaning to include this animation in a presentation for a bit now, and with the 40 minute afforded me in this presentation, I finally had the time to do so. Difference smooths are a type of plot that aid in the interpretation of GAMs and you can learn more about them in Mártin Sóskuthy’s tutorial on GAMs. Unfortunately for those of us that fit GAMs to vowel formant data, they look awfully like vowel formant curves, so they can be tricky to interpret. I’ll probably expand this into a full blog post later on, but for now, here’s a brief explanation of (my interpretation of) difference smooths.\nLet’s say we have some data, a blue curve and a red curve, sampled at 11 time points. Here, these 11 data points are plotted, with lines connecting the dots.\n\nWhen you fit a generalized additive model to this data, you can get two fit lines (left, below), which is basically a smoothed version of the jagged line above. It’s as if you had sampled continuously rather than at 11 discrete timepoints. When you plot a difference smooth, you get the plot on the right (below), which is essentially one curve “minus” the other curve.\n\n\n\n\n\n\n\n\n\n\nNow, it may not be completely transparent how the difference smooth relates to the two fit lines. So, to help out, the two plots below show the exact same curves, only several vertical lines have been added. On the fit lines, the vertical lines connect the two curves, with the height (and color) of the line representing the distance between them. On the right, the vertical lines connect the difference smooth and a horizontal line. The kicker: the height of the vertical lines in both plots is identical.\n\n\n\n\n\n\n\n\n\n\nIf you’re like me, it still might not be clear how they connect. The following animation may help. It starts with the two curves with the vertical lines between them. Since I’m getting the difference between the two, I’m “subtracting” the bottom from the top. This has the effect of flattening out the bottom one to a perfectly straight, horizontal line. In order to keep the vertical lines the same height, the amount of “bend” that has to happen to the bottom line has to apply equally to the top line. The result is a new curve called the difference smooth.\n\nOnce you’ve grasped that, you can then add some additional information to the plot. Typically, difference smooths come with confidence intervals, which I highlight in gray below. Wherever the confidence interval does not overlap with the horizontal (zero) line, the curves are interpreted as being statistically significantly different from each other.Exactly how these confidence intervals are calculated is something I’m still learning.\n\nIn this case, since the original red line is subtracted from the original blue one, with the confidence intervals on this difference smooth, I can say that the blue line is significantly higher than the red for some region.\n Finally, the plot below is the version of the difference smooths I use in this presentation, my dissertation, and anywhere else I’ve needed them so far.There are R packages that can create difference smooths (I think itsadug might be most well-known). I’m not a huge fan of the aesthetics of that plot, so I’ve created my own version using the same data, and in ggplot2. I’d like to release it in a package sometime soon so you can use them too if you’d like.\n\nI’ve just added some additional annotation to better highlight the region of statistical significance:\n\nThe center line is blue and slightly thicker in contrast with the gray, thinner lines.\nThe horizontal axis is blue in the region of statistical significance.\nAt the point where the confidence interval intersects 0 (the horizontal axis), I’ve added vertical dotted lines.\nI’ve also added the timepoint where that intersection happens. In this case, the time spans from 0 to 1, so these two lines are statistically significantly different from one another between 0.122s and 0.526s. NOTE: It’s important that you take these ranges with a grain of salt and perhaps interpret them very broadly rather than paying too much attention to the exact number.\n\nI just kinda like the look of this style of plot, so that’s the one I’ve been using."
  },
  {
    "objectID": "blog/colloquium-2020/index.html#conclusion",
    "href": "blog/colloquium-2020/index.html#conclusion",
    "title": "UGA Linguistics Colloquium 2020",
    "section": "Conclusion",
    "text": "Conclusion\nI’ve at least benefited from interpreting difference smooths in this way. Hopefully the attendees of my talk today (and now you!) will have slightly better understanding of them as well."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html",
    "href": "blog/kohler-tapes-update/index.html",
    "title": "Kohler Tapes (Update)",
    "section": "",
    "text": "In February, I acquired a goldmine of data that I can use for linguistic analysis. Here’s an update on that project, now that I have some more solid numbers."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#cataloguing",
    "href": "blog/kohler-tapes-update/index.html#cataloguing",
    "title": "Kohler Tapes (Update)",
    "section": "Cataloguing",
    "text": "Cataloguing\nSoon after getting the tapes, I had to organize them in some way. Since I don’t really know what I’m doing, I reached out to a few people I know who have worked with collections comparable in size to ask for their advice. (I also had to ask some Gen Xers how to handle and store cassette tapes because, well, I’ve never done that before 🤷🏻‍♂️).\nSo, over the course of a couple months, I took the tapes home in batches of fifty or so and cataloged them. By this, I mean that I labeled each with a sequential identifier and took photos of all sides of the tape, its case (if it had one), and any other slips of paper it came with.Thanks, Charlie, for that tip!\nWhile I was there, I wrote down whatever information was written on the outside of the tape. Things like the student’s name, the interviewee’s name, the date of the interview, their age, and the relationship between the two people (grandparent, great-grandparent, etc.). It looks like most of the tapes were done between 1986 and 1999. The ones from the 80s weren’t documented as consistently and I think Mr. Kohler caught on to that and asked his students to do a better job because the ones in the 90s were much more well-documented.\nThere are a couple really fun gems for me as I went through the names. First, I went to Heber in 2018 to collect some audio data myself. I talked to several older people then. As it turns out, some of these tapes contain recorded interviews with some of the people I talked to! Again, it’s a small town, so not a complete surprise, but it’s still pretty cool. Also, my sister-in-law has family from Heber, and sure enough, the collection contains interviews with about 10 of her distant relatives (great-grand-uncle, etc.). I’m sure I’ll discover some other gems as I dig deeper into this collection."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#digitizing",
    "href": "blog/kohler-tapes-update/index.html#digitizing",
    "title": "Kohler Tapes (Update)",
    "section": "Digitizing",
    "text": "Digitizing\nThanks to a great tip from my colleague Chris Rogers, I found out that the Humanities Learning Resource Center in my building can digitize tapes. So I dropped everything and talked to them. Not only can they do it, but they said they do it for free! Wow!\nSo, I dropped off a box of tapes for them and two weeks later the files magically appeared in my Box drive! So I went and collected them, dropped off another several dozen and repeat the process over and over. It was a good morning when I woke up to the notification from Box saying that another 100 files were ready to download.\nI want to just pause and do a huge shout-out to the student employees who processed all this audio! They had to hear the whir of the digitizing machine going for 8–9 hours days for 86 straight work days. Not to mention get up and flip the tape over or insert a new tape every 30–60 minutes, trim the audio, and upload the file. As someone who can’t stand unnecessary white noise or interruptions, that sounds like awful work to me. I’m sure if I had had to do it myself, it’d take me three or four times as long to get it all done and I wouldn’t be a happy camper."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#file-structure",
    "href": "blog/kohler-tapes-update/index.html#file-structure",
    "title": "Kohler Tapes (Update)",
    "section": "File Structure",
    "text": "File Structure\nSomething that I haven’t yet figured out though is the best way to store all this. When I worked with the Linguistic Atlas Project, it was simple: one speaker per file. Most interviews were longer and were recorded on multiple reels, so a single speaker may be on as many as ten or so files, but it was otherwise pretty well-organized.\nThis collection though is a hot mess. Here’s a list of the kinds of things I’m working with:\n\nMany students did the cleanest route and interviewed one person and turned in one tape. Great.\nHowever, some students went overboard and turned in multiple (as many as five!) tapes for a single interview. I welcome more data, so that’s not too bad.\nTo complicate things, sometimes the same person was interviewed by different students on different years. One person was interviewed six different times!\nSometimes, if the first interview wasn’t long enough, a student would conduct another interview with a different person. So there are two interviews and two different people on a single tape.\nThe messier route was if a student interviewed two grandparents at the same time. So the husband and wife would alternate back and forth. So one interview, but two people. This will be the trickiest to process.\nAnd, of course, there are a few cases where multiple joint interviews are tagged on to each other, and this collection of interviews was spread across multiple tapes. Ugh.\n\nCurrently, my file structure is one-folder-per-tape, but as I get my hands dirty, I’m realizing I need to switch to a one-folder-per-person structure.\nAnd of course, all this necessarily will need to come with multiple spreadsheets and some minor databasing to keep track of it all. Currently, I’ve got a spreadsheet for tapes, a spreadsheet for sides of tapes, and a spreadsheet for individuals. It’s not as clean as a single spreadsheet but it works."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#some-numbers",
    "href": "blog/kohler-tapes-update/index.html#some-numbers",
    "title": "Kohler Tapes (Update)",
    "section": "Some numbers",
    "text": "Some numbers\nSo now that they’ve all been cataloged and digitized, I can give some more concrete numbers than I did in my previous blog post.\n\nNumber of tapes: 751\nMy estimate before was 600–700, so it’s a little more than I expected (which is great!)\n\n\nHours of audio: 631\nI know that some of that is music (some students taped over mix tapes) so the number may go down as I listen to it all. I anticipated “only” 467 hours of audio at first, so this is 33% more than what I originally thought.\n▼ I tried to estimate how much I’d end up with before they were all done, and it looks like by around 200 tapes or so I had a pretty good idea. The blue line is the predicted number and the black lines are some error. The pink line shows the true total.\n\n\n\nMinutes per tape: 51.7 (on average).\nMy estimate in February was 40 minutes, so not only did I end up with more tapes than I expected, but they were 30% longer than I expected. I think the assignment was to have 30 minutes, but I didn’t expect so many students to go that much longer.\n▼ Here’s the distribution of how many minutes of audio were on each tape (both sides). I’m pretty sure the peaks at 30, 65, and 95 or so reflect how much audio a cassette tape can hold.\n\n\n\nDigitizing pace: 8.46 tapes a day (on average)\nSince digitization happens in real-time, that means these students had this going for like nine hours a day. It took them 86 work days to do it all.\n▼ Based on the creation date of the files, here’s how much work they did per day. They worked Monday through Saturday every week. You can see that after the semester ended in April it was slightly less consistent. Sometimes I wasn’t on campus the day they finished a box so they had to wait a day or two to get the next batch.\n\n\n\nNumber of people: 806\nThat’s just interviewees. If you add the 667 students, that’s 1473 total people. My guess is that that number will go down a small amount as I clean up the metadata. I’ve already had to change “Mr. Norman” and “Mrs. Norman” to their full names once I listened to them. Correcting any typos may change the number too if people were interviewed multiple times. I mentioned this in my last blog post, but Heber only had a few thousand people living in it at the time, so this is decent proportion of the total population of Heber. And if you just focus on the age group that this collection represents, that proportion goes up quite a bit!\nThere are a fair number of common family names like Jones, Smith, Johnson, McDonald, Thompson, Anderson, Davis of course. But there are also a lot of names like Giles, Bethers, Jensen, Web, Allred, Broadhead, Casper, Duke, Probst, and Young, which I presume are local families.\nI will say right now that some of the students’ speech has some pretty distinctive linguistic features. These would be people born between about 1972 and 1987, or Gen Xers and early Millennials,  who grew up in Heber. Unfortunately, I won’t have enough data from them to do much of an analysis.Geriatric Millennials?\n\n\nInterview years: 1986–2001\nThat’s at least based on the 418 tapes where the interview date was written on the outside. Almost all were in April or May of each year. I suppose if I really wanted to I could track down some old yearbooks and find when the students were in 8th grade and get an exact year. I may find this information in the audio, but I haven’t listened to all of them yet. I may also be able to deduce it from people’s ages and birth years if they mention them.\n▼ Notice there are many more in the 90s. I think it’s a sampling bias though. My guess is that later on in the project, the students received more explicit instructions to write that information than the students in the 1980s did.\n\n\n\nBirth Years: 1905–1953\nAgain, that’s at least based on the 28 tapes I’ve listened to. This information was not written on the outside of the tape, so I can only get it in the audio itself. If they don’t say it explicitly, I can usually get enough information about the person to look up census records and get a confirmed date.\n▼ Here’s the spread of confirmed birth years. They’re color-coded by generation cohort in case that’s meaningful to you. I estimated everyone would be born between 1900 and 1940, so that was pretty close being right.\n\n\n\nPlaces of birth: mostly Wasatch County\nOnce again, that’s based on the 28 tapes so far. Wasatch County includes Heber, Charleston, Daniel, and Walsburg. Keep in mind these interviews took place in Heber, the county seat of Wasatch County.\n▼ Several other places in Utah are represented so far too. 14 unique cities in just 28 tapes. Only one person so far was born outside of Utah."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#looking-ahead",
    "href": "blog/kohler-tapes-update/index.html#looking-ahead",
    "title": "Kohler Tapes (Update)",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nI won’t rehash what I wrote in my earlier blog post, but now that I have some more solid numbers, I can have better estimate for what I need to get this project done.\nWith 631 hours of audio and a rough estimate of 10 hours of work for every hour of audio, that’s 6,310 hours of manual labor needed to transcribe this all. Again, that’s about 35% more than I anticipated in February. At $15 per hour of work, that’s $94,650 in student wages.\nIf I can get some more stellar RAs like I had this semester, who worked 10 hours a week, that’s 631 student-weeks. At 15 weeks a semester, that’s 42 student-semesters of work. If I want this done in two years, that’ll take an average of 14 RAs each semester, including summers. Anyone who has worked with transcribers knows that retention is not great, so I’ll most certainly need more than 14 student helpers and they’ll most certainly not last two years.\nThis doesn’t even include my pipe dream of getting some grad student workers to form a core group of researchers. They’d help with supervising and training the transcribers, quality control, other aspects of the data processing (force-aligning, formant-extraction, file management), and analysis.\nAm I already looking at external grants? Yes, yes I am. And am I already looking at different types of transcription software, methods, speech-to-text programs, and other things to speed this up and/or make it less costly? Yes, yes I am."
  },
  {
    "objectID": "blog/kohler-tapes-update/index.html#immediate-plans",
    "href": "blog/kohler-tapes-update/index.html#immediate-plans",
    "title": "Kohler Tapes (Update)",
    "section": "Immediate Plans",
    "text": "Immediate Plans\nMy first task though is to get as many of the gaps in my metadata spreadsheet filled as quickly as possible. I only know the birth years and places for 28 of the 806 people. I’d like to get a much more complete picture of what this collection is like before I start prioritizing which tapes to transcribe first. Sometimes this information is near the start of the interview but sometimes it’s not. Not quite sure how to get that information without just listening to all of them.\nFortunately, I got a grant from the Redd Center for Western Studies to go towards this project. It’s about the right amount needed to process enough data for a preliminary linguistic analysis. So I hope to get the ball rolling on some transcriptions and metadata extraction. Stay tuned for the first results at a conference near you!"
  },
  {
    "objectID": "blog/divar/index.html",
    "href": "blog/divar/index.html",
    "title": "DiVar",
    "section": "",
    "text": "I’m excited to announce I’ve been accepted to present at the first iteration of the Diversity and Variation in Language Conference (DiVar1), which will be held at Emory University in Atlanta February 10–11. I’m excited to hear that many of my colleagues at UGA have also been accepted, so it should be a fun day for us.\nIn a nutshell I’m showing that in word lists people pronounce pull and pole the same but Mary and merry different, but in a minimal pair task, pull and pole are separate while Mary and merry are the same. I spent the summer in Washington state, and I’m only looking at a little less than 3% of everything I recorded for this conference.\nI’ll post a summary of the presentation and the slides once the conference is over. Stay tuned."
  },
  {
    "objectID": "blog/jmp/index.html",
    "href": "blog/jmp/index.html",
    "title": "JMP",
    "section": "",
    "text": "As a part of my assistantship this year, I get to work with the DigiLab in the Main Library at UGA. It’s a fun little gig where I get to do presentations, workshops, and seminars on digital humanities, in addition to helping researchers one-on-one on their own projects.\nI’ve always been fairly tech savvy in my research. I minored in Linguistic Computing and had a job as an undergrad creating eBooks. I’ve always been fairly quantitative about things too, and I went over to the stats department last year and took classes on linear regression and multivariate analysis.\nThis week is my first presentation where I’ll show how to extract data from primary sources, introduce people to spreadsheets, and showcase JMP If you’ve never heard of it, JMP (pronounced “jump”) is a pretty sophisticated piece of statistical software that can do all sorts of statistical analyses and visualizations. The great part is it’s drag-and-drop interface. No coding means it’s great for beginners.\nI’ll be the first to admit that JMPs visualizations (at least as well as I can make them) are not as good as what I can do with ggplot2 in R. But I do like how easy it is to make quick and dirty visualizations in JMP. For this reason, it’s usually my go-to when exploring a new dataset. With just a few clicks, it makes it really easy to get to know your data visually as well as what’s under the hood.\nAs far as I know, JMP is not widely used in Linguistics or even in the Humanities, so showcasing it will be a lot of fun to people this week. I still think that everyone should learn R for their research, but since coding can be intimidating, JMP is a great tool for researchers wanting to get more quantitative about their projects.\nUpdate (October 7): I just finished the presentation. I had about a dozen people in attendance, and I was the youngest person in the room. And besides the one other grad student there, I was probably the only one without a Ph.D. But, I feel like it went well. Not to mention, it was the first time I was introduced before a presentation, and there was food!"
  },
  {
    "objectID": "blog/365-papers/index.html",
    "href": "blog/365-papers/index.html",
    "title": "#365papers",
    "section": "",
    "text": "Around the first of the year, I saw that several academics I follow on Twitter made a goal to read 365 papers during 2018. They tweet about their papers and use the hashtag #365papers. I don’t stand a chance at reaching that goal of 365 papers, but I decided to join in."
  },
  {
    "objectID": "blog/365-papers/index.html#whats-the-point",
    "href": "blog/365-papers/index.html#whats-the-point",
    "title": "#365papers",
    "section": "What’s the point?",
    "text": "What’s the point?\nFirst off, why bother doing this over Twitter? I can think of several reasons.\n\nTwitter is an interesting place for academics and this just sort of fits in with what goes on there.\nIf I read papers written by people I know on Twitter, I’ll @ them. It not only lets them know I’m reading their work, it’s a quick way to incorporate them into the conversation.\nIn general, keeping track of the papers I read is healthy because it’s easier to go back and find something I read when I have a spreadsheet.\nKeeping track in a way for others to see is even more motivating because I don’t want to quit halfway through.\n\nEssentially, I find it useful to keep track of things. I always feel like I need to read more academic work, so this is a nice way to motivate myself to do so."
  },
  {
    "objectID": "blog/365-papers/index.html#twitter-feed",
    "href": "blog/365-papers/index.html#twitter-feed",
    "title": "#365papers",
    "section": "Twitter feed",
    "text": "Twitter feed\nThe rest of this post will be updated periodically to include my tweets. For brevity, I’ll only post parent tweets here, but sometimes I do a tweetstorm and include some commentary, which sometimes includes comments from others.\n\n\nAll right, all you #365papers have convinced me. I'm behind, but I'll mention what I remember reading so far this year:#1–6: @americandialect's PADS vol 2 (https://t.co/Yoy5YoIfaX) with @aliciabwassink, @dialect, @cotterw, @sociolx, etc. etc.\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\n@aliciabwassink. 2015. Sociolinguistic patterns in Seattle English. Language Variation and Change 27(01). 31–58. https://t.co/4e4xSylA2B It somehow slipped through the cracks in my research on Pacific Northwest English. #365papers #7\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\nTrudgill, Peter & Tina Foxcroft. 1978. On the sociolinguistics of vocalic mergers: Transfer and approximation in East Anglia.Labov (1994) cites this as where mergers by approximation and transfer came from. Thought I'd go to the source. #365papers #8\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\n@dialect 2013. ’Flip-flop’and mergers-in-progress. English Language and Linguistics 17(02). 359–390.It's been on my to-read list for a while. Was not disappointed. #365papers #9\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\nZeller, Christine. 1997. The investigation of a sound change in progress : /æ/ to /e/ in Midwestern American English. Journal of English Linguistics 25(2). 142–155.I'm researching BAG-raising. This was one of the first. #365papers #10\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\nBauer, Matt & Frank Parker. 2008. /æ/-raising in Wisconsin English. American Speech 83(4). 403–431.More research on BAG-raising. Also, ultrasound is sweet. Also, add Ohala (2003) to my to-read list. #365papers #11\n\n— Joey Stanley (@joey_stan) March 9, 2018\n\n\n\n\nFaber & Di Paolo. 1990. Phonation differences and the phonetic content of the tense-lax contrast in Utah English. LVC 2(02). 155–204. doi:10.1017/S0954394500000326.Their last statement says it all: \"There is more to vowels than their formant frequencies.\" #365papers #12\n\n— Joey Stanley (@joey_stan) March 20, 2018\n\n\n\n\nDinkin. 2016. Phonological Transfer as a Forerunner of Merger in Upstate New York. Journal of English Linguistics 44(2). 162–188. doi:10.1177/0075424216634795.Introduces a new type of merger, distinct from Approximation, Transfer, and Expansion. #365papers #13\n\n— Joey Stanley (@joey_stan) March 21, 2018\n\n\n\n\nLisker, Leigh. 1986. “Voicing” in English: A Catalogue of Acoustic Features Signaling /b/ versus /p/ in Trochees. Language and Speech 29(1). 3–11.There are no fewer than 16 acoustic cues to differentiate /p/ and /b/ in \"rapid\" vs. \"rabid.\" #365papers #14 1/6\n\n— Joey Stanley (@joey_stan) March 23, 2018\n\n\n\n\nMilroy, James & Lesley Milroy. 1978. Belfast: Change and variation in an urban vernacular. In Peter Trudgill (ed.), Sociolinguistic Patterns in British English, 19–36. London: Edward Arnold. #365papers #15 1/2\n\n— Joey Stanley (@joey_stan) April 11, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 1: Introduction. I need to learn more about phonetics, so this will be a great book. #365papers #16 (I'm counting book chapters as papers.)\n\n— Joey Stanley (@joey_stan) April 15, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 2: American English Phonemes. #365papers #17\n\n— Joey Stanley (@joey_stan) April 15, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 3: Speech and Sound. #365papers #18A nice overview of the acoustics of sound is always good every once in a while.\n\n— Joey Stanley (@joey_stan) April 17, 2018\n\n\n\n\nGerry O. Knowles. 1978. The Nature of Phonological Variables in Scouse. In Peter Trudgill (ed.), Sociolinguistic Patterns in British English, 19–36. London: Edward Arnold. #365papers #19\n\n— Joey Stanley (@joey_stan) April 18, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 4: Static Properties of Speech Sounds #365papers #20\n\n— Joey Stanley (@joey_stan) April 19, 2018\n\n\n\n\nJames Milroy. 1981. Regional accents of English: Belfast. Chapter 5: Belfast Vowels in Detail #365papers #21.Thanks, @EstherAsprey, for the recommendation!\n\n— Joey Stanley (@joey_stan) April 19, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 5: Vowel Transitions. #365papers #22\n\n— Joey Stanley (@joey_stan) April 20, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 6: Obstruent and Vowel Transitions. #365papers #23\n\n— Joey Stanley (@joey_stan) April 30, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 6: Consonantal Sonorants and Vowels. #365papers #24\n\n— Joey Stanley (@joey_stan) April 30, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 8: Consonant Interactions. #365papers #25\n\n— Joey Stanley (@joey_stan) May 7, 2018\n\n\n\n\n@karmaglow. 2018. A remedial path to merger: Merger by phonological transfer in British Columbia English. Toronto Working Papers in Linguistics 40(1). https://t.co/bonUfob51K #365papers #26Working on pre-velar raising myself right now so this was perfect for me!\n\n— Joey Stanley (@joey_stan) May 10, 2018\n\n\n\n\nOlive, Joseph P., Alice Greenwood, & John Coleman. 1993. Acoustics of American English Speech: A Dynamic Approach. Chapter 9: Acoustic Variability. #365papers #27Basically, this whole book can be summed up in their last two sentences:\n\n— Joey Stanley (@joey_stan) May 11, 2018\n\n\n\n\nLabov, William. 1994. Principles of Linguistic Change (Volume 1). Chapter 14: The Suspension of Phonemic Contrast. #365papers #28\n\n— Joey Stanley (@joey_stan) May 17, 2018\n\n\n\n\n@JoFrhwld 2017. Generations, lifespans, and the zeitgeist. Language Variation and Change 29(1). 1–27. doi:10.1017/S0954394517000060.Added to my todo list: 1) Acquire big corpora 2) Be able to use and explain advanced statistical models like a boss. #365papers #29\n\n— Joey Stanley (@joey_stan) May 22, 2018\n\n\n\n\nLabov, William. 1994. Principles of Linguistic Change (Volume 1). Chapter 5: General Principles of Vowel Shifting. #365papers #30Hey, it's only taken me almost 6 months to get to 30, but that's a start!\n\n— Joey Stanley (@joey_stan) May 22, 2018\n\n\nAt this point, I decided to stop Tweeting. It was more of a commitment than I was willing to get myself into. Plus, I was reading some foundational things that I felt embarrassed to admit I hadn’t read yet. 😅 But I did finish, and you can read my post about when I read my 365th paper here."
  },
  {
    "objectID": "blog/general-update/index.html",
    "href": "blog/general-update/index.html",
    "title": "General Update",
    "section": "",
    "text": "Because I know I have such a massive following, I thought I’d give an update on my research since it’s been a few months since the last time I wrote.\n\nPublications\nAt the Linguistic Atlas Office, we’re working hard on publishing some of our preliminary results. Currently, I’m on two papers submitted to Proceedings of Meetings on Acoustics that are in various stages of reviewing. I’m excited to see these come out.\nI’m also working on some of my own research. I’ve got three manuscripts going right now: one on near-mergers in Washington, one on language change within a speaker’s lifespan, and another on Amazon Mechanical Turk. I still haven’t submitted a paper to an actual journal so mentally this is a big hurdle for me to get over.Edit (September 2023): Lol, none of these manuscripts were even finished.\n\n\nConferences\nI’m happy to say I’ve been accepted into two conferences that’ll happen over the next few months. The first is a paper called “Consonantal variation in Utah English: What el[t]se is happening[k]?” that I’m doing with Kyle Vanderniet, a fellow grad student here at UGA. Using the MTurk data I collected recently, we focused on just the 14 speakers from Utah and gathered a lot of really interesting tokens of non-standard variants. I’ve also been accepted to present a poster at NWAV on some of my findings in Washington.\nI’m also still waiting to hear back from three other conferences: ASA and two at ADS.\n\n\nTeaching\nSo I’m teaching for the first time this semester. Because our normal phonetician will be gone, I was asked to teach Phonetics & Phonology. This is pretty cool because normally grad students at UGA don’t get to teach that class. I’ve really enjoyed it so far! I’ve got 29 students who are all linguistics majors or minors. It takes quite a bit of time, but I’m having a great time.\n\n\nR Series\nFortunately, my funding through the DigiLab at the UGA Main Library continues this school year. I’m really excited be giving a whole series of workshops on R. Next week I’ll start with just a basic introduction to R. Next month I’ll do a day on ggplot2 and in November will be one on the rest of the tidyverse. I plan on producing some detailed handouts that will be available on this website as a product of these workshops. I’m really looking forward to them.\n\n\nGrant\nI applied for a small grant a week or so ago that I’m waiting to hear back about. If I get it, it’ll pay for some fieldwork out West I’d like to do. Stay tuned. (Update: I got the grant.)\n\n\nDissertation\nAh, the dissertation. This should be my primary task right now, and I’m putting as much time as I can into it. I originally wanted to do something on my Washington data. I could do a purely descriptive work and it would make for a fine dissertation. I’ve read through some of those and they’re perfectly good.\nThe problem with that is if I would do that, it would be of interest only to people interested in Washington English. A very small group of dialectologists. Not that my ultimate goal is to boost the number of citations, but I feel like a dissertation, which is supposed to make a real contribution to the field, should be more than that.\nI’ve read a couple other dissertations that are heavily based in dialectology and sociolinguistic fieldwork, but they make a bigger statement. They make some advancement on the study of language change, using the data they’ve collected as an illustration. For example, Ruth Herold’s (1990) UPenn dissertation is based on the cot-caught merger parts of Pennsylvania. Instead of simply describing the data, she makes some very cool statements about how language change works, especially in relation to mergers and generational changes, using the data she collected as evidence. The data is supporting the claim rather than the claim itself.\nI’ve got around 180 hours of my own data right now, gathered from lots of sources. All of it has to do with mergers in some way. I feel like I’ve got a dissertation bubbling inside of me relating to vowel mergers. I can use my data as support, illustration, and evidence for some claim that I’d like to make.Update: I stuck with the Washington data for my dissertation, but Betsy Sneller and I did eventually write something about mergers using the Washington data, so I guess that scratched that itch.\nSo I’m working on my prospectus right now and will be pitching it to my committee this semester. I’ll keep you informed."
  },
  {
    "objectID": "blog/website-version-3/index.html",
    "href": "blog/website-version-3/index.html",
    "title": "Website Version 3",
    "section": "",
    "text": "After exactly seven years with my old website, I’ve decided to change it to what you are seeing now."
  },
  {
    "objectID": "blog/website-version-3/index.html#what-was-wrong-with-the-old-one",
    "href": "blog/website-version-3/index.html#what-was-wrong-with-the-old-one",
    "title": "Website Version 3",
    "section": "What was wrong with the old one?",
    "text": "What was wrong with the old one?\nI built my old website in September 2016. I had a research assistantship at the DigiLab at UGA, and Emily McGinn, the supervisor, suggested I find ways to increase my online presence. I learned some web design and CSS skills skills and eventually made Version 1 of my website. That version was essentially the same as what I built in the tutorial I followed, so a few months later I rewrote everything from scratch and made Version 2. (Let’s be honest though, it’s still obviously heavily based on the tutorial.) Other than very minor tweaks to a few things, that’s how my website has been since then.\nHowever, it got a bit unwieldy. The blog was organized just fine, but I also added pages here and there to go along with workshops and other presentations I gave. It got more confusing when I gave the same workshop a second time and had multiple similar pages floating around. Since I didn’t foresee some of these additions, its growth was reminiscent of unplanned suburban sprawl. For examples, sometimes images were just dumped into a folder, others were better organized. Non-blog pages were hidden and were sometimes a top-level page and other times within a dedicated subdirectory. Each individual addition wasn’t a big deal, but once I stepped back and looked at it all, it was a mess.\nThe format of my tutorials wasn’t consistent either. I have lots of handouts on my website, tucked away here and there. If they were associated with a workshop, they were separate R Markdown files that didn’t fit in with the rest of the site. Some of my earliest ones are PDFs of Word files!If they weren’t associated with a workshop, they’re regular blog posts. But because the site wasn’t connected to R, I had to do a lot of copying and pasting R Markdown code and careful insertion of images to get those tutorials to look right. In some cases, the extra work made it possible to do things like syntax highlighting in Praat and highlighting specific lines of code. But that was all done by manually inserting HTML tags and updating my CSS.\nAlso, as careful as I was about my CSS, it wasn’t perfect. I think there were some issues if like a list had only one element, and there were things with hyperlinks. Some one-off portions of blogs or tutorials sometimes didn’t look right. I had a disclaimer at the top of every page, something like, “This website is built from scratch. Pardon the flaws; I am not a web designer.” Which was a humble brag if anything. But as the site grew I didn’t want to change the CSS because it might change some blog post from years ago in unexpected ways.\nUltimately, I didn’t mind the mess because it’s what made my site unique. But, what made me finally decide to migrate to Quarto was the underlying architecture. It was built using Jekyll, which involves a programming language called Ruby in some way. After seven years I still have no idea what either of those are. I did this because it’s what the tutorial I followed used. When the site worked, it was great. But sometimes, the Ruby dependencies (called “gems”) would update or break or whatever and I had to google around trying to find a fix. I had no idea what I was doing and it led to a lot of frustrated late nights trying to get my website up and running again.\nThen Quarto comes along, which makes it easy to make a blog entirely within R Studio. I have been very familiar with the R world for a while. In 2017, I was an early adopter of Shiny (at least in linguistics, I think), so I was able to integrate all my html, CSS, and R skills into the Gazetteer of Southern Vowels. In 2020, I also started dabbling with creating my own R Packages and using the amazing pkgdown to make dedicated websites for them (see joeyr, futurevisions, barktools, and joeysvowels). Finally, I have a side project that involves collecting and analyzing data about what hymns are sung in LDS congregations, and in 2023 I decided to build the site entirely in Quarto.\nSo, I’ve gradually built up to web development in R over the years and Quarto seems like the logical place to migrate to. Plus, it has some features that I’ve always wanted, like scrolling table of contents and a search feature. After some encouragement from folks on Twitter, I decided it’s time to bite the bullet and go for it."
  },
  {
    "objectID": "blog/website-version-3/index.html#what-does-it-take-to-migrate",
    "href": "blog/website-version-3/index.html#what-does-it-take-to-migrate",
    "title": "Website Version 3",
    "section": "What does it take to migrate?",
    "text": "What does it take to migrate?\nI’m doing this page by page. Here’s the order I took:\n\nMy homepage and any links on it. I didn’t clean up the linked pages, but at least there weren’t any dead links.\nMy blogs."
  },
  {
    "objectID": "blog/website-version-3/index.html#things-that-are-the-same",
    "href": "blog/website-version-3/index.html#things-that-are-the-same",
    "title": "Website Version 3",
    "section": "Things that are the same",
    "text": "Things that are the same\nI’ve tried to keep as much of the original structure of the site the same as I could. However, as I migrate"
  },
  {
    "objectID": "blog/website-version-3/index.html#changes",
    "href": "blog/website-version-3/index.html#changes",
    "title": "Website Version 3",
    "section": "Changes",
    "text": "Changes\nHere’s a list of the changes I’ve made.\n\nEach blog is now in its own self-contained folder. The previous structure had all posts in a single folder and all images in another folder. This time, the images associated with a blog post are contained within that folder. So, instead of this:\n├──📁blog\n|  ├──📄blog post 1.md\n|  ├──📄blog post 2.md\n├──📁images\n|  ├──🌅image1.png\n|  ├──🌅image2.png\nIt’s now this:\n├──📁blog\n|  ├──📁blog post 1\n|  |  ├──  📄index.qmd\n|  |  ├──  🌅image1.png\n|  ├──📁blog post 2\n|  |  ├──  📄index.qmd\n|  |  ├──  🌅image2.png\nIt shouldn’t affect any urls to existing blog posts because the url blog/blog post 1 in the old format would go to the blog post 1.md file and in the new one it’ll go to the blog post 1 directory, which’ll display index.qmd by default. I was concerned about changing the url because I know some people have cited my turorials in published work and I didn’t want those urls to break. I think this’ll work and it’ll keep the site better organized.\nWithin each post, I need to update the header. I change from “tags” to “categories”, from “redirect_from” to “aliases.” I add a date-modified if needed. I remove excerpts because I only used them with my “big-link” style button. I’m replacing big-links with a standard callout box. I haven’t figured out if I can do redirects-to, which is a bummer for the GSV.\n\n\n\nBy the way, I’m stealing this way of visualizing file structure directly from TJ Mahr’s Migrating-to-Quarto page.."
  },
  {
    "objectID": "blog/ads-and-lsa-2020/index.html",
    "href": "blog/ads-and-lsa-2020/index.html",
    "title": "LSA and ADS 2020",
    "section": "",
    "text": "For the first time in a few years, I did not attend the ADS/LSA annual meetings. It would have been nice to hang out in New Orleans this weekend. However, my research will still be represented: my colleagues presented some research focusing on Southern American English vowels that I’ve been a part of this year."
  },
  {
    "objectID": "blog/ads-and-lsa-2020/index.html#fridays-ads-poster-on-southern-american-english-vowels",
    "href": "blog/ads-and-lsa-2020/index.html#fridays-ads-poster-on-southern-american-english-vowels",
    "title": "LSA and ADS 2020",
    "section": "Friday’s ADS poster on Southern American English vowels",
    "text": "Friday’s ADS poster on Southern American English vowels\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nFriday morning at the American Dialect Society poster session, Bill Kretzschmar presented some research on behalf of the entire Linguistic Atlas research team at UGA (Peggy Renwick, Katie Kuiper, Lisa Lipani, Mike Olsen, Rachel Olsen, and me). In it he presents findings from our now-complete dataset from the Digital Archive of Southern Speech. With a dataset this size, we can make comparisons to other varieties of American English regarding the distribution of vowels in the F1-F2 space."
  },
  {
    "objectID": "blog/ads-and-lsa-2020/index.html#sundays-lsa-presentation-on-formant-trajectories",
    "href": "blog/ads-and-lsa-2020/index.html#sundays-lsa-presentation-on-formant-trajectories",
    "title": "LSA and ADS 2020",
    "section": "Sunday’s LSA presentation on formant trajectories",
    "text": "Sunday’s LSA presentation on formant trajectories\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nSunday, Peggy Renwick presented our research on formant dynamics of back vowels in Southern American English vowels. Focusing on just the White Americans in the Digital Archive of Southern Speech (DASS), we wanted to see how formant trajectory shapes and their relative positions vary across male and female speakers in different generations. Many of our findings can be summarized in this animation:\n\ngoose doesn’t change much, especially for the women, suggesting goose fronting is an old shift and was nearing completion by the time our speakers acquired language. Meanwhile goat lowers between the Lost and GI generations of women, and then a generation later for the men. foot doesn’t change much, suggesting it doesn’t pattern with goose-fronting. The low vowels lot and thought are near each other, but not merged because of their different positions in the F1-F2 space and their different formant shapes.\nSomething that stands out to me is that each vowel’s trajectory didn’t change all that much over time. This wasn’t a product of how our GAMM was specified: each combination of vowel, sex, and generation was interacted and therefore, fit independently of the other. So the fact that the trajectories show remarkable stability is really interesting to me. So, at least among these DASS speakers, Southern American English vowels appear to shift the nucleus and glide in tandem."
  },
  {
    "objectID": "blog/simulating_chutes_and_ladders/index.html",
    "href": "blog/simulating_chutes_and_ladders/index.html",
    "title": "Simulating Chutes and Ladders",
    "section": "",
    "text": "We tried teaching our little almost-three-year-old Chutes and Ladders today. She wasn’t very good at counting tiles. But, as I was sitting there climbing up and sliding down over and over, I wondered what the average number of turns it would take to finish the game. So I decided to take a stab at simulating the game. So here’s a post on a simple simulation of Chutes and Ladders that demonstrates absolutely nothing about linguistics and instead shows off some R skills.\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(scico)\nlibrary(readxl)\nlibrary(gganimate)\nI’m trying to increase my Github presence, so the code for this project can be found on my Github."
  },
  {
    "objectID": "blog/simulating_chutes_and_ladders/index.html#the-game",
    "href": "blog/simulating_chutes_and_ladders/index.html#the-game",
    "title": "Simulating Chutes and Ladders",
    "section": "The game",
    "text": "The game\nFor those of you deprived people who have never played Chutes and Ladders, the game is quite simple. There are 100 tiles arranged in a 10 by 10 board. With 1–3 of your closest friends, you start at Tile 1, roll a die, and advance that number of tiles. Players take turns moving up the board boustrophedonically. until one person reaches 100. The catch is that there are various “chutes” and “ladders” on the board. If you land on the bottom of one of about half a dozen ladders, you climb to the top, advancing anywhere from 10 to 54 tiles. But, if you land at the top of about a dozen chutes, you slide down anywhere from 4 to 63 tiles. There is no skill and it’s 100% luck—perfect for small kids.I’ll admit that half the reason I wrote this post was so I could use this word!\nHere’s a simplified version of the board. First, I’ll read in the data I prepared ahead of time. It just gives the x-y coordinates for each of the 100 tiles.\n\ntile_data &lt;- readxl::read_excel(\"chutes_ladders_data.xlsx\", sheet = 1)\nhead(tile_data)\n\n# A tibble: 6 × 3\n   tile     x     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     1     1\n2     2     2     1\n3     3     3     1\n4     4     4     1\n5     5     5     1\n6     6     6     1\n\n\nNow, I’ll read in information about the chutes and ladders themselves, as in where they start and where they stop.\n\nchutes_and_ladders_data &lt;- readxl::read_excel(\"chutes_ladders_data.xlsx\", sheet = 2) %&gt;%\n  rowid_to_column(\"id\") %&gt;%\n  gather(position, tile, start, end) %&gt;%\n  left_join(tile_data, by = \"tile\")\nchutes_and_ladders_data\n\n# A tibble: 38 × 6\n      id type   position  tile     x     y\n   &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 ladder start        1     1     1\n 2     2 ladder start        4     4     1\n 3     3 ladder start        9     9     1\n 4     4 ladder start       21     1     3\n 5     5 ladder start       28     8     3\n 6     6 ladder start       36     5     4\n 7     7 ladder start       51    10     6\n 8     8 ladder start       71    10     8\n 9     9 ladder start       80     1     8\n10    10 chute  start       16     5     2\n# ℹ 28 more rows\n\n\nJust some information about the lines for the chutes and ladders, mostly for visual purposes.\n\nlines_data &lt;- read_excel(\"chutes_ladders_data.xlsx\", sheet = 3) %&gt;%\n  rowid_to_column(\"id\") %&gt;%\n  gather(point, value, beg_x, beg_y, end_x, end_y) %&gt;%\n  separate(point, into = c(\"location\", \"axis\")) %&gt;%\n  spread(axis, value)\nhead(lines_data)\n\n# A tibble: 6 × 5\n     id type    location     x     y\n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     1 outside beg        0.5   0.5\n2     1 outside end       10.5   0.5\n3     2 outside beg        0.5  10.5\n4     2 outside end       10.5  10.5\n5     3 outside beg        0.5   0.5\n6     3 outside end        0.5  10.5\n\n\nNow make a basic plot.\n\nggplot(tile_data, aes(x, y)) + \n  geom_line(data = lines_data, aes(group = id, linetype = type)) + \n  geom_text(aes(label = tile), size = 3, nudge_x = -0.25, nudge_y = 0.25) +\n  labs(title = \"A Simplified Chutes and Ladders Board\",\n       caption = \"joeystanley.com\") +\n  geom_path(data = chutes_and_ladders_data, aes(group = id, linetype = type),\n            arrow = arrow(angle = 20, length = unit(0.1, \"in\"), type = \"closed\")) +\n  scale_linetype_manual(values = c(\"solid\", \"solid\", \"dashed\", \"solid\", \"dotted\", \"dotted\")) +\n  coord_fixed(ratio = 1) + \n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\nThe game we bought is actually a knock-off (I’m a poor grad student—what do you expect?), but I found an image of the authentic version online so I’ll go with that for this blog post. And for simplicity, I’ll just simulate a one-person game."
  },
  {
    "objectID": "blog/simulating_chutes_and_ladders/index.html#the-simulation",
    "href": "blog/simulating_chutes_and_ladders/index.html#the-simulation",
    "title": "Simulating Chutes and Ladders",
    "section": "The simulation",
    "text": "The simulation\nSimulating the game is relatively straightforward. All you really need is a way to keep track of what tile you’re on, a way to roll the die, and sequence of if-else statements to simulate the chutes and ladders. The die rolling is pretty simple with the help of sample.\n\n\n\n\nsample(6, 1)\n\n[1] 1\n\n\nWe could wrap that up into a function to make it slightly more transparent too:\n\nroll_die &lt;- function() { \n  sample(6, 1) \n}\nroll_die()\n\n[1] 4\n\nroll_die()\n\n[1] 1\n\nroll_die()\n\n[1] 2\n\n\nNow, in this game, every time you roll a die, you’ll need to advance your token by that many pieces. I could write a separate advance_token function, but with functions this simple, I’ll just combine them into one. This time, it’ll take an argument, spot, which is the current tile number (from 1 to 100) that you’re on. The function takes this spot, rolls a die, and adds that value to it, returning the tile you’ll land on.\n\nroll_die &lt;- function(spot) { \n  spot + sample(6, 1)\n}\nroll_die(1)\n\n[1] 6\n\nroll_die(10)\n\n[1] 13\n\nroll_die(50)\n\n[1] 56\n\nroll_die(80)\n\n[1] 82\n\n\nAwesome. Now with this, it makes for a pretty uneventful game, so we’ll have to simulate the chutes and ladders. I know base R has switch syntax, but I’ve never been able to get it to work, so I’ll use case_when from the dplyr package to do this. I’ll of course wrap it up in another function verbosely called check_for_chute_or_ladder. Here, I input the start and end points to all the chutes and ladders. So for example, if you land on the very first tile, there’s a ladder that’ll take you to tile 38. If you land on tile 16, you’ll slide down a chute to tile 6. The function will return the new tile you’ll end up on. If you don’t land on any of them, the function will return the same number you sent in.\n\ncheck_for_chute_or_ladder &lt;- function(spot) {\n  case_when(\n    \n    # Ladders (9)\n    spot ==  1 ~  38,\n    spot ==  4 ~  14,\n    spot ==  9 ~  31,\n    spot == 21 ~  42,\n    spot == 28 ~  84,\n    spot == 36 ~  44,\n    spot == 51 ~  67,\n    spot == 71 ~  91,\n    spot == 80 ~ 100,\n    \n    # Chutes (10)\n    spot == 16 ~   6,\n    spot == 47 ~  26,\n    spot == 49 ~  11,\n    spot == 56 ~  53,\n    spot == 62 ~  19,\n    spot == 64 ~  60,\n    spot == 87 ~  24,\n    spot == 93 ~  73,\n    spot == 95 ~  76,\n    spot == 98 ~  78,\n    \n    # No change\n    TRUE ~ spot)\n}\ncheck_for_chute_or_ladder(1)\n\n[1] 38\n\ncheck_for_chute_or_ladder(4)\n\n[1] 14\n\ncheck_for_chute_or_ladder(5)\n\n[1] 5\n\n\nGreat. Now we’ve got a full turn. For kicks, I can wrap all this up into yet another function that’ll simulate taking a turn in the game:\n\ntake_turn &lt;- function(spot) {\n  spot %&gt;%\n    roll_die() %&gt;% \n    check_for_chute_or_ladder()\n}\ntake_turn(1)\n\n[1] 14\n\ntake_turn(3)\n\n[1] 6\n\ntake_turn(6)\n\n[1] 7\n\ntake_turn(10)\n\n[1] 15\n\ntake_turn(6)\n\n[1] 11\n\n\nSo as it turns out, these are all the functions I need to simulate an entire game. But, the way it’s set up now, I have to run each turn one at a time, check the output, and run it again. It would be better if I could automate the whole thing and save the results into a dataframe or something.\nSo I’ve created the larger simulate_game function below. When I run this function, it’ll simulate an entire (1-player) game. First, it’ll create a mostly empty data frame that will be populated as the turns advance. I know that some programming languages are slow if you try to append rows to a dataframe with each iteration of a loop, so I wanted to make sure there was room for a full game before we do anything else. Also, in theory, the game could last forever because of looping chutes and ladders, so I made it large enough to handle a game with 1000 turns—probably way too many for this, but I wanted to make sure. In that dataframe, I have columns for the turn number, what the dice roll was (those are all predetermined), where you landed, whether it has a chute or a ladder, and finally, where you ended up after traveling on that chute or ladder.I think Perl doesn’t care, and I miss that…I started with 100 turns, but that actually wasn’t enough turns for some of the simulated games!\nI’ve never done a simulation in R before, so I don’t know what the protocol is for looping through something an unknown number of times, so I did this whole while(keep_playing) thing. The keep_playing object is initially true, and at the end of each iteration, I check to see if we’ve gotten to 100; if so, I’ll set that to false, which’ll kill the loop. However, I wanted some sort of iterating number (like in a for loop), so I added i myself.I tried just looping through the turn_num column, but I couldn’t get the loop to stop after the player hit tile 100.\nOkay, so then inside that loop, there are several main chunks. Most of it is fluff for handling the data and keeping track of stuff and the actual game portion is just two lines in the middle.\n\nFirst, if it’s the first iteration of the loop, set the start tile number to 0. Otherwise, set it to wherever we ended up last time.\nThen, I add the dice roll to to the start tile to get the (potentially) temporary land tile. I then send that number off to check_for_chute_or_ladder and get the actual end tile.\nI then do another conditional to tell whether I had a chute or ladder. In theory, I should just be able to tell that with the check_for_chute_or_ladder function, but I’m not sure how to return two values at once in R like I can with Perl.\nFinally, I’ll do one more conditional to see if we’ve reached tile 100. If not, go on to the next iteration of the loop. If so, we’re done.\n\nAfter the looping is done, we’ve completed a game. Remember that I started by declaring enough space for 1000 turns. I don’t need all the extra rolls, so just before I return the dataframe with all the game information, filter out the rolls that didn’t happen.\n\nsimulate_game &lt;- function(game_num = 0) {\n  \n  # Declare space for the full game.\n  n &lt;- 1000\n  turns &lt;- tibble(turn_num = 1:n,\n                  start    = NA,\n                  roll     = sample(6, n, replace = TRUE),\n                  land     = NA,\n                  chute_or_ladder = NA,\n                  end      = NA)\n  \n  # Loop until the game is over\n  i &lt;- 1\n  keep_playing &lt;- TRUE\n  while(keep_playing) {\n    \n    # Step 1: Start at zero\n    if (i == 1) {\n      turns$start[[i]] &lt;- 0\n      \n      # Otherwise, start where the last turn ended.\n    } else {\n      turns$start[[i]] &lt;- turns$end[[i - 1]]\n    }\n    \n    # Step 2: This is where the game actually happens.\n    # Add dice roll to the start tile\n    turns$land[[i]] &lt;- turns$start[[i]] + turns$roll[[i]]\n    # Check for chute or ladder\n    turns$end[[i]] &lt;- check_for_chute_or_ladder(turns$land[[i]])\n    \n    # Step 3: Keep track of whether I had a chute or ladder.\n    if (turns$land[[i]] &gt; turns$end[[i]]) {\n      turns$chute_or_ladder[[i]] &lt;- \"ladder\"\n    } else if (turns$land[[i]] &lt; turns$end[[i]]) {\n      turns$chute_or_ladder[[i]] &lt;- \"chute\"\n    } else {\n      turns$chute_or_ladder[[i]] &lt;- NA\n    }\n    \n    # Step 4: Check if it's game over.\n    if (turns$end[[i]] &gt;= 100) {\n      keep_playing &lt;- FALSE\n    } else {\n      i &lt;- i + 1\n    }\n  }\n  \n  turns %&gt;%\n    filter(turn_num &lt;= i) %&gt;%\n    return()\n}\n\nNow let’s see it in action!\n\nsimulate_game()\n\n# A tibble: 21 × 6\n   turn_num start  roll  land chute_or_ladder   end\n      &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1        1     0     2     2 &lt;NA&gt;                2\n 2        2     2     6     8 &lt;NA&gt;                8\n 3        3     8     6    14 &lt;NA&gt;               14\n 4        4    14     2    16 ladder              6\n 5        5     6     1     7 &lt;NA&gt;                7\n 6        6     7     5    12 &lt;NA&gt;               12\n 7        7    12     5    17 &lt;NA&gt;               17\n 8        8    17     1    18 &lt;NA&gt;               18\n 9        9    18     1    19 &lt;NA&gt;               19\n10       10    19     6    25 &lt;NA&gt;               25\n# ℹ 11 more rows\n\n\nHooray! A complete game. This one appears to have been completed in just 15 moves after hitting four chutes and one ladder."
  },
  {
    "objectID": "blog/simulating_chutes_and_ladders/index.html#the-end",
    "href": "blog/simulating_chutes_and_ladders/index.html#the-end",
    "title": "Simulating Chutes and Ladders",
    "section": "The End",
    "text": "The End\nSo that’s it. Thanks for joining me on my journey of simulating Chutes and Ladders!"
  },
  {
    "objectID": "blog/jealousy-list-1/index.html",
    "href": "blog/jealousy-list-1/index.html",
    "title": "Jealousy List 1",
    "section": "",
    "text": "This year, FiveThirtyEight started a monthly Jealousy List, which is essentially a list of really cool articles they saw other people do that they wish they had been the ones to write. This is an idea they got from Bloomberg and I think others are starting to do their own as well. It’s kind of a fun way to showcase some of the best stuff that has come out recently and to share others’ work. I kinda like the idea so I thought I’d start an occasional jealousy list of my own.\nI’m a linguist at heart, but because I’m interested learning more about R and statistics, I tend to keep tabs on a lot of what’s going on in the data science world. I follow a lot of data scientists on Twitter and I’m also subscribed to something like 50 academics’ blogs that I read when I have a spare minute. Consequently, most of what I’ll post here will probably not be directly related to linguistics, but they’ll show off some really cool R skills. Things that I find really cool or I wish I could have been the one to do write it. I tend to share some of this kind of stuff on Twitter, but I thought I’d start compiling them here.\nSo, in no particular order, here is a list of some of the things I liked the most in the past few weeks/months. Some are tutorials, others are just information, but they’re all stuff I jealously wish I could have written:\n\nHelen Graham. “Now that’s what I call text mining and sentiment analysis”\nThis is an analysis of all the lyrics in the 100 Now! music albums. (Yeah, those are still a thing, and the UK series just released their hundredth album!) First, they use rvest for web scraping and geniusR for extracting the lyrics from genius.com. But the real fun comes from tidytext and looking at how word usage, sentiments, and other trends change over time. I’ve never done we scraping but I feel like maybe it’s not so hard after reading this.\nListen Data. “15 Types of Regression you should Know”\nThis is just a quick overview of lots of different kinds of regression analysis. I took a course in regression so I felt pretty confident going into this, but I learned about some cool types of models that I hadn’t heard of like Lasso, Poisson, and other fancier ones. There’s some brief R code and a simple explanation of how each is different from the others. I’m particularly interested in the Poisson regression and wonder how that might be used in corpus studies.\nLaura Ellis. “Highlighting with ggplot2: The Old School and New School Way”\nSomething that you might need to do in ggplot2 is to highlight data. This author explains two ways to do that. I had been doing what they refer to as the “old school” way, which is essentially overlay a second layer using just a subset of the data (which is exactly what I did here). After reading this, I think I should switch to the new school way, which uses Hiroaki Yutani’s gghighlight package. I can’t wait to try it out.\nJosef Fruehwald. “Why does Labov have such weird transcriptions?”\nThe title of this post is literally something I’ve asked myself. If I had really dug around for an answer, I probably could have found some of the early sources that explain it, but this blog post summarizes it all up nicely for you. One of the reasons is that the “transcriptional system is now encoding a phonological analysis.”\nTimo Grossenbacher. “Categorical spatial interpolation with R”\nI recently got a fair amount of spatial data and I’ve been meaning to learn how to visualize some of what I got. This post gives a complete tutorial for how to do k-nearest neighbor categorical interpolation using the kkNN package. Because this involves some pretty complicated calculations, I also learned I could utilize the multiple cores I have on my computer for super intense stuff, which is always fun. I ended up creating this map, thanks to this tutorial:\n\nAs far as what this map shows, well, no spoilers, but I’m waiting to hear back from ADS about whether I get to present on this data.\n\nI can say now that it shows prevelar raising. I presented on it at ADS and eventually published the results in American Speech.\nSo that’s my jealousy list. Text analysis, regression, ggplot2, vowel transcriptions, and GIS. That’s actually a nice smattering of the things I read about the most."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "",
    "text": "In the summer of 2016, I went to Cowlitz County, Washington to do traditional sociolinguistic interviews. I talked to 54 people and gathered my first audio corpus. It took a lot of preparation beforehand and it took a lot of time in the field. What I could not have expected was the amount of time it would take to transcribe that corpus. Now, two years later, I have finally finished transcriptions.\nThis come after a lot of work. Since others might be going through the same thing, I thought I’d share some thoughts on transcribing a sociolinguistic corpus."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#finding-the-motivation",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#finding-the-motivation",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "Finding the motivation",
    "text": "Finding the motivation\nI think my original goal was to have it all transcribed by the end of 2016. So I gave myself about five months. But then I did the first hour of audio and it took me about 5 hours. Yikes!  At that rate, I estimated it would take about 200 hours of work to finish. I think staring down the barrel of any 200 hour task is a motivation killer. So I put it off.I don’t know what I expected—of course it’s going to take a long time to transcribe!\nI wrote a blog post nine months later when I had my first wake up call that I needed to get transcriptions done. I talked about some of the struggles I had getting started but mostly made excuses for why I hadn’t done much. And I got a lot of work done over the next month or so and made it about a quarter of the way through. I remember though just getting burned out after just 10 or 15 minutes of work and would call it a day after half an hour. At that rate, yeah, it’ll take forever.\nSo I put it off for an entire year. In the meantime I was getting a lot done—mostly to distract me from the task I inevitably have to do before graduating. For some reason this distraction was in the form of collecting more audio. I got some laboratory audio, and gathered another corpus using Amazon Mechanical Turk, and in January I went out to Utah to do some more fieldwork. And yet, this audio from 2016 was collecting dust on my computer, just waiting to be analyzed. I think I found that it was easier to collect new data than it was to finish processing the old stuff. Consequently, I had collected something like 150 hours of audio over two years for various projects—and less than 5% of it was processed.\nWhen I finally defended my prospectus in April, it occurred to me that if I wanted to graduate in 2019, I needed to have data to write about. And the only way to do that was to transcribe that darn audio. So, that was what finally got me to crack down and work at this every day. Even then, it took three months of grinding to finally finish. But I’m so glad it’s done."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#a-rite-of-passage",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#a-rite-of-passage",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "A rite of passage",
    "text": "A rite of passage\nI mentioned as a part of my celebratory tweetstorm that doing this kind of work might be something like a rite of passage for sociolinguists.\n\n\nI think putting this much work into a corpus is some sort of rite of passage for sociolinguists. I'm glad I went through it, but ugh, never again.\n\n— Joey Stanley (@joey_stan) July 11, 2018\n\n\nIt seems like a lot of sociolinguists do research on their own corpora, and while the flashy part of statistical analysis, data visualization, or even fieldwork stories are what you see and hear about the most, a significant portion of what we do is the behind-the-scenes tedium on the computer. My university doesn’t have a huge group of sociolinguists and there’s no sort of shared corpus that we can use. So if I want to study contemporary spoken English, I was going to have to collect the audio myself. I think would have done fieldwork myself anyway though. I think it was always something I’ve wanted to do. Plus, there’s this:Yes, the Linguistic Atlas Project has been here since the 80s, but very few of those recordings are transcribed, so they’re of little use in their current state.\n\n\nAlso, shout out to @DrDialect, who I heard say at a Q&A at SECOL in 2015 something like, \"the best career move you can do is to create a corpus: you'll be able to analyze it forever.\" Some of the best advice I've ever heard.\n\n— Joey Stanley (@joey_stan) July 11, 2018\n\n\nAnd from the looks of it, this corpus that I now have is definitely going to last me a while, that’s for sure!"
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#what-software-did-i-use",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#what-software-did-i-use",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "What software did I use?",
    "text": "What software did I use?\nFor transcription, I think there are two ways of doing it. The first method is to find some software that will automatically transcribe it for you, and since it’s not going to be perfect, then spend the time to correct that transcription. I considered doing that, specifically using the transcriber in DARLA. But I found that it took much longer to correct the transcriptions that it would have taken me to just do it by hand. However, DARLA specifically says on their website that their automatic transcriber is not great, so my rate might have been better if I had used a different transcriber. DARLA was what came to mind because it’s easy to use and free. You might have better luck if you use a more sophisticated transcriber.\nThe other option therefore was to just do it myself. As far as I can tell, there are two or three main pieces of software you can use. One is Transcriber. This is one that we use in the Linguistic Atlas Office when we have our undergrads do transcriptions. It’s free and easy to use. One concern is that it’s a little bit tricky to get its output to a TextGrid format. The other concern was that I couldn’t see the spectrogram to accurately place boundaries. Another option is ELAN, which I hear is fantastic. The only reason I didn’t use it was frankly because I didn’t want to take the time to learn a new program.\nWhat I settled on was just plain ol’ Praat. It’s software that I’m comfortable with and I’ve used a lot. I can zoom in as close as I want so I can easy skip over stutters or other noise. Plus, I create a TextGrid right there, which is the format I’m most comfortable working with for scripting purposes. The downside to Praat is that I ended up having to use my trackpad on my laptop more than I wanted to (for scrolling side to side and placing boundaries). I wanted to avoid using my mouse as much as possible because I feel like it hurts my wrist more and I don’t want carpel tunnel.\nBased on my own experience, what I would recommend not doing is hiring out the transcriptions unless you’re not able to do it yourself. For one, I’m cheap, and didn’t want to pay however much per minute of audio. But more importantly, going through my audio a second time gave me a chance to pick up on things that I didn’t catch or forgot about when I was doing the interviews in person. Things like interesting linguistic phenomena  or passages I may want to quote later. Using the Praat textgrids, I just added a separate tier for my own annotations and could make whatever notes I wanted to about a particular section of audio. I learned so much about my people going through it a second time, and I don’t think I would have gotten those intuitions about their speech if I had hired it out. Of course, if you need the transcriptions sooner than you can process them or if you’re not able to do the work yourself, then of course hiring it out might be the better option.I got a token of liketa and two people said I and John instead of John and I which was super cool. I don’t remember those specifically and would never have caught them if I didn’t do the transcriptions myself."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#the-next-steps",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#the-next-steps",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "The next steps",
    "text": "The next steps\nSo while finishing those transcriptions was a monster step, unfortunately the work wasn’t done.\n\n\nNow I've just got to do forced alignment (which includes spell checking) and extract some formants and I'll be ready to go!\n\n— Joey Stanley (@joey_stan) July 11, 2018\n\n\n\nForced alignment\nI’ve been using DARLA for the past few years, but I had some trouble getting the long audio files to process using their web interface. So this gave me a great opportunity to download and install the Montreal Forced Aligner on my own computer. Having this in-house provides lots of benefits like processing the files in bulk and quicker turnaround time since I don’t have to upload the files.\nThe bad news was that I had to do the spell-checking myself. I completely took for granted that DARLA can handle out-of-dictionary words by guessing their pronunciation. So since the Montreal Forced Aligner doesn’t do that, I had to check the words myself. When you run it, it’ll produce a list of out-of-dictionary words for you, so all you need to do is add them to the dictionary or correct the spelling in Praat. It seems simple, but it takes a long time. I had at least 20 or 30 typos or new words in every interview, so I probably spent 15 or 20 hours just doing the spell-checking (I think I added over 1000 new dictionary entries too!).\nLuckily, all this was made easier with the help of some custom Praat scripts I wrote for this project. One does pre-processing to get the files ready for forced-alignment. It splits the audio and textgrid into two halves (it was easier to process that way), it moves these files into a specific directory, and renames the tiers so that they’re consistent. As a bonus, it spits out the command that I need to use to run the aligner on those specific files, so all I need to do is copy and paste that into my terminal and it’ll go on its merry way. This was super helpful because typing path names over and over got old real quick.\nOnce the spell-checking was done and the files were aligned, I had a post-processing script that I used. This one rejoins the two halves into one TextGrid again, adds the new phoneme and word tier to the top of the main TextGrid (so I’ve got the phoneme, word, sentence, and other tiers all in one file), and saves this in that speaker’s directory on my hard drive. Super handy.\nNow ideally, I would go back and hand-check all the boundaries. Maybe one day I’ll have the time to do that, but oh my goodness that’s not going to happen any time soon.\n\n\nFormant extraction\nSo keep in mind that all this work, the nearly 200 hours I’ve put into transcribing and force aligning, was mostly just so I could have Praat know where the vowels were in the audio.\nSo, I modified a couple scripts I wrote to do formant extraction. Of course, I’ve mostly worked with shorter passages of audio (word lists and reading passages and stuff), so what I didn’t anticipate was that Praat kind of has trouble working with audio longer than about 30 minutes. So I had to modify the script so that it splits the audio into roughly five minute chunks, processes each one individually, and then stitches all the output back together.\nAnd of course, the formant measurements ideally should be handchecked. But again, I just spent way too much time transcribing, so I’m not about to spend even more time hand-checking these. Not yet at least."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#the-end-result-a-giant-spreadsheet",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#the-end-result-a-giant-spreadsheet",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "The end result: A giant spreadsheet!",
    "text": "The end result: A giant spreadsheet!\nSo what were the main steps here?\n\nCollect audio.\nTranscription.\nForced alignment.\nFormant extraction.\n\nWhat do I have now? A giant spreadsheet. All this work has been so that I can get a big ol’ spreadsheet that I can then analyze in R. That’s where I am right now. I’ve got the finalized dataset that I’ll use for my dissertation, so I don’t even need to open up Praat much anymore, or even plug in my external hard drive. Almost all my work is in R now. But it is quite satisfying to have this monster spreadsheet of my own data."
  },
  {
    "objectID": "blog/transcribing-a-sociolinguistic-corpus/index.html#conclusions",
    "href": "blog/transcribing-a-sociolinguistic-corpus/index.html#conclusions",
    "title": "Transcribing a Sociolinguistic Corpus",
    "section": "Conclusions",
    "text": "Conclusions\nTranscribing (and the subsequent processing of) a sociolinguistic corpus takes a ton of time, patience, diligence, and determination. My eyesight may have suffered a little bit from staring at the computer, my headphones are a little worn down, my keyboard has had to endure well over a million keystrokes, and my wrists and fingers sure took a hit. But, y’know what? It’s a lot better than it used to be. At least we have tools like forced-alignment, FAVE, and Praat to make our lives easier. But in the end, it is really awesome to have completed this corpus."
  },
  {
    "objectID": "blog/linguistic-atlas-of-the-pacific-northwest/index.html",
    "href": "blog/linguistic-atlas-of-the-pacific-northwest/index.html",
    "title": "The Linguistic Atlas of the Pacific Northwest",
    "section": "",
    "text": "As a part of my research assistantship this year, I work with the Linguistic Atlas Project, under the direction of Dr. William Kretzschmar. It’s an exciting project to be a part of.\nThere is a lot going on in the lab right now. We’ve got a team of over a dozen undergrad transcribers working dutifully on an NSF grant awarded to Kretzschmar and Dr. Peggy Renwick, not to mention the web developers for the Atlas Project and for Complex Systems.\nOne of the things I’m excited about is that I now have access to all the data for the Atlas Projects from over half a century ago. In a nutshell, what happened is that in the 30s and through the 50s and 60s, Hans Kurath and a team of researchers set out to document the language geography of the United States and Canada. Armed with whatever recording devices they could afford, several hours’ worth of interview questions, and expert phonetic transcribers, they set out to document all the accents and dialects of English in North America.\nThey were partially successful. Starting in New England and in the East, they talked to a couple thousand people, painstakingly analyzed the data, and published a couple multi-volume works specifically focused on certain areas of the United States. Thus, we have the Linguistic Atlases of New England (LANE), the Gulf States (LAGS), and the Upper Midwest (LAUM). However, funding was cut short. Realizing they may have bitten off more than they can chew, the data collected in other portions of the country was never published, other than some brief overviews by some of the researchers. Time passed, and for one reason or another, the majority of this unpublished data disappeared into obscurity.\nBy the late 1970s and into the 1980s, the original handwritten field notes and any extant recordings were scattered across multiple locations. The original researchers’ dream to publish this data for a general audience was never fulfilled, let alone the majority of potential publications for a more specialized audience. The data was always supposed to be accessible to anyone interested, and just a few decades later it was collecting dust in basements, accessible to probably the half a dozen people that knew about it.\nIn 1983 some of the data was under threat of being thrown out. Luckily, William Kretzschmar offered to take all the data from all projects and house it at the University of Georgia. Since then, he has been in the process of realizing the original researchers’ dream of making the data accessible. In the 21st century, that means digitizing it all and putting it online. And there has been success in that endeavor.\nThis is where I come in. As a lowly out-of-state grad student, I’m not particularly concerned with language around Georgia, as interesting as it is. I do however like research on the opposite side of the country: the Pacific Northwest. Only after reading about the Linguistic Atlas of the Pacific Northwest (LAPNW) did I realize that all that data was being housed by my own university. So as soon as I was offered the assistantship in the Linguistic Atlas office, I expressed interest in the LAPNW data. Well, just today, I made a visit to the repository where all the data is held.\nAfter sitting alone for half an hour on the concrete floor literally in the furthest corner in that warehouse, I was quickly able to assess the situation. From what I’ve been able to tell, there are just four boxes of LAPNW data. Compared to the dozens of boxes in the large-scale projects (LANE, LAGS, etc.), it’s a meager project. One box contains the original handwritten notes for about half of the 51 participants, which is great, but I’m a little sad that some of the originals have been lost. But the other three boxes were all copies, including a complete set for all participants and another partial set. I don’t know who did the photocopies or when they were done, but I’m really glad we have them.\nSo, I brought them back to the office and I’ve started to look through them. It’s a bit exciting for me actually. Since being housed at UGA, I don’t know if these boxes have been opened by anyone. As far as I know, there are probably ten people in the world that would be interested in the LAPNW data, and certainly none of them have had the ability to peruse UGA’s repository. So this stuff literally hasn’t seen the light of day for decades. I don’t know what I’m going to do with this goldmine, but I’d sure like to revive it somehow and possibly do what I can to make it accessible. It’s an exciting time for me."
  },
  {
    "objectID": "blog/365-papers-update/index.html",
    "href": "blog/365-papers-update/index.html",
    "title": "#365 Papers (Update)",
    "section": "",
    "text": "At the beginning of 2018, I set the ambitious goal of reading 365 papers during that year. I tweeted about it and blogged about it, but ultimately didn’t achieve my goal. Turns out 365 is a lot. Well, after 1338 days, I can finally say I’ve ready 365 papers! So here’s just some visuals to see what kinds of things I’ve been reading.\nWhat counts as a “paper” and what counts as “reading” it? I didn’t have any hard and fast rules, but these were the guidelines I laid out before starting.\nIn a few cases, I counted full books as a single entry, like if they had short chapters. I think a 3-page chapter of a book shouldn’t count the same as an article in Language, for example. Similarly, a lot of the Masters Theses I read were shorter and about the same as an article reading so those counted as one.\nI will say that re-reading something counts a second time if I do it just as thoroughly. Things like textbook chapters from classes I’m teaching are the main culprit, but it’s nice to revisit things after a few years.\nAnd to be clear, this doesn’t represent all the papers I’ve ever read. In fact, I’d say the bulk of reading for my dissertation happened before I started keeping track. I’ve kept decent notes about what I’ve read since about 2010, but I’ll just focus on the most recent 365 for now."
  },
  {
    "objectID": "blog/365-papers-update/index.html#pace",
    "href": "blog/365-papers-update/index.html#pace",
    "title": "#365 Papers (Update)",
    "section": "Pace",
    "text": "Pace\nIf I wanted to read 365 papers in a year, that’s obviously one paper a day. What was my actual pace and did it change? The following plot answers this. From left to right are the months of the year. The colored lines go up as I finished a paper that year. In dashed gray lines, I have benchmarks for where the colored lines would be if I had maintained a constant rate.\n\nLooks like in 2018 and 2019 (when I was in the throes of dissertation-writing), my pace was usually somewhere around one paper every 4 to 8 days. So about one a week or occasionally two a week, on average. Starting in 2020 and continuing into this year, my pace is quicker and I’m reading a paper at least every three days on average.\nMy pace ebbed and flowed within a single year quite a bit and it’s interesting to see the patterns. In August of 2018 for example, I started really hunkering down and writing my dissertation, so there’s a sudden increase in pace (in the blue line). In early 2019 you can see I read in short bursts (I binge-read several 3rd Wave sociolinguistics papers). In June 2019 I took GIS and Stats courses so that uptick was from those classes. In September I was in a data visualization phase. And it looks like the time between when I submitted my dissertation and when I defended it (in December 2019), I didn’t do much reading at all.\nMy pace went up quite a bit in 2020 as I was transitioning from dissertation work to teaching. I read some material related to my job talk and was working on submitting my chapter in Speech in the Western States: Volume III. The biggest jump was in March 2020. Yes, that’s when COVID hit, but I was also fortunate to be hired as an “instructional designer” for BYU so I was prepping a course and doing a lot of reading. Things waned as I moved to Utah but when Fall semester hit, I kept that pretty quick pace up as I was prepping two new courses. This continued into 2021 as I prepped another two new courses. And you can see my recent uptick as I start getting ready to teach again."
  },
  {
    "objectID": "blog/365-papers-update/index.html#content",
    "href": "blog/365-papers-update/index.html#content",
    "title": "#365 Papers (Update)",
    "section": "Content",
    "text": "Content\nSo now that we’ve got the pace covered, let’s look at the content itself.\n\nYears\nFirst, I’ll show the publication years of the things I read. Note that I do have two colums for “no date” and forthcoming: those are mostly reviews I did or other sneak-peaks at unpublished work.\n\nI’m happy to see that a large proportion of what I read was recent, having come out since I started this little project. Looks like half of the papers I read came out in 2008 or later (or rather, within the last 10–14 years); a third was 2017 or later (the last 1–4 years). I honestly wish I had read even more recent stuff though because I feel a little behind the times. A quarter of what I read was before 1993. It’s good to read the classics, but I think I need to be staying more up to date though. Something that certainly accounts for this older skew is that I read while walking and the things I read are typically older (Trudgill 1978, Petyt 1980, Preston 1989, etc). I’m happy I read some older things, but I wish this plot had been more skewed towards the right.\n\n\nTopics\nNext, here’s a plot of the broad topic the papers fell in. I only gave each paper a single tag, and sometimes the decision to call something sociolinguistics vs dialectology, for example, was somewhat arbitrary. But this should give you a rough idea of what things I read.\n\nIt should come to no surprise that most of what I read was sociolinguistic in nature, followed closely by dialectology. The socio stuff is relevant to research and teaching and the dialectology stuff is mostly for research. Phonetics and statistics coming next are also exactly what I’d expect. I wish I had a bit wider range of topics though so that I can be more well-rounded of a linguist.\n\n\nPublication Type\nNext, here’s a basic plot on the publication type. I’ve divided everything into three broad categories: journal articles (which include conference proceedings), monographs, and edited volumes.\n\nThis is where I think I fall short. I’m happy to see that journal articles were the most common, but I think I should be reading a higher proportion of newer articles than I am. In fact, monographs and edited volumes combined make up 54% of what I read. This may also be because I read as I walk to and from my car and around my building, so I do get more regular book-reading time than sit-and-read-an-article time.\n\n\nPublication Venue\nFinally, the publication venue. Just focusing on the journal articles, here’s a list of the venues I read from the most. (I’ve filtered out venues that I only read from once or twice, for space issues).\n\nBased on my research, it should come as no surprise that American Speech and LVC are the top two. I have a print subscription to American Speech, and I had a habit of reading through all the articles while on the bus. My guess is that the PWPL ones are mostly proceedings from NWAV too. JASA, JEngL, and J. Soc. are also not much of a surprise.\nHowever, this plot again highlights what I think are my shortcomings. I feel like I need to be reading more Language in Society and Journal of Sociolinguistics. I also think I need to be reading about languages other than English too. I feel like I’ve latched on to my venues to my detriment and I’m missing a lot of interesting work by not expanding my horizons."
  },
  {
    "objectID": "blog/365-papers-update/index.html#outlook",
    "href": "blog/365-papers-update/index.html#outlook",
    "title": "#365 Papers (Update)",
    "section": "Outlook",
    "text": "Outlook\nI’m glad I did this exercise ad I’ll certainly continue with it. And looking back at the past 3⅔ years has been enlightening to say the least. My goals for the next 365 papers are the following:\n\nRead a larger proportion of journal articles. Specifically, I want to go from 45% journal articles to 65%.\nRead a larger proportion of newer articles. Specifically, I want over half of what I read to be since 2015 (6–9 years old).\nFinish 365 papers sooner. It took 44 months to do this first batch. Since I’ve been keeping a pretty good pace of a paper every three days, I’ll aim for 36 months and finish by August 31, 2024.\n\nI also want to get through my to-read list. I have about 50 papers on it. It is always growing, and as I read more I find more things to read. But my current list has been nagging me so I want to knock out a bunch of those."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joey Stanley",
    "section": "",
    "text": "Important\n\n\n\nAs of September 20, 2023, I am in the process of migrating my website from Jekyll to Quarto. This is the new Quarto version. It is buggy, many of the links are broken, and content is missing. I will slowly add content when I have time. Please go to joeystanley.com for the complete version of my website.\nI’m an Assistant Professor in Department of Linguistics at Brigham Young University specializing in sociophonetics, dialectology, and quantitative methods. In May 2020, I received my Ph.D in linguistics from the Department of Linguistics at the University of Georgia.\nMy primary area of research is on English in the western United States. My dissertation focused on English in Cowlitz County, Washington with an emphasis on uncovering variation in vowel formant trajectories. I am involved in several research projects on Utah English (vowels, consonants and intonation), including a long-term project analyzing a collection of 750 interviews with residents of Heber, Utah born before WWII.\nAnother strand of my research is focused on sociophonetic data analysis methods.  Recently, I’ve run simulations on real and artificial sociophonetic data, uncovering some overlooked aspects of methods that may in fact be rather important, like order of operations and Pillai scores.\nI publish under the more grown-up-sounding version of my name, Joseph A. Stanley, but anyone who has ever met me knows I go by Joey.  I live in Spanish Fork with my wife (Kelly) and kids (Lena, Walter, and Douglas). I enjoy running, breadmaking, succulents, and playing the organ too loudly. A description of my idiolect can be found here."
  },
  {
    "objectID": "index.html#what-am-i-up-to-right-now",
    "href": "index.html#what-am-i-up-to-right-now",
    "title": "Joey Stanley",
    "section": "What am I up to right now?",
    "text": "What am I up to right now?\nAs of July 10 and through the end of 2023, I am…\n\n\n\n\n\n\n\n\n\n\n👶🏻 Doing nothing but taking care of a newborn."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I am currently teaching…\nPreviously, I’ve taught…\nThe rest of this page serves as a repository for teaching materials and workshops I have prepared."
  },
  {
    "objectID": "teaching.html#courses",
    "href": "teaching.html#courses",
    "title": "Teaching",
    "section": "Courses",
    "text": "Courses\n\nPhonetics and Phonology\nI taught this course Fall 2017 and Spring 2019 at UGA. Here is a syllabus.\nPraat Tutorial—For homework, I often assign students mini-projects that involve the use of Praat. This assignment serves as a useful tutorial to downloading and running Praat for the first time. In it, I give detailed instructions on how to make and transcribe a sentence.\nFeatures Chart—When I was preparing to teach features for the first time, I found it hard to keep track of all of them so I started making a chart. It turned out to be pretty useful and I thought my students might like it too. It’s meant to act as reference rather than instruction, so you’ll still have to read through the relevant chapters to understand everything. It’s a lot of information to cram into a single page, so it’s still a work in progress to make it look better. It’s sort of one of those things that makes sense to the person who made it and no one else, so you’re milage may vary. I should have an accompanying “How to use this chart” guide. Maybe next time.\n\n\nEnglish Phonetics and Phonology\nI taught this Winter 2022 and Winter 2023. It’s similar to the Phonetics and Phonology course I taught already, but it cuts out most non-English material and goes into more depth into English. Here is a syllabus.\n\n\nIntroduction to Sociolinguistics\nI taught this undergraduate version of sociolinguistics in Winter 2021. You can look through the syllabus here. We followed Allan Bell’s The Guidebook to Sociolinguistics for the most part, though I threw in some additional topics towards the end.\n\n\nSociolinguistics\nI taught this course in Winter 2021, Fall 2021, and Fall 2022. This is the graduate-level version of the Introduction to Sociolinguistics course above (so it’s still introductory course) and is similar to the undergraduate version except we take one day a week to read and discuss articles. Here is my most recent syllabus.\n\n\nAfrican American English\nI taught this course Spring of 2023 under the auspices of the senior capstone course, as well as as a linguistics elective, in the linguistics department. (So, technically it was cross-listed as LING 495R: Senior Capstone, ELING 495R: The Senior Course, LING 421R: Studies in Linguistics, and ELING 421R: Studies in Language or Editing).\nThe capstone courses in my deaprtment are designed to be open-ended and end up being special topics courses that cover a wide range of subfields in linguistics. Most of the faculty who had been teaching these capstone courses though had recently retired, so we have been in dire need for new courses to put into the rotation. I had wanted to teaching something about language ideology/discrimination/attitudes but I was asked to do something to do with American English; we settled on African American English, which I think was a nice compromise.\nSo, with the help and input from many friends and colleagues, I designed and prepped the course from scratch, using Lisa Green’s African American English as my main textbook. I went into the course knowing some things here and there about AAE from just being exposed to the literature on it in sociolinguistics, and it turned out to be a very challenging but richly rewarding course for me. Here is the syllabus.\n\n\nSociolinguistic Fieldwork\nThis is a special topics course that I taught with Lisa Johnson in Winter 2023. We used Natalie Schilling’s textbook of the same name as our guide and did a Utah County–based implementation of similar courses taught in other universities. (Our “fieldsite” that we chose was the city of Lehi.) The course involved going out to the field to conduct sociolinguistic interviews. You can see a syllabus here.\n\n\nIntroduction to Varieties of English\nI’ve taught this course four times so far. In Summer 2020, it was completely online and had a focus on the British Isles to make up for the study abroad to the UK and Ireland that was cancelled due to covid. In Fall 2020, Fall 2021, and Fall 2022 we covered around 30 varieties of English in three broad units: North America, the British Isles, and everywhere else. Here is my syllabus from Fall 2022.\n\n\nLinguistics Tools 1\nI taught this course in Fall 2020 and Fall 2021. It covers how to conduct a linguistics study and covers software like Zotero, Word, Excel, Praat, AntConc, Qualtrics, and Jamovi with an introduction to statistics scattered throughout. Here is a syllabus from when I taught it in Fall 2021.\n\n\nLinguistic Data Analysis\nThis is a special topics course that I taught with Earl Brown in Fall 2022. We be built upon the version of the course he taught in Winter 2020. Here is the syllabus we started off with (though it did change quite a bit in the end).\n\n\nResearch Design in Linguistics\nI team-taught this course in Winter 2021 with Dan Dewey and then by myself Winter 2022. Students end the semester with a prospectus for their MA thesis and walk away with some introductory statistics. Here is my most recent syllabus."
  },
  {
    "objectID": "teaching.html#workshops",
    "href": "teaching.html#workshops",
    "title": "Teaching",
    "section": "Workshops",
    "text": "Workshops\n\nLaTeX Workshops\nCaleb Crumley, Jonathan Crum, and I led a series of three workshops on LaTeX as a way to introduce the new UGA Dissertation LaTeX template. I discussed basic LaTeX skills, Caleb showed how to use the template, and Jonathan illustrated more advanced topics. This series got the stamp of approval from the UGA Graduate School and had over 100 registered attendees.\n\n\nPraat Scripting Workshops\nLisa Lipani and I led three workshops on Praat and Praat scripting. We discussed the basics of the Praat interface, how to code basic things, and then did one devoted to automatic extraction of formant measurements.\n\n\nData Visualization Workshops\nIn a suite of five workshops, I discussed data visualization. Three of them were focused on ggplot2 (see the R workshops) but two were platform-independent and discussed the use of color and Edward Tufte’s principles of data visualization. Though they’re outside my discipline, I really enjoyed these workshops.\n\n\nR Workshops\nI have led more than a dozen workshops on R on a variety of topics. In addition to just an intro to R, I’ve talked about the tidyverse, RMarkdown, and Shiny. I’ve done a handful just on ggplot2, and I have handouts ready for more detailed workshops on customizing plots in ggplot2 for future workshops.\n\n\nProfessionalization (“Brand Yourself”) Workshops\nIn this workshops, I discuss ways to boost your online presence as an academic. I’ve given this workshop several times over the past few years and it has evolved quite a bit based on my own experiences. The gist: make a website and possibly also get active on Twitter.\n\n\nAcademic Poster Workshop\nI gave a brief—and very opinionated—workshop on how to make an academic poster. I discuss overall design and layout, nit-picky things like font sizes and color, and content.\n\n\nExcel Workshop\nI once gave a workshop on Excel. It was a while ago."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "This page highlights some of my research, organized thematically. I am a dialectologist, so several of my research interests are on specific regions. But intersecting these regions is where the sociolinguist in me comes out, and I try to uncover the meaning in linguistic variants—particularly the infrequent ones. And I go about these questions in these areas as a phonetician, analyzing speech production data and vowel trajectories. Finally, in nearly every study, I put on my quantitative linguist/data scientist/statistician cap when I actually do the analysis and interpretation. I think it’s a great intersection of subdisciplines within linguistics and I’m kept on my toes trying to keep up with each field."
  },
  {
    "objectID": "research.html#utah-english",
    "href": "research.html#utah-english",
    "title": "Research",
    "section": "Utah English",
    "text": "Utah English\n\nAnother area I focus on in the West is Utah. I worked on a poster involving vowel mergers over time in a single speaker. Most recently, I’ve been working with Kyle Vanderniet on consonantal variation in Utah English, as in the words mountain and false or in word-final -ing. I was also able to conduct additional fieldwork in Utah County but I’d really like to do some more. Utah is a unique place: its settlement history and highly concentrated Mormon population has had an impact on the English spoken in the region. I think it deserves some more attention by sociolinguists and dialectologists.\n\nCV Highlights\n\nJoseph A. Stanley. “Utahns sound Utahn when they avoid sounding Utahn.” The 97th Annual Meeting of the Linguistic Society of America. Denver, CO. January 6. 2023. \nJoseph A. Stanley & Lisa Morgan Johnson. “Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English.” The 96th Annual Meeting of the Linguistic Society of America. Washington, D.C. January 6–9, 2022. \nJoseph A. Stanley (2019). “(thr)-Flapping in American English: Social factors and articulatory motivations.” Proceedings of the 5th Annual Linguistics Conference at UGA, 49–63.  \nJoseph A. Stanley & Kyle Vanderniet (2018). “Consonantal Variation in Utah English.” Proceedings of the 4th Annual Linguistics Conference at UGA, 50–65.  \nAlso, keep an eye out for blog posts tags with Utah!"
  },
  {
    "objectID": "research.html#cowlitz-county-washington",
    "href": "research.html#cowlitz-county-washington",
    "title": "Research",
    "section": "Cowlitz County, Washington",
    "text": "Cowlitz County, Washington\n In grad school, I did some fieldwork in Cowlitz County, Washington, and since not a lot of research is done in the Pacific Northwest, I’ve been trying to document the English spoken there. In my dissertation, I showed that the younger people are shifting their front lax vowels like in other parts of the US and Canada. I’ve also done a couple spin-off studies on bag-raising, prelateral mergers, and prerhotic mergers. It’s a fun area and look forward to uncovering more detail on southwest Washington!\n\nCV Highlights\n\nJoseph A. Stanley. “Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift.” Poster presentation at the 181st Meeting of the Acoustical Society of America (ASA). Seattle, WA. November 29, 2021. \nJoseph A. Stanley (2020). “The Absence of a Religiolect among Latter-day Saints in Southwest Washington.” In Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall (eds.) Speech in the Western States Volume III: Understudied Dialects. (Publication of the American Dialect Society 105), 95–122. Durham, NC: Duke University Press. https://doi.org/10.1215/00031283-8820642. \nJoseph A. Stanley (2020). “Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington.” Ph.D Dissertation. University of Georgia, Athens, GA. \nJoseph A. Stanley (2018). “Changes in the Timber Industry as a Catastrophic Event: bag-Raising in Cowlitz County, Washington” Penn Working Papers in Linguistics, 24(2)."
  },
  {
    "objectID": "research.html#the-south",
    "href": "research.html#the-south",
    "title": "Research",
    "section": "The South",
    "text": "The South\n Having firmly established myself as a researcher of Western American English, I was reluctant to start doing research on the South since the amount of existing literature is vast. But, as a research assistant for the Linguistic Atlas Project for four years, I couldn’t help but begin analysis on DASS, a newly transcribed corpus of interviews from the 1970s and 1980s. Together with Peggy Renwick and the others at the Linguistic Atlas team, we’ve dug deep into the phonetics of how southerners, and more specifically Georgians, sounded a couple generations ago. You can see and interact with some of this data by going a site I made called the Gazetteer of Southern Vowels.\n\nCV Highlights\nJoseph A. Stanley, Jon Forrest, Lelia Glass, & Margaret E. L. Renwick. “Perspectives on Georgia vowels: From legacy to syncrhony.” The American Dialect Society Annual Meeting. Washington, D.C., January 6, 2022. \nJoseph A. Stanley, Margaret E. L. Renwick, Katie Ireland Kuiper, & Rachel Miller Olsen (2021). “Back vowel dynamics and distinctions in Southern American English.” Journal of English Linguistics 49(4): 389–418. https://doi.org/10.1177/00754242211043163. \n\n\nMargaret E. L. Renwick & Joseph A. Stanley (2020). “Modeling dynamic trajectories of tense vs. lax vowels in the American South.” Journal of the Acoustical Society of America 147(1): 579–595. doi: 10.1121/10.0000549. \nJoseph A. Stanley & Margaret E. L. Renwick. “Back vowel distinctions and dynamics in Southern US English.” The 94th Annual Meeting of the Linguistic Society of America. New Orleans, LA. January 2–5, 2020. \n\nRachel M. Olsen, Michael L. Olsen, Joseph A. Stanley, Margaret E. L. Renwick, & William A. Kretzschmar, Jr. (2017). “Methods for transcription and forced alignment of a legacy speech corpus.” Proceedings of Meetings on Acoustics 30, 060001; doi: http://dx.doi.org/10.1121/2.0000559."
  },
  {
    "objectID": "research.html#methods-in-sociophonietic-data-analysis",
    "href": "research.html#methods-in-sociophonietic-data-analysis",
    "title": "Research",
    "section": "Methods in Sociophonietic Data Analysis",
    "text": "Methods in Sociophonietic Data Analysis\nI’m increasingly becoming preoccupied with how sociophoneticians process their data.\nJoseph A. Stanley & Betsy Sneller. 2023. Sample size matters in calculating Pillai scores. Journal of the Acoustical Society of America 153(1). 54–67. https://doi.org/10.1121/10.0016757.  \nJoseph A. Stanley (2022). “Interpreting the Order of Operations in Sociophonetic Analysis.” Linguistics Vanguard 8(1). 279–289. https://doi.org/10.1515/lingvan-2022-0065.  \nJoseph A. Stanley (2022) “Order of Operations in Sociophonetic Analysis,” University of Pennsylvania Working Papers in Linguistics: Vol. 28: Iss. 2, Article 17. Available at: https://repository.upenn.edu/pwpl/vol28/iss2/17"
  },
  {
    "objectID": "research.html#vowel-formant-trajectories",
    "href": "research.html#vowel-formant-trajectories",
    "title": "Research",
    "section": "Vowel Formant Trajectories",
    "text": "Vowel Formant Trajectories\n Traditional sociophonetic research analyzes vowels using a pair of measurements, usually somewhere near the midpoint of the vowel. We can get a greater understanding of vowels by extracting data at multiple timepoints per vowel. In particular, I think there is important sociolinguistic meaning encoded in formant trajectories. Much of my recent research has gone this route, and has used generalized additive mixed-effects models to analyze the resulting data. This type of analysis yields complex results and visualizations, but I think we’re starting to get a better idea of how vowels work while uncovering exciting new research questions and possibilities.\n\nCV Highlights\nJoseph A. Stanley & Lisa Morgan Johnson. “Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English.” The 96th Annual Meeting of the Linguistic Society of America. Washington, D.C. January 6–9, 2022. \nJoseph A. Stanley, Margaret E. L. Renwick, Katie Ireland Kuiper, & Rachel Miller Olsen (2021). “Back vowel dynamics and distinctions in Southern American English.” Journal of English Linguistics 49(4): 389–418. https://doi.org/10.1177/00754242211043163. \nJoseph A. Stanley. “Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift.” Poster presentation at the 181st Meeting of the Acoustical Society of America (ASA). Seattle, WA. November 29, 2021. \n\nJoseph A. Stanley (2020). “The Absence of a Religiolect among Latter-day Saints in Southwest Washington.” In Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall (eds.) Speech in the Western States Volume III: Understudied Dialects. (Publication of the American Dialect Society 105), 95–122. Durham, NC: Duke University Press. https://doi.org/10.1215/00031283-8820642. \nJoseph A. Stanley (2020). “Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington.” Ph.D Dissertation. University of Georgia, Athens, GA. \nMargaret E. L. Renwick & Joseph A. Stanley (2020). “Modeling dynamic trajectories of tense vs. lax vowels in the American South.” Journal of the Acoustical Society of America 147(1): 579–595. doi: 10.1121/10.0000549."
  },
  {
    "objectID": "research.html#infrequent-phonological-variables",
    "href": "research.html#infrequent-phonological-variables",
    "title": "Research",
    "section": "Infrequent Phonological Variables",
    "text": "Infrequent Phonological Variables\n I’m interested in phenomena on the margins of English phonology. There are some speech patterns that are quite infrequent because the particular sequence of sounds only exists in a handful of words. For example, I’ve found phonological and regional patterns in words with /ɛɡ/ (beg, leg). I’ve also found that people in Utah have a tap in /θɹ/ clusters (three, throw) but Washingtonians don’t. Utahns also insert stops in /ls/ clusters (false, salsa). How infrequent can a variable be and still exhibit language-internal, regional, and sociolinguistic variability?\n\nCV Highlights\nJoseph A. Stanley (2022). “Regional patterns in prevelar raising.” American Speech. 97(3): 374–411. http://doi.org/10.1215/00031283-9308384.\nJoseph A. Stanley & Lisa Morgan Johnson. “Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English.” The 96th Annual Meeting of the Linguistic Society of America. Washington, D.C. January 6–9, 2022. \nJoseph A. Stanley. “Methodological considerations in the study of infrequent phonological variables: The case of English /eɡ/ and /ɛɡ/.” Word-specific phenomena in the realization of vowel categories: Methodological and theoretical perspectives (LabPhon 17 Satellite Workshop). Vancouver, British Columbia[Online]. September, 2020.\nJoseph A. Stanley (2019). “Phonological Patterns in beg-Raising.” UGA Working Papers in Linguistics, 4, 69–91.  \nJoseph A. Stanley (2019). “(thr)-Flapping in American English: Social factors and articulatory motivations.” Proceedings of the 5th Annual Linguistics Conference at UGA, 49–63.  \nJoseph A. Stanley & Kyle Vanderniet (2018). “Consonantal Variation in Utah English.” Proceedings of the 4th Annual Linguistics Conference at UGA, 50–65."
  },
  {
    "objectID": "research.html#mormonese",
    "href": "research.html#mormonese",
    "title": "Research",
    "section": "“Mormonese”",
    "text": "“Mormonese”\n Mormons, more properly referred to as members of the Church of Jesus Christ of Latter-day Saints, were once distinct enough to be considered an ethnic group. Today, their behavior, dress, and culture are much more mainstream, but their speech patterns might not always be. I’m curious about how Mormons talk, particularly in relation to non-Mormons and ex-Mormons. I’d also like to disentangle Mormonese from Utah English, because while they must overlap, I think they’re different in subtle ways. My new position at Brigham Young University will certainly give me better access to this population and I hope to soon uncover some new findings on Mormonese.\n\nCV Highlights\nJoseph A. Stanley (2020). “The Absence of a Religiolect among Latter-day Saints in Southwest Washington.” In Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall (eds.) Speech in the Western States Volume III: Understudied Dialects. (Publication of the American Dialect Society 105), 95–122. Durham, NC: Duke University Press. https://doi.org/10.1215/00031283-8820642. \nJoseph A. Stanley (2019). “(thr)-Flapping in American English: Social factors and articulatory motivations.” Proceedings of the 5th Annual Linguistics Conference at UGA, 49–63.  \nJoseph A. Stanley & Kyle Vanderniet (2018). “Consonantal Variation in Utah English.” Proceedings of the 4th Annual Linguistics Conference at UGA, 50–65.  \nStanley, Joseph A. (2016). “When do Mormons Call Each Other by First Name?” University of Pennsylvania Working Papers in Linguistics, 22(1)."
  },
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "Joey Stanley",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "todo.html#top-level-pages",
    "href": "todo.html#top-level-pages",
    "title": "Joey Stanley",
    "section": "Top-level pages",
    "text": "Top-level pages\nIndex.qmd * Instead of linking to OoO and Pillai papers, link to the appropriate section in research.qmd.\nThings to add to idiolect * CURE lexical set * American Raising detail. When that’s done, go back to /ar/-raising post and link to it. * /ar/-Raising, with link to the blog post.\n404.qmd * Update it with more helpful text."
  },
  {
    "objectID": "todo.html#css-and-appearance",
    "href": "todo.html#css-and-appearance",
    "title": "Joey Stanley",
    "section": "CSS and appearance",
    "text": "CSS and appearance\nstyles.css * Get the links to look right\nDissertation.qmd * Add rounded corners to the image. The CSS is “border-radius: 7pt” but I’m not sure how to add that in. * Do the same thing to the NWAV46 and NWAV47 pictures.\nAll blogs * If I don’t like the cover image on blog posts, switch it to something I do like."
  },
  {
    "objectID": "todo.html#blogs",
    "href": "todo.html#blogs",
    "title": "Joey Stanley",
    "section": "Blogs",
    "text": "Blogs\nLinking to past blog posts * Make sure the Pillai tutorials work in the NWAV50 post. * Make sure post to Lexical sets works in ADS2022.\nLinking to future posts * Any time a conference paper eventually got published, put a link on the page somewhere with that published version. (Maybe the top of the page?) - Pillai scores has a bit of a thread, starting with ASA181, then NWAV50. - The Gen X–Boomer thing can be traced pretty far back. At least since my LCUGA presentation. But we first showed it more clearly in ADS2019. * In 10 years of linguistics, cite the languages I know. * In PWPL post about OoO, link to the Vanguard post, twice.\nFuture blog posts * That one article and my system for keeping track of my research * Tutorial about colors in the feeding Douglas plot * Showing my Praat scripts for pre- and post-processing MFA. I mention it in /blog/transcribing a sociolinguistic corpus * Retroactively do a blog post about every publication and conference. * Retroactively do a student kudos blog posts. * Retroactivly flesh out the interpreting difference smooths blog post from colloquium 2020 * Follow-up on /ar/-raising but with GAMMs. Towards the bottom of the /ar/ post, I say I don’t know GAMMs yet. * Kohler Tapes update based on more complete metadata.\nUpdate images from the joeystanley theme * Testing VOT Durations * /ar/-raising * LCUGA6"
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html",
    "href": "blog/the-importance-of-twitter/index.html",
    "title": "The Importance of Twitter",
    "section": "",
    "text": "I’m preparing a workshop right now for the DigiLab here at UGA on how to increase your web presence. I’ll give a more detailed explanation of that later on, but I just wanted to point how how cool Twitter has been for me.\nI don’t remember when or why I got a Twitter account, but I remember early on that I wanted to keep it professional. I don’t follow very many friends or family: just other random linguists I find. That means my feed is nothing but linguistics stuff, and mini posts that other linguists find interesting. Granted, a lot of these folks post non-linguistic stuff as well, so I do have to sift through those sometimes. But there have been some really valuable gems I’ve found because of Twitter."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#fun-stuff",
    "href": "blog/the-importance-of-twitter/index.html#fun-stuff",
    "title": "The Importance of Twitter",
    "section": "Fun Stuff",
    "text": "Fun Stuff\nFirst, we’ll start with the fun stuff.\n\n\nCheck out @JWGrieve’s wordmapper app (before it gets overwhelmed by traffic!) – plots Twitter usage across U.S.: https://t.co/9nBh3h9z9v\n\n— Ben Zimmer (@bgzimmer) January 29, 2016\n\n\nThis didn’t lead to anything in my work, but it was pretty awesome to see what Jack Grieve had done. In case the link doesn’t load above, it shows an interactive program where you can type in a word and see its regional distribution across Twitter. It’s a lot of fun to play around with."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#datasets",
    "href": "blog/the-importance-of-twitter/index.html#datasets",
    "title": "The Importance of Twitter",
    "section": "Datasets",
    "text": "Datasets\nTwitter has also been good for me to discover new datasets. This tweet for example let me know that the entire contents of Reddit had been extracted and were available for download.\n\n\n1 terabyte corpus of Reddit comments, up to may 2015, from @internetarchive. What a glorious day http://t.co/cHtmhKZyHW\n\n— heather froehlich (@heatherfro) July 9, 2015\n\n\nAbout a month or so later I was starting a course in Digital Humanities, so this corpus became the main tool for my term paper for that class. I ended up downloading all the Reddit comments from its inception (2007) until October 2015. It was a whopping 50 billion words of text sitting on a terabyte of storage. If this were printed on standard book-sized sheets of paper, it would be something like 2½ miles long! And growing at about 4 feet an hour. That’s a lot of data. I don’t have a terabyte of storage available for something like this, so I wrote a Perl script that cut it down to a hundredth of its original size (“only” 500 million words!).\nI ended up having to download it all using lab computers in the student center off and on for a week. In fact, I had four computers going simultaneously, all downloading Reddit files, one month at a time, and running Perl scripts to make them smaller. I wasn’t surprised when IT came over and wondered what the heck I was doing. Turns out it was the fact that my login was used on four computers that triggered their systems, not the fact that I was running four computers at full speed for a couple hours. Once they saw what I was doing they shrugged their shoulders and said it was totally fine.\nHandling this much data, even though it was a hundredth of the original size, was rough. I made a frequency list of all the words, which ended up being about half a million rows long. I wanted to track language across time so I had information about how often each word was used every month for about 100 months. That’s a lot of columns for all those rows. I pushed Excel (and my little laptop) to its limits.\nAnyway, this project turned into a fun term paper that I never published. I wanted to look at the language of the most upvoted comments as compared to all other comments and see if there were any differences. I found a few, but with biggish data like this, statistical significance is everywhere so you have to be more careful about things.\nBottom line: Because of Twitter I got to work with an enormous corpus which was a lot of fun."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#new-methodology",
    "href": "blog/the-importance-of-twitter/index.html#new-methodology",
    "title": "The Importance of Twitter",
    "section": "New Methodology",
    "text": "New Methodology\nOn Twitter people also post new things they see at conferences and other places. During NWAV44, I followed the live tweets and saw this gem:\n\n\n@wgi_02445_temp has given us another gift. Bhattacharyya’s affinity to measure overlap: https://t.co/26byi2KpRk #NWAV44\n\n— Paul De Decker (@pmdedecker) October 25, 2015\n\n\nBasically, Daniel Johnson talked about another way to measure vowel overlap—something I do a lot in my research. In the Shiny App linked above, Johnson compares Pillai scores and something called Bhattacharyya affinity. I ended up using this in a poster (abstract here) I did with Peggy Renwick at LabPhon, and will continue to use this new measure of overlap, not exclusively, but in addition to the other measures out there."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#live-tweeting-conferences",
    "href": "blog/the-importance-of-twitter/index.html#live-tweeting-conferences",
    "title": "The Importance of Twitter",
    "section": "Live Tweeting Conferences",
    "text": "Live Tweeting Conferences\nI’m a lowly grad student and don’t have a ton of funding for conferences, so I can’t attend some of the big ones all the time. Luckily, a lot of people live tweet what’s going on at most major conferences, so I can follow along and feel like a part of the group.\nI myself live tweeted for the first time at a linguistics conference here at UGA. I don’t have a ton of followers, and the conference isn’t super well-known. But I did try to find people’s Twitter handles whenever possible, as well as their department’s, and would include them in the tweets. Well as it turns out I got about half a dozen new followers from that conference. Not a huge deal, but it does spread my name just a little bit further, and maybe onto the right person’s computer screen."
  },
  {
    "objectID": "blog/the-importance-of-twitter/index.html#twitter-is-great",
    "href": "blog/the-importance-of-twitter/index.html#twitter-is-great",
    "title": "The Importance of Twitter",
    "section": "Twitter is great",
    "text": "Twitter is great\nSo in the end, having a Twitter account is a lot of fun. I’ve benefited personally and professionally, and it’s definitely a worthy investment of my time."
  },
  {
    "objectID": "blog/admission-to-candidacy/index.html",
    "href": "blog/admission-to-candidacy/index.html",
    "title": "Admission to Candidacy",
    "section": "",
    "text": "This morning I successfully defended my second qualifying paper, “Near-Mergers in Cowlitz County, Washington,” which means I’m officially a doctoral candidate! (Okay, actually, a couple forms need to be signed, but that’s no biggie.) What an important step for me!\n\n\n\"Doctoral candidate\" just sounds so much cooler than mere \"Ph.D. student.\" I like my new title :)\n\n— Joey Stanley (@joey_stan) May 4, 2017\n\n\nThe paper I defended was essentially what I presented at the ADS conference in January and at DiVar in February. I presented some of my data that I collected in Washington State and discussed the Mary-merry-marry merger (or lack thereof) and a collection of mergers involving higher back vowels before coda laterals (pool, pull, pole, and pulp). I got some great feedback from my committee.\nWhat are my plans for the rest of my schooling? I’m still deciding if I want to finish my dissertation and graduate next year or the year after. I’ll start things up and see how I’m feeling in six months, but I’m on the fence. On the one hand, while I will probably be able to secure funding for my fifth year, no matter what it is it’s probably going to be less pay than a real job, so I’ll want to graduate after my fourth year. On the other hand, giving myself two years to finish will allow me to really put a lot of time into the dissertation, as well as the couple other sizable side projects I have going on, thus beefing up my CV before graduating.Edit: I ended up graduating in 2020 after my sixth year.😳\nI’ll have to weigh my research goals against my personal, familial, and financial needs and make some decisions at some point. But for now I’m just happy to have made that leap to ABD."
  },
  {
    "objectID": "blog/asa181/index.html",
    "href": "blog/asa181/index.html",
    "title": "ASA181",
    "section": "",
    "text": "I’m in Seattle at the 181st Meeting of the Acoustical Society of America right now! This is my first in-person conference since October 2019, so it’s great to be here. I presented two posters today, which you can read about and download below."
  },
  {
    "objectID": "blog/asa181/index.html#beyond-midpoints-vowel-dynamics-of-the-low-back-merger-shift",
    "href": "blog/asa181/index.html#beyond-midpoints-vowel-dynamics-of-the-low-back-merger-shift",
    "title": "ASA181",
    "section": "Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift",
    "text": "Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nFor some reason, I hadn’t yet presented any of my dissertation findings at a conference, not even while I was working on them or writing them up. Anyway, I’m happy to finally present some of my results at a conference. The purpose of this paper is to describe changes in vowel trajectory that accompany changes in midpoints. The Low-Back-Merger Shift is a now-widespread shift across much of North America. My data from Washington shows it pretty clearly across generations. But when I take a wide-angle lens at the vowel trajectories, I found that there was much more to the story than just a global lowering/centralizing of the front lax vowels.\n\n\n\n\n\nThere were perhaps three patterns I noticed when I modeled vowel trajectories. First is that the trajectory length was different between them. The low vowel /æ/ was much longer, then /ɛ/, and then /ɪ/. There’s also a general U-shaped pattern. Finally, the “angle” of this U-shaped was more towards the “left” for /æ/, more towards the right for /ɪ/, and in the middle for /ɛ/. These descriptions are consistent across generations and between the two geneders modeled, so it may say more about American English articulation than anything sociolinguistic.\nPerhaps more interestingly though was how these trajectories changed—within the parameters just described—across generations. Older people’s vowels traversed through much more of the F2 space than younger generations did. The result is that the older people’s vowels look more like a shallow U-shape while the younger people’s is more of V-shape or even a “bounce” straight up and down in the F1-F2 vowel space. The fact that this was consistent across all three front lax vowels and between the genders suggests some interesting sociolinguistic change.\nAt this point, this is largely descriptive work. I don’t know how perceptible these differences are and I’m not even sure if everything I just described is statistically significant. It’ll take additional work to confirm both of these. Trajectories are often ignored because they’re chalked up articulatory causes; are we comfortable saying that trajectories are 100% phonetic and 0% sociolinguistic? (Meanwhile, if I may be a bit snarky, are the arbitrary single-point measurements that are typically analyzed magically sociolinguistically important?) I think people can exploit trajectories for sociolinguistic purposes. I just don’t know how or to what extent yet."
  },
  {
    "objectID": "blog/asa181/index.html#sample-size-matters-when-calculating-pillai-scores",
    "href": "blog/asa181/index.html#sample-size-matters-when-calculating-pillai-scores",
    "title": "ASA181",
    "section": "Sample size matters when calculating Pillai scores",
    "text": "Sample size matters when calculating Pillai scores\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nI’m very excited about this Pillai scores paper with a new colleague, Betsy Sneller! The background for this papers is that a while ago I was analyzing some cot-caught merger data I had collected. I noticed that, without exception, I got higher pillai scores in wordlists than I did in conversational data. I thought I had stumbled upon some interesting style shifting! But it was too clean of a pattern, so I did some digging and found that it’s likely because of the sample size between the two subsets. I had less data from the wordlists than I did from the conversation. I hypothesized then that less data leads to higher Pillai scores.\n\nMethods and Experiements\nSo in this paper, we test this hypothesis specifically by running a bunch of simulations. We started with a single bivariate normal distribution. We then randomly drew 5 numbers from that distribution and called it “group 1.” We then drew another 5 numbers from the exact same distribution and called it “group 2.” The fact that they’re drawn from the same underlying distribution represents a true underlying vowel merger. We then calculated the Pillai score of those two groups. We repeated with these group sizes 1000 times. Then we drew 6 tokens from each group 100 times and calcualted Pillai scores. Then 7. And all the way up to 100.\nAs seen in the main figure in the poster (slightly modified below), the results were clear: the larger the sample sizes, the lower the Pillai scores were. In theory, the Pillai scores should all be around zero since they’re from the same distribution. But with small samples sizes (&lt;10) observations per group, we very often got pretty high Pillai scores—scores that some researchers have considered to be distinct. It took around 30 observations per group to reliably (meaning 95% of the time) get the Pillai score under the somewhat conservative threshold of 0.1. It took 60 observations per group to get Pillai scores reliably below 0.05. This was concerning to us because few sociophonetic studies have sample sizes that large!\n\n\n\n\n\nWe were also concerned about unequal sample sizes betwen groups. So we reran the experiment, except the group sizes weren’t the same size. Each group could be anywhere from 5 to 100 observations, and we ran all 9000 or so combinations. The results were surprising to us—unequal group sizes doesn’t matter at all. The only thing that mattered was the total sample size. You can see this in the figure in the top right of the poster (or below): as you go from bottom-left to top-right, the average log Pillai scoreWe used log(pillai) because it worked better for this visual and for the math. goes from high to low. But the fact that there is no pattern from the top-left to the bottom-right diagonal means that unequal sizes don’t matter.\n\n\n\n\n\nIn other words, if Group A has 25 observations and Group B has 5, the Pillai score will average around 0.07. That’s the same as if you had two groups of 15. If Group A has 25 observations and Group B has 100, the Pillai score will average around 0.01. That’s the same as if you had two groups of 62.\n\n\nImplications\nWe can think of a lot of implications for these findings. For one, mergers are probably underreported and splits/distinctions are probably overreported. This is because many sociophonetic studies run Pillai scores on somewhat smaller samples.\nBecause of sample size differences, comparison across studies is difficult. A study that collects lots of data per person will likely report lower Pillai scores than a study that is based on fewer observations per person.\nGoing back to the main impetus for this paper, comparison within studies is difficult. Since more careful speech styles typically elicit fewer observations, reading tasks will have higher Pillai scores than conversational data. To a naive researcher, this will be interpreted as style differences, when it is really just a reflection of the underlying math! This is such an important point and you can count on hearing more from me and Betsy about this in later venues.\nFinally, one way to overcome the sample size difference is to look at the p-value that comes out the MANOVA test that the Pillai score came from. These p-values do seem to be reported in more phonetics-oriented papers, but for some reason they’re not in sociophonetics papers. So rather than us coming up with arbitrary and ad hoc thresholds for what a merged Pillai score should be, let’s us the p-value instead. Not reporting this p-value, to me at least, is kinda like reporting a t-statistic or F-ratio but without the accompanying p-value.\nAs a final note, and this is more of an after-thought for us, I wonder if it would be more helpful to report log(pillai) rather than raw pillai scores. Since Pillai ranges from 1 (completely distinct) to 0 (complete overlap), log Pillai would range from 0 (completely distinct) to negative infinity (complete overlap). In practical terms, it would mostly be betwen 0 and around –4 (the latter corresponding to a raw Pillai score of about 0.01). We’ll probably talk more about this in other venues so stay tuned for that."
  },
  {
    "objectID": "blog/how-i-implemented-the-links-in-this-site/index.html",
    "href": "blog/how-i-implemented-the-links-in-this-site/index.html",
    "title": "How I Implemented the Links in this Site",
    "section": "",
    "text": "A while ago I stumbled across Butterick’s Practical Typography. I’ve never been that into typography, but I do make sure my documents look good. I’ve yet to implement good typography into this website, but I did find a neat trick that I was able to pull off.\nHyperlinks traditionally are in blue and underlined. It’s been the standard for so long now that we’ve gotten used to it and don’t notice how bad it looks. In Butterick’s book though he has linked text the same color and style as the rest of the page, but there’s a little red circle after it, as if it was a link to a footnote or something. It’s a pretty clever way to soften the look, but still get the point across that that particular text is a link.\nI like to write stuff with links everywhere. I figure it might save people a google search if I just do it for them. On my CV, I link readers to the PDF and other documents, my co-authors’ pages, and other pages that might be useful. The result though makes it look like Wikipedia, unless I can find a way to soften how links look.\nThanks to some CSS skills I’ve been learning from Lynda.com, I’ve found out that I can add those little red circles by adding this to my .css file:\na[href*=\"http\"] {\n    color: black;\n}\na[href*=\"http\"]:after {\n    content: 'º';\n    color: darkred;\n}\nWhat the first block does is it targets just the &lt;a&gt; tags that contain an href, meaning they’re an external link, and it changes the color to black, overwriting the default blue. From there, the :after selector in the next block puts specific text, the little circle, after the hyperlinked text in the tag every time. I may need to change some things later to make sure it does exactly what I want on all the pages, but it works for now.\nI don’t know if I’ll keep it the way it is, but it does look a lot better than tons of blue underlined text."
  },
  {
    "objectID": "blog/website-version-2/index.html",
    "href": "blog/website-version-2/index.html",
    "title": "Website Version 2",
    "section": "",
    "text": "Today I finally rolled out a new version of my website! The previous version was great and was an excellent stepping stone into web design, but it was mostly borrowed code. Unsatisfied with some of the way it was designed, I decided to go ahead and just write a new site completely from scratch. It has taken about a month or so to get it going, but I think it’s a lot better than before."
  },
  {
    "objectID": "blog/website-version-2/index.html#new-site",
    "href": "blog/website-version-2/index.html#new-site",
    "title": "Website Version 2",
    "section": "New Site?",
    "text": "New Site?\nAs I’ve mentioned previously, I went through a course on Lynda.com, called “Jekyll for Web Designers” that showed me the basics in how to get that website up. I was still in unfamiliar territory though as I navigated my way around CSS and HTML and any changes I wanted to make to the site were difficult to do. I liked the instructor for that course (James Willliamson) though, so when I looked up some of his other ones and saw that he did several others that would be relevant to me, I decided to go ahead an take them as well.\nI first took his “CSS Core Concepts” where I learned all about CSS and how it works. At a whopping 9 hours long, I knew I was going to get a thorough treatment of CSS. Essentially I learned how to make things look the way I want on a webpage. I learned how to do anything I want to text like change the font, size, and color as well as add things like bold, italic, and small caps. I also learned how to add space around text or between elements on a webpage.\n\n\nI learned about #Web on https://t.co/n13drggo1T. I completed CSS: Core Concepts by @jameswillweb https://t.co/at3VlMgyPD via @lynda\n\n— Joey Stanley (@joey_stan) February 5, 2017\n\n\nWhen I finished that, I continued on to the next course called “CSS Page Layouts” which I’m pretty close to finishing. This is more about web design and how to get from a sketch book drawing of a webpage to the screen. Most of the time was in CSS still (as opposed to the HTML) but I learned how to position things on the webpage.\nThe layout of site I had before was simply copied from the last tutorial in the first course. Since I didn’t write the HTML or the CSS, I didn’t know exactly what was going on, so if I wanted to make changes it was really hard to do. Now that I’ve written this new site from scratch, I know exactly what’s going on in all the webpages and in CSS.\nOf course, the sacrifice is that this new site isn’t quite as clean as the old one was. For example, I know it looks good on my Apple laptop in Safari, but I don’t know all the code I need to watch out for to make it fully compatible with other browsers, let alone other versions of other browsers. The site also doesn’t adapt to smaller screens like phones and tablets. I’m still working on that."
  },
  {
    "objectID": "blog/website-version-2/index.html#so-whats-the-difference",
    "href": "blog/website-version-2/index.html#so-whats-the-difference",
    "title": "Website Version 2",
    "section": "So what’s the difference?",
    "text": "So what’s the difference?\nI didn’t do any major changes to the overall structure or the general typographic details that I use in everything I do. The fonts are still Iowan Old Style and Avenir and the background is still “whitesmoke” (96% white, 4% black). The blue from before was, coincidentally, very similar to the blue I use in my power point slides, but not exactly, so I went ahead and changed it to my blue to match my slides.\nJust to give you an idea of what the site looked like before, here are some screenshots. This first one is my home page from my old site:\n\n\n\nOld site’s homepage\n\n\nAnd this is my new home page:\n\n\n\nNew site’s homepage\n\n\nA couple layout changes. First, I’ve widened it from something like 70% of the screen to a standard 960 pixels. That gave me the option of keeping my ideal width at roughly 66 characters still while giving me room for a sidebar.\nI changed the header as well. I felt like it was the weakest part of my page, and I didn’t like how it looked. I knew I wanted my social media links to be somewhere prominent, and of course the navigation links should be there too. But I wanted to include temporary links to recent presentations so people can download the slideshow if they visit my site, and those didn’t quite fit up there. And I didn’t have anywhere to put a photo.\nOne solution was to have the top just include my name and the navigation buttons, which are now more descriptive. A sidebar could then have my photo, social media, and temporary links to recent presentations. There was also room to put an excerpt of my most recent blog post too, which I figured out how to do dynamically, which is pretty cool."
  },
  {
    "objectID": "blog/website-version-2/index.html#a-new-research-tab",
    "href": "blog/website-version-2/index.html#a-new-research-tab",
    "title": "Website Version 2",
    "section": "A new Research tab!",
    "text": "A new Research tab!\nI also added a Research tab. I’ve seen this on lots of other people’s pages and I thought I’d include one in my own. I guess I figured everyone would clearly see what my research was by skimming through my CV, but putting it in prose like that makes a lot more sense.\nThe layout was a blatant copy from the UGA DigiLab projects tab. I think adding images, even if they’re simple like mine, contribute a lot to the page.\nEventually, I’d like to create a separate page for each of these. There I’d go into more detail on my findings, put a list of publications, and have links to any relevant blog posts. I’ll get there eventually. For now, I’ll just make the the excerpts a little longer."
  },
  {
    "objectID": "blog/website-version-2/index.html#blogi-mean-news",
    "href": "blog/website-version-2/index.html#blogi-mean-news",
    "title": "Website Version 2",
    "section": "Blog—I mean, “News”",
    "text": "Blog—I mean, “News”\nMy blog is now relabeled “News” just because it sounds less, well, blog-like. Here’s the old one:\n\n\n\nOld site’s blog\n\n\nAnd here’s the new one:\n\n\n\nNew site’s blog\n\n\nI also redesigned the blog page itself. First off, I didn’t like that you could only see 4 at a time and that you’d have to scroll back to see older entries. I don’t like it that way. I’ve still got the code still there, but it’ll only create a second page once I’ve got 50 posts. I’ve got like 15 for now so I don’t think it’ll slow anyone down by loading this page.\nThe rest of the page was completely revamped. To be quite honest, I googled around for great examples of blog layouts and I found this one, which I copied quite a bit of. I think it works well for my site.\nAnother big thing I didn’t like about the old layout was that the “Archive” tab didn’t make sense as its own page and the label didn’t really make sense.\n\n\n\nOld site’s “archive” page\n\n\nInstead, I moved it to a separate sidebar on the blog. That way it’s clear that the tags and the blog work together.\n\n\n\nNew site’s “tags” page\n\n\nIf I could change one thing on the main “news” page, it would be that sidebar though. Right now, if you click on a tag, it’ll take you to a page where it lists all the blog posts by category. I don’t like that separate page. I wanted to do something fancy like you click on a tab and a list of posts will expand down. Turns out I couldn’t do that without Java scripting and I don’t know how to do any of that. I also looked into just having pages for each category created on the spot, with blog excerpts instead of just the titles for that one category, but I couldn’t figure out how to do that."
  },
  {
    "objectID": "blog/website-version-2/index.html#anything-else",
    "href": "blog/website-version-2/index.html#anything-else",
    "title": "Website Version 2",
    "section": "Anything else?",
    "text": "Anything else?\nNope. That’s about it. My CV page has remained essentially the same. I added “Resources” and “Teaching” tabs, but those are blank for now. I’ll add content to them eventually. There are still a few layout things I need to work on, but I thought I’d launch the site anyway—imperfect as it is for now—because it’s a major improvement over the last one."
  },
  {
    "objectID": "blog/thank-you/index.html",
    "href": "blog/thank-you/index.html",
    "title": "Thank You",
    "section": "",
    "text": "Some of you may have noticed that at the bottom of a lot of the pages on my website, I’ve got this button:\nI’ve created an account on ko-fi.com, which is a platform that, in their words, provides “a friendly way for fans to support your work for the price of a coffee.” To put it more bluntly, I’ve created a way for people to give me money for my tutorials and stuff. At the risk of sounding arrogant, I’ll brag for just a little bit before geeking out about the new books I’ve gotten because of that little button."
  },
  {
    "objectID": "blog/thank-you/index.html#let-me-brag-for-just-a-sec",
    "href": "blog/thank-you/index.html#let-me-brag-for-just-a-sec",
    "title": "Thank You",
    "section": "Let me brag for just a sec",
    "text": "Let me brag for just a sec\nSo I’ve put together half a dozen tutorials and another dozen or so workshops. The most popular ones are the tutorials on how to make vowel plots, how do formant extraction, measuring vowel overlap, and my series of R workshops. You can find them all on my Resources page.\nThese were skills that aren’t typically taught in a linguistics course, but they’re expected of sociophoneticians, and I knew I had to learn how to do them. So, I hunkered down, did lots of googling, and figured out this stuff on my own by frankensteining what I could find from lots of other pages online. After a while, word spread among my peers that I had acquired some skills, and I ended up helping lots of people do these things on their own data.\nEventually, I realized that there was a niche to be filled: my peers could benefit from a clear tutorial on some basic scripting techniques in sociophonetics. So, I started to write the tutorials that I wish I had had when I was learning this stuff. When I finish them, I share them on Twitter, and they’ve consistently remained among my most visited pages on my website. People from about ten different countries on four continents have contacted me for various reasons, explaining how useful they’ve found the tutorials. And there are likely many more anonymous folks too.\nSince then, lots of other people have produced excellent tutorials that cover some of the same topics mine have. And that’s awesome! I’m just glad that people have so many resources available to them to learn these skills."
  },
  {
    "objectID": "blog/thank-you/index.html#so-whats-this-ko-fi-thing",
    "href": "blog/thank-you/index.html#so-whats-this-ko-fi-thing",
    "title": "Thank You",
    "section": "So what’s this Ko-fi thing?",
    "text": "So what’s this Ko-fi thing?\nSo someone (I forget who) offhandedly mentioned that I should charge people for my tutorials. There was no way I was going to do that. I got this information for free, so I give it for free.\nBut, I figured if people wanted to donate money, well, who was I to stop them? So I looked around and settled on Ko-fi as a way to give people that opportunity if they wanted to take it. It seems simple enough: the button above will take you to my Ko-fi page, and from there you can donate $3, or about the price of a coffee.  The money gets sent to my PayPal account and I can deposit into my bank account.Ironically, I don’t drink coffee, so you can think of it as a fancy glass bottle of Coca-Cola or something.\nSo I set it up, put and put it at the bottom of my tutorials, and didn’t think anything of it because I didn’t expect much to happen. But slowly, here and there—and much to my surprise—the donations came in. It’s not bumping me up to the next tax bracket or anything, but it does make me feel that the work I put into the tutorials is appreciated.\nUpdate: Now that I’m faculty, I have a real salary, and I feel back taking money from people, especially grad students. But, I’ve been told that sometimes people appreciate having a way to express their appreciation."
  },
  {
    "objectID": "blog/thank-you/index.html#what-am-i-doing-with-the-money",
    "href": "blog/thank-you/index.html#what-am-i-doing-with-the-money",
    "title": "Thank You",
    "section": "What am I doing with the money?",
    "text": "What am I doing with the money?\nInstead of just dumping it into my bank account, I figure I should do something special with the money. I decided to get a couple books. In particular, I wanted to get some books on data visualization, since that’s relevant to the tutorials themselves.\nMy first goal was Data Visualization by Kieren Healy. I had been following the book as it was approaching publication and was excited to see it’s positive reception. Yes, it’s pretty much all available online, but I wanted the physical copy for two reasons. For one, it’s a beautiful book and a paragon of excellence in typography. But also, I should return the favor of financially supporting someone who produces tutorials online. Thanks to one large donation that bumped me well past my goal, I’m now a proud owner of Healy’s book!\n\nSo I tweeted about it and did a small humble brag. And again, much to my surprise, some more donations came in! So in the same week, I met my second goal of purchasing Fundamentals of Data Visualization by Claus O. Wilke.\n I’m now realizing the front phone on my camera is quite fuzzy.\nWith these two books in hand, I hope my knowledge of data visualization will increase and that this percolates into future tutorials and workshops.\nSo I didn’t think this whole thing would work. Apparently it has. I’ve now started to compile a list of additional books I’d like to get, including Edward Tufte’s quartet of data visualization books, some linguistics books I’ve been meaning to get, and a couple statistics manuals."
  },
  {
    "objectID": "blog/thank-you/index.html#thank-you",
    "href": "blog/thank-you/index.html#thank-you",
    "title": "Thank You",
    "section": "Thank You",
    "text": "Thank You\nThe reason for all this is to say thank you. I’m so thrilled to hear that people seem to find some utility in my tutorials, and I’m humbled that some folks went out of their way to send me a little extra pocket money as a token of their appreciation. I’m honored to be helping the linguistics community in a small way. Thank you."
  },
  {
    "objectID": "blog/ads-and-lsa-2019/index.html",
    "href": "blog/ads-and-lsa-2019/index.html",
    "title": "LSA and ADS 2019",
    "section": "",
    "text": "Thanks for attending my presentations. At the 2019 annual meetings of the American Dialect Society and the Linguistic Society of America in New York City, I was fortunate to present three presentations!"
  },
  {
    "objectID": "blog/ads-and-lsa-2019/index.html#thursdays-lsa-poster-on-southern-vowels",
    "href": "blog/ads-and-lsa-2019/index.html#thursdays-lsa-poster-on-southern-vowels",
    "title": "LSA and ADS 2019",
    "section": "Thursday’s LSA poster on southern vowels",
    "text": "Thursday’s LSA poster on southern vowels\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nThursday, Peggy Renwick and I presented our poster on social patterns in static and dynamic measurements of Southern American English vowels. Our dataset was the Digital Archive of Southern Speech (DASS), a collection of 64 interviews from the 1970s. We looked at Pillai scores to measure the degree of “swapping” between pairs of front vowels (/i ɪ/ and /e ɛ/) and we used vector length, trajectory length, and spectral rate of change to see how dynamic the vowels were.\nWe found a bunch of cool patterns! You can see the poster for all of them, but one of the cooler ones was that /e/ and /ɛ/ swapped more in younger speakers. This plot shows the trajectories of these two vowels split up by generation and you can see how they get closer together (though keep in mind that because the trajectories are drastically different, these aren’t merging).\n\nThis is to be expected for speakers with the Southern Vowel Shift. But, the African American Vowel Shift doesn’t have this same swapping. So sure enough, if we look at the data split by ethnicity, we see that the African American speakers had less speakers than the European Americans.\n\nSo our results are mostly what we expected to find. But this corpus of older recordings give us a unique peek into the past while these changes were developing. And by using both static and dynamic measurements, we can get a more complete picture of what’s going on."
  },
  {
    "objectID": "blog/ads-and-lsa-2019/index.html#fridays-ads-presentation-on-prevelar-raising",
    "href": "blog/ads-and-lsa-2019/index.html#fridays-ads-presentation-on-prevelar-raising",
    "title": "LSA and ADS 2019",
    "section": "Friday’s ADS presentation on prevelar raising",
    "text": "Friday’s ADS presentation on prevelar raising\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nEarly Friday morning, I talked about regional patterns in beg- and bag-raising in North American English. There’s been a lot of research on these phenomena, but only in places like the Upper Midwest, Western Canada, and the Pacific Northwest. Perhaps there is prevelar raising in other areas too?\nI used the same dataset that I used for my NWAV47 presentation: I set up an online survey and asked people how they pronounced several dozen prevelar words. For geographic data, I used GPS coordinates of people’s childhood homes. I ended up with over 500,000 data points!\nIt was clear that there were differences between the two vowels. bag-raising was pretty clear cut: either people had it or they did not. And most people didn’t. There just weren’t too many people that were sort of on the middle ground. However, with and beg-raising, while there were lots of people with no raising, there were tons of people with varying amounts of raising. Basically, beg was much more variable than bag.\n\n\nAnd then I showed some plots. The first was bag-raising which was reported in pretty much all the areas we expected.\n\nBut then other map showed that bag-raising was pretty much everywhere except for the South. It’s particularly widespread in the West and the Midlands.\n\nWhen you combine the two maps, you can start to see where one occurs without the other. Here, green areas are those that have bag-raising but not beg-raising and purple are areas with beg-raising without bag-raising.\n\nBasically, the purpose of this study was to see whether there were differences between beg-raising and bag-raising in where they occur regionally. I think these maps show that there are differences. Now I just want to go confirm these patterns with phonetic data!"
  },
  {
    "objectID": "blog/ads-and-lsa-2019/index.html#sundays-ads-presentation-on-the-perception-of-southern-american-english",
    "href": "blog/ads-and-lsa-2019/index.html#sundays-ads-presentation-on-the-perception-of-southern-american-english",
    "title": "LSA and ADS 2019",
    "section": "Sunday’s ADS presentation on the perception of Southern American English",
    "text": "Sunday’s ADS presentation on the perception of Southern American English\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nMy last presentation of the weekend was Sunday morning at the American Dialect Society. On behalf of my coauthors, Rachel Olsen, Mike Olsen, Lisa Lipani, and Peggy Renwick, I talked about our research on comparing acoustic data with fieldworker transcriptions in the Digital Archive of Southern Speech. Doing this kind of comparison is not new, and people have found that Linguistic Atlas transcriptions are not really that reliable, but we wanted to look into this for ourselves in our newly transcribed corpus.\nSo for now, we’re just focusing on the canonical diphthongs /aɪ, aʊ, ɔɪ/ because they’re quite a bit more monophthongal in the South than in other areas. To measure this acoustically, we used trajectory length. For the perception, we looked at the original fieldworker’s protocols they created for each informant, which has example tokens and how those people would pronounce them (in IPA).\nBasically there was no correlation between how monophthongal the vowels were acoustically and how they were transcribed. Besides that, in transcriptions the vowels were more monophthongal when they were before /r/, but acoustically they were more monophthongal before /l/ and among European Americans. The two give different results.\nSo we’ve at least found that whatever the fieldworkers heard when they transcribed these words, it wasn’t trajectory length. Perhaps in the future we can explore some other acoustic measures and see if they correlate better with the transcriptions."
  },
  {
    "objectID": "blog/reviewer-feedback/index.html",
    "href": "blog/reviewer-feedback/index.html",
    "title": "Reviewer Feedback",
    "section": "",
    "text": "Yesterday I got the reviewer feedback for the paper I’m going to be presenting at the American Dialect Society in January.\nThis is not the first time I’ve gotten reviewer feedback: I’ve submitted several things to big enough conferences where reviewers give their feedback. But they’re usually something like, “Okay yeah that’s cool and all, but here are some obvious things you should consider. I’ll reluctantly give you a pass, but I expect major changes at the conference.” Either that, or they’re brutally honest and say it’s garbage.\nI’m grateful for the feedback every time because it’s completely spot-on, and reading an uncomfortable email alone at my desk is merciful compared to what I might potentially hear at conferences. Or worse, what I might not hear but what people think.\nBut, I’m happy to report I had three glowing reviews this time! On top of that, I was offered advice on some studies to look up and some ways to strengthen my argument. Maybe this is an interesting topic after all!"
  },
  {
    "objectID": "blog/brand-yourself-2/index.html",
    "href": "blog/brand-yourself-2/index.html",
    "title": "Brand Yourself 2",
    "section": "",
    "text": "Today, I was asked to do a professionalization workshop on different ways grad students can boost their online presence through building a personal webpage, utilizing social media, and finding their field’s conversation—basically, how to make yourself more googleable. At the end, I challenged people to not leave the room until they had built some sort of new online profile they didn’t have when they walked in."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html#social-media",
    "href": "blog/brand-yourself-2/index.html#social-media",
    "title": "Brand Yourself 2",
    "section": "Social Media",
    "text": "Social Media\n\nAcademia.edu\nIf you have zero online presence, Academia.edu is a great way to get something up and running quickly. Basically, it’s a freemium social networking site for academics. Users can create profiles, upload their papers, and follow particular research topics. They can also follow others that have done the same. It’s a great resource for finding papers that may be behind a paywall, although it has gotten a lot of criticism for this. Papers you upload can be found by Google Scholar, which is a nice perk. The website will keep track of your analytics, and there’s nothing more thrilling than getting an email saying someone has found your profile!\n\nThere are some negative aspects of Academia.edu. The site got some criticism a few years ago for offering authors the chance to promote their work for a fee. As of about two years ago, they launched a Premium version ($8.25 a month) with extra features and they constantly bug you about it. There’s also a chance at any time the site could get shut down because publishers aren’t happy about it, but with 30 million users, I don’t know if that’s going to happen any time soon. Full disclosure, I deleted my account about a year ago, now that I have this site up and running.\n\n\nResearchGate\nI’m less familiar with ResearchGate, but in my cursory look, there’s a lot of overlap with academia.edu as far as its features. A big difference I noticed is that it seems like it’s more focused on creating networks based on people (folks you you cite and co-author with) while academia.edu is more focused on following topics. Two thing I don’t like about ResearchGate is that the number of emails it sends you is borderline spam and that it’ll create a profile for you based on information that other people put up without you knowing. I think in some fields ResearchGate is more popular than Academia.edu, so it might be worth it to have a profile on both.\n\n\nGoogle Scholar\nI would imagine most researchers use Google Scholar regularly, but did you know you can create a profile for others to see? You can tell a researcher has done that when you see their name underlined in a search:\n\nIn this screenshot (live link here), you can see that Walt Wolfram, Natalie Schilling, Sali Tagliamonte have created their profiles, but Shanna Poplack and Penny Eckert have not. I’d like to see what else the last two researchers have written, but I can’t simply click on their names like I can with the first three. When you do click on their links, you can see the full profile including what else they have written and how many times each has been cited.\n\nIt does take a bit of work to get a full profile going, because Google’s data can be a bit messy, so you’ll have to add stuff in by hand. But I think the payoff is worth the effort.\n\n\nOther Academic Social Media\n There are a handful of other websites out there that can help you build an online presence. Impact Story is one that can keep track of how much of an impact you have on people by keeping track of when people cite, mention, read you and your work. For $10 a month, it might not be worth it for a grad student, but for a professor applying for tenure this might be worth it.I am indebted to the Impact Challenge blog series, with the accompanying 200+ page pdf, from which I learned a lot about all this. I would highly recommend that you download it and take a look. Not only does it include much more than what I’ve mentioned here, including step-by-step how-to guides to getting these profiles set up, but also many more topics to get yourself more visible. Thanks, Impact Challenge.\n\n\nWhat about LinkedIn?\nI haven’t found LinkedIn terribly useful for people looking for academic jobs (like me). It might be worth it to set up a low-maintenance page that gives a good view of you in a nutshell, just in case people look."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html#building-a-personal-webpage",
    "href": "blog/brand-yourself-2/index.html#building-a-personal-webpage",
    "title": "Brand Yourself 2",
    "section": "Building a Personal Webpage",
    "text": "Building a Personal Webpage\nKeeping track of all these profiles can be tedious. Do you need to update seven different profiles every time you present at a conference? Is it worth it to invest the time in these sites that don’t communicate with each other? One solution is to choose one site to be your main one and build a full profile there. Then, create a “highlights” version on the other social media sites and redirect people to your main page. What should be your main site though? For this reason, it’s nice to have a personal webpage.\nThe problem with personal webpages it that they come with a cost, either in money or skills (and sometimes both). You can set up a webpage through Word Press, Wix.com, or Square Space, which take little technical skill to get a professional page set up. These can be free, but you can get some extra features for $10 a month or more. To me, that’s a pretty penny to pay for a relatively simple webpage.\nAnother option, which is what I’m doing for [Edit: did for the old version of] this website], is to host the page on Github. It’s free, but it takes a bit of skill. I’ve had to learn to use Jekyll, Markdown, and CSS, but through some help on ProgrammingHistorian.com and Lynda.com, I was able to get this site up. The benefit of going this route is I have unlimited flexibility in how the site looks, and I really, really like that.\nEither way, it’s probably worth it to set up a personal domain name. For as little as $1 a month, you can buy your own domain name (like www.joeystanley.com), which looks much more professional than www.blogsplot.com/joeystanley or www.github.com/joeystanley."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html#finding-your-conversation",
    "href": "blog/brand-yourself-2/index.html#finding-your-conversation",
    "title": "Brand Yourself 2",
    "section": "Finding Your Conversation",
    "text": "Finding Your Conversation\nThe last thing we talked about in our workshop is to find where the big names in your field are having their online conversations. This sounds a little weird at first, but every field has some secret space where people are collaborating and sharing ideas informally as well as posting calls for papers, invitations for publications, and job openings. The problem is that where is space is is different for every field.\nIn some fields, these are a listserv. As far as I know, network analysis and Slavic languages each have a well-known listserv where all the conversation happens. If you’re not on that listserv, you’re out of the loop. Digital Humanities has a space on Slack where over 800 researchers get together and talk. For some fields, it might just be at coffee breaks during certain conferences. You may have to ask around established academics in your field to find that space.\nOne thing I will mention is that a lot of action happens on Twitter, like livetweeting conferences. I’ve covered this in more depth in an earlier blog post, but basically a lot of good stuff can come out of following the right people and seeing just the right tweets.\n\nI’ve given this presentation multiple times now. You can see the (very similar) slides from April 13, 2017 and the ones from November 11, 2016."
  },
  {
    "objectID": "blog/brand-yourself-2/index.html#conclusion",
    "href": "blog/brand-yourself-2/index.html#conclusion",
    "title": "Brand Yourself 2",
    "section": "Conclusion",
    "text": "Conclusion\nIn today’s job market, there’s no surefire way to ensure you’ll get hired. But increasing your online presence and making yourself more googleable probably helps."
  },
  {
    "objectID": "blog/a-survey-of-western-american-english-using-mturk/index.html",
    "href": "blog/a-survey-of-western-american-english-using-mturk/index.html",
    "title": "A Survey of the Western American English using MTurk",
    "section": "",
    "text": "I’m so happy to announce I’ve been selected as a recipient of the UGA Graduate School Innovative and Interdisciplinary Research Grant! This $2,500 grant is part of the Graduate School’s strategic initiative to support innovation and interdisciplinary in the research being conducted by doctoral students. My project is entitled “A Survey of Western American English using Amazon Mechanic Turk.”\nAmazon Mechanical Turk (“MTurk”) is a crowdsourcing marketplace through Amazon.com where people and business can post tasks for others to perform for a small amount of payment. They are usually menial tasks like completing surveys or data entry. At NWAV last year, Kim, Reddy, Wyschogrod, and Stanford did a presentation on how they cleverly used MTurk to gather recordings of people across the Northeast. After some discussion with some of the authors, I decided to apply for a grant that would pay for the same kind of data collection but targeting the West.Kim, Chaeyoon, Sravana Reddy, Ezra Wyschogrod & James Stanford. 2016. A large-scale online study of dialect variation in the US Northeast: Crowdsourcing with Amazon Mechanical Turk. Paper presented at the New Ways of Analyzing Variation 45, Victoria, BC.\nCompared to other parts of the country, the West has relatively little linguistic research. Some of that is changing, thanks to some of the folks in Stanford, University of Washington, and other universities. But there are large portions of the country like Montana, Wyoming, and other places that have very little written about them. The population isn’t huge, but there are people there, and those people do speak. So what do they sound like?\nThis project will hopefully get recordings from possibly up to 500 people in specific western states. This will result in a corpus of over 100 hours of audio and roughly 200,000 words—a sizable linguistic corpus which I can analyze for many years to come. This will allow a slightly deeper look into some of these places, acting as a launchpad for further research in specific cities.\nI’m really excited to have gotten this grant and to get started. I will post updates as they come. Stay tuned."
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html",
    "href": "blog/new-publication-in-pwpl/index.html",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "",
    "text": "I’ve just been informed that a manuscript I submitted to the Penn Working Papers in Linguistics has been published! It’s called “Order of Operations in Sociophonetic Analysis” and is available here. In a nutshell, I discuss the various processing steps that are typical of a sociophonetic analysis an I show that changing the order that you run them can have a non-negligible impact on the overall results. I end the paper with a recommended order that we can use."
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html#overview",
    "href": "blog/new-publication-in-pwpl/index.html#overview",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "Overview",
    "text": "Overview\nI originally started this paper because I noticed, by accident, that I was getting different results after moving some code around. I was in the middle of an analysis (of the very dataset I use in the paper) and had already gotten some results, but I had to change a few things in the processing and ended up moving a chunk of code up a little bit so that it happened before another chunks. Lo and behold, the results changed. Nothing about the underlying data changed, and nothing about the functions I ran changed—other than their order.\nSo this sent me down a bit of a rabbit hole. I identified seven processing steps that I did in my analysis pipeline. I then wrote some code that arranged those seven steps into all possible orderings, all 5,040 of them. I then took the exact same input dataset, and processed it 5,040 times, once for each permutation. With each resulting spreadsheet, I then calculated things like how shifted or overlapped people’s vowels were. Again, to be clear, the underlying data was identical, and the functions I ran were identical. The only thing that changed was the order that I ran them.\nWell, as you can read in the paper, the results were a little concerning. First, I got many unique values for each person. Not 5,040 unique values, because some pipeline result in identical outputs. But sometimes hundreds of unique values. Some of these were admittedly very similar, but others were quite drastic. In one case, I point out that if I use some pipelines, a particular individual may be interpreted as being a linguistic innovator since she had advanced measures of some of the shifts. But in other pipelines, she would be seen as linguistically conservative since those same measures returned less-advanced measures. Taking the entire group of 53 speakers collectively, some pipelines showed that about half had shifted vowels while other pipelines showed that nearly all of them did. So, just by changing the order that I run my code can have an impact on how my dataset is interpreted!"
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html#takeaways",
    "href": "blog/new-publication-in-pwpl/index.html#takeaways",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "Takeaways",
    "text": "Takeaways\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that this is a different order than what I suggest in my NWAV49 talk! The version shown here is what I currently stand by.\n\n\nSo, in addition to pointing out the problem, I do offer a potential solution by prescribing an order of operations. Here’s what that is:\n\nReclassify your data into allophones\nThis is important because subsequent steps really need to run by allophone rather than by phoneme. I’d even say that we should separate the data into allophones even for vowels we’re not interested in (like allophones of /u/ even if we’re only interested in front vowels) because it has implications for what data gets excluded which could affect the centering when you normalize.\nRemove “bad” data\nThis is when you remove outliers. Again, I argue that it should be done by allophone. I also recommend that stopwords and unstressed vowels be treated as their own separate category as well. But, I’m no married to that idea.\nNormalize\nRegardless of what normalization procedure you use, I think it is at this point in the pipeline that the procedure should happen. The normalization should include all good data (even if it’s not pertinent to the study) and not include any bad data.\nRemove “good” but otherwise uninteresting data.\nFinally, here is where you should subset your data to focus on just the stuff you’re interested. So, get rid of vowels or allophones you don’t want, stopwords, unstressed vowels, etc. Importantly though, it is only at this point in the pipeline that you should toss vowel trajectories! If you never extracted them in the first place, or just ignored those columns in FAVE’s output, then you’re excluding trajectory data as step 1 and that has an impact on your results!\n\nFor now, these steps are based more on logic and theory (and I appreciate the input from Rich Ross and Thomas Kettig for helping me out with that)! I have a manuscript at the moment that dives more into these steps and provides some more quantitative justification for them. Stay tuned!"
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html#my-soapbox",
    "href": "blog/new-publication-in-pwpl/index.html#my-soapbox",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "My Soapbox",
    "text": "My Soapbox\nFinally, towards the end, I do get a little preachy. First, I recommend that Order of Operations be explained in detail in methods sections so that we can evaluate them better. In fact, it was only after I submitted this paper that I found Brand, Hay, Clark, Watson, & Sóskuthy’s (2021) paper that does exactly what I want to see! That’s exactly the kind of transparency that we need. Of course, at this point, it’s hard to interpret what effect one order has upon the data as opposed to another order, though, again, stay tuned for my manuscript which does exactly that!\nI then say that quantitative linguists should be more mindful of how the data is being processed. I basically subtweet some methods that I’m not particularly fond of (Lobanov transformations, ANAE “benchmarks”) without calling them out specifically. I also subtweet a reviewer for a different paper who said to me that I shouldn’t use New Technique X and should instead use Old Technique Y for comparability with previous studies, even though recent research has shown that Old Technique Y is flawed and New Technique X is better. Anyway, so if those paragraphs sound cryptic, now you know some background.\nFinally, I encourage quantitatively-minded linguists to continue writing methods papers. Whether it be developing a new technique or comparing existing techniques against each other, all that stuff is good for the field.\nAnyway, I took the opportunity in this non-peer-reviewed methods paper to stand on my soapbox a little bit and talk about some of my views on the current state of quantitative linguistics. And I stand by what I wrote in the paper."
  },
  {
    "objectID": "blog/new-publication-in-pwpl/index.html#conclusion",
    "href": "blog/new-publication-in-pwpl/index.html#conclusion",
    "title": "New publication in the Penn Working Papers in Linguistics",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s it! This was a fun one to write because it literally has nothing to do with language or how language works: it’s purely methods. In fact, it’s not even about one specific method. It’s about how we should run our R code! Very niche. But I hope it does some small part in improving quantitative linguistics papers.\nPS: In my notes I refer to “Order of Operations” as “OoO”. I’m 1000% okay with having OoO be a new abbreviation in people’s code and in methods sections now. Who wants to be the first? :)"
  },
  {
    "objectID": "blog/lcuga4/index.html",
    "href": "blog/lcuga4/index.html",
    "title": "LCUGA4",
    "section": "",
    "text": "This weekend, I had the opportunity to present twice at the 4th Annual Linguistics Conference at UGA. One was planned and the other was a last-minute fill-in for someone who couldn’t make it. I was happy to do both.\nFriday’s presentation was called The linguistic effects of a changing timber industry: Language change in Cowlitz County, WA. Here, I talk about some of the sudden linguistic changes that I found in apparent time and suggest that they had to do with changes in the timber industry around that time. Because this was last-minute, it is basically a precursor to (and a slideshow version of) my NWAV46 poster that I’ll be giving in a few weeks. You can download the slides for this talk here\nSaturday’s presentation with Kyle Vanderniet was called Consonantal variation in Utah English: What el[t]se is happening[k]?. We talked about three variables that seem to be particularly salient in Utah English\n\nThe various pronunciation of words like mountain, button, or satin with the last syllables as [ʔn̩], [ʔɨn], or [tʰɨn].\nInsertion of [t] between /ls/ clusters, as in fal[t]se or el[t]se.\nRealizing word-final ing as [ɪŋk].\n\nYou can see our slides for this presentation here. Edit: we later presented additional findings from this research at ADS2018.]"
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html",
    "href": "blog/interactive-guarani-dictionary/index.html",
    "title": "Interactive Guarani Dictionary",
    "section": "",
    "text": "The semester is finishing up, and as usual, the most productive week for me is during finals. Not necessarily productive regarding school work or current research projects, but I always rediscover side projects and hobbies. This week I rekindled my interest in Guarani."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html#brazil",
    "href": "blog/interactive-guarani-dictionary/index.html#brazil",
    "title": "Interactive Guarani Dictionary",
    "section": "Brazil",
    "text": "Brazil\nI’ve been working on Guarani off and on since 2009. I was living in Campo Grande, Mato Grosso do Sul, Brazil, as a Mormon missionary at the time. Fairly regularly I would meet people that spoke this language called Guarani, and I had friend (a fellow missionary), who had some pedagogical materials that taught Spanish speakers Guarani. So I had to work through the Spanish (I had only been speaking Portuguese for 9 months or at that point), but I was able to decipher some of the basic Guarani morphology and grammar. A while later my dad sent me a copy of the Book of Mormon in Guarani and said I ought to learn what I could. So I sat there with the Guarani, Portuguese, and English translations and would try to figure out new words and morphology.\nAgain, I was a Mormon missionary at the time, so I didn’t have a lot of time to spent learning this language. I hadn’t begun studying linguistics yet, so I had no idea what a non-Indo-European language could possibly be like and there were a few things that had me stumped. I also didn’t have access to a computer, so I couldn’t keep track of notes and vocabulary very well. So every couple of weeks I’d sit there with a dozen sheets of paper spread all over my desk, trying in vain to keep things alphabetized as I added vocabulary and translations. My Brazilian buddies all thought I was insane for trying to learn this language, but I found it to be a LOT of fun.\nOne of the more frustrating things was that I wanted to see how a single word was used in other contexts. If I was looking through a sentence and there were three Guarani words I didn’t know, I often had no way of knowing which word corresponded with the meaning in the English sentence. If only I could control+F the book and find the Guarani words in other contexts and figure out the meaning."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html#self-study",
    "href": "blog/interactive-guarani-dictionary/index.html#self-study",
    "title": "Interactive Guarani Dictionary",
    "section": "Self-study",
    "text": "Self-study\nAfter I came back to the United States and went back to college at BYU, I found that there were some books written about Guarani grammar, but they were mostly older ones. I didn’t know it at the time, but a former Department Chair in the Department of Linguistics and English Language at BYU was Robert W. Blair, who published some Guarani pedagogical material. I found his Guarani Basic Course at the library as well as his student, Charles Graham’s, Guarani Intermediate Course, and did what I could going through those. There were some other more descriptive grammars of the language written in the mid 20th Century, and I even sat in on a Guarani course for a semester.Yes BYU offers a course in Guarani! The class was taught only every once in a while and was intended for Mormon missionaries who had spent time in Paraguay. The class was taught in Spanish (again—not a language I’ve studied) by a native Guarani speaker, and was intended to add some formal instruction to people already familiar with the language. I was overwhelmed with other courses so I couldn’t keep up for more than a few weeks."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html#translation-program",
    "href": "blog/interactive-guarani-dictionary/index.html#translation-program",
    "title": "Interactive Guarani Dictionary",
    "section": "Translation Program",
    "text": "Translation Program\nI was in my last year at BYU. I was working as a programmer, creating eBooks for WordCruncher and had access to an HTML file of the Guarani Book of Mormon. I had taken a class in Perl already and had gotten pretty proficient through that job. I had also taken Mark Davies’ Corpus Linguistics course. So when I took an NLP course as the capstone to my minor in Linguistic Computing, I decided to write a Guarani translator.\nThe program worked pretty well and was exactly what I was dreaming of in Brazil. I had paired the Guarani and English text as a “parallel corpus”, meaning each line in one file corresponded to a translated line in the other. What the translator does is it takes an input string (say, mba’apo) and it displays all the Guarani sentences with that word with the English underneath it. Made it very handy to see how words (or parts of words) were used in other contexts.This corpus might actually be the largest Guarani-English parallel dictionary. It had 329K Guarani words when first wrote the translator, but it’s now up to 606K after adding some more translated church material. I’ve got another ≈250K to add to it, whenever I get the time. I could nearly double it even then if I get access to the Guarani Bible, though I don’t know if that’ll happen anytime soon. Granted, these are all translated texts from English, and are religious-based, obviously representing a very different style than naturally occurring, spoken Guarani.\nWhat it then does it is look at all the words in both the English and Guarani sentences with the word, keeps track of their frequencies, then looks at the frequencies for all words in the entire corpus and compares the two. Words that have nothing to do with the translation will occur with roughly the same frequency in the matched sentences as they do in the full corpus. But words that correspond to the same meaning will occur relatively much more often in the matched sentences compared the corpus as a whole. So say the word work appears once every 1000 words in the whole corpus. If it suddenly appears once every 25 words in the matched words, statistically that’s a big difference, and odds are pretty good that work is a translation for mba’apo (and it is).\n\n\n\nGuarani Search engine\n\n\nSo using this I could find out which English words correlated with which Guarani words. Not a perfect translator, especially since it didn’t use any fancy NLP processing, but not bad.\nMy interest in Guarani, which was mostly about its nasal harmony, verbal morphology, and trying to document the grammar as a whole, started to wane as I started grad school and focused more on sociolinguistics and dialectology. But my reading comprehension is still… okay let’s face it, not that great, but I’m surprised at how much I was able to learn through self-study and a custom computer program."
  },
  {
    "objectID": "blog/interactive-guarani-dictionary/index.html#interactive-dictionary",
    "href": "blog/interactive-guarani-dictionary/index.html#interactive-dictionary",
    "title": "Interactive Guarani Dictionary",
    "section": "Interactive Dictionary",
    "text": "Interactive Dictionary\nI think what started this recent resurgence in Guarani was, strangely enough, making this website. I’ve acquired some more HTML and CSS skills and realized that I could make something useful with a web browser. So I dusted off my old files and started something fun.\nIn just a week I was able to make a pretty useful website (locally hosted only for now) with two main pages. The first is the entire corpus. Unlike what I had before, I could take advantage of the formatting to display useful information. All the words I know are in regular black text, but the words I don’t know stand out in blue. That makes it easy to figure out which ones I need to learn next. For the words I do know, the roots are underlined, so I can quickly see the base and what morphology is stemming off of it. The interactive part is that if I mouseover the root, a basic definition shows up in the form of a tooltip. So if I’ve forgotten a word, I can very quickly remind myself of what it means. Very handy.\n\n\n\nGuarani Corpus screenshot\n\n\nHow am I keeping track of what I know and don’t know? The other page on the site is a dictionary. I usually kept all this stuff in a spreadsheet somewhere, but here I can utilize the formatting to make it look like a real dictionary. I’ve got roots, possible word forms, derivatives, translations, parts of speech, etymology, other notes, and the infrastructure to include example sentences and other metadata. All this is stored on a file on my computer, and when I learn a new word, I just add it to the bottom of the file and a Perl script will take care of alphabetizing it and making sure it looks good for the CSS to take over.\n\n\n\nGuarani Dictionary screenshot\n\n\nThe result is a slick system where I can quickly see what words I need to learn and I can easily add them to the dictionary. I then run a lightning fast Perl script and refresh my browser, and I’ve got an updated corpus and dictionary.\nThe system is set up to handle as big of a corpus or dictionary as I’m willing to feed it. For now, I’m only a couple paragraphs in and I’ve got over 100 entries in the dictionary. It will take hundreds of hours to go through my entire corpus. But for the first time I’ll be creating a decent Guarani dictionary, which is kinda what I had in mind to do the whole time."
  },
  {
    "objectID": "blog/laboratory-research/index.html",
    "href": "blog/laboratory-research/index.html",
    "title": "Laboratory Research",
    "section": "",
    "text": "Recently, I’ve presented on words like pool, pull, and pole and how the difference between them can be really hard to describe, both by me and the non-specialist alike. Based on my findings in Washington, I decided I wanted to dig a little deeper into what these words are like, so I started a study that is less sociolinguistic and more laboratory phonology-based, which is a little unusual for me.\nBroadly, I want to look at the phonetics of English vowels before coda laterals. So, after making a list of lots and lots of possible words, cutting them down based on frequency and other factors, I’ve got a decent list of targeted words.\n\n\nList of all ~401 monosyllabic English words with a coda lateral, organized by vowel and syllable structure? Check. My favorite? Squelched.\n\n— Joey Stanley (@joey_stan) May 7, 2017\n\n\nI got IRB approval just a little too late into the semester to recruit people and offer them extra credit in courses, so I had to wait a few weeks to get started. Now that Maymester has started, I’ve approached some professors and asked them to offer participation in my research as an extra credit opportunity. I even hand out little business cards after I’ve done my pitch to the class, so they have my contact information—an idea that proved very effective for me in Washington:\n\n\n\nRecruitment business card\n\n\nSo I’m now meeting with students in the linguistics laboratory that we have here at UGA. It’s unfortunately under-utilized but nonetheless very good. Inside the already very quiet recording studio is a tiny booth where the best recordings can be made. I have participants reading a bunch of carefully selected sentences that target key sounds and then taking a quick follow-up survey. It amounts to about 30–40 minutes of speech from each person, which is kind of a lot.\nI don’t have a specific goal for how many people I want to get, but I should have 20 by the end of the month and potentially up to 50 by the end of the summer. My only limitation is how much time I can put into this. I’ll do some preliminary analyses on those and see if I need to recalibrate the sentences or maybe collect more data. This will probably be an ongoing project for a while: the IRB and consent forms are purposely pretty open-ended to allow me to modify things where needed without much hassle.\nAnyway, it’s been fun being in the lab, and I’m excited to analyze really clean audio for a change."
  },
  {
    "objectID": "blog/nwav50/index.html",
    "href": "blog/nwav50/index.html",
    "title": "NWAV50",
    "section": "",
    "text": "Today I gave a talk that Betsy Sneller and I have been working on called “How Sample Size Impacts Pillai Scores – and What Sociophoneticians Should Do About It” at the 50th New Ways of Analyzing Variation conference in San Jose! This is an updateto what we presented at ASA2021.\nWe have three things you can download.\n\nFirst is the actual powerpoint file. In the notes of each slide though you can see the actual script I read, so you can read every word that was said during the talk.\nNext, if that’s too much for you, you can download just a PDF of the talk. In case you want this ligherweight version of the slides.\nFinally, here is the current manuscript that is under review with the Journal of the Acoustical Society of America. The final product will likely change somewhat, but most of the information is there. [Edit (December 14, 2022): Here is the accepted version.]\n\nIf you need to calculate Pillai scores in R, I’ve got a two-part tutorial for you (here and here). I also did a blog post (here) about how Pillai scores don’t seem to change after normalization."
  },
  {
    "objectID": "blog/mount-st-helens-and-vowels/index.html",
    "href": "blog/mount-st-helens-and-vowels/index.html",
    "title": "Mount St. Helens and Vowels",
    "section": "",
    "text": "Today in our Linguistics Colloquium here at UGA, I got to present on some of my ongoing research on English in a smaller town in Washington. For the past few months I’ve mostly looked at vowel mergers and using lots of statistical tests to show some very subtle changes. Over the past week or so as I’ve prepared for this presentation, I’ve discovered something pretty awesome about my data. And it has to do with Mount St. Helens!\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nIn the presentation, I focus on a couple linguistic variables. The first is what linguists call /æg/-raising, which is where words like bag, flag, and dragon to sound more like bayg, flayg, or draygon. The other variable is what we call /o/-fronting and /o/-monophthongization, which is where vowel sounds in words like go, snow, or show sound kinda like they would in stereotypical “Minnesohhta”. These have been studied extensively by researchers in the West regarding Pacific Northwest English and surrounding regions. So, nothing new here.\nBut looking at the data in relation to speaker age, I noticed a striking pattern: there’s a clear difference between the speech of people born before 1970 and those born after. I mean really clear. In my sample, /æg/ raising virtually disappears after 1970, and /o/ is suddenly diphthongal. /o/ admittedly gradually fronts, so the 1970 date isn’t quite as drastic in that regard.\nSo what happened in 1970? Well, not much. But in 1980, Mount St. Helens erupted and seriously affected the logging-based economy of Longview. Up until then, it was easy to find work in the logging industry with only a high school degree, if that. And the salary was relatively good considering it was blue collar work. But when some of the mills started to close, there was a drastic change in the dynamics of the town. Now you need a college education to get a job and even then it’s not paying well.\nSo though nothing happened in 1970, those who were born around then were teenagers at the time of this change, and were the first affected by the lack of easy-to-get, high-paying jobs. This marks a paradigm shift in the culture of Longview, and I believe it had to do with the clear changes in the speech. In other words, I think Mount St. Helens played a role in linguistic change in Longview. (The title of the talk was “Volcanic Vocalic Changes”—a title I’m quite proud of!)\nThis is super exciting for me because up until now most of my work has been phonetic-based and focused on vowel mergers. This the first clearly sociolinguistic project I’ve done—something I’ve been meaning to do this whole time—and I think the results are cool. It’s uncharacteristically qualitative and the statistics don’t play a huge role, which is weird for me. I like this change and I hope I can do more with this research."
  },
  {
    "objectID": "blog/lcuga5/index.html",
    "href": "blog/lcuga5/index.html",
    "title": "LCUGA5",
    "section": "",
    "text": "Today, I was fortunate to give two presentations on very different areas of my research at the 5th Annual Linguistics Conference at UGA, one on an obscure consonantal phonological pattern in the West using new recordings and another on well-studied vowel shifts in the South using very old recordings."
  },
  {
    "objectID": "blog/lcuga5/index.html#thr-flapping",
    "href": "blog/lcuga5/index.html#thr-flapping",
    "title": "LCUGA5",
    "section": "(thr)-flapping",
    "text": "(thr)-flapping\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nFirst, I presented on something I’ve noticed in a few speakers, something I call (thr)-flapping. Some people pronounce the /ɹ/ after /θ/ as a flap [ɾ]. In this presentation, I presented some data supporting this hunch.\nThe articulatory motivations are clear: as the tongue tip moves from between the teeth to a retroflexed position, it may make brief contact with the alveolar ridge. What may have started as an accidental gesture appears to have been phonologized by some speakers.\nLooking at phonological factors, in my data (thr)-flapping happened more when the following vowel was non-high and non-front. So it happened more in throb, throng, throne, thrust, and thrive than in thrash, threaten, thread, thrill, three and through. I offer some tentative explanations for this, but without articulatory data, I can’t know for sure.\nTo my surprise, the social factors I looked at were the opposite of what I expected. Age and sex were not significant, meaning there’s probably not a lot of change in time. But what state people came from (Washington verses Utah—my two fieldsites) was significant. Utah English has a lot of hyperarticulated consonants and at ADS this year, Di Paolo & Johnson (2018) hypothesize that this has to do with the high proportion of members of the Church of Jesus Christ of Latter-day Saints in Utah. Since public speaking is a common part of their worship services (even as early as the age of 3!), elements of this hyperarticulated register may have spread into other speech styles. (thr)-flapping may be just another manifestation of that. But without attitude or perceptual data, I can’t know for sure.\n(thr)-flapping is something I’ve noticed for a while now, and despite the shortcomings of this study, and I was glad to finally present solid evidence that it is a thing!"
  },
  {
    "objectID": "blog/lcuga5/index.html#vowel-shifts-in-southern-american-english",
    "href": "blog/lcuga5/index.html#vowel-shifts-in-southern-american-english",
    "title": "LCUGA5",
    "section": "Vowel Shifts in Southern American English",
    "text": "Vowel Shifts in Southern American English\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nIn the next session, I presented research I been doing with Peggy Renwick on the vowel shifts in the South. The Southern Vowel Shift has front lax vowels raising and front tense vowels lowering, resulting in vowel pairs swapping. Meanwhile, back vowels are fronting. The African American Vowel Shift has the front lax vowels raising, but tense vowels are not lowering, so there wouldn’t be any swapping. Furthermore, back vowels typically aren’t fronted.\nWe use the Digital Archive of Southern Speech, a corpus of interviews from the 1970s and 1980s. The people in these recordings were born while these shifts were going on, so we can see their development in a way that newer recordings wouldn’t be able to do.\nUsing Pillai scores and linear mixed-effects models, we find that younger, European American women are leading in the front vowel swapping and that African Americans are participating less in the back vowel fronting. These findings are exactly what we expect, showing that these older recordings confirm what newer ones suggest."
  },
  {
    "objectID": "blog/lsa2017/index.html",
    "href": "blog/lsa2017/index.html",
    "title": "LSA2017",
    "section": "",
    "text": "Last weekend, I had the opportunity to present at the 2017 Annual Meeting of the American Dialect Society, as well as attend the other meetings of the Linguistic Society of America annual meeting in Austin, TX. There were a lot of really awesome things about the whole thing.\nFirst off, I feel like I had a great experience giving my presentation. I presented Thursday afternoon in the session called “Vowels, Vowels, Vowels” which was chaired by Erik Thomas, and saw other presentations by Charlie Farrington & Tyler Kendall, Matthew Gordon, and Michol Hoffman. There were about 30 people in the room, and I could name about half of them. In fact, while summarizing previous research, I realized that half the people I cited were sitting right there. Afterwards, I had a lot of discussion and great feedback. I couldn’t have asked for a better experience.As part of the presentation, I showed this video.\nDuring the course of the next several days, I made a point to introduce myself to people. Networking is an important part of going to conferences, and I haven’t really taken that opportunity in the past. So I was able to meet several of the greats and tell them how much I enjoyed some of the things they’ve written. I also met some grad students that have similar interests as me. I feel a lot more connected to other researchers than I did before.\nGoing into this conference, I knew I wanted to give it everything I had. I got funding from UGA for the first time, both through the Linguistics Program and the Graduate School, and I wanted to make sure the money I received went to a good cause. The conference was busy, but I attended as many presentations as I could. In fact, I hardly had time to eat, and ended up only eating one meal a day during the four days. I actually lost four pounds attending this conference! I also stayed at an Airbnb for the first time, and I didn’t want to take the bus all the way to the apartment when I knew there were things to do at the conference. I ended up attending 35 presentations and visited about a dozen posters. It made for four very long and busy days, but they were extremely productive.\nI also made an effort to be active on Twitter during the conference, but I have a separate blog post about that which you can read here.\nOverall, a fantastic experience. The best I could have hoped for, and the best conference I’ve been to."
  },
  {
    "objectID": "blog/nwav47/index.html",
    "href": "blog/nwav47/index.html",
    "title": "NWAV47",
    "section": "",
    "text": "Today, I gave a poster presentation on prevelar raising. As it turns out, despite beg and bag being relatively small lexical classes, I found phonological, morphological, and lexical effects on the degree of raising, and that the two vowel classes reacted to these influences differently.\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\n\n\n(Photo by Maciej Baranowski.) \n\nSo for some speakers, mostly in the Pacific Northwest, the Upper Midwest, and Canada, they raise the dress (beg) and trap (bag) vowels before voiced velars. The idea for this research mostly came about because I was trying to figure out whether I have beg-raising. Turns out, I do, except for in a few words like integrity, negligible, and segregate which seems somewhat unexpected.\nSo I set up a categorization task that I distributed via Reddit. I showed people a bunch of beg and bag words and asked them whether the vowel in their pronunciation is most like that in bake, deck, or back. Almost 7,000 people took the survey and I got over 500,000 observations.\nAs it turns out, there were clear phonological effects for beg. If the /ɡ/ was followed by a sonorant (as in regular or segment), particular a liquid (negligible or segregate), it was indicated to be raised less. Adding suffixes had different effects: people indicated a raised vowel more when -ed was added to verbs, but only for bag. There were lexical effects too: for bag, more frequent words were raised more, borrowings with beg were raised more, and beg words with orthographic &lt;ex&gt; (like exile or exit) were virtually never raised.\nThe takeaway is that even though these are relatively marginal lexical classes, they still have interesting language-internal factors. And, even though beg- and bag-raising are generally considered to be the same phenomenon of prevelar raising, the two are different. There are some major limitations here, particularly because reported speech is never completely reliable, so I’d like to follow up with an acoustic study. But, I think it’s important to include many more words in a word list because some of these patterns would not have been found if a just small one was used."
  },
  {
    "objectID": "blog/nwav49/index.html",
    "href": "blog/nwav49/index.html",
    "title": "NWAV49",
    "section": "",
    "text": "I’m at New Ways of Analyzing Variation 49 online right now! Other than an quick online satellite session of LabPhon last summer, I haven’t attended a conference since November 2019 when we hosted LCUGA at UGA. Anyway, I’m excited to be conferencing again and while I miss seeing colleagues in-person, this online format isn’t bad. Anyway, on this page you’ll find links to the slides and YouTube videos of my two talks."
  },
  {
    "objectID": "blog/nwav49/index.html#order-of-operations-in-sociophonetic-analysis",
    "href": "blog/nwav49/index.html#order-of-operations-in-sociophonetic-analysis",
    "title": "NWAV49",
    "section": "Order of Operations in Sociophonetic Analysis",
    "text": "Order of Operations in Sociophonetic Analysis\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nI’m very excited and nervous about this Order of Operations one. As it turns out, even if you start with the exact same spreadsheet and use the exact same functions, if you do those function in different orders, it’ll produce different results. Sometimes drastically different results. I did this by processing a spreadsheet 5,040 unique ways and got a whole range of results. To me at least, it’s making me rethink how I process my data and how I can interpret others’ results when the order isn’t explicitly reported in the methods section of a paper."
  },
  {
    "objectID": "blog/nwav49/index.html#years-of-georgia-english",
    "href": "blog/nwav49/index.html#years-of-georgia-english",
    "title": "NWAV49",
    "section": "100 Years of Georgia English",
    "text": "100 Years of Georgia English\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nAs a continuation of some work I did as a grad student, Peggy Renwick and I presented our research on Georgia English vowels and how they’ve changed over 100 years. Basically, all of them have. The Southern Vowel Shift seems to have undergone a rise and fall, perhaps peaking in those born around WWII. Meanwhile, back vowels are fronting. Younger people today have something like the Low-Back-Merger Shift, but flavored with some Southernisms still. You’ll hear more about this project in later conferences too."
  },
  {
    "objectID": "blog/lots-of-transcribing/index.html",
    "href": "blog/lots-of-transcribing/index.html",
    "title": "Lots of Transcribing",
    "section": "",
    "text": "Last summer, I collected roughly 40 hours of conversation from people in Washington State last year. Not an enormous corpus, but I’m quite proud of that dataset. Well, my goal was to transcribe one speaker gradually over the course of the year, finishing around now. Well, in 9 months, I’ve done about… one hour. I realized this week that I really really need to get these done.\nNow, I haven’t just been slacking off doing nothing for the past school year. I’ve been very busy with my research assistantships and with other tasks. I’ve given workshops and seminars here at UGA, and my name has been on a lot of conference presentations in the past few months, even if I haven’t been able to attend them. I’ve put together this website as well.\nI’ve got my second qualifying paper coming up (edit: see my post about it here and it occurred to me that if I want to graduate in a year, I had better get these recordings transcribed!\nBecause I am the way I am, I kept track of my transcription rate. I did the first hour or so in about 5 hours. Okay, so at that rate, it’ll take me around 200 hours of work to finish the transcriptions. Okay, so at an hour a day that’ll take me like 8 months. At two hours a day I’ll finish in like August. Yikes. That’s a lot of work.\n\n\n39 hours of audio left. At two hours of transcribing a day I'll finish by… Thanksgiving. Am I old enough to hire undergrads yet?\n\n— Joey Stanley (@joey_stan) April 28, 2017\n\n\nSo I decided to just get to it. I can calculate and project and write about it all I want, but it’s not going to get done unless I just go for it. And I’ve got to be the one to do it. Even though there are automatic transcribers out there, I’d have to double-check their work anyway, and if I need to listen to it all I might as well do it all myself. Plus, it’s nice to go through them again and listen to linguistic features I didn’t catch before. And there are a lot!\n\nEdit: I’ve decided to tweet every so often to keep track of my progress and to keep myself motivated.\n\n\nI put in four hours today and finished an entire speaker, averaging 3.2 minutes of work time for every minute of transcription time. Nice.\n\n— Joey Stanley (@joey_stan) May 2, 2017\n\n\n\n\nI'm 15% finished transcribing. Just finished listening to my interview with a professional radio announcer. Wow. Talk about clear audio.\n\n— Joey Stanley (@joey_stan) May 12, 2017\n\n\n\n\nJust hit the 20% mark for transcribing my interviews. I'm currently reliving the one with a guy who worked falling trees for 18 years.\n\n— Joey Stanley (@joey_stan) May 16, 2017\n\n\n\n\n25% done with transcribing. Would have hit it earlier, but I’ve got two other projects going right now, which means more transcribing later…\n\n— Joey Stanley (@joey_stan) May 31, 2017\n\n\n\n\nAfter a bit of a hiatus, I'm back to transcribing and just hit the 30% mark for my corpus! Hooray!\n\n— Joey Stanley (@joey_stan) August 3, 2017\n\n\n\n\nAfter another long hiatus, I'm back to transcribing. I just hit the 35% mark of my corpus. I'm averaging around 4.6 hours of work for every hour of audio, and I've got about 26 hours of interviews left. https://t.co/AyjOL9gIXX\n\n— Joey Stanley (@joey_stan) April 17, 2018\n\n\n\n\nOkay I'm now 40% finished transcribing. I've put 78 hours into this already and I've got approximately 105 hours of work left. I'm hoping to be done by July. This is crazy. #amtranscribing\n\n— Joey Stanley (@joey_stan) April 25, 2018\n\n\n\n\nI'm now 45% done with transcription. It'll be really satisfying to heat that 50% mark in a week or so. Only 60 workdays left! #amtranscribing\n\n— Joey Stanley (@joey_stan) May 2, 2018\n\n\n\n\nHooray! I'm HALFWAY done with transcribing these interviews! At slightly more than two hours a day and my current pace of 4.5 hours of work for every hour of audio, I should be done by July. #amtranscribing\n\n— Joey Stanley (@joey_stan) May 8, 2018\n\n\n\n\nJust hit 55%. Learning a lot about welding right now. 1022 minutes of interviews left to transcribe, but who's counting anyway? #amtranscribing\n\n— Joey Stanley (@joey_stan) May 16, 2018\n\n\n\n\nOkay, so now I'm 60% done. I'm enjoying all the technical terms I'm learning about: welding on the last interview and quilting on this one. Only 65 more hours of work left! #amtranscribing\n\n— Joey Stanley (@joey_stan) May 22, 2018\n\n\n\n\nOkay, I'm now 65% done with transcribing my corpus! About 6 more weeks and I'll be finished. #amtranscribing\n\n— Joey Stanley (@joey_stan) May 29, 2018\n\n\n\n\nAlright, I just hit the 70% mark. Just 10½ hours of interview left, which should take me about 45 hours to do. In less than a month I'll be done! #amtranscribing\n\n— Joey Stanley (@joey_stan) June 7, 2018\n\n\n\n\nHooray! 75% done! Just finished an interview with a native Washingtonian who totally has existential \"it\" and \"liketa.\" That was cool. #amtranscribing\n\n— Joey Stanley (@joey_stan) June 13, 2018\n\n\n\n\nIt's been a busy week of transcribing, and even though I hit 80% on Tuesday, I just finished out the week at 85%. I did an entire interview yesterday and another one (plus some change) today! #amtranscribing\n\n— Joey Stanley (@joey_stan) June 29, 2018\n\n\n\n\nSo apparently, I've been miscalculating my percentage. Turns out I'm 95% done! I only have two more interviews to transcribe! The end is near! #amTranscribing #NotForMuchLonger #CantWaitToStartAnalysis\n\n— Joey Stanley (@joey_stan) July 10, 2018\n\n\n\n\nI saved the funnest interview for last. This guy's a fourth generation euphonium player and I did trombone before switching to linguistics, so most of the interview is us geeking out on low brass topics. #amtranscribing #ButIllBeDoneTODAY #StayTuned\n\n— Joey Stanley (@joey_stan) July 11, 2018\n\n\nTo see my excited tweetstorm for when I finished, see this new post."
  },
  {
    "objectID": "blog/full-house-at-my-first-latex-workshop/index.html",
    "href": "blog/full-house-at-my-first-latex-workshop/index.html",
    "title": "Full house at my first LaTeX workshop!",
    "section": "",
    "text": "Today I had the opportunity to teach LaTeX for the first time. Caleb Crumley, an RA for the DigiLab at UGA, has been working on a dissertation template in LaTeX that conforms with UGA’s formatting check. He finished it, and it’s got the stamp of approval from the Graduate School. To advertise the template, Caleb, Jonathan Crum, and I put on a three-part series to introduce the template and teach a little LaTeX as well.\nAs always, marketing these workshops is tricky and it’s rare to get more than about a dozen attendees. But, with some persistence from the DigiLab’s coordinator, Emily McGinn, the Grad School agreed to advertise a little for us, and sent out the information to the approximately 6000 grad students at UGA!\nWithin minutes, we had dozens of people registering for the workshop. After day or two, we doubled, and then tripled the registration cap! Sure enough, the DigiLab was fuller than I had ever seen it, and we decided to host repeat sessions of the three workshops as soon as the first cycle is over.\n\n\n\nAction shot of me showing how to make a bulletted list!\n\n\nKnowing there would be so many people there, I was a little nervous prepping the workshop because I’m not as comfortable with LaTeX as I am with R. I’ve been dabbling with it for a couple year but I’ve only been using it seriously for about a year.  All things considered, I think it went well, and I enjoyed it a quite a lot.I began using it when I switched my dissertation over from Word. It took about 20 hours to do so, but seriously, best decision ever.\n\n\nThis'll be my first time teaching LaTeX, but I had a lot of fun putting this workshop together with @CalebCrumley and Jonathan Crum. The materials for today's workshop can be found at https://t.co/uyGNAq1EMe https://t.co/uhnz7sptRz\n\n— Joey Stanley (@joey_stan) January 31, 2020\n\n\nYou can see more details about the workshop series here, the handout for the workshop on my github, and the UGA LaTeX dissertation template that Caleb made on the DigiLab’s github."
  },
  {
    "objectID": "blog/updated_mvnorm.etest-function/index.html",
    "href": "blog/updated_mvnorm.etest-function/index.html",
    "title": "Updated mvnorm.etest() function",
    "section": "",
    "text": "In Levshina’s How to do Linguistics with R, the function mvnorm.etest() from the energy library is used. This runs what’s called the “E-statistic (Energy) Test of Multivariate Normality” which used to test whether multivariate data is normally distributed. This is important because it’s an assumption that should be met for several statistical tests like MANOVA and for testing statistical significance of a correlation. Well, the code from the book is broken.Levshina, Natalia. 2015. How to do Linguistics with R: Data exploration and statistical analysis. Amsterdam: John Benjamins Publishing Company.Maria L. Rizzo and Gabor J. Szekely (2016). energy: E-Statistics: Multivariate Inference via the Energy of Data. R package version 1.7-0. https://CRAN.R-project.org/package=energy\nI looked into it and it turns out that the book was based on an older version of the energy package (&lt;1.7). But if you’ve updated the package since August 2016 to version 1.7 or later, the code breaks. What happened? Here’s the old code:\nmvnorm.etest(cbind(x, y))\nWhile this worked with the old versions, in the newer versions this returns a p-value of “NA”. This function does some bootstrapping meaning it runs some function on the data over and over some number of times. In the old version of the package, the default was 999 replicates. In the new version there is no default, so you have to specify the number of replicates with the R=999 argument:\nmvnorm.etest(cbind(x, y), R=999)\nYou can of course change this number to whatever you want, but 999 was the default before so I figure it’s a good number to keep. Just thought you ought to know."
  },
  {
    "objectID": "blog/ads-and-lsa-2022/index.html",
    "href": "blog/ads-and-lsa-2022/index.html",
    "title": "ADS and LSA 2022",
    "section": "",
    "text": "I’m attending the Annual Meeting of the Linguistic Society of America and the American Dialect Society and I’ve got three presentations to tell you about! Please find links, summaries, and images from these presentations below!"
  },
  {
    "objectID": "blog/ads-and-lsa-2022/index.html#perspectives-on-georgia-vowels-from-legacy-to-synchrony",
    "href": "blog/ads-and-lsa-2022/index.html#perspectives-on-georgia-vowels-from-legacy-to-synchrony",
    "title": "ADS and LSA 2022",
    "section": "Perspectives on Georgia Vowels: From Legacy to Synchrony",
    "text": "Perspectives on Georgia Vowels: From Legacy to Synchrony\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nOn Thursday afternoon, Peggy Renwick, Jon Forrest, Lelia Glass, and I kicked off the ADS sessions with our presentation on English in Georgia. The four of us have been collaborating for about a year, pooling together datasets and sharing resources, on a project focusing on English in Georgia. This is our first talk showcasing some of our findings. Our results are largely descriptive at this point. Here are the main plots we used (in a fun dark mode!) split up by generation, gender, and ethnicity:For those of you that were there, this was the one that was horribly Zoombombed!\n\n\n\nTurns out pretty vowel changes if you give it 100 years. We’re just excited to see acoustic data from such a large span of time analyzed together."
  },
  {
    "objectID": "blog/ads-and-lsa-2022/index.html#homogeneity-and-heterogeneity-in-western-american-english",
    "href": "blog/ads-and-lsa-2022/index.html#homogeneity-and-heterogeneity-in-western-american-english",
    "title": "ADS and LSA 2022",
    "section": "Homogeneity and Heterogeneity in Western American English",
    "text": "Homogeneity and Heterogeneity in Western American English\n\n\n\n\n\n\nTip\n\n\n\nDownload the poster here!\n\n\nAt the ADS poster session on Friday, I presented a poster with two students, Jessica Shepherd and Auna Nygaard. As a bit of background, in Speech in the Western States: Volume 2, Fridland et al (2012:172) point out that pretty much every study of the front lax vowels in the Western US has been based on independent, isolated studies. Because each research collects and processes data their own way, it’s difficult to disentangle differences that may be due to region and differences that may be due to methodological choices. They say that “clearly, collecting the same type of data from all sites would be optimal in allowing us the most reliable cross-region assessment.”\nThis project is a direct response to that call. When I was a grad student I recruited people via Amazon Mechanical Turk to a bunch of recordings of people reading sentences and wordlists. In total, 212 people completed the task, scattered all across the Western US. This poster describes the first results from this project. As it turns out, our findings match the West’s description as exhibiting both “homogeneity and heterogeneity” (Fridland et al. 2012:172). We find homogeneity in that most people have the LBMS to some degree and that education level and region weren’t statistically significant predictors. However, there’s a wide range of variation for the LBMS and ban-raising, with younger people and sometimes women appearing to lead both of these sound changes. Here are some sample plots from four representative speakers.\n\nAnd here’s an overall look at the vowel space in our dataset, with some additional allophones we don’t analyze here.\n\nWe look forward to digging into this dataset a little bit more in the future!"
  },
  {
    "objectID": "blog/ads-and-lsa-2022/index.html#vowels-can-merge-because-of-changes-in-trajectory-prelaterals-in-rural-utah-english",
    "href": "blog/ads-and-lsa-2022/index.html#vowels-can-merge-because-of-changes-in-trajectory-prelaterals-in-rural-utah-english",
    "title": "ADS and LSA 2022",
    "section": "Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English",
    "text": "Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English\n\n\n\n\n\n\nTip\n\n\n\nDownload the slides here!\n\n\nFinally, on Friday afternoon, Lisa Johnson and I talked about vowel trajectories and what they can tell us about vowel merger. We look at prelaterals in rural Utah and find that, on the surface, they look like mergers by approximation. However, when we looked at the trajectories (with the help of some pretty cool animations!), it seems like the lateral gradually increases its influence on the vowel so that the merger happens “leftward,” from the coda to the onset. In this example, we have zeal and guilt, representing /il/ (feel, deal, meal) and /ɪl/ (fill, dill, mill), respectively.\n\n\n\n\n\n\n\nThis was a pretty consistent pattern across all the pairs of prelateral vowels we looked at. We suspect that we might find this among other conditioned and vowel shifts, like prevelar raising, the mary-merry-marry merger, and post-coronal /u/-fronting. The point is, we think trajectories should be considered more when looking at vowel mergers because even among these supposed monophthongs, trajectories really illuminated how that merger happened."
  },
  {
    "objectID": "blog/jealousy-list-3/index.html",
    "href": "blog/jealousy-list-3/index.html",
    "title": "Jealousy List 3",
    "section": "",
    "text": "This is the third iteration of my Jealousy List, which is a list of articles so good I wish I had been the one to write them. My first two lists were posted about a year ago (see the list of lists here) and this one is long overdue, so I apologize for some of the posts being a little less recent. Regardless, here are a list of posts I’ve found in the past few weeks and months that I found exceptional in some way, entertaining, informative, or just plain cool.\n\nSaskia Freytag. “Workshop: Dimension reduction with R”.\n\n\nSo I wrote a tutorial on dimension reductions in #rstats. It has actually turned out to be fairly comprehensive. It uses a fun example dataset on cereals (🎉 - not looking for at iris) I would love some feedback: https://t.co/VsiHX2KYdT\n\n— Saskia Freytag (@trashystats) August 16, 2019\n\n\nYou may have heard of PCA (Principle Components Analysis) as a way to reduce a bunch of variables down to a more manageable number. As it turns out, this is just one way to do a dimension reduction on your data. Freytag’s workshop does a really nice job at explaining some of the different dimension reduction techniques that are out there, including helpful plots for the visual learners out there. It also goes into detail about the pros and cons of each method, and gives some sample R code showing you how to run the analysis yourself. I wish there were more workshops like there floating around! Plus, it uses data from a bunch of cereals, and I’m pretty sure I’ve used that dataset before in my workshops…\n\nAustin Wehrwein. “Burden of roof: Revisiting housing costs with tidycensus”.\n\n\nIn this episode I use the A+ tidycensus #rstats package to examine housing costs along with income data AND stretch wordplay as far as it will go. Burden of roof: revisiting housing costs with tidycensus. https://t.co/gDVlOb9N9H pic.twitter.com/i3gxzz6C8G\n\n— Austin Wehrwein (@awhstin) August 2, 2019\n\n\nThis is a short blog post, but I really like it because is succinctly shows how to quickly produce a really complelling story with some data and a nice visual. It uses the tidycensus package by Kyle Walker to extract some information about median income and housing prices per US county, and then creates a stunning map to display the data. I also learned that the county I live in now, Clarke County, Georgia, was among the top 25 worst counties in the country in this regard. I guess that’s where all my money is going!\n\nGarrick Aden-Buie. “Custom Discrete Color Scales for ggplot2”.\n\n\nI wrote up a short blog post on creating custom ggplot2 color scales. I focused on discrete color scales to demo a setup that makes binary colors easy, but I hope the post is helpful if you're working on a #ggplot2 theme for your org or brand. #rstats https://t.co/jQDxE61K3W\n\n— Garrick Aden-Buie (@grrrck) August 16, 2019\n\n\nI’ve become a bit of a color snob, so I appreciate a good post on colors in data visualization. This one is less about the colors themselves, and more about how to more easily implement your own custom color scheme in ggplot2. Aden-Buie even goes so far as to provide helpful tips for when you compile all these custom commands into an R package. I’ll definitely be using this whenever I get my package off the ground.\n\nRafael Irizarry. “Dynamite Plots must Die”. From Simply Statistics.\n\n\nOpen letter to journal editors: dynamite plots must die. Dynamite plots, also known as bar and line graphs, hide important information. Editors should require authors to show readers the data and avoid these plots. https://t.co/0GNKEIUCJL pic.twitter.com/OS9ytEFRZN\n\n— Rafael Irizarry (@rafalab) February 22, 2019\n\n\nData visualization must have been on my mind for a while now, because five months ago I bookmarked this blog post so that it’d make it on my next Jealousy List. This is a nice critique about Dynamite Plots, or basically bar plots with those little error bars at the top. Basically, they obscure the underlying distribution and can be replaced by a very small table. Fortunately, the complaint comes with a few recommendations for alternative visuals.\n\nJulia Silge. “Introducing Tidylo”.\nThere will probably always be at least one Julia Silge post on my Jealousy Lists. This one introduces a new R package, tidylo, which calculates weighted log odds using within the framework of the tidyverse. The post itself is, as always, a fun read and there are some great visuals. This’ll make it really easy to choose a baby name characteristic of like the 1920s for my next kid or something.\n\n\nSo that’s it for my long-overdue Jealousy List: statistical procedures, succinct tutorials, color, data visualization, and more statistics. Again, a decent representation of what I’ve been reading recently."
  },
  {
    "objectID": "blog/ads2018/index.html",
    "href": "blog/ads2018/index.html",
    "title": "ADS2018",
    "section": "",
    "text": "Thanks for attending my presentations. At the 2018 annual meeting of the American Dialect Society in Salt Lake City, Utah, I was fortunate to present on two aspects of my research."
  },
  {
    "objectID": "blog/ads2018/index.html#thursdays-presentation-on-the-gsv",
    "href": "blog/ads2018/index.html#thursdays-presentation-on-the-gsv",
    "title": "ADS2018",
    "section": "Thursday’s presentation on the “GSV”",
    "text": "Thursday’s presentation on the “GSV”\n\n\n\n\n\n\nTip\n\n\n\nDownload the slideshow here!\n\n\nThursday, I represented Peggy Renwick, Bill Kretzschmar, Rachel Olsen, and Mike Olsen and introduced a website called The Gazetteer of Southern Vowels. This is a online tool that makes it easy to visualize linguistic atlas data (specifically, the Digital Archive of Southern Speech, or DASS) that is currently being processed at the University of Georgia. The site has several features:\n\nSide-by-side plots make it easy to compare two subsets of our data, whether it be by demographic factors, language-internal factors, or methodological differences.\nA “point-pattern analysis” page shows an underlaid grid on the plot as an alternative way of visualizing the vowel space.\nAt the top of each page are options to subset the data however you like. Speakers can be selected by typical demographic factors. You can filter out stop words or examine specific words. You can subset by vowel, stress, and following consonant. Different transcription systems, filtering algorithms, and normalization procedures are available.\nThe plots themselves are highly customizable. Users can display any combination of points, ellipses, averages, and words. For each of these, the size and opacity can be controlled. This makes it easy to visualize the same data in lots of different ways.\n\nWe hope that you enjoy the Gazetteer of Southern Vowels and find it useful for visualizing linguistic atlas data."
  },
  {
    "objectID": "blog/ads2018/index.html#sundayss-presentation-on-consonants-in-utah",
    "href": "blog/ads2018/index.html#sundayss-presentation-on-consonants-in-utah",
    "title": "ADS2018",
    "section": "Sundays’s presentation on consonants in Utah",
    "text": "Sundays’s presentation on consonants in Utah\n\n\n\n\n\n\nTip\n\n\n\nDownload the slideshow here!\n\n\nOn Sunday afternoon, Kyle Vanderniet and I presented on consonantal variation in Utah English. We looked at three variables:\n\nWe found that words like mountain, cotton, and Latin have three pronunciations in Utah. The most common is what most other North American Engilsh speakers say: moun[ʔn̩]. Some women in our sample frequently used a second form, moun[ʔɨn], which has become almost stereotypical in Utah and has a lot of stigma. Finally, a third form, moun[tʰɨn], appears to be a hyperarticulated form in response to the stigma associated with the glottal stop. This was relatively frequent in our sample: about 25% of tokens had it, which is much more than similar audio from other states. Men tended to use this more, especially younger men.\nThen we looked at [t]-epenthesis in words like false, also, and else and found that while this isn’t particularly common overall, some women had it a fair amount in their speech.\nFinally, we looked at [k]-epenthesis after velar nasals. Despite being very frequent in other Utah English studies (like Di Paolo and Johnson’s study just before ours), this was rarely attested in our sample, so we have to figure out why.\n\nOverall, we feel that consonants in Utah deserve further study because of the high amount of variation."
  },
  {
    "objectID": "blog/making-a-website-is-fun/index.html",
    "href": "blog/making-a-website-is-fun/index.html",
    "title": "Making a website is fun!",
    "section": "",
    "text": "In the past month or so I’ve been putting a lot of time and effort into increasing my professional web presence. In about a year I’ll be applying for academic positions, and it would sure be nice to be more visible to my potential employers. The sheer fact that you’re reading this means you’ve seen some of the fruits of my labor."
  },
  {
    "objectID": "blog/making-a-website-is-fun/index.html#why-now",
    "href": "blog/making-a-website-is-fun/index.html#why-now",
    "title": "Making a website is fun!",
    "section": "Why now?",
    "text": "Why now?\nIt took me a while before I wanted to find out what area of linguistics I wanted to go into. I’ve been interested in a lot of things at one time or another: typology, documentation, indigenous languages of South America, language change, simulation, morphology, network analysis, forms of address, among other things. I’ve even gone to conferences presenting some of this research. But I knew at the time that whatever it was that I was presenting on wasn’t going to be what I wanted to be known for. So I didn’t bother networking with other people, and I hardly took people’s advice because I would brush this off and say it was just a glorified term paper.\nBut I’ve found my niche. I’m interested in sociolinguistics, dialectology, phonetics, phonology, and using computer and statistics to help me out. This is something I’d like to be known for. Right now I’m working on English in the Pacific Northwest, and I’m familiar with a lot of the work that’s been done in that area, so I know who to talk to at conferences because I’ve read a lot of their work."
  },
  {
    "objectID": "blog/making-a-website-is-fun/index.html#github",
    "href": "blog/making-a-website-is-fun/index.html#github",
    "title": "Making a website is fun!",
    "section": "Github",
    "text": "Github\nOver the past five years or so in my undergrad and graduate education, I’ve acquired some computer skills. I minored in linguistic computing, so I learned Perl and C# as a part of the required coursework. More importantly though, I learned that learning to use computers is pretty darn useful for my research. So I’ve learned a few other skills along the way to help me out with my larger linguistics questions.\nJust this month I presented a paper on Quechua morphology. I mentioned in it that I wrote a computer script to help me out with generating the correct forms of the paradigm. One of the participants in the Q&A session asked if the code was available on Github and I said it wasn’t. But why shouldn’t it be? That acted as a catalyst into getting a github profile and uploading some code. I also found out I could host a webpage (this webpage!) on there too. Well sweet."
  },
  {
    "objectID": "blog/making-a-website-is-fun/index.html#creating-the-webpage",
    "href": "blog/making-a-website-is-fun/index.html#creating-the-webpage",
    "title": "Making a website is fun!",
    "section": "Creating the webpage",
    "text": "Creating the webpage\nHaving never done web design before, I had a lot to learn. Turns out you’ve gotta host the webpage somewhere. By that I mean that all the files and formatting and content and stuff in a webpage has to be stored on some computer somewhere. It can’t be mine, because I’ve set it up as a server and stuff and that’s way beyond me expertise—plus I’m pretty sure I don’t want to have my laptop as a web server. So luckily, Github, will host mine for free. Okay good.\nThe next task was to figure out how. I had recently heard about a website called The Programming Historian, which has a lot of slick, easy-to-follow tutorials on how to do useful computer stuff for research. Well, one of their pages is called “Building a static website with Jekyll and GitHub Pages”. Awesome. Now, to be clear, it wasn’t the easiest tutorial. You have to download all sorts of stuff to your computer using the command line and manage a bunch of files and stuff. But that’s fine. I got it. I was able to create a simple webpage.\nWell, I’m never really satisfied with the default design of things, and I like having unlimited flexibility in how things look. So, I went over to Lynda.com, which I have access to through UGA, and took a more detailed course on how to build a webpage using Jekyll. And it was great. I learned a lot and figured out what a lot of stuff means.Edit: Looks like the “Jekyll Web Design” course is no longer available. Makes sense; it was from 2016!\nBut, I’m still not satisfied with the way it looks. I learned how the website works in that course, but not a lot of formatting. So I’m currently taking another Lynda.com , which I think will help me a lot. As it turns out, the concepts are very similar to the job I did as an undergrad, where I essentially created eBooks for a program called WordCruncher.Looks like this one is missing too. It was called CSS Core Concepts. I wish I knew who the instructor was.\nMy goal in all this is to be able to make a beautiful webpage that is uniquely mine. I want all the flexibility I could ever want in how it works and looks. By so doing, I’m learning a lot of new skills like CSS, but that’s perfectly okay with me. So, today the webpage is still a hack off of the tutorial I went through, but hopefully over the course of the next few weeks and months it’ll slowly transform into my own.Now that I’ve switched to Quarto, I’ve lost much of that uniqueness and flexibility.\nI guess the end goal of this is to wow potential employers still. But, let’s be honest, I’m sure enjoying the journey."
  },
  {
    "objectID": "blog/tweeting_LSA2017/index.html",
    "href": "blog/tweeting_LSA2017/index.html",
    "title": "Tweeting LSA2017",
    "section": "",
    "text": "In addition to the awesome experiences I had overall at the LSA2017 conference (which you can read about here), I made an effort to be active on Twitter during the conference.\nI’ve followed conferences in the past (such as LSA and NWAV last year) when I wasn’t able to attend them, and really enjoy them. I livetweeted the Linguistics Conference at the University of Georgia (LCUGA) in October, which was my first experience as a livetweeter, though I didn’t do much other than introduce who was presenting next. So this year, not only did I follow the twitter feed, but I also contributed as much as I could myself. Here are some of my more popular tweets:\n\n\nFor anyone at #LSA2017, you should come to the first session of #ADS2017 (\"Vowels, vowels, vowels\") and hear me talk about Washington State!\n\n— Joey Stanley (@joey_stan) January 5, 2017\n\n\nThis one got a surprising amount of traffic (over 800 people saw it on Twitter) for being self promotion, but it’s because the LSA account retweeted it. I don’t think they retweet everyone’s self-advertising tweets though, so I wonder why mine made the cut… Either way, I didn’t mind the advertising!\n\n\nThomas & Kendall: “You can't get a full sociolinguistic picture of a community by looking at only one kind of variable.” #ADS2017 #LSA2017\n\n— Joey Stanley (@joey_stan) January 6, 2017\n\n\nThis one was one of many tweets during Erik Thomas and Tyler Kendall’s presentation, a direct quote from one of their last slides. It’s a really good quote overall and a lot of people seemed to like it.\n\n\nAntieau: Double modals. One Utahn used “might usually would”. Nice! #ADS2017 #LSA2017\n\n— Joey Stanley (@joey_stan) January 7, 2017\n\n\nThis was one of many things that Lamont Antieau found in the Linguistic Atlas of the Middle Rockies. Having lived in Utah, most of his results were surprising to me, but this one was especially so. Apparently others thought so too.\n\n\n@BrentPWoo: \"and/or\" as a fully lexicalized coordinator with the union set of constraints of on \"and\" and \"or.\" #lsa2017\n\n— Joey Stanley (@joey_stan) January 7, 2017\n\n\nBrent Woo from the University of Washington is doing some really interesting research. We met when he presented at the Linguistics Conference at the University of Georgia in October. He must have some dedicated twitter followers though because anything I tweet at him gets a lot of traffic. I mean, his findings are pretty cool though.\n\n\nPreston: “I grew up in a paint store so I have female-like familiarity with color, and it hasn’t costed me my masculinity.”#ADS2017 #LSA2017\n\n— Joey Stanley (@joey_stan) January 7, 2017\n\n\nThis was a random side comment Dennis Preston made after explaining the color scheme in is graphs. Instead of red, green, and blue or something, he used lavender, forest green, chartreuse, and a couple others. After explaining which colors represented which variable, he said that line. Even though it had a typo (hasn’t costed me my masculinity), a lot of people liked it. I mean, Dennis Preston seems like a pretty funny guy, so this was great.\n\n\nAfter #LSA2017 I feel simultaneously intimidated, inspired, and exhausted. A clear sign that this was a fantastic conference.\n\n— Joey Stanley (@joey_stan) January 8, 2017\n\n\nAt the end of conferences, there’s a lot of “I’m so sad the conference is over” tweets. As I’ve mentioned above, I put as much as I could into the conference, and I did feel exhausted. I was intimidated because seeing what great stuff other grad students are doing I suddenly feel less employable. But I was inspired to do more and better research. This tweet succinctly summed up what my conference experience, and it looks like others felt the same way too.\n\n\nThis was my first time & it took more multitasking than expected. I have a newfound respect for tweeters and I appreciate 'em even more now. https://t.co/1HgUFmHzvV\n\n— Joey Stanley (@joey_stan) January 8, 2017\n\n\nThis was part of a small thread that was going. Basically, people were thanking all the livetweeters out there, and I got a mention. I thought I’d thank the other livetweeters as well, now that I know how hard it is.\nAll this tweeting paid off though because I think people are noticing me on twitter now, and I think I’m seen as “one of the live-tweeters”. Not a bad reputation to have. I’m about to get my hundredth follower, now that I’ve gotten about 10 more since this conference. (I got about 10 just by following NWAV and liking tweets, and another half dozen when I tweeted very basic things at the LCUGA conference.) My goal is for my followers to outnumber the people I follow, but I don’t know if that’ll happen anytime soon. I’m not very active on twitter outside of conferences, other than shameless self-promotion. Maybe I should get better at that.\nIf you’re interested in getting more into twitter, I’d recommend this guide:\n\n\n@joey_stan have you seen @GretchenAMcC's excellent guide? https://t.co/03AjgiGtuq\n\n— Rachael Tatman (@rctatman) January 9, 2017\n\n\nI enjoy being active on twitter for lots of reasons, most of them completely selfish, but starting to get a small following is pretty exciting and it’s a fun group to be a part of."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html",
    "href": "blog/ten-years-of-linguistics/index.html",
    "title": "10 Years of Linguistics",
    "section": "",
    "text": "On this day, ten years ago, I decided to major in linguistics. Today, I’m an assistant professor. To celebrate this decade of linguistics, I thought I’d write a little bit about where I came from and how I came to the decision to go into linguistics."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#music",
    "href": "blog/ten-years-of-linguistics/index.html#music",
    "title": "10 Years of Linguistics",
    "section": "Music",
    "text": "Music\n\nGrowing up, I was a total band geek. I’ll spare you the details, but I took piano lessons when I was eight, started saxophone in 6th grade band, hopped around to pretty much all the instruments I could for a few years, and finally settled on bass trombone junior year of high school. I wasn’t bad either: I made the district band most years (on trombone plus a couple other instruments) and even the Missouri All-State band my senior year.If you peek into the code of this webpage on Github, there’s a 1200-word summary of my band geekiness and musical background.\nWhen I started my freshman year at BYU, I auditioned to get into the Brass Performance major, but I didn’t get accepted. So I was officially a “pre-music major”, or rather, the “I-haven’t-realized-I’m-not-cut-out-for-this-yet major.” That gave me a year to knock out most of my general education courses though, which was nice.\nI had a fun music hobby though. I’d find movie scores or other songs from the library and I’d arrange them for piano ensembles (like a six-person, two-piano arrangement of a piece from Star Wars). I had been doing that since high school, and BYU seems to have a disproportionate number of people who can play piano, so it was a cinch to find a handful of good sight-readers to play with me. So at the recommendation of a music faculty member I met with one time, I auditioned a second time for the music school, but this time it was to be a Media Music Studies major. And I was accepted!\nBut, I was 19 years old, and the important thing to do was to go on a full-time, two-year mission. So I took time off from school, knowing my spot in the music school would be waiting for me. I’m so grateful for this break though because without it I would have just barreled through my music major. But the time off gave me the chance to stop and figure out what I was doing with my life."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#early-signs-that-i-wanted-to-be-a-linguist",
    "href": "blog/ten-years-of-linguistics/index.html#early-signs-that-i-wanted-to-be-a-linguist",
    "title": "10 Years of Linguistics",
    "section": "Early signs that I wanted to be a linguist",
    "text": "Early signs that I wanted to be a linguist\nUntil May 2008, linguistics was not on my radar at all. Like, I didn’t even know what it was. I didn’t even take any foreign language courses in high school.\nThere were a few signs though. The one I remember most was from when I was in a play my freshman year of high school. I didn’t have a big role, so I had to kill a lot of time in the drama room while the others rehearsed. I was flipping through one of the textbooks and I saw this chart with what’s called the International Phonetic Alphabet. My brother, who had taken a couple theater classes, had mentioned the IPA to me a few years prior. He described it as basically if you can transcribe it well and read it well, you can use it to, in theory, speak in any accent. I remember thinking it was so cool so I copied down all the symbols from that book. [Edit: I found the paper! Here it is!]\n\n\n\nI copied this IPA guide from a theater book in 2003!\n\n\nAnyway, it was in May of 2008 that I got the call that I’d be a missionary in Brazil. Meaning I’d have to learn Portuguese. Even though I would be spending the first two months of my mission in an intensive language school, I went ahead and tried learning as much as I could beforehand. And it was then that I realized that learning languages was pretty cool! I finally saw the IPA in action, and was able to use it to learn the sounds. But I remember it blew my mind that, for hundreds of millions of people, “house blue” sounds totally normal and “blue house” sounds totally wrong. Blew. My. Mind.\nSo even before I had left for my mission, I had already been thinking about linguistics. I had even added to my Facebook profile was that I was considering a minor in linguistics. So, getting called to a foreign mission is what put linguistics on my radar for sure."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#my-time-in-brazil",
    "href": "blog/ten-years-of-linguistics/index.html#my-time-in-brazil",
    "title": "10 Years of Linguistics",
    "section": "My time in Brazil",
    "text": "My time in Brazil\nSo I went to Brazil and served my mission. While I was down there, I really enjoyed learning Portuguese. I practiced vocabulary like crazy and studied as much grammar as I could. Some of the other missionaries joked that if anyone needed to talk to a lawyer about the gospel that they should call Elder Stanley because he’s the only one that could understand him. After a year and a half or so, I could convince people that I was Brazilian (my darker complexion helped there)—not a local Brazilian, mind you, but I’d tell them I was from another part of the country. I would get phone calls from other missionaries—native Portuguese-speaking Brazilians!—asking about some nuanced aspect of grammar. It was fun.\nYou may know that Mormon missionaries have pretty strict rules about what they can and can’t do. At the time, the internet was completely off-limits except to email our parents once a week. Well, I would occasionally sneak on to Wikipedia and look up linguistics pages and print them out and stuff. Of all the ways to rebel, I think looking up IPA symbols was a pretty tame way to do so.\nAt one point, I was in a city relatively close to Paraguay and would occasionally run into Guaraní speakers. I wrote to my parents about the language, and my dad sent me a Guaraní translation of the Book of Mormon and encouraged me to learn as much of the language as I could. I also met someone who had what was basically a “Teach Yourself Guaraní” textbook (written in Spanish, so I had to quickly learn to read some basic Spanish) so I used that to learn some of the morphology. So in the little free time I had, I spent it trying to learn Guaraní.In retrospect, I’m not sure if they spoke Paraguayan Guaraní because they were Brazilians, but I wonder if they spoke some other Tupí language in that part of the country.\nTowards the end of my mission, I served in a college town and my companion and I went to the university’s bookstore. I bought a Portuguese Phonetics and Phonology textbook and really had fun reading that. I learned about minimal pairs and basic phonological distributions, and I especially enjoyed reading the dialectal variation that it mentioned. During my last month or so, I bought a comprehensive grammar book, one that was written entirely in Portuguese and was meant for Brazilian university students. In fact, my mission president saw me reading it one time and he’s like, “Elder Stanley, aren’t you going home in like three weeks? Why are you reading that?” Why not? It was fascinating!It was Thaïs Cristóforo Silva’s 2007 textbook, Fonetica e Fonologia do Portugues."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#realizing-music-wasnt-for-me",
    "href": "blog/ten-years-of-linguistics/index.html#realizing-music-wasnt-for-me",
    "title": "10 Years of Linguistics",
    "section": "Realizing music wasn’t for me",
    "text": "Realizing music wasn’t for me\n\nOctober–December 2010\nI got home from my mission in October so I had a few months before the next semester of school started. My plans hadn’t changed yet: I wanted to be the next John Williams so my mind was set on Media Music Studies. But I knew I also wanted to at least minor in linguistics, if not double-major.\nBecause I was starting in January, I couldn’t start all the the theory and other core classes with the other first-year students because those were only offered in the fall. So I had to sort of fill my semester with fluff. I was able to sign up for the Songwriting class though, which was kinda like the intro to the Media Music Studies major. And since I had a time slot available, I went ahead and signed up for Intro to Linguistics. I also signed up for Acoustics for Music and Speech, band, a non-audition choir, and private bass trombone lessons. So about as much music as I could do without those core classes.\nMy family visited Utah soon after I got back so I took the opportunity to meet with a linguistics advisor, just to see what classes they’d recommend. It was Alan Melby that I met with, and he recommended I minor in Linguistics Computing. I thought that was a pretty good idea, so I went ahead and signed up for an Intro to Linguistic Computing class as well.Now that I’m on the faculty side of the department, I can see why he pushed the minor: enrollment was low and they were struggling to keep the minor!\nIn the meantime, I worked for my dad and spent my free time getting the right equipment and software for my music studies. But I also spent a lot of time studying linguistics. Mostly looking at Wikipedia and other resources online, including lectures that I could listen to while driving. So even though I didn’t know linguistics would eventually be my major (and career), I was already investing a lot of time into learning it and had a decent grasp of a lot of basic topics.\n\n\nWednesday, January 5th, 2011\nFirst week of classes comes and I walk into my songwriting class full of confidence. This was going to be the first day of the rest of my life[Although, as my brothers point out, this is technically true every day :)]{.aside} That class turned out to be a pivotal moment like I had anticipated, it just wasn’t quite pivotal moment I was expecting.\nAfter going over the syllabus, we learn that the final project was going to be to write and record a pop song. Uh-oh. I don’t listen to pop music. We got a homework assignment that day too: submit the names of three pop artists you think most closely resemble your own style of music. Ummm… what? I was there to learn to write movie music, not learn about pop singers. I had just barely gotten back to the country, so I literally couldn’t even name three artists on the radio[I’d struggle with that today, to be fair.]{.aside} Pop music wasn’t my thing. But the assignment had to be pop music.\nFunnily enough, my Intro to Linguistics class was right after that. And I freaking loved it. I wrote in my journal that night that I was already considering changing majors. I think I had known for a long time—really deep down—that I wasn’t cut out for music. This experience in my Songwriting class was just what I needed to come to that realization.\n\n\nThursday, January 6th, 2011\nThe next day, I wrote to my songwriting professor expressing my concern. He said something along the lines of this: “Too bad. I accepted you into this major as a favor to a friend. The odds of you making it as a film score composer are basically zero. Either you expand your horizons or you’re not getting anywhere in the music world. Pop music is where the jobs are so if you can’t keep up, you’re in the wrong major.”\nOuch.\nSince middle school, I’d thought of nothing but music and for two years in Brazil, I eagerly anticipated the day when I’d finally start my music classes. And literally in the first hour of the first one, I get a slap in the face, a reality check, and a rude awakening to the fact that I was not going to have a career in music. What was my life for then? Was all that music a waste?"
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#switching-to-linguistics",
    "href": "blog/ten-years-of-linguistics/index.html#switching-to-linguistics",
    "title": "10 Years of Linguistics",
    "section": "Switching to linguistics",
    "text": "Switching to linguistics\n\nFriday, January 7th, 2011\nI spent several hours that evening looking through classes and figuring out what I was going to do. I considered switching to just a general Music major, but now that the rose-colored glasses were off, it occurred to me that the classes that looked the most fun (like orchestration and score analysis) were only possible after three or four long years of coursework that was not very fun-sounding. I’d have to slog through years of alone time in the practice room and classes I didn’t want to take just to finally get to those fun ones at the very end.\nMeanwhile, after taking a closer look at the courses in the linguistics major, I realized that they all sounded really fun! Phonetics? Phonology? Morphology? Sociolinguistics? Language documentation? Sign me up!\n\n\nSaturday, January 8th, 2011\nAt this point, I was learning towards a double major in (general) music and linguistics, but I was weighing my options. To help me out, I made some charts to see how my semesters would be spent, credit-hour wise. According to my journal, 65% of my time would be spent in music classes. I had already decided my career wouldn’t be in music at that point, so that’d be a lot of time spent doing something that wasn’t going to lead me anywhere.…and this offers a peek into how my mind works and was a early sign I’d do a lot of quantitative work\nSo at that point, the decision was clear. If all these music classes sounded lame and all the linguistics classes sounded fun, what was stopping me from switching to linguistics?\n\n\nSunday, January 9th, 2011\nMy parents have always been extremely supportive of everything I do. They were in the loop on all the developments up to that point, but that afternoon, I Skyped with them to hash a few things out. As expected, they were just as shocked as I was that I was considering switching, but still extremely supportive.\nI don’t recall exactly how it all went down, but I know that by the end of that conversation with my parents, my mind was made up: I was going to major in linguistics and minor in linguistics computing.\nIt’s pretty interesting what my future plans were at that time. I had these grand plans of learning lots of languages (Mandarin, Arabic, and potentially Hebrew), minoring in TESOL, and teaching abroad somewhere. I was already considering a Master’s program (probably based on conversations with my Intro to Linguistics professor). None of that really panned out, but at that point, a PhD and academia were not in the picture.I did end up taking two semesters of Mandarin the next school year.\nI thought it was interesting that I wrote in my journal how much I was looking forward to the Varieties of English class. When I did finally take it a couple semesters later, I was stoked (and it did not disappoint)! Little did I know I’d grow up to be a dialectologist and that I’d be teaching that very course in 10 years’ time. In fact, I taught it in the very same classroom where my Intro to Linguistics class was!\nSo, at 11:00 on Wednesday the 5th, I was still confident that I’d be a music major. By the evening of Sunday the 9th, I had made up my mind to major in linguistics and was emotionally ready to abandon music studies. It was a tough five days, but that’s all it took. I don’t regret doing music or being in band or learning trombone at all. I still get opportunities to play piano (I’m playing church this Sunday) and I’m still doing arrangements of movie music (I’m working on Jurassic Park right now). But I am soooo glad I didn’t major in it."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#starting-linguistics",
    "href": "blog/ten-years-of-linguistics/index.html#starting-linguistics",
    "title": "10 Years of Linguistics",
    "section": "Starting linguistics",
    "text": "Starting linguistics\nI met with a humanities advisor a few days later and showed her my pie charts and she basically said she had nothing to say because it’s clear my mind was made up already. I don’t know when I officially made the switch according to the university systems, but it must not have been much later because soon after that I was calling myself a linguistics major in my journal.\nThe advisor also recommended I do a study abroad. So the next day, I was looking through potential study abroad programs and found one that went to Ecuador to study Pastaza Kichwa with Dr. Janis Nuckolls. I’ve told Janis this sense then, but that study abroad was what set me into motion to get a PhD and ultimately go into academia. Being there in the field doing rigorous linguistic documentation was a total blast.\nLater, she invited me to join her research team, which ultimately led me to a presentation at SSILA and attending my first LSA conference in Boston in 2013. I attended as many sessions as I could, including many in the American Dialect Society meetings. And that’s when I caught the bug. I knew then that I had to do my own research go to more conferences. I had already applied to grad schools at that point, but it was that conference that gave me the determination to present at conferences my first year of grad school (and many times since then).\nI’m a little fuzzy on the details about deciding to do a PhD and figuring out what my research focus would be. I know I mentioned to my mission president during my closing interview with him that I was considering a PhD, but it didn’t seem like it was on my radar when I switched to linguistics. I did apply to PhD programs though so it must have been that study abroad that sent me that direction. As far as my research focus, I started off wanted to do language documentation, but at some point I decided on sociolinguistics and, more specifically, dialectology. I’ll have to continue reading my journal and seeing if I can pinpoint exact dates for those too.I do have specifics about when I decided on my dissertation topic. Maybe I’ll do a blog post about that."
  },
  {
    "objectID": "blog/ten-years-of-linguistics/index.html#conclusion",
    "href": "blog/ten-years-of-linguistics/index.html#conclusion",
    "title": "10 Years of Linguistics",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s it. Ten years ago today is when I decided to major in linguistics. After one difficult homework assignment (that I never finished by the way) and a strongly worded reality check from a professor, it took just five days to abandon the previous decade’s worth of plans to major in music. I look forward to another decade of linguistics and to see where this career will take me!"
  },
  {
    "objectID": "blog/youre_a_statistician_harry/index.html",
    "href": "blog/youre_a_statistician_harry/index.html",
    "title": "You’re a Statistician, Harry!",
    "section": "",
    "text": "The job hunt was not successful this year. I applied to about two dozen positions, got interviewed for five of them (yay!) but ultimately got zero offers (boo…). I’m disappointed, sure, but it’s probably for the best anyway: it took longer to write my dissertation than I anticipated, so it probably wouldn’t have been feasible to finish it and graduate by August. Plus, I have funding for one more year. But, the funny thing is I’m now in this weird position where the bulk of my dissertation has been written, but I have about another year left as a student. What can I during this time? I considered a lot of options, but I think I’ve settled on something fun: I’m going to try and get an M.S. in Statistics!"
  },
  {
    "objectID": "blog/youre_a_statistician_harry/index.html#how-is-this-possible",
    "href": "blog/youre_a_statistician_harry/index.html#how-is-this-possible",
    "title": "You’re a Statistician, Harry!",
    "section": "How is this possible?",
    "text": "How is this possible?\nAs it turns out, UGA offers a “secondary” Master’s degree in statistics. It designed so that UGA students who are seeking degrees in other departments can walk away with a Master’s in statistics as well. I’d take the required courses, fill out some paperwork, write a thesis, and boom—I’ve got an MS. I wouldn’t be part of the statistics department so I’ll have to continue getting funded through my home department (linguistics), but I would get a full-fledged degree from them by the time I graduate.\nI’ve known about this option for a while now, and the main thing holding me back was my lack of a strong mathematical background. I took calculus in high school and quite enjoyed it, but that was more than 10 years ago, and I’d actually need three semesters of calculus to be able to do well in some of the core stats courses. So I decided it was a nice thought but was ultimately not going to happen. When the realization came that I’d probably be around for another year though, I looked at the requirements, talked with the graduate coordinator, and put together a schedule. It’ll be a busy year, but I think I can make it work.\nThe math problem still hasn’t gone away, so my goal this summer is to fly through differential, integral, and multivariate calculus on my own before the semester starts in August. Fortunately, it looks like I can learn everything I need through Khan Academy, which is awesome. I’m already auditing Mathematical Statistics during the summer so when take it for real in the fall I’ll know the material well enough to pass. So far, it has been coming back pretty quickly, so I feel pretty good about it all."
  },
  {
    "objectID": "blog/youre_a_statistician_harry/index.html#why-bother",
    "href": "blog/youre_a_statistician_harry/index.html#why-bother",
    "title": "You’re a Statistician, Harry!",
    "section": "Why bother?",
    "text": "Why bother?\nWhen I was weighing my options for what I could do during my last year, it was all based on how I could improve my job prospects for next year. My first and the most obvious option was to just hunker down and crank out a bunch of publications to lengthen that CV. This is a good plan, but ultimately I decided I probably wouldn’t be able to get them done in time for potential employers to see them anyway. I have a couple projects in various stages of the publication pipeline right now, so I’ll continue working on those, but that won’t be what I spend 100% of my time on.\nWell so then I looked at perhaps strengthening the teaching part of my job applications. I’ve been funded through research assistantships this whole time, which has been great! But that means I’ve only had the opportunity to teach twice and I think universities like to see more teaching experience than that. So I thought about getting the graduate certificate in University Teaching, which would involve some actual courses in pedagogy. I figure if I couldn’t impress them with experience, maybe I could with training and certification. I’ll admit, it didn’t sound that fun, but some of the courses did look somewhat interesting. However, I found out that I’m ineligible for the certificate anyway because I need to have taught four sections while at UGA to qualify, which I have not. So, I scratched that idea.I think I would have taken ones like adult learning, how to do proper assessment, and skills for being able to teach an online course.\nSo then I considered getting the graduate certificate in GIS. It’s not totally out of the blue: as a dialectologist, it would be nice to have some actual geography training and it might put me ahead of other potential applicants for positions that wanted a hard-core sociolinguist. The certificate would have been feasible too: I’d take the Intro to GIS course over the summer, and then it would be just two courses in the Fall and two courses in the Spring. In fact, that was what I planned on, and I’m currently taking the Intro to GIS course online right now (and loving it!). But, as I looked through the course offerings, it turns out that the really fun ones that I wanted to take  weren’t going to be offered this year. That means I’d have to take things like working with aerial photography or forestry-related courses or something to satisfy those electives, which wouldn’t be particularly fun or easy.I really wanted to do things like spatial statistics, programming in GIS, and cartography.\nAll the while, this statistics possibility has always intrigued me. I have always had a knack for quantitative methods in linguistics and feel like I have a better-than-normal grasp of the statistics that linguists use. My undergraduate minor in linguistic computing exposed me to coding, and I’ve learned a lot of R programming since coming to UGA. A lot of the jobs I applied for wanted someone with skills in quantitative linguistics, and while I think I have the skills they want, I had a hard time proving it since my publications so far haven’t been particularly quantitative. I figure an entire degree in statistics might do the trick this time around and would definitely make me stand out. Plus, it’s a nice skill to have to fall back on: it opens doors to college-level teaching opportunities in statistics and to industry jobs as well. So I finally reached out to the statistics graduate coordinator, explained the situation, and he said it would definitely be a possibility."
  },
  {
    "objectID": "blog/youre_a_statistician_harry/index.html#so-whats-the-plan",
    "href": "blog/youre_a_statistician_harry/index.html#so-whats-the-plan",
    "title": "You’re a Statistician, Harry!",
    "section": "So what’s the plan?",
    "text": "So what’s the plan?\nRight away, I had to consider what track I wanted to be on with this degree. The stats department offers a thesis and a non-thesis option for their MS. The non-thesis track has a comprehensive exam at the end and requires two extra elective courses. That sounds great because there are lots of courses I really do want to take. The problem is I just don’t have the time.\nMy other option was to do the thesis track. There are slightly fewer classes required, which is good for me, but that of course means I’ll have to do a thesis. So I guess I’m writing a master’s thesis in statistics! I don’t know what I’ll write on yet, but it’ll definitely pertain to statistics used in (socio)linguistics, possibly involving some simulations and stuff, but we’ll see. I’m kind of excited about the idea of doing that actually. Plus, it’ll be really weird I think to write a master’s thesis after doing a Ph.D. dissertation.\nSo I’ve got a busy year ahead of me. After learning as much calculus as I can this summer, I’ll take three courses in the fall, three in the spring, and do some consulting work over the summer while I write my thesis. It should be a blast."
  },
  {
    "objectID": "blog/I-got-a-job/index.html",
    "href": "blog/I-got-a-job/index.html",
    "title": "I got a job at BYU!",
    "section": "",
    "text": "I can’t believe I get to say this, but in June I’ll start a new job as an Assistant Professor in the Department of Linguistics at Brigham Young University!\nFirst off, I can’t believe my odds. There are so many amazingly talented grad students out there. Seriously. Several times during the past few years, I’ve seen and met some super accomplished folks, and I’m shocked they’re only grad students. Unfortunately, the academic job market is fierce and not all of them go on to get academic jobs. Especially this year. I’m convinced that once you get to a certain level, it mostly comes down to luck after that. I was dealt good cards this year and that’s really the only reason I get to write this right now and other people can’t."
  },
  {
    "objectID": "blog/I-got-a-job/index.html#byu-is-going-to-be-awesome",
    "href": "blog/I-got-a-job/index.html#byu-is-going-to-be-awesome",
    "title": "I got a job at BYU!",
    "section": "BYU is going to be awesome!",
    "text": "BYU is going to be awesome!\nI’m really excited for BYU for several reasons. First, it’s where I got my undergrad. I’ll be teaching in the same rooms where I myself learned linguistics. In fact, during my campus visit, my teaching demonstration was in the very room that my Intro to Linguistics course was. I’ll also be colleagues with my former professors. (Calling them by first name will take some getting used to!) The campus is beautiful and right in the foothills of an enormous mountain. The Humanities building is big and beautiful. It’ll be great to be there again.\nWhat I didn’t know as a student, but what I can really appreciate now as a future teacher, is the flexibility in teaching that BYU Linguistics offers. I’m slated to teach sociolinguistics, dialectology, phonetics & phonology, and quantitative methods this year. I don’t know if too many schools could have offered me such a perfect assortment of courses. Plus, they’ve got several courses on the books that are specifically designed for faculty interests. I probably won’t be able to teach those for a few years, but if there’s some cool topic I want to try out, I won’t need to go through too many hoops in trying it out once.Update: By the end of my third year, I already been able to teach African American English, and team-taught Linguistic Data Analysis and Sociolinguistic Fieldwork!\nIn fact, I’ve already started some work for BYU Linguistics. I’m helping to prepare some course material to be used online during a study abroad next year. I’ll also get started a little earlier than normal (end of June) so I can teach a course this summer. It’s crazy to think that my family and I will be moving in less than three months!\nI’m also excited to begin research in Utah! I’ve always been interested in dialectology, especially in the American West. How many dialectologists land jobs in the heart of the geographic region that most interests them?? I have so many research questions that have to do with Utah English and Mormon English, not to mention surrounding areas like Wyoming and Idaho. To think that a few years ago I had to get a grant to collect fieldwork in Utah—now all I’ll need to do is go get groceries! I’m looking forward to collecting some data as soon as I get there."
  },
  {
    "objectID": "blog/I-got-a-job/index.html#the-job-hunt",
    "href": "blog/I-got-a-job/index.html#the-job-hunt",
    "title": "I got a job at BYU!",
    "section": "The Job Hunt",
    "text": "The Job Hunt\nI wanted to post this chart summarizing my job hunt over the past two (or three-ish) academic years. I applied to basically everything I could possibly see myself doing. I’ve heard stories of people applying to over 100 jobs. Well, for a sociolinguist, there really aren’t 100 jobs out there—this was pretty much all of them.\n\nOf the 41 jobs I applied for, 26 were tenure-track, six were visiting professor positions, six were post-docs, two were lecturer positions, and one was Alt-Ac. I got 10 Skype interviews, one campus visit, and one offer. It has never crossed my mind to apply for an industry job (heck, I don’t even know how to!), so there aren’t any there. If things hadn’t worked out this year, I would have started down that route. I think finally did get the hang of applying for jobs towards the end, but I’m just so glad I don’t need to apply for jobs anymore. (Grants though, that’s a different story…)\nAgain, I’m aware that I basically won the lottery this year. So many grad students won’t be able to get academic jobs—especially with COVID-19 destroying the economy right now. I can’t imagine how bummed I would have been if nothing had panned out. And it’s depressing to think that many of my grad student colleagues will have to open that crushing final rejection letter. I express my deepest sympathy for them."
  },
  {
    "objectID": "blog/I-got-a-job/index.html#next-steps",
    "href": "blog/I-got-a-job/index.html#next-steps",
    "title": "I got a job at BYU!",
    "section": "Next steps",
    "text": "Next steps\nToday I submitted the revisions for my dissertation to my advisor, and I anticipate submitting the final version to UGA next week. I’ve still got some loose ends to tie up with my assistantship with the Linguistic Atlas Project that I’ll work on this month. After that, it’ll be course prep, house-hunting, and (gulp!) a 28 hour drive to Provo, UT. At least the weather will be nicer there!"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Updated May 8, 2023. Download PDF here."
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\nPh.D. Linguistics, University of Georgia, 2020\n\nDissertation: Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington  \n\n\nCommittee: L. Chad Howe (chair), Margaret E. L. Renwick, William A. Kretzschmar, Jr.\n\nB.A. Linguistics, Brigham Young University, 2013\n\nMinor: Linguistics Computing"
  },
  {
    "objectID": "cv.html#academic-employment",
    "href": "cv.html#academic-employment",
    "title": "Curriculum Vitae",
    "section": "Academic Employment",
    "text": "Academic Employment\nAssistant Professor, Department of Linguistics, Brigham Young University. 2020–\nInstructional Designer, Brigham Young University. March–May, 2020."
  },
  {
    "objectID": "cv.html#publications",
    "href": "cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\nEdited Volumes\nJoseph A. Stanley, Julia Steele Josephs, Jonathan Crum, & Frithjof Timo Wöhrmann (2022). Proceedings of the 6th Annual Linguistics Conference at UGA. Linguistics Society at UGA, Athens, GA. \nJoseph A. Stanley and Conni Covington, co-editors (2019). UGA Working Papers in Linguistics, Volume 4. Linguistics Society at UGA, Athens, GA. \n\n\nRefereed Articles\nMargaret E. L. Renwick, Joseph A. Stanley, Jon Forrest, & Lelia Glass (in press). “Boomer Peak or Gen X Cliff? from SVS to LBMS in Georgia English.” Language Variation and Change. https://doi.org/10.1017/S095439452300011X.  \n\nStefano Coretta, Joseph V. Casillas, [128 other authors], Joseph A. Stanley, [23 other authors], & Timo B. Roettger (2023). “Multidimensional Signals and Analytic Flexibility: Estimating Degrees of Freedom in Human-Speech Analyses.” Advances in Methods and Practices in Psychological Sciences 6(3). https://doi.org/10.1177/25152459231162567.  \nJoseph A. Stanley & Betsy Sneller. 2023. Sample size matters in calculating Pillai scores. Journal of the Acoustical Society of America 153(1). 54–67. https://doi.org/10.1121/10.0016757.  \nJoseph A. Stanley (2022). “Interpreting the Order of Operations in Sociophonetic Analysis.” Linguistics Vanguard 8(1). 279–289. https://doi.org/10.1515/lingvan-2022-0065.  \nJoseph A. Stanley (2022). “Regional patterns in prevelar raising.” American Speech. 97(3): 374–411. 97(3): 374–411. http://doi.org/10.1215/00031283-9308384.  \nJoseph A. Stanley (2022). “A comparison of turn-of-the-century and turn-of-the-millennium speech in Georgia.” Proceedings of the 6th Annual Linguistics Conference at UGA. Linguistics Society at UGA, Athens, GA.  \nJoseph A. Stanley, Margaret E. L. Renwick, Katie Ireland Kuiper, & Rachel Miller Olsen (2021). “Back vowel dynamics and distinctions in Southern American English.” Journal of English Linguistics 49(4): 389–418. https://doi.org/10.1177/00754242211043163. \nMargaret E. L. Renwick & Joseph A. Stanley (2020). “Modeling dynamic trajectories of tense vs. lax vowels in the American South.” Journal of the Acoustical Society of America 147(1): 579–595. http://doi.org/10.1121/10.0000549.  \nJoseph A. Stanley (2019). “Phonological Patterns in beg-Raising.” UGA Working Papers in Linguistics, 4, 69–91.  \nNuckolls, Janis, Joseph Stanley, Elizabeth Nielson, & Roseanna Hopper (2016). “The Systematic Stretching and Contracting of Ideophonic Phonology in Pastaza Quichua”. International Journal of American Linguistics, 82(1). 95–116.  \n\n\nBook Chapters\nDominic Watt, Margaret E. L. Renwick, & Joseph A. Stanley (forthcoming). “Regional dialects.” In Christopher Strelluf (ed.) The Routledge Handbook of Sociophonetics. \nJoseph A. Stanley (2020). “The Absence of a Religiolect among Latter-day Saints in Southwest Washington.” In Valerie Fridland, Alicia Wassink, Lauren Hall-Lew, & Tyler Kendall (eds.) Speech in the Western States Volume III: Understudied Dialects. (Publication of the American Dialect Society 105), 95–122. Durham, NC: Duke University Press. https://doi.org/10.1215/00031283-8820642.  \n\n\nOther Publications\n(* indicates student collaborator)\nJohnson, Heather J.*, Wendy Baker-Smemoe, Joseph A. Stanley, & Alessandro Rosborough (2023). “Ducks in the Pond: Elementary-School-Age Children’s Perceptions of Standard American English, African American English, and Spanish-Accented English on Scales of Status and Solidarity.” In Paris Gappmayr & Jackson Kellogg (eds.), Proceedings of the 47th annual Boston University Conference on Language Development, 394–407. Somerville, MA: Cascadilla Press. \nJoseph A. Stanley (2022). “Order of Operations in Sociophonetic Analysis,” University of Pennsylvania Working Papers in Linguistics: Vol. 28: Iss. 2, Article 17. Available at: https://repository.upenn.edu/pwpl/vol28/iss2/17 \nJoseph A. Stanley (2022). “Perceptual Dialectology of Utah.” Schwa: Language and Linguistics: 26. 1–10.  \nJoseph A. Stanley (2019). “(thr)-Flapping in American English: Social factors and articulatory motivations.” Proceedings of the 5th Annual Linguistics Conference at UGA, 49–63.  \nJoseph A. Stanley (2018). “Changes in the Timber Industry as a Catastrophic Event: bag-Raising in Cowlitz County, Washington” Penn Working Papers in Linguistics: Vol. 24: Iss. 2, Article 16. Available at: https://repository.upenn.edu/pwpl/vol24/iss2/16 \nJoseph A. Stanley & Kyle Vanderniet (2018). “Consonantal Variation in Utah English.” Proceedings of the 4th Annual Linguistics Conference at UGA, 50–65.  \nMargaret E. L. Renwick & Joseph A. Stanley (2017). “Static and dynamic approaches to vowel shifting in the Digital Archive of Southern Speech.” Proceedings of Meetings on Acoustics 30, 060003; doi: http://dx.doi.org/10.1121/2.0000582. \nRachel M. Olsen, Michael L. Olsen, Joseph A. Stanley, Margaret E. L. Renwick, & William A. Kretzschmar, Jr. (2017). “Methods for transcription and forced alignment of a legacy speech corpus.” Proceedings of Meetings on Acoustics 30, 060001; doi: http://dx.doi.org/10.1121/2.0000559. \nStanley, Joseph A. (2016). “Pacific Northwest English: Historical Overview and Current Directions”. UGA Working Papers in Linguistics, 3.  \nStanley, Joseph A. (2016). “When do Mormons Call Each Other by First Name?” Penn Working Papers in Linguistics: Vol. 22: Iss. 1, Article 31. Available at: https://repository.upenn.edu/pwpl/vol22/iss1/31.  \nStanley, Joseph A. (2015). “Merging Phonemes in Real Time”. Proceedings of LSUGA’s Second Interdisciplinary Conference in Linguistics (LSUGA2), 1."
  },
  {
    "objectID": "cv.html#conference-presentations",
    "href": "cv.html#conference-presentations",
    "title": "Curriculum Vitae",
    "section": "Conference Presentations",
    "text": "Conference Presentations\n\nOral Presentations\n(* indicates student collaborator)\nZoe Eldredge* & Joseph A. Stanley. “Exploring the Effects of Cross-Cultural Variation and Tourism in Utah English.” New Ways of Analyzing Variation 51. New York City. October 13–15, 2023.\nKateryna Kravchenko* & Joseph A. Stanley. “An analysis of Ukrainians’ language attitudes and ideology: A conflict-catalyzed identity shift.” New Ways of Analyzing Variation 51. New York City. October 13–15, 2023.\nChad Huckvale* & Joseph A. Stanley. “Perceptions of ‘Southern’ Utah English”. New Ways of Analyzing Variation 51. New York City. October 13–15, 2023.\nKateryna Kravchenko* & Joseph A. Stanley. “Surzhyk: Attitudes and Usage among Ukrainian People.” 5th annual Sociolinguistics Symposium. Champaign, Illinois. March 2–3, 2023. \nJoseph A. Stanley & KaTrina Jackson*. “Is Idaho English really ‘the Epitome of Average English’?”. The American Dialect Society Annual Meeting. Denver, CO. January 6, 2023. \nJoseph A. Stanley. “Utahns sound Utahn when they avoid sounding Utahn.” The 97th Annual Meeting of the Linguistic Society of America. Denver, CO. January 6. 2023. \nHeather Johnson, Wendy Baker-Smemoe, Joseph A. Stanley, & Alessandro Rosborough. “Ducks in the pond: Elementary-school-age children’s perception of Standard American English, African American English, and Spanish-accented English on scales of status and solidarity.” The 47th Boston University Conference on Language Development (BUCLD). Boston, MA. November 4, 2022.\nJoseph A. Stanley & Betsy Sneller. “How Sample Size Impacts Pillai Scores – and What Sociophoneticians Should Do About It.” New Ways of Analyzing Variation 50. San Jose, CA. October 14, 2022. \nMargaret E. L. Renwick, Joseph A. Stanley, Jon Forrest, & Lelia Glass. “A Mid-Century Peak for the Southern Vowel Shift: Evidence from Georgia.” LabPhon 18. Online. June 23–25, 2022.\nJoseph A. Stanley. “Generational Change in Formant Trajectories: The Low-Back-Merger Shift in Longview, Washington.” The 4th Cascadia Workshop in Sociolinguistics. Online, May, 2022. \nDot-Eum Kim & Joseph A. Stanley. “The Participation in Non-Local Changes and the Rejection of Southern Speech by Korean Americans in Georgia.” Southeastern Conference on Linguistics 89. Online, March 31–April 2, 2022.\nJon Forrest, Margaret E. L. Renwick, Joseph A. Stanley, & Lelia Glass. “Consistent Variability: African-American Vowel Systems in Georgia.” Southeastern Conference on Linguistics 89. Online, March 31–April 2, 2022.\nJoseph A. Stanley, Jon Forrest, Lelia Glass, & Margaret E. L. Renwick. “Perspectives on Georgia vowels: From legacy to syncrhony.” The American Dialect Society Annual Meeting. Washington, D.C., January 6, 2022. \nJoseph A. Stanley & Lisa Morgan Johnson. “Vowels can merge because of changes in trajectory: Prelaterals in rural Utah English.” The 96th Annual Meeting of the Linguistic Society of America. Washington, D.C. January 6–9, 2022. \nJoseph A. Stanley. “Order of Operations in Sociophonetic Data Analysis.” New Ways of Analyzing Variation 49. Online. October 19–24, 2021.  \nJoseph A. Stanley & Margaret E. L. Renwick. “100 Years of Georgia English.” New Ways of Analyzing Variation 49. Online. October 19–24, 2021.  \nJoseph A. Stanley. “Methodological considerations in the study of infrequent phonological variables: The case of English /eɡ/ and /ɛɡ/.” Word-specific phenomena in the realization of vowel categories: Methodological and theoretical perspectives (LabPhon 17 Satellite Workshop). Vancouver, British Columbia[Online]. September, 2020.\nJoseph A. Stanley. “Beyond midpoints: Vowel dynamics of the Low-Back-Merger Shift.” Cascadia Workshop in Sociolinguistics. Vancouver, British Columbia. April, 2021. (Cancelled due to COVID-19.)\nJoseph A. Stanley & Margaret E. L. Renwick. “Back vowel distinctions and dynamics in Southern US English.” The 94th Annual Meeting of the Linguistic Society of America. New Orleans, LA. January 2–5, 2020. \nJoseph A. Stanley. “Real Time Vowel Shifts in Georgia English.” The 6th Annual Linguistics Conference at UGA (LCUGA6). Athens, GA. October 4–5, 2019. \nWilliam A. Kretzschmar, Jr. & Joseph A. Stanley. “Visualization of Big Data Phonetics.” Digital Humanities Conference 2019. Utrecht, the Netherlands. July 9–12, 2019.\nJoseph A. Stanley. “Are beg and bag-raising distinct? Regional patterns in prevelar raising in North American English.” American Dialect Society Annual Meeting. New York City, NY. January 3–6, 2019. \nRachel M. Olsen, Joseph A. Stanley, Michael Olsen, Lisa Lipani, & Margaret E. L. Renwick. “Reconciling perception with production in Southern speech.” American Dialect Society Annual Meeting. New York City, NY. January 3–6, 2019. \nJoseph A. Stanley & Margaret E. L. Renwick. “Finding pockets of social variation in the Digital Archive of Southern Speech.” The 5th Annual Linguistics conference at UGA (LCUGA5). Athens, GA. October 12–13, 2018. \nJoseph A. Stanley. “(thr)-tapping in American English: Articulatory motivations and social factors.” The 5th Annual Linguistics conference at UGA (LCUGA5). Athens, GA. October 12–13, 2018. \nJoseph A. Stanley & Kyle Vanderniet. “What el[t]se is happening[k] with Utah English consonants?” American Dialect Society (ADS) Annual Meeting. Salt Lake City, UT. January 5–8, 2018. \nJoseph A. Stanley, Margaret E. L. Renwick, William A. Kretzschmar Jr., Rachel M. Olsen, & Michael Olsen. “The Gazetteer of Southern Vowels.” American Dialect Society (ADS) Annual Meeting. Salt Lake City, UT. January 5–8, 2018.   \nJoseph A. Stanley & Kyle Vanderniet. “Consonantal variation in Utah English: What el[t]se is happening[k]?” The 4th Annual Linguistics Conference at UGA (LSUGA4). Athens, GA. October 6–8, 2017.  \nJoseph A. Stanley. “The linguistic effects of a changing timber industry: Language change in Cowlitz County, WA.” The 4th Annual Linguistics Conference at UGA (LSUGA4). Athens, GA. October 6–8, 2017. \nRachel Olsen, Michael Olsen, Katherine Kuiper, Joseph A. Stanley, Margaret E. L. Renwick, & William A. Kretzschmar, Jr. “New Perspectives on Historical Southern Speech.” Panel presented at the 2017 Integrative Research and Ideas Symposium (IRIS). Athens, GA. March 20, 2017.\nRachel Olsen, Michael Olsen, Joseph A. Stanley & Margaret E. L. Renwick. “Transcribing the Digital Archive of Southern Speech: Methods and Preliminary Analysis.” 84th Meeting of the SouthEastern Conference on Linguistics (SECOL84). Charleston, SC. March 8–11, 2017. \nWilliam A. Kretzschmar, Joseph Stanley, & Katherine Kuiper. “Automated Large-Scale Phonetic Analysis: DASS.” 84th Meeting of the SouthEastern Conference on Linguistics (SECOL84). Charleston, SC. March 8–11, 2017. \nJoseph A. Stanley. “V[ɛ]ry v[e]ried vowel mergers in the Pacific Northwest.” Diversity and Variation in Language (DiVar 1). Atlanta, GA. February 10–11, 2017. \nJoseph A. Stanley. “The perception and production of two vowel mergers in Cowlitz County, Washington.” The American Dialect Society (ADS) Annual Meeting. Austin, TX. January 5–8, 2017.  \nJoseph A. Stanley. “An EWP model of Quechua agreement: Further evidence against DM”. The Third Annual Linguistics Conference at the University of Georgia (LCUGA3). Athens, GA. October 7–9, 2016.  \nJoseph A. Stanley. “Separate Phonemes /ɔr/ Merging? The Cord-Card Merger in Real-Time”. The Second Interdisciplinary Linguistics Conference at UGA (LCUGA2). Athens, GA. October 9–11, 2015.   \nJoseph A. Stanley. “Brother Bell’s Audience Types: Forms of Address among Latter-day Saint Young Adults”. 82nd Meeting of the Southeastern Conference on Linguistics (SECOL82). Raleigh, NC. April 9–11, 2015.  \nJoseph A. Stanley. “Brother Bell’s Audience Design: Forms of Address among Latter-day Saint Young Adults”. 39th Annual Penn Linguistics Conference (PLC39). Philadelphia, PA. March 19–21, 2015.  \nJanis Nuckolls, Joseph Stanley, Elizabeth Nielsen, and Roseanna Hopper. “The systematic stretching and adjusting of ideophonic phonology in Pastaza Quichua”. Annual Meeting of the Society for the Study of Indigenous Languages of America (SSILA 2013). Boston, MA. January 3–6, 2013. \n\n\nPoster Presentations\n(* indicates student collaborator)\nAmmon Hunt*, Mark Tanner, Joseph A. Stanley, and Jeff Parker. “Using Corpus Data to Empirically Investigate Native English Speakers’ Pausing Patterns.” PSLLT2023 (Pronunciation in Second Language Learning and Teaching). West Lafayette, IN. September 6–8, 2023.\nMargaret E. L. Renwick, Jon Forrest, Lelia Glass, & Joseph A. Stanley. “Vowel trajectories of African Americans in Georgia, USA.” Poster presentation at the 183rd Meeting of the Acoustical Society of America (ASA). Nashville, TN. December 9, 2022.\nJoseph A. Stanley, Jessica Shepherd*, & Auna Nygaard*. “Homogeneity and Heterogeneity in Western American English.” Poster presentation at the American Dialect Society Annual Meeting. Washington, D.C. January 7, 2022. \nJoseph A. Stanley & Betsy Sneller. “Sample size matters when calculating Pillai scores.” Poster presentation at the 181st Meeting of the Acoustical Society of America (ASA). Seattle, WA. November 29, 2021. \nJoseph A. Stanley. “Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift.” Poster presentation at the 181st Meeting of the Acoustical Society of America (ASA). Seattle, WA. November 29, 2021. \nWilliam A. Kretzschmar, Jr., Margaret E. L. Renwick, Joseph A. Stanley, Katie Kuiper, Lisa Lipani, Michael Olsen, & Rachel Olsen. “The View of Southern Vowels from Large-Scale Data.” Poster presentation at the American Dialect Society Annual Meeting. New Orleans, LA. January 2–5, 2020. \nJoseph A. Stanley & Margaret E. L. Renwick. “Social factors in Southern US speech: Acoustic analysis of a large-scale legacy corpus.” Poster presentation at the 93rd Annual Meeting of the Linguistic Society of America. New York City, NY. January 3–6, 2019. \nJoseph A. Stanley. “The differences between and within beg and bag: Phonological, morphological, and lexical effects in prevelar raising.” Poster presentation New Ways of Analyzing Variation 47. New York City, New York. October 18–21, 2018. \nShawn Foster, Joseph A. Stanley, & Margaret E. L. Renwick. “Vowel Mergers in the American South.” Poster presentation at the 174th Meeting of the Acoustical Society of America (ASA). New Orleans, LA. December 4–8, 2017. \nJoseph A. Stanley. “Changes in the Timber Industry as a Catalyst for Linguistic Change.” Poster presentation at New Ways of Analyzing Variation 46. Madison, WI. November 2–5, 2017.  \nMargaret E. L. Renwick & Joseph A. Stanley. “An acoustic perspective on vowel shifting: Acoustic analysis of the Digital Archive of Southern Speech” Poster presentation at the 173rd Meeting of the Acoustical Society of America (ASA). Boston, MA. June 25–29, 2017. \nMargaret E. L. Renwick, Michael Olsen, Rachel M. Olsen, & Joseph A. Stanley. “Transcription and forced alignment of the Digital Archive of Southern Speech.” Poster presentation at the 173rd Meeting of the Acoustical Society of America (ASA). Boston, MA. June 25–29, 2017. \nJoseph A. Stanley & Margaret E. L. Renwick. “Phonetic Shift /ɔr/ Phonemic Change? American English mergers over 40 years”. Poster presentation at the 15th Conference on Laboratory Phonology (LabPhon15). Ithaca, NY. July 13–16, 2016."
  },
  {
    "objectID": "cv.html#workshops",
    "href": "cv.html#workshops",
    "title": "Curriculum Vitae",
    "section": "Workshops",
    "text": "Workshops\n2020 LaTeX Workshop Series with Caleb Crumley and Jonathan Crum. University of Georgia DigiLab. Athens, GA\n\n\nIntroduction to LaTeX. January 31 & February 21. \n\n\nThe UGA LaTeX Template. February 7 & 28. \n\n\nAdvanced Topics in LaTeX February 14. \n\n\n\n2019 Praat Scripting Workshop Series with Lisa Lipani. University of Georgia DigiLab. Athens, GA.\n\n\nPraat Basics: Introduction to the software. September 11. \n\n\nPraat Scripting Basics: Loops, I/O, and TextGrids. September 18.\n\n\nAutomatic Formant Extraction (and other acoustic measures) in Praat. October 2.\n\n\n\n2019 Data Visualization Workshop Series. University of Georgia DigiLab. Athens, GA.\n\n\nAn intro to data visualization in R using ggplot2. August 21. \n\n\nCustomizing your plots to make that perfect visual. August 28. \n\n\nGeneralize your aesthetics using custom themes in ggplot2. September 4. \n\n\nFidelity, integrity, and sophistication: Edward Tufte’s principles of data visualization. October 16.\n\n\nSend the right message: The dos and don’ts of color in data visualization. October 23. (with Meagen Duever)\n\n\n\n2018 R Workshop Series. University of Georgia DigiLab. Athens, GA.\n\n\nIntro to R (Part 1). January 19.  \n\n\nIntro to R (Part 2). January 26.  \n\n\nBuilding Interactive Webpages in R: Introduction to Shiny (Part 1). February 2. \n\n    &lt;li&gt;Building Interactive Webpages in R: Introduction to Shiny (Part 2). February 9.\n    &lt;a href=\"https://joeystanley.shinyapps.io/intro_to_shiny/\" class=\"link\" target =\"_blank\" title=\"view the handout online\"&gt;&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Visualizations I: Introduction to ggplot2. February 16.\n    &lt;a href=\"/downloads/180216-ggplot2-part1\" class=\"link\" target =\"_blank\"  title=\"view the handout online\"&gt;&lt;/a&gt; \n    &lt;a href=\"/downloads/180216-ggplot2-part1.pdf\" class=\"paper\" target =\"_blank\" title=\"download the handout for this presentation\"&gt;&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Visualizations II: Customizing plots in ggplot2. February 23.\n    &lt;a href=\"/downloads/180223-ggplot2-part2\" class=\"link\" target =\"_blank\"  title=\"view the handout online\"&gt;&lt;/a&gt; \n    &lt;a href=\"/downloads/180223-ggplot2-part2.pdf\" class=\"paper\" target =\"_blank\" title=\"download the handout for this presentation\"&gt;&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Clean and tidy data: Tidyverse Part 1. March 2.\n    &lt;a href=\"/downloads/180302-tidyverse_part1\" class=\"link\" target =\"_blank\"  title=\"view the handout online\"&gt;&lt;/a&gt; \n    &lt;a href=\"/downloads/180302-tidyverse_part1.pdf\" class=\"paper\" target =\"_blank\" title=\"download the handout for this presentation\"&gt;&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Transform, reshape, and modify your data: Tidyverse Part 2. March 23.\n    &lt;a href=\"/downloads/180323-tidyverse_II\" class=\"link\" target =\"_blank\" title=\"view the handout online\"&gt;&lt;/a&gt; \n    &lt;a href=\"/downloads/180323-tidyverse_II.pdf\" class=\"paper\" target =\"_blank\" title=\"download the handout for this presentation\"&gt;&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Communicate to your audience with R Markdown. March 9.\n    &lt;a href=\"/downloads/180309-rmarkdown\" class=\"link\" target =\"_blank\"  title=\"view the handout online\"&gt;&lt;/a&gt; \n    &lt;a href=\"/downloads/180309-rmarkdown.pdf\" class=\"paper\" target =\"_blank\" title=\"download the handout for this presentation\"&gt;&lt;/a&gt; &lt;/li&gt;\n\n\n2017 R Workshop Series. University of Georgia DigiLab. Athens, GA.\n\n\nAn Introduction to R: Learn the Basics. September 13.  \n\n\nAn Introduction to ggplot2. October 12.  \n\n\nAn Introduction to the Tidyverse. November 10. \n\n\n\nProfessionalization Workshops\n\n\nHow to make an Academic Poster: An Opinionated Tutorial. Workshop given to members of the BYU Linguistics Department. Provo, UT. March 10, 2022.\n\n\nHow to make an Academic Poster: An Opinionated Tutorial. Workshop given to members of the BYU Linguistics Department. Provo, UT. October 20, 2021. \n\n\nA workshop on preparing conference abstracts. Given by invitation by the Linguistics Club at the University of Georgia. Athens, GA. March 5, 2020. \n\n\nBrand Yourself: Boosting Your Online Presence (for linguistics grad students). Workshop given by invitation by the University of Georgia Department of Linguistics. Athens, GA. March 3, 2020. \n\n\nBrand Yourself: Boosting Your Online Presence. Workshop given at the University of Georgia as a part of the Graduate Research Workshop Series sponsored by UGA Libraries. Athens, GA. September 20 and 26, 2019. \n\n\nHow to make an Academic Poster: An Opinionated Tutorial. Workshop given to members of the Linguistics Society at UGA and the Linguistics Club. Athens, GA. September 11, 2019. \n\n\nBrand Yourself: Creating a Digital, Professional Presence. Invited workshop at the DigiLab, Main Library, University of Georgia, Athens, GA. September 27, 2018.  \n\n\nBrand Yourself: A professionalization workshop for grad students. Guest lecturer in ANTH 8755: Topics in (Anthropology) Research. Athens, GA. April 13, 2017. \n\n\nBrand Yourself: A professionalization workshop for grad students. Workshop given at the University of Georgia DigiLab. Athens, GA. November 11, 2016. (with Emily McGinn)    \n\n\n\nExcel Workshops\n\n\nBe a Data Magician: An Excel Workshop for Humanists. Workshop given at the University of Georgia DigiLab. Athens, GA. January 27, 2017."
  },
  {
    "objectID": "cv.html#other-presentations",
    "href": "cv.html#other-presentations",
    "title": "Curriculum Vitae",
    "section": "Other Presentations",
    "text": "Other Presentations\n\nInvited Presentations\n“Enriching our understanding of language change with vowel formant trajectories.” Macquarie University Centre for Language Sciences (CLaS) Colloquium series. Sydney, Australia (delivered remotely). August 31, 2023.\n“Modeling Change in American English Accents.” BYU Statistics Department Seminar. Brigham Young University, Provo, Utah. September 8, 2022.\n“A Brief Intro to the study of Varieties of English.” BYU’s Summer of Academic Refinement program (SOAR). Brigham Young University, Provo, Utah. June 22, 27, and July 11.\n“Is Editing Racist, Elitist, or Discriminatory?” Panel member in “Situating Editing in a Linguistics Department: A Seminar Series.” Brigham Young University, Provo, Utah. March 1, 2022.\n“Explaining Office Hours.” Panel member in a seminar called “Make a Good First Impression with Your Syllabus.” Center for Teaching and Learning, Brigham Young University, Provo, Utah. February 18, 2022.  \n“What can vowel formant trajectories tell us about language change?” Sociolinguistics Group, University of Washington, Seattle, Washington. November 30, 2021. \n“What can vowel formant trajectories tell us about language change?” Linguistics Colloquium Talks, University of Utah, Salt Lake City, Utah. October 28, 2021. \n“Prepping for the future with a degree in linguistics.” The Linguistics Club at the University of Georgia. Athens, GA (delivered remotely). November 12, 2020.\nMargaret E. L. Renwick & Joseph A. Stanley. “100 years of speech in Georgia.” Workshop on Language, Technology, and Society series. Georgia Institute of Technology, Atlanta, GA (delivered remotely). November 11, 2020.\n“Data Visualization and Basic Statistical Modeling in R.” Invited workshop for an NSF-funded “Research Experience for Undergraduates” program, helping bioanthropology students analyze osteological data from skeletons at the 7–5th c. BCE Greek colony of Himera. June 21 and 25, 2018. \n“Build a better project: Starting a DH project from primary sources.” Presented at the First DigiLab Colloquium. Athens, GA. October 6, 2016.   \n\n\nGuest Lectures\n“Why I’m glad I learned statistics and coding as a linguistics major.” Guest lecturer in LING 198: Career Explorations in Linguistics. Brigham Young University, Provo, UT. April 3, 2023.\n“Generalized Additive Mixed Effects Models.” Guest lecturer in LING 604: Research Methods. Brigham Young University, Provo, UT. March 28, 2023.\nGuest discussant in Advanced Methods in Sociophonetics. Georgetown University, Washington DC (delivered remotely). January 23, 2023.\n4-day lecture series on Praat: Introduction to the Software, Analyzing Consonants, Analyzing Vowels, and Audio Manipulation & Visualization. LING 240: Linguistics Tools. Brigham Young University, Provo, UT. October 16–23, 2020.\n“Modeling non-linear changes in time using generalized additive models.” LING 4886/6886: Text and Corpus Analysis. University of Georgia, Athens, GA. November 12, 2019.\n“Basics, Review, Summary, Help.” LING 4400/6400: Quantitative Methods in Linguistics. University of Georgia, Athens, GA. April 20.\n“Phonology of Tone.” LING 3060: Phonetics and Phonology. University of Georgia, Athens, GA. April 20.\n\n\nMiscellaneous Presentations\n“The Boomer–Gen X Transition and its effect on American English.” Linguistics Discussion Group. Provo, UT. April 11, 2023.\n“Why I’m glad I learned statistics and coding as a linguistics major.” Professor’s Story night. Linguistics Student Society. Provo, UT. January 24, 2023.\n“What can vowel formant trajectories tell us about language change?” Linguistics Discussion Group. Provo, UT. October 12, 2021.\n“Order of Operations in Sociophonetic Data Analysis.” Linguistics Discussion Group. Provo, UT. January 26, 2021.\nProfessor’s Story night. Linguistics Student Society. Provo, UT. November 19, 2020.\n“Vowel Dynamics of the Low Vowels in Cowlitz County, Washington”. University of Georgia Linguistics Colloquium. Athens, GA. January 10, 2020. \n“Vowel Dynamics of the Elsewhere Shift: A Sociophonetic Analysis of English in Cowlitz County, Washington.” Dissertation defense. Athens, GA. December 19, 2019. \n“Linguistics Papers Collection in Athenaeum” with Mariann Burright and Mary Willoughby. Libraries Spring Forum. Athens, GA. February 26, 2019.\n“/ɛɡ/-raising is straightforward? I beg to differ!” The Linguistic Society of the University of Georgia (LSUGA) Tiny Talks. Athens, GA. April 13, 2018. \n“Hey, Siri. Can you understand me?” Three Minute Thesis (3MT™) Competition at the University of Georgia. March 22, 2018.\n“New methods in outlier detection and formant measurement using a modified Mahalanobis distance and ‘mistplots.’” University of Georgia Linguistics Colloquium. Athens, GA. February 9, 2018.\n“Near-mergers in Cowlitz County, Washington.” Second Qualifying Paper defense. Program in Linguistics, Univeristy of Georgia, Athens, GA. May 4, 2017.\n“Volcanic Vocalic Changes”. University of Georgia Linguistics Colloquium. Athens, GA. April 7, 2017. \n“Linguistic Identity in Longview, Washington.” Three Minute Thesis (3MT™) Competition at the University of Georgia. March 23, 2017.\n“An EWP model of Quechua agreement: Further evidence against DM.” Presented at the Linguistic Society of the University of Georgia (LSUGA) Tiny Talks. Athens, GA. September 15, 2016. \n“Southeastern Washington English: What We Know So Far”. The Linguistic Society of the University of Georgia (LSUGA) Tiny Talks. Athens, GA. February 18, 2016. \n“When do Mormons Call Each Other by First Name?” First Qualifying Paper defense. Program in Linguistics, Univeristy of Georgia, Athens, GA. January 29, 2016.\n“Brother Bell’s Audience Design: Forms of Address among Latter-day Saint Young Adults”. University of Georgia Linguistics Colloquium. Athens, GA. February 27, 2015. \n“The systematic stretching and adjusting of ideophonic phonology in Pastaza Quichua” with Janis Nuckolls, Elizabeth Nielsen, and Roseanna Hopper. Presentation at the Brigham Young University Linguistics Department Brown Bag Meeting. Provo, UT. December 6, 2012."
  },
  {
    "objectID": "cv.html#digital-output",
    "href": "cv.html#digital-output",
    "title": "Curriculum Vitae",
    "section": "Digital Output",
    "text": "Digital Output\n\nWebsite\nJoseph A. Stanley, William A. Kretzschmar Jr., Margaret E. L. Renwick, Michael L. Olsen, and Rachel M. Olsen (2017) Gazetteer of Southern Vowels. Linguistic Atlas Project, University of Georgia. lap3.libs.uga.edu/u/jstanley/vowelcharts/.\n\n\nR Packages\njoeysvowels: An R package of datasets, based on my own speech, for use when demonstrating R code. R packgage version 0.1.0. http://joeystanley.github.io/joeysvowels/\nbarktools: Functions to help when working with Barks. R package version 0.2.0. http://joeystanley.github.io/barktools\nfuturevisions: Color Palettes based on Visions of the Future Poster Series. R package version 0.1.1. http://github.com/JoeyStanley/futurevisions\njoeyr: Functions for Vowel Data. R package version 0.3.5. http://github.com/JoeyStanley/joeyr\n\n\nMedia\nKatie Cowart. ``Classic Georgia accent fading fast.’’ UGA Today. September 7, 2023. Based on Renwick et al. (2023). \nJustin Higginbottom. KZMU, based in Moab, Utah. June 28, 2022. A 3.5-minute radio clip about my research on Utah English and “Mormonese.” \n\nLauren Paterson. “Language Of The Rockies: Linguist Seeks Speaking Patterns In Idaho And Utah.” Northwest Public Broadcasting, based in Coeur d’Alene, Idaho. April 13, 2022. A 1-minute radio clip about a project on Idaho English. \n“PhD. Candidate Seeks to Interview Multigenerational Wasatch County Families.” The Wasatch Wave. January 3, 2018. A local paper heard about my fieldwork and ran an article on the front page to help me find research participants. (Print only.)\nEhrenberg, Rachel. “The southern drawl gets deconstructed.” ScienceNews. June 30, 2017. Based on Peggy Renwick’s and my poster presentation, “A historical perspective on vowel shifting: Acoustic analysis of the Digital Archive of Southern Speech” at the June 2017 ASA conference. \nGuest host on Faith Promoting Rumors podcast. “Brother Joseph,” April 10, 2017, wherein I discuss my 2016 paper, “When do Mormons Call Each Other by First Name” published in the Penn Working Papers in Linguistics."
  },
  {
    "objectID": "cv.html#teaching",
    "href": "cv.html#teaching",
    "title": "Curriculum Vitae",
    "section": "Teaching",
    "text": "Teaching\n\nAt Brigham Young University (as faculty)\nAfrican American English (LING 495R). Spring 2023.\nEnglish Phonetics & Phonology (ELANG 327). Winter 2022, Winter 2023.\nIntro to Sociolinguistics (LING 452). Winter 2021 (×2).\nIntroduction to Varieties of English (ELANG 468). Summer 2020 (focus on the British Isles), Fall 2020, Fall 2021.\nLinguistic Data Analysis (LING 580R). Fall 2022 (with Earl Brown).\nLinguistic Tools 1 (LING 240). Fall 2020, Fall 2021.\nResearch Design in Linguistics (LING 604). Winter 2021 (with Dan Dewey), Winter 2022.\nSociolinguistic Fieldwork (LING 580R). Winter 2023 (with Lisa Johnson).\nSociolinguistics (LING 550). Winter 2021, Fall 2021, Fall 2022.\n\n\nAt the University of Georgia\nPhonetics and Phonology (LING 3060). Fall 2017, Spring 2019.\nQuantitative Methods in Linguistics (LING 4400/6400). Teaching Assistant for Peggy Renwick. Spring 2017.\n\n\nAt Brigham Young University (as a student)\nLinguistic Computing and Programming 1 (LINGC 220). Teaching Assistant for Jason Dzubak. Winter 2013.\nBasic Humanities Computing Skills (LINGC 200). Teaching Assistant for Monte Shelley. Fall 2011, Winter 2012, Fall 2012."
  },
  {
    "objectID": "cv.html#advising",
    "href": "cv.html#advising",
    "title": "Curriculum Vitae",
    "section": "Advising",
    "text": "Advising\n\nMA Thesis Chair\nKateryna Kravchenko (MA, Linguistics, current).\nChad Huckvale (MA, Linguistics, 2023). Thesis title: Utah English: A Perceptual Dialectology Study\n\n\nMA Thesis Committee Member\nMax Jensen (MA, Linguistics, current)\nRebecca Brenkman (MA, Linguistics, 2023): Afrikaans Taboo Words: Offensiveness Ranking and Reflections of Usage\nAmmon Hunt (MA, TESOL, 2023). Thesis title: Pausing Patterns in American English\nYing Suet Michelle Lung (MA, TESOL, 2022). Thesis title: The Impact of Rubric Training on Students’ Self-Efficacy and Self-Regulated Learning\nHeather Johnson (MA, Linguistics, 2022). Thesis title: Ducks in the pond: Elementary-school-age children’s perceptions of Standard American English, African American English, and Spanish-accented English on scales of status and solidarity\n\n\nUndergraduate Honors Thesis Chair\nJoshua Stevenson (BA, Linguistics, 2023). Thesis title: The “Missionary Voice”: Bona Fide Sociolect or Figment of the Mormon Linguistic Imagination?\n\n\nOther Positions\nMentor for Sarah Beecroft, 2022 Recipient of a Jack & Mary Lois Wheatley Endowed Leadership Scholarship. Project Title: Foreign Language Return Missionaries and Accent Opinions. 2023.\nGraduate Thesis Reader for Shawn C. Foster, UGA Center for Undergraduate Research Opportunities Assistantship. Thesis title: Conditioned Front Vowel Mergers in the American South. 2018."
  },
  {
    "objectID": "cv.html#funding-and-awards",
    "href": "cv.html#funding-and-awards",
    "title": "Curriculum Vitae",
    "section": "Funding and Awards",
    "text": "Funding and Awards\n\nGrants\nBYU College of Humanities Research Funding ($2,500). “Idaho English.”\nJohn Topham and Susan Redd Butler BYU Faculty Research Award ($2,000), awarded by the Charles Redd Center for Western Studies. “The Development of Utah English in Heber City.”\nBYU College of Humanities Research Funding ($3,500). “The Low-Back-Merger Shift in the Western United States.”\nSummer Doctoral Research Fellowship ($3,500), awarded by the University of Georgia Graduate School. Project title: A New Method for Extracting Acoustic Measurements from Speech Audio. May–June, 2018\nAmerican Dialect Society student travel grant. ADS. Salt Lake City, UT. January 2018.\nGraduate Research Award ($1000), awarded by the University of Georgia Willson Center for Humanities and Arts. Project title: “Intra-Family Language Variation in Utah County, Utah.” October 12, 2017.\nInnovative and Interdisciplinary Research Grant ($2,500), awarded by the University of Georgia Graduate School. Project title: “A Survey of Western American English using Amazon Mechanical Turk.” April 20, 2017.\nUniversity of Georgia Graduate School Dean’s Award ($1,250). Project title: “Linguistic Identity and the Founder Effect in Longview, Washington.” January 5, 2016.\n\n\n\nResearch Assistantships\nComplex Systems and the Humanities Research Assistantship awarded by the Graduate School at the University of Georgia ($17,664 per year). Split time between the Linguistic Atlas Project and the DigiLab, under the direction of William A. Kretzschmar, Jr. and Emily McGinn. 2016–2020.\nUniversity of Georgia stipend enhancement awarded by the Franklin College of Arts and Sciences. 2014–2015.\nUniversity of Georgia Graduate Research Assistantship. Included work for Chad Howe, Peggy Renwick, Mi-Ran Kim, Vera Lee-Schoenfeld, and Pilar Chamorro. 2014–2016.\nResearch Assistant for Monte Shelley, Maxwell Institute for Religious Scholarship, Brigham Young University. 2012–2013.\nResearch Assistant for Janis Nuckolls. Department of Linguistics and English Language, Brigham Young University. 2012–2013.\n\n\nTravel Awards\nLinguistic Atlas Project travel award. DH2019. Utrecht, the Netherlands. July 2019.\nLinguistic Atlas Project travel award. ADS and LSA. Salt Lake City, UT. January 2019.\nUniversity of Georgia Graduate School travel award. NWAV47. New York City, NY. October, 2018.\nLinguistic Atlas Project travel award. ADS. Salt Lake City, UT. January 2018.\nUniversity of Georgia Department of Linguistics travel award. NWAV46. Madison, WI. November, 2017.\nUniversity of Georgia Graduate School travel award. NWAV46. Madison, WI. November, 2017.\nLinguistics Society of the University of Georgia travel award. DiVar1. Atlanta, GA. February, 2017.\nUniversity of Georgia Graduate School travel award. ADS. Austin, TX. January, 2017.\nUniversity of Georgia Linguistics Program travel award. ADS. Austin, TX. January, 2016.\nBrigham Young University Department of Linguistics and English Language conference travel grant. SSILA. January, 2013.\n\n\nOther Employment\nUS English Linguistics Expert for DefinedCrowd. Summer 2019.\nAssistant for Dr. Peggy Renwick in developing materials and establishing an online repository for “Quantitative Methods in Linguistics” at the University of Georgia. 2016–2017.\nProgrammer for the Maxwell Institute for Religious Scholarship. 2013."
  },
  {
    "objectID": "cv.html#professional-service",
    "href": "cv.html#professional-service",
    "title": "Curriculum Vitae",
    "section": "Professional Service",
    "text": "Professional Service\n\nManuscript reviewer\nAmerican Speech (2021, 2022, 2023)\nFrontiers in Artificial Intelligence. Handling Editor. (2021)\nFrontiers in Communication. Reviewer. (2021)\nJournal of English Linguistics (2021)\nJournal of the Acoustical Society of America (2022)\nLaboratory Phonology (2021)\nLanguage and Linguistic Compass (2022)\nLanguage and Speech (2021, 2023)\nLanguage Variation and Change (2020, 2022)\nLinguistics Vanguard (2021, 2022 (×3))\nStudia Linguistica Universitatis Iagellonicae Cracoviensis (2021)\nUniversity of Georgia Working Papers in Linguistics (2023)\n\n\nAbstract Reviewer\nNew Ways of Analyzing Variation 50 (2022)\n96th–97th Annual Meetings of the Linguistic Society of America (2021–2022)\n6th Linguistics Conference at UGA (2019)\n5th Linguistics Conference at UGA (2018)\n4th Linguistics Conference at UGA (2017)\n\n\nConference Committees\n6th Linguistics Conference at UGA (LCUGA6). Social media subcommittee, webmaster, organizer of the Undergraduate Poster Session, typesetter, and miscellaneous other duties. 2019.\n5th Linguistics Conference at UGA (LCUGA5). Social media. 2018.\n4th Linguistics Conference at UGA (LCUGA4). Social media. 2017.\n\n\nOther Service\n\nLinguistic Atlas Project Advisory Board. 2022–\nFaculty liason to BYU Linguistics Department’s Graduate Student Society. 2022–.\nThree Minute Thesis (3MT™) Competition Judge for the BYU Linguistics Department MA students. 2021, 2022.\nMember of the American Dialect Society New Words Committee. 2018–.\nUGA Linguistics Graduate Student Representative. 2018–2019.\nWeb developer for the Linguistics Society at UGA. 2018–2020.\nOrganizer of the UGA Sociolinguistics reading group. 2017.\nOrganizer of the UGA Perl Study group. 2016.\nOrganizer of the UGA Typology reading group. 2016.\nEnglish-to-Portuguese simultaneous interpreter, Missionary Training Center, Provo, Utah. 2011–2013.\n\n\nProfessional Affiliations\nLinguistic Society of America (LSA), lifetime member\nAmerican Dialect Society (ADS), lifetime memeber\nLinguistc Society at the University of Georgia (LSUGA)\nSouthEastern Conference on Linguistics (SECOL)\nSociety for the Study of the Indigenous Languages of the Americas (SSILA)"
  },
  {
    "objectID": "cv.html#skills-and-experience",
    "href": "cv.html#skills-and-experience",
    "title": "Curriculum Vitae",
    "section": "Skills and Experience",
    "text": "Skills and Experience\n\nComputer Skills & Tools\nExpert (=I could teach a course on these): R(Studio), tidyverse, ggplot2, Praat, LaTeX, Perl, Word, Excel, DARLA\nProficient (=I’ve used these regularly and could help with some things): COCA (and related corpora), ELAN, FastTrack, FAVE, Jamovi JMP, Markdown, Shiny, WordCruncher, Zotero, and enough CSS, HTML, and web design to make this site.\nFamiliar (=I still need guidance): Access, AntConc, C#, DreamWeaver, HTK, LaBB-CAT, MALLET, PCT, Photoshop, Prosody-Lab Aligner, Python, SAS, SoX, SPPAS, Transcriber, UCINET, VBA, WebMAUS, Wiki markup\n\n\n\nFieldwork Experience\nSociolinguistic fieldwork in Utah and Wasatch Counties, Utah. January, 2018.\nSociolinguistic fieldwork in Cowlitz County, Washington. July, 2016.\nStudy abroad at the Andes and Amazon Field School near Tena, Ecuador, documenting Tena and Pastaza Kichwa phonology. June–July, 2011.\nBrazil (Marília, SP; Campo Grande, MS; Cáceres, Cuiabá, and Várzea Grande, MG). 2008–2010. (Not linguistics related.)\n\n\n\nLanguages\nNative: English\nFluent: Brazilian Portuguese\nConversational: Spanish, Quechua\nAcademic Knoweldge: Guaraní, Mandarin, Tz’utujil, Tshiluba, Korean\n\n  \n\nIcon credits:  \n&lt;img src='/images/icons/audio.svg' height=\"10\"/&gt;\n&lt;img src='/images/icons/document.svg' height=\"10\"/&gt; \n&lt;img src='/images/icons/link.svg' height=\"10\"/&gt;\n&lt;img src='/images/icons/video.svg' height=\"10\"/&gt;\nwere made by &lt;a href=\"https://www.flaticon.com/authors/smashicons\" title=\"Smashicons\"&gt;Smashicons&lt;/a&gt; and\n&lt;img src='/images/icons/flyer.svg' height=\"10\"/&gt;\n&lt;img src='/images/icons/presentation.svg' height=\"10\"/&gt;\nby &lt;a href=\"http://www.freepik.com\" title=\"Freepik\"&gt;Freepik&lt;/a&gt; of &lt;a href=\"https://www.flaticon.com/\" title=\"Flaticon\"&gt;www.flaticon.com&lt;/a&gt; and are licensed by &lt;a href=\"http://creativecommons.org/licenses/by/3.0/\" title=\"Creative Commons BY 3.0\" target=\"_blank\"&gt;CC 3.0 BY&lt;/a&gt;."
  },
  {
    "objectID": "pages/idiolect.html",
    "href": "pages/idiolect.html",
    "title": "My Idiolect",
    "section": "",
    "text": "Here is a never-complete but growing description of my idiolect. I’ll add to it whenever I think of or discover new things about the way I speak English."
  },
  {
    "objectID": "pages/idiolect.html#overview",
    "href": "pages/idiolect.html#overview",
    "title": "My Idiolect",
    "section": "Overview",
    "text": "Overview\nI don’t like it when people say they don’t have an accent, however, my pronunciation is pretty close to what I’d call standard American English. I grew up in St. Charles County, Missouri, which is a suburb of St. Louis. You can read Matt Gordon and Chris Strelluf’s work for an in-depth analysis of Missouri English. See also Dan Duncan’s research which is focuses specifically on St. Charles County for an even closer match to my speech. My mom grew up in Minnesota and my dad grew up in Upstate New York and Minnesota."
  },
  {
    "objectID": "pages/idiolect.html#vowels",
    "href": "pages/idiolect.html#vowels",
    "title": "My Idiolect",
    "section": "Vowels",
    "text": "Vowels\nHere is a general look at my monophthongs. These come from a recording of me reading a bunch of real and nonce words where the vowel is flanked by coronals. (You can access this dataset with my joeysvowels package.)\n\nAs far as I can tell, I don’t have really any indication of any of the chain shifts that people are studying. Even though I grew up in the St. Louis Corridor, I don’t have the Northern Cities Shift. I also don’t have the Low-Back-Merger or the Low-Back-Merger Shift.\n\nMy low back vowels\nMy lot and thought vowels are rather close phonetically but are definitely not merged. Lot is slightly fronter and unrounded while thought is slightly backer and rounded. They are very close to what Jonathan Dowse transcribes as [ɐ̞] and [ɒ̈] here.\nYou can read a fairly comprehensive list of words that I classify as lot and thought in Appendix C (pg. 209) of my dissertation, reproduced in a blog post here. On page 161, I explain that I have a clear intuition about which words are classified into which lexical set, though admittedly a few like probably, prom, mom, bronco, and pond could go either way.\nLike a lot of Americans even with the distinction, dog is thought, as are many other pre-/ɡ/ words. However, there are a few exceptions, like cog, which has an especially fronted vowel quality. Also, the name Og, as in the author Og Mandino, which is short for Augustine, bothers me because Og is clearly lot while Augustine is thought.\nPrelaterally, I have a conditioned merger, which is best described in Aaron Dinkin’s (2016) JEngL paper. (Aaron’s paper is based in Upstate New York, close to where my dad is from, so perhaps this is something I’ve gotten from him.) I have lot if the vowel is in an open syllable (and the following lateral is the onset of the following syllable), as in collar, dollar, anthropology, ollie, and trolley. In fact, I have a rather fronted vowel, fronter than my lot normally is. However, I have thought if the lateral is tautosyllabic, as in golf, dolphin, volume, and doll. Thus, words like doll & hall rhyme, while hollar & hauler and collar & caller do not. You can hear me pronounce these words in the video in this tweet.\nAs is typical of American English, the cloth lexical set is indistinguishable from my thought. Similarly, the palm set (and any “Foreign a” words; Boberg 2009) mostly fits in with my lot. Except, ironically, palm itself (as well as psalm, qualm, and alms), because of that /l/, which makes it part of thought.\nOne important phonological distinction between my lot and thought is that, even though Hayes (2009:82) says tenseness is not specified for low vowels, lot appears to be a lax vowel. Like other lax vowels, it doesn’t appear word-finally, except in marginal cases like ma and pa and in onomatopoeia (fa-la-la-la-la, haha, blah) and interjections (aha!, hurrah!). This means that some palm words like bra are reclassifed as thought. There are some exceptions though, like spa and schwa. So yes, for me, spa /spɑ/ and bra /bɹɔ/ don’t rhyme.\n\n\n/æ/ and /ɛ/ before nasals\nI raise /æ/ before nasals. Before /n/ and /m/, it’s raised, fronted, and nasalized, such that ban is definitely not the same as bat or even bad. Before /ŋ/, it’s raised even higher to the point that naive me would classify the vowel in bang as /e/. In fact, I’m not actually 100% convinced that it even is /æ/ underlyingly; I may have rephonologized it as being truly /e/.\nAs I point out on page 74 of my dissertation, there are very few words with /ɛɡ/. A nearly complete list, as far as I know, is length, lengthen, strength, strengthen, penguin, dengue, and Bengal (tiger). For what it’s worth, those vowels are the same as /æŋ/, so that bang and the first syllable of Bengal are the same for me.\n\n\nbeg-raising\nAs I explain on page 400 of my 2022 American Speech paper on prevelar raising, I raise historic /ɛɡ/ to something like [eɪɡ] such that beg, egg, leg, and Greg all rhyme with vague. It happens in open syllables like in legacy, negative, and megaphone. It occurs in some infrequent words like renege. I’ve also got it when the vowel has secondary stress, like in Winnipeg, stegosaurus, and nutmeg.\nHowever, there are a handful of exceptions, which was a major part of the reason why I did that paper in the first place. For an unclear reason to me, integrity, segregate, interregnum, and segment all have [ɛ]. I noticed that the /ɡ/ in those words are all followed by sonorants, but that’s not a guarantee blocker of raising since regulate, pregnant, and segue are raised. Interestingly, negligible is raised but negligent is not. Also, peg is raised but JPEG is not. Finally, any word with &lt;x&gt; pronounced as [ɡz] (yes, it’s voiced for me) like exit, exile, excerpt, and exigence are firmly [ɛ] and not [e].\n\n\nRosa’s Roses\nI don’t know the technical term for this, but I have two, possibly three, unstressed vowels, such that Rosa’s and roses aren’t homophonous to me. The first has [ə] while the second is what I’d transcribe as [ɨ].\nMy [ɨ] category of words is rather large and I have it in a handful of environments. Some of the distribution is predictable. I have [ɨ] in plurals (classes, offices), 3rd person singular (loses, pushes), and past tense allomorphs (waited, decided). Word-finally, it’s always [ə], as in extra, area, and data.\nIn a lot of words, I think I’m influenced by spelling. Word-initially, if it’s spelled with an &lt;a&gt;, &lt;o&gt;, &lt;u&gt; I have [ə], as in again, among, & ago, occur, opinion & obtain, and upon, unless, & until. (This spelling preference might explain why I always have [ə] word-finally because as far as I can tell &lt;a&gt; is the only letter used for unstressed word-final vowels.) However, if it’s an &lt;e&gt;, then I have [ɨ], as in expect, edition, effect, emotion, event, and exactly.\nWord-internally though, I haven’t done enough digging to see if there are any patterns and I’m not familiar with the literature so I don’t know what to look out for. I have a suspicion that if it’s next to a coronal sound, the vowel is [ɨ] and [ə] otherwise. So I have [ɨ] in student, woman, & happen but [ə] in problem, system, & item. I have [ɨ] in minute, private, & unit, but [ə] in product, democrat, develop, proposal. Interestingly, I have both vowels in advocate [ˈædvəkɨt]. But there are exceptions to these generalizations, like stomach, perfect, galaxy, miracle, and obstacle all have [ɨ]. I have [ɨ] in regime but [ə] in machine, which makes me think spelling is a stronger factor than phonological context. Without being too exhaustive, I’m inclined to think that [ɨ] is the elsewhere allophone and that [ə] is the exception.\nAs for a possible third one, I have some [ʊ]-like vowel in words like success, support, and suggest. It seems like word-initial &lt;su&gt; might be the environment, but I also get it in to. I’ll have to dig a little deeper to think of other examples.\n\n\nPrelaterals\nI have lost the ability to intuit what’s going on with my back vowels before laterals, but I’ll try to explain what I think I have. I know I merge /ʊl/ with /ol/, so that pull and pole are homophonous. However, it’s the /ʌl/ class that is really tricky for me. When it’s in a closed syllable, like in hull, dull, cull, and mulch, I’m pretty sure I at least had it merged with /ol/. However, I’ve looked at the list of words so much and I’ve thought about this enough that I pretty much know all the words that fall into this category without thinking (at least the one-syllable words) and I apparently want to unmerge them, so now you’ll be hard pressed to find me saying hull the same as hole, even in casual situations. Words like culture, result, vulnerable, multuple, and ultimately, I have no idea what I do.\nIn fact, it was this homophone that got me interested in prelaterals in the first place! There is a small town near the University of Georgia named Hull, and I had to go there for something. I thought to myself over and over as a I drove there, “Wait, is this pronounced like Hole?” I never did really figure out what I did.\nHowever, when it’s in an open syllable, like color, gullet, and sculley. The word adult fits in this category as being firmly [ʌ] rather than [o]. Though not all open syllable words are [ʌ] because like gully and mulligan I think I said as [o] when I was younger. (Not sure what I do now.) A word like sullen could go either way, even now."
  },
  {
    "objectID": "pages/idiolect.html#some-of-my-favorite-quirks",
    "href": "pages/idiolect.html#some-of-my-favorite-quirks",
    "title": "My Idiolect",
    "section": "Some of my favorite quirks",
    "text": "Some of my favorite quirks\nHere are a list of some of my favorite things I have in my idiolect.\n\nI epenthesize a [k] in ancient, [ẽɪ̃ŋkʃɨnt]. I think what’s going on is I have [ŋ] instead of [n] in the first syllable, possibly analogous to anxious, and the [k] slips in there as I transition from the velar nasal to the post-alveolar fricative.\nbig has a bit of raising towards the end of the vowel. I’d transcribe it as [bɪi̯ɡ]. I don’t have it in any other /ɪɡ/ word, as far as I know, not even pig.\nwant is [wʌnt]. In other words wants is homophonous with once.\nI 100% say camouflague as “camel-flague”. So I have an extra /l/ in there.\nThe last syllable of kindergarten has a /d/ underlyingly rather than /t/.\nThe default way I say grandma is [ɡɹæ̃mə].\n\n ## Other things\n\nPhonological\n\n/t/ and /d/ before /ɹ/ (as in try, train, dry, and drain) are affricated to [tʃɹ] and [dʒɹ]. In other words, little kid me would spell them as “chry” and “jrain”.\nI raise /æ/ before nasals but not in other environments.\n\n\n\nLexical\n\nI pronounce the &lt;l&gt; in words like psalm, alm, palm, qualm. I also pronounce it in wolf, yolk, and folk. I know I used to insert an [ɫ] in both and local, but I don’t think I do that anymore. I do do it in only though.\nThe second syllable of caterpiller doesn’t have an /ɹ/ underlyingly: /kætəpɪlɚ/\nlair is homophonous with layer and does not rhyme with hair.\nI don’t pronounce the &lt;t&gt; in often.\nThough both my parents grew up north of the on line and therefore have lot in on, I don’t, so on is firmly thought.\nI don’t have the pin-pen merger, but I have /ɪ/ in parentheses and /ɛ/ in symmetry.\nI consistently say settler with three syllables ([sɛ.ɾl̩.ɚ]) and not two (*[sɛʔ.lɚ]), even when saying the name of the game Settlers of Catan.\nI think I say violet with two syllables, meaning it’s [ˈvɑɪ.lɨt] instead of [ˈvɑɪ.ə.lət]. However, alveolar does have a very short schwa, so it’s [æɫˈvi.ə.lɚ], which does not rhyme with velar [ˈvi.lɚ]."
  },
  {
    "objectID": "pages/idiolect.html#my-kids-speech",
    "href": "pages/idiolect.html#my-kids-speech",
    "title": "My Idiolect",
    "section": "My kids’ speech",
    "text": "My kids’ speech\nSince my kids are growing up in an area different from where I grew up, they will likely acquire a different variety of English from my own. Here’s a list of things I’ve heard my 6-year-old say that is different from my own speech.\n\nkindergarten has a clear [t], i.e [kʰɨndɚɡɑɹtʰɨn] while I definitely have an underlying /d/ there.\nOccasional use of [ʔɨn] in words like Martin"
  },
  {
    "objectID": "pages/latex.html",
    "href": "pages/latex.html",
    "title": "LaTeX Workshops",
    "section": "",
    "text": "Caleb Crumley, Jonathan Crum, and I will be hosting a series of three workshops on LaTeX during Spring 2020 as a way to introduce our new UGA Grad School–approved LaTeX dissertation template. This page will house some of the materials for those workshops, with links to GitHub repositories where you can find out more.\nNote: All workshops will be held at 3:35pm at the DigiLab (300 Main Library)"
  },
  {
    "objectID": "pages/latex.html#intro-to-latex",
    "href": "pages/latex.html#intro-to-latex",
    "title": "LaTeX Workshops",
    "section": "Intro to LaTeX",
    "text": "Intro to LaTeX\nJanuary 31, 2020—In this workshop, I introduce some essential LaTeX skills, including basic typesetting, special characters, text formatting, document structure, internal references, lists, alignment, and white space. I also get into more typographical topics like font choice, line spacing, hyphenation and justification, and a few other things. This workshop was my most well-attended so far, with 30 registered attendees and another 55 or so on the waitlist. It was a full house!"
  },
  {
    "objectID": "pages/latex.html#the-uga-latex-template",
    "href": "pages/latex.html#the-uga-latex-template",
    "title": "LaTeX Workshops",
    "section": "The UGA LaTeX Template",
    "text": "The UGA LaTeX Template\nFebruary 7, 2020—Caleb Crumley will lead this workshop and will introduce the UGA LaTeX template. This has been approved by the Graduate School. The purpose of the template is to take care of the nit-picky aspects of your document to help ensure it passes the format check so that you’re free to spend your time actually writing it. Caleb will show how to download it, toggle some of the options you may want, and show additional LaTeX skills like inserting tables and images."
  },
  {
    "objectID": "pages/latex.html#advanced-topics-in-latex",
    "href": "pages/latex.html#advanced-topics-in-latex",
    "title": "LaTeX Workshops",
    "section": "Advanced topics in LaTeX",
    "text": "Advanced topics in LaTeX\nFebruary 14, 2020—Jonathan Crum will lead this workshop and will illustrate some of the more technical aspects of LaTeX for more advanced users. Some topics include custom commands, advanced font encoding, more document structure, document splitting and compiling, environments, compilers, and everyday workflow."
  },
  {
    "objectID": "pages/dataviz-workshops.html",
    "href": "pages/dataviz-workshops.html",
    "title": "Data Visualization Workshops",
    "section": "",
    "text": "In Fall of 2019, I lead a series of five workshops on data visualization. This page houses the handouts and materials for those workshops.\n \n\n\nAugust 21, 2019—In Part 1 of this workshop I cover the basic syntax and how to make some simple types of plots.\n\n\n\n\nAugust 28, 2019—Often you’ll want to customize your plots in some way. So, in this workshop we cover how to mess with properties of the plots like the axes, colors, and legends to make the plot work better for you.\n\n\nApparently I had a lot to say about how to extend your ggplot2 skills, so I ended up creating a supplement with lots of additional detail on how to modify your plots.\n\n\n\n\n\nSeptember 4, 2019–Based on a popular blog post I wrote, this workshop wraps all customization methods together and shows how to create your own themes.\n\n\nAs I was preparing for the custom themes workshop, I got a little carried away illustrating all the components of the theme function. I decided to simplify that portion of the workshop and create this separate handout that just focuses on theme. It is not yet finished, but it may be of some help to people (including myself!).\n\n\n\n\n\nOctober 16, 2019—In this workshop, I introduce a few key concepts from Edward Tufte’s book, The Visual Display of Quantitative Information such as graphical integrity, proportional ink, data-ink ratio, removing redundant material, and general graphical sophistication. Basically, the workshop could be thought of as, “How not to make awful plots.”\n\n\n\n\nOctober 23, 2019—Meagan Duever, GIS Librarian at UGA will help me lead this workshop. We talk about general principles of color in data visualization, introduce a whole bunch of nice color palettes, and demonstrate how to customize your colors in ArcMap, QGIS, Excel, and R.\n\n\n\n\nYou may also be interested in the 2018 versions of some of these workshops that I gave (Part 1 Rmarkdown and PDF and Part 2 RMarkdown and PDF). An older version from 2017 that combines elements of Parts 1 and 2 can be found here as a PDF or RMarkdown file."
  },
  {
    "objectID": "pages/dataviz-workshops.html#part-1-intro-to-ggplot2",
    "href": "pages/dataviz-workshops.html#part-1-intro-to-ggplot2",
    "title": "Data Visualization Workshops",
    "section": "",
    "text": "August 21, 2019—In Part 1 of this workshop I cover the basic syntax and how to make some simple types of plots."
  },
  {
    "objectID": "pages/dataviz-workshops.html#part-2-extending-your-ggplot2-skills",
    "href": "pages/dataviz-workshops.html#part-2-extending-your-ggplot2-skills",
    "title": "Data Visualization Workshops",
    "section": "",
    "text": "August 28, 2019—Often you’ll want to customize your plots in some way. So, in this workshop we cover how to mess with properties of the plots like the axes, colors, and legends to make the plot work better for you.\n\n\nApparently I had a lot to say about how to extend your ggplot2 skills, so I ended up creating a supplement with lots of additional detail on how to modify your plots."
  },
  {
    "objectID": "pages/dataviz-workshops.html#custom-themes",
    "href": "pages/dataviz-workshops.html#custom-themes",
    "title": "Data Visualization Workshops",
    "section": "",
    "text": "September 4, 2019–Based on a popular blog post I wrote, this workshop wraps all customization methods together and shows how to create your own themes.\n\n\nAs I was preparing for the custom themes workshop, I got a little carried away illustrating all the components of the theme function. I decided to simplify that portion of the workshop and create this separate handout that just focuses on theme. It is not yet finished, but it may be of some help to people (including myself!)."
  },
  {
    "objectID": "pages/dataviz-workshops.html#fidelity-integrity-and-sophistication-edward-tuftes-principles-of-data-visualization",
    "href": "pages/dataviz-workshops.html#fidelity-integrity-and-sophistication-edward-tuftes-principles-of-data-visualization",
    "title": "Data Visualization Workshops",
    "section": "",
    "text": "October 16, 2019—In this workshop, I introduce a few key concepts from Edward Tufte’s book, The Visual Display of Quantitative Information such as graphical integrity, proportional ink, data-ink ratio, removing redundant material, and general graphical sophistication. Basically, the workshop could be thought of as, “How not to make awful plots.”"
  },
  {
    "objectID": "pages/dataviz-workshops.html#send-the-right-message-the-dos-and-donts-of-color-in-data-visualization",
    "href": "pages/dataviz-workshops.html#send-the-right-message-the-dos-and-donts-of-color-in-data-visualization",
    "title": "Data Visualization Workshops",
    "section": "",
    "text": "October 23, 2019—Meagan Duever, GIS Librarian at UGA will help me lead this workshop. We talk about general principles of color in data visualization, introduce a whole bunch of nice color palettes, and demonstrate how to customize your colors in ArcMap, QGIS, Excel, and R."
  },
  {
    "objectID": "pages/dataviz-workshops.html#older-versions",
    "href": "pages/dataviz-workshops.html#older-versions",
    "title": "Data Visualization Workshops",
    "section": "",
    "text": "You may also be interested in the 2018 versions of some of these workshops that I gave (Part 1 Rmarkdown and PDF and Part 2 RMarkdown and PDF). An older version from 2017 that combines elements of Parts 1 and 2 can be found here as a PDF or RMarkdown file."
  }
]