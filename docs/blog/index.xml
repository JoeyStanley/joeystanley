<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Joey Stanley</title>
<link>https://new.joeystanley.com/blog/index.html</link>
<atom:link href="https://new.joeystanley.com/blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.433</generator>
<lastBuildDate>Mon, 25 Sep 2023 04:12:00 GMT</lastBuildDate>
<item>
  <title>Website Version 3</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/website-version-3/index.html</link>
  <description><![CDATA[ 




<p>After exactly seven years with my old website, I’ve decided to change it to what you are seeing now.</p>
<section id="what-was-wrong-with-the-old-one" class="level2">
<h2 class="anchored" data-anchor-id="what-was-wrong-with-the-old-one">What was wrong with the old one?</h2>
<p>I built my old website in September 2016. I had a research assistantship at the DigiLab at UGA, and Emily McGinn, the supervisor, suggested I find ways to increase my online presence. I learned some <a href="../../blog/making-a-website-is-fun">web design and CSS skills</a> skills and eventually made Version 1 of my website. That version was essentially the same as what I built in the tutorial I followed, so a few months later I rewrote everything from scratch and made Version 2. (Let’s be honest though, it’s still obviously heavily based on the tutorial.) Other than very minor tweaks to a few things, that’s how my website has been since then.</p>
<p>However, it got a bit unwieldy. The blog was organized just fine, but I also added pages here and there to go along with workshops and other presentations I gave. It got more confusing when I gave the same workshop a second time and had multiple similar pages floating around. Since I didn’t foresee some of these additions, its growth was reminiscent of unplanned suburban sprawl. For examples, sometimes images were just dumped into a folder, others were better organized. Non-blog pages were hidden and were sometimes a top-level page and other times within a dedicated subdirectory. Each individual addition wasn’t a big deal, but once I stepped back and looked at it all, it was a mess.</p>
<p>The format of my tutorials wasn’t consistent either. I have lots of handouts on my website, tucked away here and there. If they were associated with a workshop, they were separate R Markdown files that didn’t fit in with the rest of the site. Some of my earliest ones are PDFs of Word files!If they weren’t associated with a workshop, they’re regular blog posts. But because the site wasn’t connected to R, I had to do a <em>lot</em> of copying and pasting R Markdown code and careful insertion of images to get those tutorials to look right. In some cases, the extra work made it possible to do things like syntax highlighting in Praat and highlighting specific lines of code. But that was all done by manually inserting HTML tags and updating my CSS.</p>
<p>Also, as careful as I was about my CSS, it wasn’t perfect. I think there were some issues if like a list had only one element, and there were things with hyperlinks. Some one-off portions of blogs or tutorials sometimes didn’t look right. I had a disclaimer at the top of every page, something like, “This website is built from scratch. Pardon the flaws; I am not a web designer.” Which was a humble brag if anything. But as the site grew I didn’t want to change the CSS because it might change some blog post from years ago in unexpected ways.</p>
<p>Ultimately, I didn’t mind the mess because it’s what made my site unique. But, what made me finally decide to migrate to Quarto was the underlying architecture. It was built using Jekyll, which involves a programming language called Ruby in some way. After seven years I still have no idea what either of those are. I did this because it’s what the tutorial I followed used. When the site worked, it was great. But sometimes, the Ruby dependencies (called “gems”) would update or break or whatever and I had to google around trying to find a fix. I had no idea what I was doing and it led to a lot of frustrated late nights trying to get my website up and running again.</p>
<p>Then Quarto comes along, which makes it easy to make a blog entirely within R Studio. I have been very familiar with the R world for a while. In 2017, I was an early adopter of <a href="https://www.rstudio.com/products/shiny/">Shiny</a> (at least in linguistics, I think), so I was able to integrate all my html, CSS, and R skills into the <a href="http://lap3.libs.uga.edu/u/jstanley/vowelcharts/">Gazetteer of Southern Vowels</a>. In 2020, I also started dabbling with creating my own R Packages and using the amazing <a href="https://pkgdown.r-lib.org">pkgdown</a> to make dedicated websites for them (see <a href="https://joeystanley.github.io/joeyr/">joeyr</a>, <a href="https://github.com/JoeyStanley/futurevisions">futurevisions</a>, <a href="https://joeystanley.github.io/barktools/">barktools</a>, and <a href="https://joeystanley.github.io/joeysvowels/">joeysvowels</a>). Finally, I have a side project that involves collecting and analyzing data about what hymns are sung in LDS congregations, and in 2023 I decided to build the <a href="hymnstats.joeystanley.com">site</a> entirely in Quarto.</p>
<p>So, I’ve gradually built up to web development in R over the years and Quarto seems like the logical place to migrate to. Plus, it has some features that I’ve always wanted, like scrolling table of contents and a search feature. After some encouragement from folks on Twitter, I decided it’s time to bite the bullet and go for it.</p>
</section>
<section id="what-does-it-take-to-migrate" class="level2">
<h2 class="anchored" data-anchor-id="what-does-it-take-to-migrate">What does it take to migrate?</h2>
<p>I’m doing this page by page. Here’s the order I took:</p>
<ul>
<li>My homepage and any links on it. I didn’t clean up the linked pages, but at least there weren’t any dead links.</li>
<li>My blogs.</li>
</ul>
</section>
<section id="things-that-are-the-same" class="level2">
<h2 class="anchored" data-anchor-id="things-that-are-the-same">Things that are the same</h2>
<p>I’ve tried to keep as much of the original structure of the site the same as I could. However, as I migrate</p>
</section>
<section id="changes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="changes">Changes</h2>
<p>Here’s a list of the changes I’ve made.</p>
<ul>
<li><p>Each blog is now in its own self-contained folder. The previous structure had all posts in a single folder and all images in another folder. This time, the images associated with a blog post are contained within that folder. So, instead of this:</p>
<pre><code>├──📁blog
|  ├──📄blog post 1.md
|  ├──📄blog post 2.md
├──📁images
|  ├──🌅image1.png
|  ├──🌅image2.png</code></pre>
<p>It’s now this:</p>
<pre><code>├──📁blog
|  ├──📁blog post 1
|  |  ├──  📄index.qmd
|  |  ├──  🌅image1.png
|  ├──📁blog post 2
|  |  ├──  📄index.qmd
|  |  ├──  🌅image2.png</code></pre>
<p>It shouldn’t affect any urls to existing blog posts because the url <code>blog/blog post 1</code> in the old format would go to the <code>blog post 1.md</code> file and in the new one it’ll go to the <code>blog post 1</code> directory, which’ll display <code>index.qmd</code> by default. I was concerned about changing the url because I know some people have cited my turorials in published work and I didn’t want those urls to break. I think this’ll work <em>and</em> it’ll keep the site better organized.</p></li>
<li><p>Within each post, I need to update the header. I change from “tags” to “categories”, from “redirect_from” to “aliases.” I add a date-modified if needed. I remove excerpts because I only used them with my “big-link” style button. I’m replacing big-links with a standard callout box. I haven’t figured out if I can do redirects-to, which is a bummer for the GSV.</p></li>
</ul>


<div class="no-row-height column-margin column-container"><span class="">By the way, I’m stealing this way of visualizing file structure directly from <a href="https://github.com/tjmahr/quarto-blog/blob/main/posts/migrating-from-jekyll-to-quarto/index.qmd">TJ Mahr’s Migrating-to-Quarto page.</a>.</span></div></section>

 ]]></description>
  <category>Meta</category>
  <guid>https://new.joeystanley.com/blog/website-version-3/index.html</guid>
  <pubDate>Mon, 25 Sep 2023 04:12:00 GMT</pubDate>
</item>
<item>
  <title>NWAV50</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/nwav50/index.html</link>
  <description><![CDATA[ 




<p>Today I gave a talk that Betsy Sneller and I have been working on called “How Sample Size Impacts Pillai Scores – and What Sociophoneticians Should Do About It” at the 50th New Ways of Analyzing Variation conference in San Jose! This is an updateto what we presented at <a href="../../blog/asa2021">ASA2021</a>.</p>
<p>We have three things you can download.</p>
<ol type="1">
<li><p>First is <a href="../../downloads/221014-NWAV50_pillai.pptx">the actual powerpoint file</a>. In the notes of each slide though you can see the actual script I read, so you can read every word that was said during the talk.</p></li>
<li><p>Next, if that’s too much for you, you can download just <a href="../../downloads/221014-NWAV50_pillai.pdf">a PDF of the talk</a>. In case you want this ligherweight version of the slides.</p></li>
<li><p><del>Finally, here is the current manuscript that is under review with the <em>Journal of the Acoustical Society of America</em>. The final product will likely change somewhat, but most of the information is there.</del> [Edit (December 14, 2022): <a href="../../downloads/221202-Pillai-preprint.pdf">Here</a> is the accepted version.]</p></li>
</ol>
<p>If you need to calculate Pillai scores in R, I’ve got a two-part tutorial for you (<a href="../../blog/a-tutorial-in-calculating-vowel-overlap">here</a> and <a href="../../blog/vowel-overlap-in-r-advanced-topics">here</a>). I also did a blog post (<a href="../../blog/pillai-scores-dont-change-after-normalization">here</a>) about how Pillai scores don’t seem to change after normalization.</p>



 ]]></description>
  <category>Conferences</category>
  <category>Methods</category>
  <category>Phonetics</category>
  <category>Presentations</category>
  <category>Research</category>
  <category>Simulations</category>
  <category>Statistics</category>
  <category>Vowel Overlap</category>
  <guid>https://new.joeystanley.com/blog/nwav50/index.html</guid>
  <pubDate>Fri, 14 Oct 2022 03:30:00 GMT</pubDate>
</item>
<item>
  <title>ASA181</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/asa181/index.html</link>
  <description><![CDATA[ 




<p>I’m in Seattle at the 181st Meeting of the Acoustical Society of America right now! This is my first in-person conference since October 2019, so it’s great to be here. I presented two posters today, which you can read about and download below.</p>
<p><br></p>
<section id="beyond-midpoints-vowel-dynamics-of-the-low-back-merger-shift" class="level2">
<h2 class="anchored" data-anchor-id="beyond-midpoints-vowel-dynamics-of-the-low-back-merger-shift">Beyond Midpoints: Vowel Dynamics of the Low-Back-Merger Shift</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the poster <a href="../../downloads/211129-ASA2021-trajs.pdf">here</a>!</p>
</div>
</div>
<p>For some reason, I hadn’t yet presented any of my dissertation findings at a conference, not even while I was working on them or writing them up. Anyway, I’m happy to finally present some of my results at a conference. The purpose of this paper is to describe changes in vowel trajectory that accompany changes in midpoints. The Low-Back-Merger Shift is a now-widespread shift across much of North America. My data from Washington shows it pretty clearly across generations. But when I take a wide-angle lens at the vowel trajectories, I found that there was much more to the story than just a global lowering/centralizing of the front lax vowels.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://new.joeystanley.com/blog/asa181/asa2021_trajs.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>There were perhaps three patterns I noticed when I modeled vowel trajectories. First is that the trajectory length was different between them. The low vowel /æ/ was much longer, then /ɛ/, and then /ɪ/. There’s also a general U-shaped pattern. Finally, the “angle” of this U-shaped was more towards the “left” for /æ/, more towards the right for /ɪ/, and in the middle for /ɛ/. These descriptions are consistent across generations and between the two geneders modeled, so it may say more about American English articulation than anything sociolinguistic.</p>
<p>Perhaps more interestingly though was <em>how</em> these trajectories changed—within the parameters just described—across generations. Older people’s vowels traversed through much more of the F2 space than younger generations did. The result is that the older people’s vowels look more like a shallow U-shape while the younger people’s is more of V-shape or even a “bounce” straight up and down in the F1-F2 vowel space. The fact that this was consistent across all three front lax vowels and between the genders suggests some interesting sociolinguistic change.</p>
<p>At this point, this is largely descriptive work. I don’t know how perceptible these differences are and I’m not even sure if everything I just described is statistically significant. It’ll take additional work to confirm both of these. Trajectories are often ignored because they’re chalked up articulatory causes; are we comfortable saying that trajectories are 100% phonetic and 0% sociolinguistic? (Meanwhile, if I may be a bit snarky, are the arbitrary single-point measurements that are typically analyzed magically sociolinguistically important?) I think people can exploit trajectories for sociolinguistic purposes. I just don’t know how or to what extent yet.</p>
<p><br></p>
</section>
<section id="sample-size-matters-when-calculating-pillai-scores" class="level2">
<h2 class="anchored" data-anchor-id="sample-size-matters-when-calculating-pillai-scores">Sample size matters when calculating Pillai scores</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the poster <a href="../../downloads/211129-ASA2021_pillai.pdf">here</a>!</p>
</div>
</div>
<p>I’m very excited about this Pillai scores paper with a new colleague, <a href="https://betsysneller.github.io">Betsy Sneller</a>! The background for this papers is that a while ago I was analyzing some <em>cot-caught</em> merger data I had collected. I noticed that, without exception, I got higher pillai scores in wordlists than I did in conversational data. I thought I had stumbled upon some interesting style shifting! But it was <em>too</em> clean of a pattern, so I did some digging and found that it’s likely because of the sample size between the two subsets. I had less data from the wordlists than I did from the conversation. I hypothesized then that less data leads to higher Pillai scores.</p>
<section id="methods-and-experiements" class="level3">
<h3 class="anchored" data-anchor-id="methods-and-experiements">Methods and Experiements</h3>
<p>So in this paper, we test this hypothesis specifically by running a bunch of simulations. We started with a single bivariate normal distribution. We then randomly drew 5 numbers from that distribution and called it “group 1.” We then drew another 5 numbers from the <em>exact same distribution</em> and called it “group 2.” The fact that they’re drawn from the same underlying distribution represents a true underlying vowel merger. We then calculated the Pillai score of those two groups. We repeated with these group sizes 1000 times. Then we drew 6 tokens from each group 100 times and calcualted Pillai scores. Then 7. And all the way up to 100.</p>
<p>As seen in the main figure in the poster (slightly modified below), the results were clear: the larger the sample sizes, the lower the Pillai scores were. In theory, the Pillai scores should all be around zero since they’re from the same distribution. But with small samples sizes (&lt;10) observations per group, we very often got pretty high Pillai scores—scores that some researchers have considered to be distinct. It took around 30 observations per group to reliably (meaning 95% of the time) get the Pillai score under the somewhat conservative threshold of 0.1. It took 60 observations per group to get Pillai scores reliably below 0.05. This was concerning to us because few sociophonetic studies have sample sizes that large!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://new.joeystanley.com/blog/asa181/asa2021_equal.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>We were also concerned about unequal sample sizes betwen groups. So we reran the experiment, except the group sizes weren’t the same size. Each group could be anywhere from 5 to 100 observations, and we ran all 9000 or so combinations. The results were surprising to us—unequal group sizes doesn’t matter at all. The only thing that mattered was the total sample size. You can see this in the figure in the top right of the poster (or below): as you go from bottom-left to top-right, the average log Pillai score<span class="sidenote">We used log(pillai) because it worked better for this visual and for the math.</span> goes from high to low. But the fact that there is no pattern from the top-left to the bottom-right diagonal means that unequal sizes don’t matter.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://new.joeystanley.com/blog/asa181/asa2021_unequal.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>In other words, if Group A has 25 observations and Group B has 5, the Pillai score will average around 0.07. That’s the same as if you had two groups of 15. If Group A has 25 observations and Group B has 100, the Pillai score will average around 0.01. That’s the same as if you had two groups of 62.</p>
</section>
<section id="implications" class="level3">
<h3 class="anchored" data-anchor-id="implications">Implications</h3>
<p>We can think of a <em>lot</em> of implications for these findings. For one, mergers are probably underreported and splits/distinctions are probably overreported. This is because many sociophonetic studies run Pillai scores on somewhat smaller samples.</p>
<p>Because of sample size differences, comparison across studies is difficult. A study that collects lots of data per person will likely report lower Pillai scores than a study that is based on fewer observations per person.</p>
<p>Going back to the main impetus for this paper, comparison <em>within</em> studies is difficult. Since more careful speech styles typically elicit fewer observations, <strong>reading tasks will have higher Pillai scores than conversational data</strong>. To a naive researcher, this will be interpreted as style differences, when it is really just a reflection of the underlying math! This is such an important point and you can count on hearing more from me and Betsy about this in later venues.</p>
<p>Finally, one way to overcome the sample size difference is to look at the <em>p</em>-value that comes out the MANOVA test that the Pillai score came from. These <em>p</em>-values do seem to be reported in more phonetics-oriented papers, but for some reason they’re not in sociophonetics papers. So rather than us coming up with arbitrary and ad hoc thresholds for what a merged Pillai score should be, let’s us the <em>p</em>-value instead. Not reporting this <em>p</em>-value, to me at least, is kinda like reporting a <em>t</em>-statistic or <em>F</em>-ratio but without the accompanying <em>p</em>-value.</p>
<p>As a final note, and this is more of an after-thought for us, I wonder if it would be more helpful to report log(pillai) rather than raw pillai scores. Since Pillai ranges from 1 (completely distinct) to 0 (complete overlap), log Pillai would range from 0 (completely distinct) to negative infinity (complete overlap). In practical terms, it would mostly be betwen 0 and around –4 (the latter corresponding to a raw Pillai score of about 0.01). We’ll probably talk more about this in other venues so stay tuned for that.</p>


</section>
</section>

 ]]></description>
  <category>Conferences</category>
  <category>Dissertation</category>
  <category>Methods</category>
  <category>Pacific Northwest</category>
  <category>Phonetics</category>
  <category>Presentations</category>
  <category>R</category>
  <category>Research</category>
  <category>Simulations</category>
  <category>Statistics</category>
  <category>Vowel Overlap</category>
  <guid>https://new.joeystanley.com/blog/asa181/index.html</guid>
  <pubDate>Mon, 29 Nov 2021 06:00:00 GMT</pubDate>
</item>
<item>
  <title>NWAV49</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/nwav49/index.html</link>
  <description><![CDATA[ 




<p>I’m at New Ways of Analyzing Variation 49 online right now! Other than an quick online satellite session of LabPhon last summer, I haven’t attended a conference since November 2019 when we hosted LCUGA at UGA. Anyway, I’m excited to be conferencing again and while I miss seeing colleagues in-person, this online format isn’t bad. Anyway, on this page you’ll find links to the slides and YouTube videos of my two talks.</p>
<p><br></p>
<section id="order-of-operations-in-sociophonetic-analysis" class="level2">
<h2 class="anchored" data-anchor-id="order-of-operations-in-sociophonetic-analysis">Order of Operations in Sociophonetic Analysis</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the poster <a href="../../downloads/211019-NWAV49_OoO.pdf">here</a>!</p>
</div>
</div>
<p>I’m very excited and nervous about this Order of Operations one. As it turns out, even if you start with the exact same spreadsheet and use the exact same functions, if you do those function in different orders, it’ll produce different results. Sometimes drastically different results. I did this by processing a spreadsheet 5,040 unique ways and got a whole range of results. To me at least, it’s making me rethink how I process my data and how I can interpret others’ results when the order isn’t explicitly reported in the methods section of a paper.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/8TEip-Fixyw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
</section>
<section id="years-of-georgia-english" class="level2">
<h2 class="anchored" data-anchor-id="years-of-georgia-english">100 Years of Georgia English</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the poster <a href="../../downloads/211019-NWAV49_Georgia.pdf">here</a>!</p>
</div>
</div>
<p>As a continuation of some work I did as a grad student, <a href="http://faculty.franklin.uga.edu/mrenwick/">Peggy Renwick</a> and I presented our research on Georgia English vowels and how they’ve changed over 100 years. Basically, all of them have. The Southern Vowel Shift seems to have undergone a rise and fall, perhaps peaking in those born around WWII. Meanwhile, back vowels are fronting. Younger people today have something like the Low-Back-Merger Shift, but flavored with some Southernisms still. You’ll hear more about this project in later conferences too.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/USF6fspxiGU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
<p><br></p>


</section>

 ]]></description>
  <category>Conferences</category>
  <category>Methods</category>
  <category>Presentations</category>
  <category>Research</category>
  <category>Simulations</category>
  <category>South</category>
  <guid>https://new.joeystanley.com/blog/nwav49/index.html</guid>
  <pubDate>Mon, 18 Oct 2021 12:00:00 GMT</pubDate>
</item>
<item>
  <title>Kohler Tapes</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/kohler-tapes/index.html</link>
  <description><![CDATA[ 




<p>So, I just acquired a goldmine of data that I can use for linguistic analysis. Sitting in my office are 452 cassette tapes, each containing at least 30 minutes of recorded interviews with an older folks from Heber City, Utah. And that’s about half of the collection: the other half is with a historian in Midway, Utah. So, I’m looking at roughly 400–500 hours of audio. Not sure how I’m going to process it all, but I wanted to kick off the beginning of this long-term project with a blog post describing the history of the tapes, why I’m interested in them, and speculations about the future.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/kohler-tapes/kohler_tapes.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">452 tapes sitting on my shelf!</figcaption>
</figure>
</div>
<section id="background" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>I first heard about the tapes a little over three years ago. In January 2018, the LSA annual meeting was in Salt Lake City. Wanting to take advantage of the trip out there, I applied for and received a grant from the University of Georgia to collect audio in Heber City, aiming for multiple generations within a family to track language change over time. I decided on Heber partly because it was a region of Utah that had never been the subject of acoustic (let alone linguistic) analysis, as far as I know. My parents were living there at the time too, so they could hook me up with some potential contacts.</p>
<div class="page-columns page-full"><p>So on the morning of the first day of my fieldwork, the first thing I did was go to the Heber Valley Visitor’s Center as a way to potentially find some contacts. Literally the first person I talked to told me about a man who had a huge collection of tapes. One person led me to another, and I was talking to an elderly man named Norm Kohler in his nursing home.</p><div class="no-row-height column-margin column-container"><span class="">Side note, it’s amazing that I heard about this goldmine <em>literally</em> through the first person I talked to while doing fieldwork? Who knew that there’d be such an amazing collection of audio sitting in someone’s basement nearby? In fact, there could be lots of collections like these, just collecting dust in people’s basements. All it takes is to find the right person!</span></div></div>
<p>Norm was a beloved middle school teacher in Heber City in the 1980s and 1990s. As a history project, he had each of his students get a cassette tape and interview a grandparent. I don’t know what the interview questions were, but I think they mostly concerned life in Heber Valley. He kept all the tapes his students turned in and, over the course of two decades, he ended up with over 1200 interviews! Norm intended to compile them and put together an oral history of the town, but unfortunately was unable to do so. So, just weeks before I met him, he decided it was best to return the tapes to the family members’ of his students and the people they interviewed. So he put an ad in the paper and hundreds of people claimed their tapes and were able to hear their ancestors’ voices, perhaps for the first time.</p>
<p>However, not all the tapes were claimed. I was told a few hundred remained. So, after Norm passed away a few months later, his family held on to them for a while before finally donating them to the Midway Historical Society. (Midway is the town next door to Heber.) Several complications made it difficult for me to get access to the tapes, including outdated contact information on Midway’s website, the society going on an extended hiatus, me living in Georgia, and then covid. But I did my best to reach out to anyone who might know about where the tapes were being stored.</p>
<p>Finally, on Thursday this week, I was contacted by the historian in custody of the tapes. She asked if I was still interested in them, and I most definitely am! So we had a nice chat about what my goals were for them and what the goals were for the Historical Society, and we think there’s mutual interest in getting them digitized and transcribed. So, the next day, yesterday, she happened to be in Provo so she dropped off about half of the tapes—452 of them!—at my office!</p>
<p>So after three years of following tenuous leads, I finally have the tapes!</p>
</section>
<section id="why-am-i-so-interested" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-am-i-so-interested">Why am I so interested?</h2>
<p>I am a linguist, so why should I care about these tapes? Well, the obvious reason is that it’s a <em>lot</em> of audio. For my dissertation I analyzed about 40 hours of interviews and that was already a lot of data. This is at least 10 times the amount of audio. In fact, it’s about the size of the <em>Digital Archive of Southern Speech</em>, a subset of the <em>Linguistic Atlas of the Gulf States</em>, that I spent four years in grad school analyzing. So having access to this much audio is absolutely incredible.</p>
<p>But it’s not just the amount of audio. There are dozens of oral history projects even in Utah. This particular set is attractive for several reasons:</p>
<ol type="1">
<li><p>The nature of the homework assignment ensured good metadata. A few tapes have already been digitized and they all start off introducing the interviewer (the middle-schooler) and the interviewee, with information like the date of interview, their age, and where they grew up.</p></li>
<li><p>Because these were all students in the same smallish town in Utah the sample will be relatively homogeneous geographically. While it doesn’t ensure that the interviewees (the grandparents) were from Heber or Heber Valley generally, my guess is that a significant number of them are.</p></li>
<li><p>Typically, interviews happen with a historian or someone that the interviewer is unfamiliar with. In sociolinguistics, it’s generally accepted that the degree of familiarity with the interviewer can have an influence on a person’s speech. In all cases with these tapes, the interviewer is a teenager and a grandchild of the interviewee. So that lowers the formality of the situation and will likely mean that the interviewees’ speech will be more casual.</p></li>
<li><p>Heber Valley has been the focus of very little acoustic research. There may be occasional interviews as parts of the Linguistic Atlas Project or the <em>Dictionary of Regional American English</em>, but no study, as far as I know, has focused on Heber. Instead, most research looks at people from Utah Valley and Salt Lake Valley. This collection of interviews will offer a new spot on the map of Utah dialectology and a nice point of comparison between more urban and more rural areas of the state.</p></li>
<li><p>I have virtually no metadata about the interviewees right now, but if their grandchildren were about 14 years old in the 1980s and 1990s, then the speakers in these tapes were born perhaps sometime between 1900 and 1940. There has been some research on the development of Utah English, mostly by David Bowie, but he acknowledges that it was based on public sermons given by upper-class white men. This collection offers a unique look into how other Utahns born around that time talked. And since I have some comparable data from contemporary Heber City residents, I can begin to look at language change in real time.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><span class="">There were about 4500 people living in Heber in the 1980s and 1990s, which means this sample is a significant chunk of the community!</span></div><p>So there are lots of reasons for why I’m really interested in this collection of tapes. And that’s on top of the oral history the Midway Historical Society wants to create based on them.</p>
</section>
<section id="looking-ahead" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead">Looking Ahead</h2>
<p>Luckily, I’ve had some experience working on a project of this size. For four years at the University of Georgia, I was a part of the team that processed the <a href="http://www.lap.uga.edu/Site/DASS.html"><em>Digital Archive of Southern Speech</em></a>, which is a 367-hour subset of the <em>Linguistic Atlas of the Gulf States</em>. So I’ve sat in on transcriber training sessions, seen what kinds of obstacles get in the way of processing, managed thousands of files, and analyzed spreadsheets with a couple million acoustic measurements in them. However, that was only as a graduate student. I’m sure there’s a lot that goes on behind the scenes as a PI that I didn’t see.</p>
<p><em>Transcribers</em>—Some back-of-the-envelope calculations suggest that I’ll need a sizable grant to get this all processed. Again, I don’t have definite numbers for anything, I know my 452 tapes are a little over half of them, so let’s say there are 700 tapes total. They’re all at least 30 minutes long and I know many went longer, so if I average say 40 minutes per tape, that’s 28,000 minutes or roughly 467 hours. I think the the transcribers for <em>DASS</em> averaged about 13 hours per 50 minutes of audio or so, but this audio is newer and I presume Utah transcribers will be more familiar with Utah speakers I think, so I’ll estimate 10 hours of work per tape. That’s 4670 hours of transcription. At $15 per hour, I’m looking at about $70,000 in student wages. Obviously, I can’t get that much coin internally so it sounds like this is only going to happen with an external grant.</p>
<p><em>Grad student workers</em>—That’s of course assuming that the only wages I’ll need to pay for are transcribers. This might be getting into “If you give a mouse a cookie” territory, but it would be nice to have some grad students helping out with the project. At UGA, we had at least four and as many as six grad students involved in the project at a time. There was a lot of overlap between our duties, but very roughly speaking, one managed the transcribers, one managed the spot-checks, one managed the acoustic analysis, and one did miscellaneous duties. We were all involved in analysis, and a few others popped in for a semester or two to do additional analysis or perform other duties. To lighten my load, it would be handy to have perhaps three grad students manage the transcribers, check their work, and do the acoustic analysis. I’m fuzzy on what costs are associated with RA-ships at BYU, but I do know it’ll add significantly to the total cost of the project.</p>
<p><em>Time</em>—How long will transcriptions take? I’ve done transcriptions and they’re soul-sucking work. Even when I was highly motivated to process my own dissertation data, that I collected myself, and under a bit of a time crunch, I could barely put in more than about two hours a day. I surely don’t expect undergraduate transcribers to do more than 10 hours a week. When motivated by money, I’ve seen some at UGA do more, but those students were exceptional. I’ll estimate five hours of work per transcriber per week. So under the assumption of 4670 hours of work total, that’s 934 transcriber-weeks. If a semester is fifteen weeks, that’s 62 transcriber-semesters. If I set a goal of getting all the work done in two years (six semesters if you include summers), it would take ten or eleven transcribers to do it in two years. Of course, these are all very rough estimates, but managing several tens of thousands of dollars and almost a dozen workers for two years is not something I expected to do right away!</p>
<p><em>Digitizing</em>—Regardless of the cost, number of workers, and time involved, the first step of the process will be digitization. Fortunately, it sounds like the Office of Digital Humanities can take care of that for me! Wow! So my short term goal is to get a batch—maybe 30 or 50 tapes—done first. While they work on digitizing the next batch, I can get started on listening to the first few minutes of the completed tapes and extracting whatever metadata I can from them. Eventually, all the tapes will be digitized and I can have a more concrete idea of how much audio (and consequently, people, time, hours, and money) I’m looking at.</p>
<p><em>Metadata</em>—After digitizing all of them, my next step will be to finish collecting the metadata. It’ll be nice to have a clear picture of birth years, genders, and birthplaces for all 700 or so people. The most likely scenario is that I <em>won’t</em> get an external grant because they’re extremely competitive, so I’ll have to prioritize which ones to transcribe first. The Historical Society would like to start with some of the prominent members of the community and descendants of the town’s founders. I’d like to find a balance of genders and birth years too, so we’ll probably settle on a subset that satisfies both of our needs. How big? I’m thinking between 35 and 70 (5% to 10% of the tapes). That’s a more reasonably-sized project that I could possibly get funded internally. It could provide me at least a beginning look at the speech community which would help seed an external grant.</p>
<p><em>Follow-up project?</em>—In case I just need more data to analyze (ha!) wouldn’t it be cool to track down some of the tapes that were given away? Presumably, if an ad in the paper is what it took for the families to get them, then an ad might be a good place to start to find them. We’d digitize the tapes right there for people, give them a copy and return the tape to them of course, but then also add that to the collection for the oral history. I think it would be especially cool to interview those people themselves! That way we can get some contemporary data to compare the tapes to, as well as track change within the family. That’ll have to wait until I get NSF grant number two!</p>
<p><em>Publications</em>—What’s the end goal? Well, I’ll obviously start cranking out some papers as soon as a reasonable amount of data has been processed. There is a <em>lot</em> going on in Utah English. Many of the stereotyped features are dying out, so these people may provide good acoustic data for what would otherwise be hard to study phonetically today. But there are also lots of other features that I believe are recent innovations, so if they’re infrequent or missing from these speakers, it’ll help establish the timing of when they did develop. Even before I had the tapes, I’ve been thinking a full analysis of this collection deserves a book-length treatment. It likely won’t get done before I’m up for tenure, but maybe it’ll go towards my application for full professor.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The history of the Kohler Tapes is pretty cool, and I’m lucky to be a part of the creation of an oral history of Heber City. It’s so satisfying teaming up with a historical society and finding ways to help the community I’m studying too. Linguistically, they’re interesting to me for lots of reasons, but I think everyone benefits from seeing these tapes get processed. As far as how I’m going to go about processing all of them, I really have no idea what I’m doing so there will be a lot of learning involved. But I’m excited to be involved and to have a clear research trajectory for the next decade or so!</p>


</section>

 ]]></description>
  <guid>https://new.joeystanley.com/blog/kohler-tapes/index.html</guid>
  <pubDate>Sat, 13 Feb 2021 21:16:00 GMT</pubDate>
</item>
<item>
  <title>Dissertation</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/dissertation/index.html</link>
  <description><![CDATA[ 




<p>I’m happy to report that I successfully defended my dissertation today! The <a href="../../downloads/191209-dissertation_defense.pdf">defense</a> was held in the DigiLab (300 Main Library). The study itself is called “Vowel Dynamics of the Elsewhere Shift: A sociophonetic analysis of English in Cowlitz County, Washington.”</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/dissertation/dissertation_defense.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Me with my committee: Chad Howe, Peggy Renwick, and Bill Kretzschmar (Skyping in).</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can download my dissertation <a href="../../downloads/200417-dissertation-revised.pdf">here</a>!</p>
</div>
</div>
<p>The version linked above is a revision that I’ve made after correcting some small typos. Click <a href="https://search.proquest.com/docview/2412335574/398995AF274140D8PQ/1?accountid=4488">here</a> to view the official, submitted version.</p>



 ]]></description>
  <category>Dissertation</category>
  <category>Pacific Northwest</category>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/dissertation/index.html</guid>
  <pubDate>Wed, 04 Dec 2019 13:00:00 GMT</pubDate>
</item>
<item>
  <title>LCUGA6</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/lcuga6/index.html</link>
  <description><![CDATA[ 




<p>I presented at the 6th annual Linguistics Conference at UGA today! My presentation, which was called “Real Time Vowel Shifts in Georgia English” compared Georgians born around the 1890s to those born in the 1990s—100 years of change! The gist: nearly every vowel has changed, and it seems like the trajectory of that change is in the direction of the Elsewhere Shift, rather than just a simple recession of Southern features.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the slides <a href="../../downloads/191004-lcuga6.pdf">here</a>!</p>
</div>
</div>
<p>The data came from two very different sources. The older speakers, were born between 1887 and 1903, were recorded doing Linguistic Atlas interviews and are part of the <em>Digital Archive of Southern Speech</em>. The younger speakers, who were born between 1994 and 1997) were UGA undergraduates that I recorded as they read sentences in a lab. Two very different datasets, but they’re all Georgian natives, so it starts to give us a glimpse into how things have changed around here.</p>
<p>Here’s a summary of the changes I found.</p>
<ul>
<li><p><sc>price</sc> was monophthongal for older speakers and definitely a diphthong in the younger speakers.</p></li>
<li><p>The Euclidean distance between <sc>fleece</sc> and <sc>kit</sc> and between <span class="smallcaps">face</span> and <sc>dress</sc> was grew over 100 years, suggesting the younger speakers’ vowels are not as “Southern” sounding.</p></li>
<li><p>While the older speakers’ <sc>lot</sc> and <sc>thought</sc> were close, the younger speakers’ were almost entirely merged.</p></li>
<li><p>The front lax vowels, <sc>trap</sc>, <sc>dress</sc>, and <sc>kit</sc> were front and monophthongal in the older speakers while the younger speakers realized them more centralized and more diphthongal. <sc>trap</sc> underwent the most change, then <sc>dress</sc>, and then <sc>kit</sc>.</p></li>
<li><p><span class="smallcaps">goose</span> was centralized even in the older speakers, but the younger speakers took it further by using an even more fronted onset.</p></li>
</ul>
<p>This animation basically sums up the entire talk. Plus I just had fun making animations! Here’s the change for the women.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://new.joeystanley.com/blog/lcuga6/georgia_animation_female.gif" class="img-fluid figure-img"></p>
</figure>
</div>
<p>And here’s the change for the men.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://new.joeystanley.com/blog/lcuga6/georgia_animation_male.gif" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Overall, it appears that it’s not just that the younger speakers lack features associated with the South, it’s that they’re also using realizations characteristic of other regions like California and Canada. It makes sense, since Atlanta is a large metropolitan area and is kind of known as a non-Southern city in the middle of the South. But this might be an indication that the Elsewhere Shift has made its way into the South.</p>



 ]]></description>
  <category>Animations</category>
  <category>Conferences</category>
  <category>Linguistic Atlas</category>
  <category>Presentations</category>
  <category>Research</category>
  <category>South</category>
  <guid>https://new.joeystanley.com/blog/lcuga6/index.html</guid>
  <pubDate>Fri, 04 Oct 2019 04:30:00 GMT</pubDate>
</item>
<item>
  <title>DH 2019</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/dh2019/index.html</link>
  <description><![CDATA[ 




<p>At the Digital Humanities 2019 conference in Utrecht, the Netherlands, I presented with Bill Kretzschmar on ways to visualize a lot of phonetic data.</p>
<section id="the-gazetteer-of-southern-vowels" class="level2">
<h2 class="anchored" data-anchor-id="the-gazetteer-of-southern-vowels">The Gazetteer of Southern Vowels</h2>
<p>The first half of the presentation was essentially me showcasing the <em>Gazetteer of Southern Vowels</em> (or <em>GSV</em>), a website I created in Shiny to help visualize 1.3 million acoustic measurements from the <em>Digital Archive of Southern Speech</em>.<span class="sidenote-left">The full web address is <a href="http://lap3.libs.uga.edu/u/jstanley/vowelcharts/">http://lap3.libs.uga.edu/ u/jstanley/vowelcharts/</a>, but I’ve got a redirect at <a href="joeystanley.com/gsv">joeystanley.com/gsv</a> that’s easier to type.</span> In the talk I spend most of the time in the “Vowel Plot Comparison” tab (below) and show how you can interact with the data.</p>
<p><img src="https://new.joeystanley.com/blog/dh2019/side_by_side.png" class="img-fluid"></p>
<p>First, you can subset the data by demographic factors. The <em>Speaker Selection</em> tab has menu items for speakers’ sex, age, ethnicity, home state, social class, and a couple other variables. When you select one or more of these, the plot automatically updates to reflect that subset.</p>
<p><img src="https://new.joeystanley.com/blog/dh2019/tab_speaker.png" class="img-fluid"></p>
<p>You can also subset the data by linguistic factors. In the <em>Words</em> tab, you’ll see that a list of stopwords is displayed and that those are excluded by default. You can add to or remove words from that stoplist, or switch it so display only those words (or some other set of words like numbers or colors).</p>
<p><img src="https://new.joeystanley.com/blog/dh2019/tab_words.png" class="img-fluid"></p>
<p>In the <em>Vowels</em> tab, you’ve got a whole bunch of options. First, you can choose what vowel is being displayed, what kind of stress it can have, and what its phonetic environment is(based on following segment only). There are some methodological choices too, like ways of filtering and normalizing the data. You can also choose what transcription system is being used.</p>
<p><img src="https://new.joeystanley.com/blog/dh2019/tab_vowels.png" class="img-fluid"></p>
<p>Then, there are ways for you to customize the plot. The <em>Plot Style</em> tab as four main options (points, ellipses, means, and words), that act independently with their own controls for size and opacity. So if you want means and ellipses but no dots, you can do that. If you want to display the words themselves, but in a small font and transparent, be my guest.</p>
<p><img src="https://new.joeystanley.com/blog/dh2019/tab_plot_style.png" class="img-fluid"></p>
<p>The <em>Plot Customization</em> tab lets you change things like the zoom, axes, aspect ratio, and (some control over) colors. I’m hoping to add more options to this tab in the future. With these two tabs, I feel like you can make a lot of very different plots, all based on the same data, which is pretty cool.</p>
<p><img src="https://new.joeystanley.com/blog/dh2019/tab_plot_customization.png" class="img-fluid"></p>
<p>Finally, the <em>Download Options</em> is the newest tab. You could always take a screenshot, but you’re limited to how your web browser displays the image and your computer’s screensize. In this tab, you can set the height, width, quality, and format, so you can make publication-quality images. In fact, the plots in this blog post were all created using this download button, so you can recreate them yourself!</p>
<p><img src="https://new.joeystanley.com/blog/dh2019/tab_download.png" class="img-fluid"></p>
<p>So that’s it! My goal in creating these options was to allow users to create any type of plot using any conceivable subset of DASS, and I think the GSV does a pretty good job at that. Here’s a quick gallery of six different plots:</p>
<p><img width="50%" style="float:left;" src="https://new.joeystanley.com/blog/dh2019/sample_points_big.jpeg"> <img width="50%" style="float:right;" src="https://new.joeystanley.com/blog/dh2019/sample_ellipse_means.jpeg"> <img width="50%" style="float:left;" src="https://new.joeystanley.com/blog/dh2019/sample_words_ellipse_means.jpeg"> <img width="50%" style="float:right;" src="https://new.joeystanley.com/blog/dh2019/sample_points_trans_means.jpeg"> <img width="50%" style="float:left;" src="https://new.joeystanley.com/blog/dh2019/sample_points_ellipse.jpeg"> <img width="50%" style="float:right;" src="https://new.joeystanley.com/blog/dh2019/sample_words_ellipse.jpeg"></p>
</section>
<section id="point-pattern-analysis" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="point-pattern-analysis">Point Pattern Analysis</h2>
<p>In the second half of the presentation, Bill Kretzschmar took over and discussed using point pattern analysis in the visualization of vowel data. He has found that when you overlay a grid on the F1-F2 space (just as geographers do with geospatial data), you can see the central tendency of vowels by which “cells” in this new grid are the densest. They roughly follow the 80-20 rule, with a few cells being heavy concentrated, some having some tokens, and many with very few. Here’s just one image of a Georgia man’s <sc>fleece</sc> vowel.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">Incidentally, because the grid itself is new element, there are additional controls in the <em>Plot Customization</em> tab, like how many cells and the opacity or size of the labels. And since there’s only one color being used, I’ve got controls over whether the shading is discrete or continuous, if it’s discrete then how many levels, and you can even put a custom color in hex notation. Also, I’m still working on getting the ranges in the legend to display integers only, since 54.6 data points in a cell is somewhat nonsensical.</span></div></div>
<p><img src="https://new.joeystanley.com/blog/dh2019/point_pattern.jpeg" class="img-fluid"></p>
<p>Bill finds that if you plot them in order of density, the resulting curve is an asymptotic hyperbolic curve, or just A-curve for short. And, as it turns out, this distribution is fractal in nature, so regardless of how much you subset the data, you’ll find the same distribution. The <em>GSV</em> makes it easy to see this distributions interactively.</p>
<p>At the very end, we hinted at some additional visualizations we’d like to develop to make it easier to view trajectory data, taking advantage of a third-dimension in the plot itself. Hopefully, we’ll have more to say about that in the future.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>So that’s our presentation! We’ve got a lot of data and we needed lots of plots to make sense of it all. Instead of saving plot after plot, we decided an interactive Shiny app might be a better option. Most importantly, I think we’ve learned a little more about how language works because of the size of the data and the interactivity of this tool.</p>


</section>

 ]]></description>
  <category>Conferences</category>
  <category>Linguistic Atlas</category>
  <category>Presentations</category>
  <category>Research</category>
  <category>South</category>
  <guid>https://new.joeystanley.com/blog/dh2019/index.html</guid>
  <pubDate>Mon, 08 Jul 2019 22:30:00 GMT</pubDate>
</item>
<item>
  <title>LSA and ADS 2019</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/ads-and-lsa-2019/index.html</link>
  <description><![CDATA[ 




<p>Thanks for attending my presentations. At the 2019 annual meetings of the American Dialect Society and the Linguistic Society of America in New York City, I was fortunate to present three presentations!</p>
<p><br></p>
<section id="thursdays-lsa-poster-on-southern-vowels" class="level2">
<h2 class="anchored" data-anchor-id="thursdays-lsa-poster-on-southern-vowels">Thursday’s LSA poster on southern vowels</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the poster <a href="../../downloads/190103-lsa2019.pdf">here</a>!</p>
</div>
</div>
<p>Thursday, <a href="http://faculty.franklin.uga.edu/mrenwick/">Peggy Renwick</a> and I presented our poster on social patterns in static and dynamic measurements of Southern American English vowels. Our dataset was the Digital Archive of Southern Speech (DASS), a collection of 64 interviews from the 1970s. We looked at Pillai scores to measure the degree of “swapping” between pairs of front vowels (/i&nbsp;ɪ/ and /e&nbsp;ɛ/) and we used vector length, trajectory length, and spectral rate of change to see how dynamic the vowels were.</p>
<p>We found a bunch of cool patterns! You can see the poster for all of them, but one of the cooler ones was that /e/ and /ɛ/ swapped more in younger speakers. This plot shows the trajectories of these two vowels split up by generation and you can see how they get closer together (though keep in mind that because the trajectories are drastically different, these aren’t merging).</p>
<p><img src="https://new.joeystanley.com/blog/ads-and-lsa-2019/ads2019_EY_EH_gen.jpeg" class="img-fluid"></p>
<p>This is to be expected for speakers with the Southern Vowel Shift. But, the African American Vowel Shift doesn’t have this same swapping. So sure enough, if we look at the data split by ethnicity, we see that the African American speakers had less speakers than the European Americans.</p>
<p><img src="https://new.joeystanley.com/blog/ads-and-lsa-2019/ads2019_EY_EH_eth.jpeg" class="img-fluid"></p>
<p>So our results are mostly what we expected to find. But this corpus of older recordings give us a unique peek into the past while these changes were developing. And by using both static <em>and</em> dynamic measurements, we can get a more complete picture of what’s going on.</p>
<p><br> <br></p>
</section>
<section id="fridays-ads-presentation-on-prevelar-raising" class="level2">
<h2 class="anchored" data-anchor-id="fridays-ads-presentation-on-prevelar-raising">Friday’s ADS presentation on prevelar raising</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the slides <a href="../../downloads/190104-ads2019_prevelar.pdf">here</a>!</p>
</div>
</div>
<p>Early Friday morning, I talked about regional patterns in <sc>beg</sc>- and <sc>bag</sc>-raising in North American English. There’s been a lot of research on these phenomena, but only in places like the Upper Midwest, Western Canada, and the Pacific Northwest. Perhaps there is prevelar raising in other areas too?</p>
<p>I used the same dataset that I used for my <a href="../../nwav47">NWAV47</a> presentation: I set up an online survey and asked people how they pronounced several dozen prevelar words. For geographic data, I used GPS coordinates of people’s childhood homes. I ended up with over 500,000 data points!</p>
<p>It was clear that there were differences between the two vowels. <sc>bag</sc>-raising was pretty clear cut: either people had it or they did not. And most people didn’t. There just weren’t too many people that were sort of on the middle ground. However, with and <sc>beg</sc>-raising, while there were lots of people with no raising, there were tons of people with varying amounts of raising. Basically, <sc>beg</sc> was much more variable than <sc>bag</sc>.</p>
<p><img src="https://new.joeystanley.com/blog/ads-and-lsa-2019/ads2019_bag_raising_histogram.jpeg" class="img-fluid"></p>
<p><img src="https://new.joeystanley.com/blog/ads-and-lsa-2019/ads2019_beg_raising_histogram.jpeg" class="img-fluid"></p>
<p>And then I showed some plots. The first was <sc>bag</sc>-raising which was reported in pretty much all the areas we expected.</p>
<p><img src="https://new.joeystanley.com/blog/ads-and-lsa-2019/ads2019_bag_raising_map.jpg" class="img-fluid"></p>
<p>But then other map showed that <sc>bag</sc>-raising was pretty much everywhere except for the South. It’s particularly widespread in the West and the Midlands.</p>
<p><img src="https://new.joeystanley.com/blog/ads-and-lsa-2019/ads2019_beg_raising_map.jpg" class="img-fluid"></p>
<p>When you combine the two maps, you can start to see where one occurs without the other. Here, green areas are those that have <sc>bag</sc>-raising but not <sc>beg</sc>-raising and purple are areas with <sc>beg</sc>-raising without <sc>bag</sc>-raising.</p>
<p><img src="https://new.joeystanley.com/blog/ads-and-lsa-2019/ads2019_combined_map.jpg" class="img-fluid"></p>
<p>Basically, the purpose of this study was to see whether there were differences between <sc>beg</sc>-raising and <sc>bag</sc>-raising in where they occur regionally. I think these maps show that there are differences. Now I just want to go confirm these patterns with phonetic data!</p>
<p><br> <br></p>
</section>
<section id="sundays-ads-presentation-on-the-perception-of-southern-american-english" class="level2">
<h2 class="anchored" data-anchor-id="sundays-ads-presentation-on-the-perception-of-southern-american-english">Sunday’s ADS presentation on the perception of Southern American English</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the slides <a href="../../downloads/190106-ads2019_dass.pdf">here</a>!</p>
</div>
</div>
<p>My last presentation of the weekend was Sunday morning at the American Dialect Society. On behalf of my coauthors, Rachel Olsen, Mike Olsen, Lisa Lipani, and Peggy Renwick, I talked about our research on comparing acoustic data with fieldworker transcriptions in the Digital Archive of Southern Speech. Doing this kind of comparison is not new, and people have found that Linguistic Atlas transcriptions are not really that reliable, but we wanted to look into this for ourselves in our newly transcribed corpus.</p>
<p>So for now, we’re just focusing on the canonical diphthongs /aɪ, aʊ, ɔɪ/ because they’re quite a bit more monophthongal in the South than in other areas. To measure this acoustically, we used trajectory length. For the perception, we looked at the original fieldworker’s protocols they created for each informant, which has example tokens and how those people would pronounce them (in IPA).</p>
<p>Basically there was no correlation between how monophthongal the vowels were acoustically and how they were transcribed. Besides that, in transcriptions the vowels were more monophthongal when they were before /r/, but acoustically they were more monophthongal before /l/ and among European Americans. The two give different results.</p>
<p>So we’ve at least found that whatever the fieldworkers heard when they transcribed these words, it wasn’t trajectory length. Perhaps in the future we can explore some other acoustic measures and see if they correlate better with the transcriptions.</p>
<p><br></p>


</section>

 ]]></description>
  <guid>https://new.joeystanley.com/blog/ads-and-lsa-2019/index.html</guid>
  <pubDate>Thu, 03 Jan 2019 11:20:00 GMT</pubDate>
</item>
<item>
  <title>NWAV47</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/nwav47/index.html</link>
  <description><![CDATA[ 




<p>Today, I gave a poster presentation on prevelar raising. As it turns out, despite <sc>beg</sc> and <sc>bag</sc> being relatively small lexical classes, I found phonological, morphological, and lexical effects on the degree of raising, and that the two vowel classes reacted to these influences differently.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the poster <a href="../../downloads/181019-nwav47.pdf">here</a>!</p>
</div>
</div>
<img src="https://new.joeystanley.com/blog/nwav47/nwav47_in_action.jpg" class="img-fluid">
<center>
(Photo by <a href="https://www.research.manchester.ac.uk/portal/maciej.baranowski.html">Maciej Baranowski</a>.) <br><br>
</center>
<p>So for some speakers, mostly in the Pacific Northwest, the Upper Midwest, and Canada, they raise the <sc>dress</sc> (<sc>beg</sc>) and <sc>trap</sc> (<sc>bag</sc>) vowels before voiced velars. The idea for this research mostly came about because I was trying to figure out whether I have <sc>beg</sc>-raising. Turns out, I do, except for in a few words like <em>integrity</em>, <em>negligible</em>, and <em>segregate</em> which seems somewhat unexpected.</p>
<p>So I set up a categorization task that I distributed via Reddit. I showed people a bunch of <sc>beg</sc> and <sc>bag</sc> words and asked them whether the vowel in their pronunciation is most like that in <em>bake</em>, <em>deck</em>, or <em>back</em>. Almost 7,000 people took the survey and I got over 500,000 observations.</p>
<p>As it turns out, there were clear phonological effects for <sc>beg</sc>. If the /ɡ/ was followed by a sonorant (as in <em>regular</em> or <em>segment</em>), particular a liquid (<em>negligible</em> or <em>segregate</em>), it was indicated to be raised less. Adding suffixes had different effects: people indicated a raised vowel more when <em>-ed</em> was added to verbs, but only for <sc>bag</sc>. There were lexical effects too: for <sc>bag</sc>, more frequent words were raised more, borrowings with <sc>beg</sc> were raised more, and <sc>beg</sc> words with orthographic &lt;ex&gt; (like <em>exile</em> or <em>exit</em>) were virtually never raised.</p>
<p>The takeaway is that even though these are relatively marginal lexical classes, they still have interesting language-internal factors. And, even though <sc>beg</sc>- and <sc>bag</sc>-raising are generally considered to be the same phenomenon of prevelar raising, the two are different. There are some major limitations here, particularly because reported speech is never completely reliable, so I’d like to follow up with an acoustic study. But, I think it’s important to include many more words in a word list because some of these patterns would not have been found if a just small one was used.</p>



 ]]></description>
  <category>Conferences</category>
  <category>Pacific Northwest</category>
  <category>Presentations</category>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/nwav47/index.html</guid>
  <pubDate>Thu, 18 Oct 2018 12:00:00 GMT</pubDate>
</item>
<item>
  <title>LCUGA5</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/lcuga5/index.html</link>
  <description><![CDATA[ 




<p>Today, I was fortunate to give two presentations on very different areas of my research at the <a href="http://www.linguistics.uga.edu/lcuga-5">5th Annual Linguistics Conference at UGA</a>, one on an obscure consonantal phonological pattern in the West using new recordings and another on well-studied vowel shifts in the South using very old recordings.</p>
<section id="thr-flapping" class="level2">
<h2 class="anchored" data-anchor-id="thr-flapping">(thr)-flapping</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the slides <a href="../../downloads/181012-lcuga5_thr.pdf">here</a>!</p>
</div>
</div>
<p>First, I presented on something I’ve noticed in a few speakers, something I call (thr)-flapping. Some people pronounce the /ɹ/ after /θ/ as a flap [ɾ]. In this presentation, I presented some data supporting this hunch.</p>
<p>The articulatory motivations are clear: as the tongue tip moves from between the teeth to a retroflexed position, it may make brief contact with the alveolar ridge. What may have started as an accidental gesture appears to have been phonologized by some speakers.</p>
<p>Looking at phonological factors, in my data (thr)-flapping happened more when the following vowel was non-high and non-front. So it happened more in <em>throb</em>, <em>throng</em>, <em>throne</em>, <em>thrust</em>, and <em>thrive</em> than in <em>thrash</em>, <em>threaten</em>, <em>thread</em>, <em>thrill</em>, <em>three</em> and <em>through</em>. I offer some tentative explanations for this, but without articulatory data, I can’t know for sure.</p>
<p>To my surprise, the social factors I looked at were the opposite of what I expected. Age and sex were not significant, meaning there’s probably not a lot of change in time. But what state people came from (Washington verses Utah—my two fieldsites) <em>was</em> significant. <a href="../../downloads/180108-ads2018-utah.pdf">Utah English has a lot of hyperarticulated consonants</a> and at ADS this year, Di Paolo &amp; Johnson (2018) hypothesize that this has to do with the high proportion of members of the Church of Jesus Christ of Latter-day Saints in Utah. Since public speaking is a common part of their worship services (even as early as the age of 3!), elements of this hyperarticulated register may have spread into other speech styles. (thr)-flapping may be just another manifestation of that. But without attitude or perceptual data, I can’t know for sure.</p>
<p>(thr)-flapping is something I’ve noticed for a while now, and despite the shortcomings of this study, and I was glad to finally present solid evidence that it is a thing!</p>
</section>
<section id="vowel-shifts-in-southern-american-english" class="level2">
<h2 class="anchored" data-anchor-id="vowel-shifts-in-southern-american-english">Vowel Shifts in Southern American English</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the slides <a href="../../downloads/181012-lcuga5_dass.pdf">here</a>!</p>
</div>
</div>
<p>In the next session, I presented research I been doing with Peggy Renwick on the vowel shifts in the South. The Southern Vowel Shift has front lax vowels raising and front tense vowels lowering, resulting in vowel pairs swapping. Meanwhile, back vowels are fronting. The African American Vowel Shift has the front lax vowels raising, but tense vowels are not lowering, so there wouldn’t be any swapping. Furthermore, back vowels typically aren’t fronted.</p>
<p>We use the Digital Archive of Southern Speech, a corpus of interviews from the 1970s and 1980s. The people in these recordings were born while these shifts were going on, so we can see their development in a way that newer recordings wouldn’t be able to do.</p>
<p>Using Pillai scores and linear mixed-effects models, we find that younger, European American women are leading in the front vowel swapping and that African Americans are participating less in the back vowel fronting. These findings are exactly what we expect, showing that these older recordings confirm what newer ones suggest.</p>


</section>

 ]]></description>
  <category>Conferences</category>
  <category>Pacific Northwest</category>
  <category>Phonetics</category>
  <category>Presentations</category>
  <category>Research</category>
  <category>South</category>
  <category>Utah</category>
  <guid>https://new.joeystanley.com/blog/lcuga5/index.html</guid>
  <pubDate>Fri, 12 Oct 2018 12:00:00 GMT</pubDate>
</item>
<item>
  <title>Transcribing a Sociolinguistic Corpus</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/transcribing-a-sociolinguistic-corpus/index.html</link>
  <description><![CDATA[ 




<p>In the summer of 2016, I went to Cowlitz County, Washington to do traditional sociolinguistic interviews. I talked to 54 people and gathered my first audio corpus. It took a lot of preparation beforehand and it took a lot of time in the field. What I could not have expected was the amount of time it would take to transcribe that corpus. Now, two years later, I have finally finished transcriptions.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
I'm DONE! 54 speakers, 46½ hours of audio, 172 hours of work (averaging about 1h50m over like 92 days), producing an audio corpus of 327,000 words! <a href="https://twitter.com/hashtag/amNOLONGERtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amNOLONGERtranscribing</a>
</p>
— Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017096903878639621?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This come after a lot of work. Since others might be going through the same thing, I thought I’d share some thoughts on transcribing a sociolinguistic corpus.</p>
<section id="finding-the-motivation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="finding-the-motivation">Finding the motivation</h2>
<div class="page-columns page-full"><p>I think my original goal was to have it all transcribed by the end of 2016. So I gave myself about five months. But then I did the first hour of audio and it took me about 5 hours. Yikes!  At that rate, I estimated it would take about 200 hours of work to finish. I think staring down the barrel of any 200 hour task is a motivation killer. So I put it off.</p><div class="no-row-height column-margin column-container"><span class="">I don’t know what I expected—of course it’s going to take a long time to transcribe!</span></div></div>
<p>I wrote a <a href="../../blog/lots-of-transcribing">blog post</a> nine months later when I had my first wake up call that I needed to get transcriptions done. I talked about some of the struggles I had getting started but mostly made excuses for why I hadn’t done much. And I got a lot of work done over the next month or so and made it about a quarter of the way through. I remember though just getting burned out after just 10 or 15 minutes of work and would call it a day after half an hour. At that rate, yeah, it’ll take forever.</p>
<p>So I put it off for an entire year. In the meantime I was getting a lot done—mostly to distract me from the task I inevitably have to do before graduating. For some reason this distraction was in the form of collecting more audio. I got some <a href="../../blog/laboratory-research">laboratory audio</a>, and gathered another corpus using <a href="../../blog/using-mturk">Amazon Mechanical Turk</a>, and in January I went out to Utah to do some more fieldwork. And yet, this audio from 2016 was collecting dust on my computer, just waiting to be analyzed. I think I found that it was easier to collect new data than it was to finish processing the old stuff. Consequently, I had collected something like 150 hours of audio over two years for various projects—and less than 5% of it was processed.</p>
<p>When I finally defended my prospectus in April, it occurred to me that if I wanted to graduate in 2019, I needed to have data to write about. And the only way to do that was to transcribe that darn audio. So, that was what finally got me to crack down and work at this every day. Even then, it took three months of grinding to finally finish. But I’m so glad it’s done.</p>
</section>
<section id="a-rite-of-passage" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-rite-of-passage">A rite of passage</h2>
<p>I mentioned as a part of my celebratory tweetstorm that doing this kind of work might be something like a rite of passage for sociolinguists.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
I think putting this much work into a corpus is some sort of rite of passage for sociolinguists. I'm glad I went through it, but ugh, never again.
</p>
— Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017096905564835842?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<div class="page-columns page-full"><p>It seems like a lot of sociolinguists do research on their own corpora, and while the flashy part of statistical analysis, data visualization, or even fieldwork stories are what you see and hear about the most, a significant portion of what we do is the behind-the-scenes tedium on the computer. My university doesn’t have a huge group of sociolinguists and there’s no sort of shared corpus that we can use. So if I want to study contemporary spoken English, I was going to have to collect the audio myself. I think would have done fieldwork myself anyway though. I think it was always something I’ve wanted to do. Plus, there’s this:</p><div class="no-row-height column-margin column-container"><span class="">Yes, the Linguistic Atlas Project has been here since the 80s, but very few of those recordings are transcribed, so they’re of little use in their current state.</span></div></div>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
Also, shout out to <a href="https://twitter.com/DrDialect?ref_src=twsrc%5Etfw"><span class="citation" data-cites="DrDialect">@DrDialect</span></a>, who I heard say at a Q&amp;A at SECOL in 2015 something like, "the best career move you can do is to create a corpus: you'll be able to analyze it forever." Some of the best advice I've ever heard.
</p>
— Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017096909041885185?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>And from the looks of it, this corpus that I now have is definitely going to last me a while, that’s for sure!</p>
</section>
<section id="what-software-did-i-use" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="what-software-did-i-use">What software did I use?</h2>
<p>For transcription, I think there are two ways of doing it. The first method is to find some software that will automatically transcribe it for you, and since it’s not going to be perfect, then spend the time to correct that transcription. I considered doing that, specifically using the transcriber in <a href="http://darla.dartmouth.edu">DARLA</a>. But I found that it took much longer to correct the transcriptions that it would have taken me to just do it by hand. However, DARLA specifically says on their website that their automatic transcriber is not great, so my rate might have been better if I had used a different transcriber. DARLA was what came to mind because it’s easy to use and free. You might have better luck if you use a more sophisticated transcriber.</p>
<p>The other option therefore was to just do it myself. As far as I can tell, there are two or three main pieces of software you can use. One is <a href="http://trans.sourceforge.net/en/presentation.php">Transcriber</a>. This is one that we use in the Linguistic Atlas Office when we have our undergrads do transcriptions. It’s free and easy to use. One concern is that it’s a little bit tricky to get its output to a TextGrid format. The other concern was that I couldn’t see the spectrogram to accurately place boundaries. Another option is <a href="http://www.mpi.nl/corpus/html/elan/index.html">ELAN</a>, which I hear is fantastic. The only reason I didn’t use it was frankly because I didn’t want to take the time to learn a new program.</p>
<p>What I settled on was just plain ol’ Praat. It’s software that I’m comfortable with and I’ve used a lot. I can zoom in as close as I want so I can easy skip over stutters or other noise. Plus, I create a TextGrid right there, which is the format I’m most comfortable working with for scripting purposes. The downside to Praat is that I ended up having to use my trackpad on my laptop more than I wanted to (for scrolling side to side and placing boundaries). I wanted to avoid using my mouse as much as possible because I feel like it hurts my wrist more and I don’t want carpel tunnel.</p>
<div class="page-columns page-full"><p>Based on my own experience, what I would recommend <em>not</em> doing is hiring out the transcriptions unless you’re not able to do it yourself. For one, I’m cheap, and didn’t want to pay however much per minute of audio. But more importantly, going through my audio a second time gave me a chance to pick up on things that I didn’t catch or forgot about when I was doing the interviews in person. Things like interesting linguistic phenomena  or passages I may want to quote later. Using the Praat textgrids, I just added a separate tier for my own annotations and could make whatever notes I wanted to about a particular section of audio. I learned so much about my people going through it a second time, and I don’t think I would have gotten those intuitions about their speech if I had hired it out. Of course, if you need the transcriptions sooner than you can process them or if you’re not able to do the work yourself, then of course hiring it out might be the better option.</p><div class="no-row-height column-margin column-container"><span class="">I got a token of <em>liketa</em> and two people said <em>I and John</em> instead of <em>John and I</em> which was super cool. I don’t remember those specifically and would never have caught them if I didn’t do the transcriptions myself.</span></div></div>
</section>
<section id="the-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="the-next-steps">The next steps</h2>
<p>So while finishing those transcriptions was a monster step, unfortunately the work wasn’t done.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
Now I've just got to do forced alignment (which includes spell checking) and extract some formants and I'll be ready to go!
</p>
— Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017096904885366784?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<section id="forced-alignment" class="level3">
<h3 class="anchored" data-anchor-id="forced-alignment">Forced alignment</h3>
<p>I’ve been using DARLA for the past few years, but I had some trouble getting the long audio files to process using their web interface. So this gave me a great opportunity to download and install the <a href="https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner">Montreal Forced Aligner</a> on my own computer. Having this in-house provides lots of benefits like processing the files in bulk and quicker turnaround time since I don’t have to upload the files.</p>
<p>The bad news was that I had to do the spell-checking myself. I completely took for granted that DARLA can handle out-of-dictionary words by guessing their pronunciation. So since the Montreal Forced Aligner doesn’t do that, I had to check the words myself. When you run it, it’ll produce a list of out-of-dictionary words for you, so all you need to do is add them to the dictionary or correct the spelling in Praat. It seems simple, but it takes a long time. I had at least 20 or 30 typos or new words in every interview, so I probably spent 15 or 20 hours just doing the spell-checking (I think I added over 1000 new dictionary entries too!).</p>
<p>Luckily, all this was made easier with the help of some custom Praat scripts I wrote for this project. One does pre-processing to get the files ready for forced-alignment. It splits the audio and textgrid into two halves (it was easier to process that way), it moves these files into a specific directory, and renames the tiers so that they’re consistent. As a bonus, it spits out the command that I need to use to run the aligner on those specific files, so all I need to do is copy and paste that into my terminal and it’ll go on its merry way. This was super helpful because typing path names over and over got old real quick.</p>
<p>Once the spell-checking was done and the files were aligned, I had a post-processing script that I used. This one rejoins the two halves into one TextGrid again, adds the new phoneme and word tier to the top of the main TextGrid (so I’ve got the phoneme, word, sentence, and other tiers all in one file), and saves this in that speaker’s directory on my hard drive. Super handy.</p>
<p>Now ideally, I would go back and hand-check all the boundaries. Maybe one day I’ll have the time to do that, but oh my goodness that’s not going to happen any time soon.</p>
</section>
<section id="formant-extraction" class="level3">
<h3 class="anchored" data-anchor-id="formant-extraction">Formant extraction</h3>
<p>So keep in mind that all this work, the nearly 200 hours I’ve put into transcribing and force aligning, was mostly just so I could have Praat know where the vowels were in the audio.</p>
<p>So, I modified a couple scripts I wrote to do formant extraction. Of course, I’ve mostly worked with shorter passages of audio (word lists and reading passages and stuff), so what I didn’t anticipate was that Praat kind of has trouble working with audio longer than about 30 minutes. So I had to modify the script so that it splits the audio into roughly five minute chunks, processes each one individually, and then stitches all the output back together.</p>
<p>And of course, the formant measurements ideally should be handchecked. But again, I just spent way too much time transcribing, so I’m not about to spend even more time hand-checking these. Not yet at least.</p>
</section>
</section>
<section id="the-end-result-a-giant-spreadsheet" class="level2">
<h2 class="anchored" data-anchor-id="the-end-result-a-giant-spreadsheet">The end result: A giant spreadsheet!</h2>
<p>So what were the main steps here?</p>
<ol type="1">
<li><p>Collect audio.</p></li>
<li><p>Transcription.</p></li>
<li><p>Forced alignment.</p></li>
<li><p>Formant extraction.</p></li>
</ol>
<p>What do I have now? A giant spreadsheet. All this work has been so that I can get a big ol’ spreadsheet that I can then analyze in R. That’s where I am right now. I’ve got the finalized dataset that I’ll use for my dissertation, so I don’t even need to open up Praat much anymore, or even plug in my external hard drive. Almost all my work is in R now. But it is quite satisfying to have this monster spreadsheet of my own data.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Transcribing (and the subsequent processing of) a sociolinguistic corpus takes a ton of time, patience, diligence, and determination. My eyesight may have suffered a little bit from staring at the computer, my headphones are a little worn down, my keyboard has had to endure well over a million keystrokes, and my wrists and fingers sure took a hit. But, y’know what? It’s a <em>lot</em> better than it used to be. At least we have tools like forced-alignment, FAVE, and Praat to make our lives easier. But in the end, it is really awesome to have completed this corpus.</p>


</section>

 ]]></description>
  <category>Dissertation</category>
  <category>Methods</category>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/transcribing-a-sociolinguistic-corpus/index.html</guid>
  <pubDate>Mon, 06 Aug 2018 19:10:00 GMT</pubDate>
</item>
<item>
  <title>ADS2018</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/ads2018/index.html</link>
  <description><![CDATA[ 




<p>Thanks for attending my presentations. At the 2018 annual meeting of the American Dialect Society in Salt Lake City, Utah, I was fortunate to present on two aspects of my research.</p>
<section id="thursdays-presentation-on-the-gsv" class="level2">
<h2 class="anchored" data-anchor-id="thursdays-presentation-on-the-gsv">Thursday’s presentation on the “GSV”</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the slideshow <a href="../../downloads/180105-ads2018-gsv.pdf">here</a>!</p>
</div>
</div>
<p>Thursday, I represented Peggy Renwick, Bill Kretzschmar, Rachel Olsen, and Mike Olsen and introduced a website called <a href="http://lap3.libs.uga.edu/u/jstanley/vowelcharts/"><em>The Gazetteer of Southern Vowels</em></a>. This is a online tool that makes it easy to visualize linguistic atlas data (specifically, the Digital Archive of Southern Speech, or <a href="https://www.lap.uga.edu/Site/DASS.html">DASS</a>) that is currently being processed at the University of Georgia. The site has several features:</p>
<ol type="1">
<li><p>Side-by-side plots make it easy to compare two subsets of our data, whether it be by demographic factors, language-internal factors, or methodological differences.</p></li>
<li><p>A “point-pattern analysis” page shows an underlaid grid on the plot as an alternative way of visualizing the vowel space.</p></li>
<li><p>At the top of each page are options to subset the data however you like. Speakers can be selected by typical demographic factors. You can filter out stop words or examine specific words. You can subset by vowel, stress, and following consonant. Different transcription systems, filtering algorithms, and normalization procedures are available.</p></li>
<li><p>The plots themselves are highly customizable. Users can display any combination of points, ellipses, averages, and words. For each of these, the size and opacity can be controlled. This makes it easy to visualize the same data in lots of different ways.</p></li>
</ol>
<p>We hope that you enjoy the <em>Gazetteer of Southern Vowels</em> and find it useful for visualizing linguistic atlas data.</p>
</section>
<section id="sundayss-presentation-on-consonants-in-utah" class="level2">
<h2 class="anchored" data-anchor-id="sundayss-presentation-on-consonants-in-utah">Sundays’s presentation on consonants in Utah</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the slideshow <a href="../../downloads/180108-ads2018-utah.pdf">here</a>!</p>
</div>
</div>
<p>On Sunday afternoon, Kyle Vanderniet and I presented on consonantal variation in Utah English. We looked at three variables:</p>
<ol type="1">
<li><p>We found that words like <em>mountain</em>, <em>cotton</em>, and <em>Latin</em> have three pronunciations in Utah. The most common is what most other North American Engilsh speakers say: <em>moun</em>[ʔn̩]. Some women in our sample frequently used a second form, <em>moun</em>[ʔɨn], which has become almost stereotypical in Utah and has a lot of stigma. Finally, a third form, <em>moun</em>[tʰɨn], appears to be a hyperarticulated form in response to the stigma associated with the glottal stop. This was relatively frequent in our sample: about 25% of tokens had it, which is much more than similar audio from other states. Men tended to use this more, especially younger men.</p></li>
<li><p>Then we looked at [t]-epenthesis in words like <em>false</em>, <em>also</em>, and <em>else</em> and found that while this isn’t particularly common overall, some women had it a fair amount in their speech.</p></li>
<li><p>Finally, we looked at [k]-epenthesis after velar nasals. Despite being very frequent in other Utah English studies (like Di Paolo and Johnson’s study just before ours), this was rarely attested in our sample, so we have to figure out why.</p></li>
</ol>
<p>Overall, we feel that consonants in Utah deserve further study because of the high amount of variation.</p>


</section>

 ]]></description>
  <category>Conferences</category>
  <category>Dissertation</category>
  <category>Linguistic Atlas</category>
  <category>Phonetics</category>
  <category>Presentations</category>
  <category>Research</category>
  <category>Utah</category>
  <guid>https://new.joeystanley.com/blog/ads2018/index.html</guid>
  <pubDate>Sun, 07 Jan 2018 20:00:00 GMT</pubDate>
</item>
<item>
  <title>NWAV46</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/nwav46/index.html</link>
  <description><![CDATA[ 




<p>At the 46th New Way of Analyzing Variation conference in Madison, Wisconsin, I presented a poster called <em>Changes in the Timber Industry as a Catalyst for Linguistic Change</em>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the poster <a href="../../downloads/171103-nwav46.pdf">here</a>!</p>
</div>
</div>
<p>In a nutshell, I found a couple interesting things going on in a small town in Washington:</p>
<p><em>Linguistic changes</em>—I focused on two variables: <sc>bag</sc>-raising and <sc>goat</sc>-diphthongization. Specifically, older people raised <span class="smallcaps">bag</span> and younger people had a more diphthongal <span class="smallcaps">goat</span>. The generational divide was around 1970 and the difference between older and younger people was sudden.</p>
<p><em>Census data</em>—Based on topics of conversation in the interviews, I took a look at census data and found some correlation with these linguistic changes. The mills laid a bunch of people off, people were earning less money, a bunch of people left town, and more of those who stayed worked outside the community. Correlation does not equal causation, but it’s paints a pretty compelling picture.</p>
<p><em>Catastrophic events</em>—One of the interesting findings was using regression with breakpoint as a way to model catastrophic change. I’m refining this methodology right now, but it does seem to work for modeling language change in time.</p>
<p><img src="https://new.joeystanley.com/blog/nwav46/nwav46_in_action.jpg" class="img-fluid"></p>
<center>
(Photo credits I believe go to <a href="https://www.research.manchester.ac.uk/portal/maciej.baranowski.html">Maciej Baranowski</a>)
</center>



 ]]></description>
  <category>Conferences</category>
  <category>Dissertation</category>
  <category>Pacific Northwest</category>
  <category>Presentations</category>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/nwav46/index.html</guid>
  <pubDate>Sat, 04 Nov 2017 00:11:00 GMT</pubDate>
</item>
<item>
  <title>LCUGA4</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/lcuga4/index.html</link>
  <description><![CDATA[ 




<p>This weekend, I had the opportunity to present twice at the <a href="http://www.linguistics.uga.edu/lcuga-4">4th Annual Linguistics Conference at UGA</a>. One was planned and the other was a last-minute fill-in for someone who couldn’t make it. I was happy to do both.</p>
<p>Friday’s presentation was called <em>The linguistic effects of a changing timber industry: Language change in Cowlitz County, WA</em>. Here, I talk about some of the sudden linguistic changes that I found in apparent time and suggest that they had to do with changes in the timber industry around that time. Because this was last-minute, it is basically a precursor to (and a slideshow version of) my <a href="../../blog/nwav46">NWAV46</a> poster that I’ll be giving in a few weeks. You can download the slides for this talk <a href="../../downloads/171006-LCUGA4-WA-slides.pdf">here</a></p>
<p>Saturday’s presentation with <a href="https://clyguy.wixsite.com/profile">Kyle Vanderniet</a> was called <em>Consonantal variation in Utah English: What el[t]se is happening[k]?</em>. We talked about three variables that seem to be particularly salient in Utah English</p>
<ol type="1">
<li><p>The various pronunciation of words like <em>mountain</em>, <em>button</em>, or <em>satin</em> with the last syllables as [ʔn̩], [ʔɨn], or [tʰɨn].</p></li>
<li><p>Insertion of [t] between /ls/ clusters, as in <em>fal</em>[t]<em>se</em> or <em>el</em>[t]<em>se</em>.</p></li>
<li><p>Realizing word-final <em>ing</em> as [ɪŋk].</p></li>
</ol>
<p>You can see our slides for this presentation <a href="../../downloads/171007-LCUGA4-UT-slides.pdf" target="_blank" title="download LCUGA4 (UT)">here</a>. Edit: we later presented additional findings from this research at <a href="../../blog/ads2018">ADS2018</a>.]</p>



 ]]></description>
  <category>Conferences</category>
  <category>Pacific Northwest</category>
  <category>Presentations</category>
  <category>Research</category>
  <category>Utah</category>
  <guid>https://new.joeystanley.com/blog/lcuga4/index.html</guid>
  <pubDate>Sat, 07 Oct 2017 15:11:00 GMT</pubDate>
</item>
<item>
  <title>/ɑr/-Raising</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/ar-raising/index.html</link>
  <description><![CDATA[ 




<div class="page-columns page-full"><p>I’ve noticed for a while in my own speech that the vowel in <em>star</em> is higher and longer than <em>start</em>. I have American Raising, which, simplifying a bit, is where /aɪ/ is raised before voiceless consonants. So I just expected this to be another manifestation of that. I had some time so I thought I’d test this empirically. So here’s a breif study on my own speech to figure out what’s going on.</p><div class="no-row-height column-margin column-container"><span class="">See <a href="https://read.dukeupress.edu/pads/issue/106/1">Davis &amp; Berkson 2021</a> for more detail on American Raising. I particularly liked <a href="https://doi.org/10.1215/00031283-9551267">Moreton’s chapter</a> because it shows how stress, syllable structure, and morphologica structure all matter.</span></div></div>
<section id="hypothesis" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis">Hypothesis</h2>
<p>/ɑr/ raises before voiceless segments.</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>In COCA’s list of the most frequent 5000 words, there are 88 that have the /ɑr/ sequence in stressed position. Here they are in order of frequency.</p>
<p><em>large, car, art, party, heart, article, artist, argue, hard, card, bar, yard, garden, partner, sorry, argument, department, apartment, farm, tomorrow, start, army, farmer, largely, hardly, smart, sharp, regarding, dark, far, mark, marketing, parking, darkness, armed, chart, remarkable, market, garlic, partly, partnership, regardless, target, carbon, borrow, margin, architect, park, architecture, march, harsh, particle, guitar, guard, alarm, starter, carve, starting, departure, sharply, hardware, garbage, cart, barn, carpet, star, jar, shark, charter, charge, partially, harm, artifact, partial, marketplace, part, harmony, remark, regard, arm, apart, marble, charm, marker, spark, harvest, depart, cargo</em></p>
<p>The words were randomized and I recorded myself reading them using Praat in a quiet place with a good mic. I segmented out the /ar/ sequence by hand (incidentally, I sounded like a seal: “ar ar ar ar”). Using a Praat script, I extracted formants at 30% of the way into the /ar/ sequence (what seemed to be about the midpoint of the vowel). Filtered out a few bad measurements. Analyzed in R. For this analysis, word-final /ar/ is being treated as pre-voiced.</p>
</section>
<section id="results" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>I’ll split my analysis up into monosyllabic words and polysyllabic words because the results were cleaner.</p>
<section id="monosyllabic-words" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="monosyllabic-words">Monosyllabic words</h3>
<p>The effect that place of articulation had on duration was clear. A simple boxplot shows the stark contrast.</p>
<p><img src="https://new.joeystanley.com/blog/ar-raising/ar_duration.png" class="img-fluid" style="width:80.0%"></p>
<p>With the exception of <em>arm</em> being short and <em>mark</em> being long, there’s a clear difference. In fact, the longest ones were mostly the word-final /ɑr/ words, but for this analysis I’ll group word-final and pre-voiced into one category.</p>
<p>Moving on to vowel quality, it’s clear that the voicing of the following segment has an effect. In fact, there’s virtually no overlap between the two vowel classes.</p>
<p><img src="https://new.joeystanley.com/blog/ar-raising/ar_plot.png" class="img-fluid" style="width:80.0%"></p>
<p>I ran a MANOVA test that took into account F1, F2, and duration, and the results show that the two groups are distinct.</p>
<pre><code>summary(manova(cbind(F1, F2, dur) ~ fol_voice, data=ar))

          Df  Pillai approx F num Df den Df    Pr(&gt;F)    
fol_voice  1 0.51552   28.375      3     80 1.348e-12 ***
Residuals 82                                                                                        </code></pre>
<div class="page-columns page-full"><p> The <em>p</em>-value is small, which indicates the two are different, but I’ve noticed they tend to be on vowel data even for what seems like merged classes. More importantly, the Pillai score is pretty high: on a scale from 0 (=complete overlap) to 1 (completely separate), it’s about in the middle.</p><div class="no-row-height column-margin column-container"><span class="">Using the formula that me and Betsy Sneller came up with in our 2022 paper, we’d expect a Pillai score less than 0.0618 with this much data if the two classes were underlyingly merged. The fact that it’s so high with a sample size of 88 is pretty indicative of separation. In case the visual wasn’t clear enough.</span></div></div>
<p>Out of curiosity, I did a k-means clustering analysis. I made it blind to phonetic environment so it finds the best two clusters based on F1 and F2 alone. As expected, the clusters essentially captured the voicing distinction, but there were a couple exceptions: <em>march</em> is voiced but clustered with the voiceless words, and both <em>yard</em> and <em>guard</em> are voiced but clustered with the voiceless words.</p>
<p>Conclusion so far: with the exception of a few words, it’s pretty clear that /ɑr/ is raised, fronted, and shorter before voiceless sounds.</p>
</section>
<section id="polysyllabic-words" class="level3">
<h3 class="anchored" data-anchor-id="polysyllabic-words">Polysyllabic words</h3>
<p>When there’s more than one syllable, things get complicated because of their distribution within the word. Most of the two-syllable words have word-initial stress, but there are a few misfits (<em>alarm</em>, <em>depart</em>, <em>guitar</em>, <em>regard</em>, <em>remark</em>). With three syllables stress is either on the first syllable (<em>architect</em>, <em>argument</em>, <em>article</em>, <em>artifact</em>, <em>harmony</em>, <em>marketing</em>, <em>marketplace</em>, <em>partially</em>, <em>particle</em>, <em>partnership</em>) or the second (<em>apartment</em>, <em>department</em>, <em>departure</em>, <em>regarding</em>, <em>regardless</em>, <em>tomorrow</em>). There were two four-syllable words, with the /ɑr/ segment on different syllables (<em>architecture</em>, <em>remarkable</em>). Most of these can be split up by the voicing of the following segment too, which spreads the data pretty thin.</p>
<p><img src="https://new.joeystanley.com/blog/ar-raising/ar_poly_plot.png" class="img-fluid" style="width:80.0%"></p>
<p>Generally, the the raised variant occurs with voiceless segments, but this data is a little bit messier. What’s interesting is to look at the exceptions.</p>
<p>There were several pre-voiced segments that appeared to be raised: <em>sorry</em>, <em>regardless</em>, <em>target</em> (that one’s hard to see), <em>regarding</em> and maybe <em>argue</em>, <em>argument</em>, <em>garden</em>, and <em>regard</em>. I can’t help but notice that a lot of those words have the sequence /gɑr/. In fact, the only other /gɑr/ words were <em>garlic</em> and <em>garbage</em>, which were the most fronted voiced words even though they weren’t raised.</p>
<p>It’s interesting that <em>guard</em> was one of the few exceptions on the k-means clustering analysis on the monosyllabic words and was raised and fronted compared to the others. A part of me wants to think that /ɑr/ is raised after /g/, which would be cool.</p>
<p>It’s also worth noting that most of the voiceless words that were outside the ellipse were after bilabials: <em>depart</em>, <em>market</em>, <em>remark</em>, <em>remarkable</em>, etc. These are actually near the voiced segments after bilabials like <em>borrow</em> and <em>tomorrow</em>. This even explains why <em>march</em> was one of the exceptions detected by the k-means clustering analysis. Perhaps bilabials have something to do with backing.</p>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>There is a phonetic explanation for the behavior of both the velars and the bilabials. Let’s talk about transition formants for a second. To start, the low vowel /ɑ/ has a high F1 and a low F2. Velars cause F1 to lower and F2 to raise (the latter as a part of the velar pinch), which is more like a higher, fronter vowel. Meanwhile, bilabials cause all formants to lower (because of the slight lengthening of the vocal tract), meaning raising and backing.</p>
<p>What’s happening is that some of these transition formants are raising the nucleus of the /gɑr/ sequence. The reason why we don’t see raising in /kɑr/ sequences such as <em>car</em>, <em>carbon</em>, <em>cargo</em>, <em>carve</em>, etc. is because of the extra padding of the aspiration. Transition formants do appear in aspiration and the fact that this VOT increases the time between the velar stop and the vowel gives my articulators time to transition to a full /ɑ/.</p>
<p>I don’t know of a good formal way to test this right now, but we can look at measurements at other points along the vowels’ durations and see if they’re predicted by the place of articulation. Moving closer to the vowel onset, the /gɑr/ words are even higher and fronter. And in fact, at 10% into the /ɑr/ sequence, the distribution is more easily predicted by the previous place of articulation.</p>
<p><img src="https://new.joeystanley.com/blog/ar-raising/ar_10_percent.png" class="img-fluid" style="width:80.0%"></p>
<p>Here, we see that the /gɑr/ sequences (highlighted in green now) are all very high and very front, together with /jɑr/ and /ʃɑr/ which have similar transition formant patterns. Similarly, the /ɑr/ words after bilabials (now in yellow) are somewhat backer, though this isn’t as stark. In fact the difference in voicing of the following stop is much smaller, and might not even be significant at all anymore.</p>
<p>If we measure later in time, the influence of the /r/ and its following consonant is increasing and the distinctions between the vowels also decreases.</p>
<p><img src="https://new.joeystanley.com/blog/ar-raising/ar_50_70_percent.png" class="img-fluid" style="width:80.0%"></p>
<p>Here we see that at halfway through the /ɑr/ sequence, the /gɑr/ words are still among the higher and fronter within the blue cloud. But at 70%, they are pretty much randomly dispersed among the other pre-voiced segments. The bilabials make it clear that the following place of articulation predicts this vowel plot: words like <em>mark</em>, <em>park</em>, and <em>spark</em> are higher and fronter due to the velar pinch.</p>
<p>Going back to that 30% point, which is roughly the middle of the vowel itself, here it is again, but with the highlights.</p>
<p><img src="https://new.joeystanley.com/blog/ar-raising/ar_30_percent_dots.png" class="img-fluid" style="width:80.0%"></p>
<p>Here, the position of each word within the voiced/voiceless cloud is already being influenced by the surrounding consonants. Word that end in velars like <em>mark</em>, <em>park</em>, and <em>spark</em> are already fronting. Nearly all the /gɑr/ words are followed by an alveolar sound, which cause some raising and fronting but not as extreme as velars. So it makes sense that they would stay high but not as high as the pre-velars.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In my speech, I definitely have evidence to reject the null hypothesis and to conclude that /ar/ is raised before voiceless segments. I think that part is pretty clear.</p>
<p>I’ll admit, I came into writing this blog in hopes of showing that my /gɑr/ sequence is also raised. But after doing more tests, visualizations, and especially after taking into account the pattern of the bilabials, I just don’t have enough evidence to suggest it’s anything more than influence of surrounding consonants.</p>
<p>This makes me think hard about how to interpret vowel plots in the future. Every combination of surrounding consonants has its own trajectory, and it’s crazy that we can find patterns anywhere. This means ideally we should take into account more consonantal influences when interpreting vowel data. But at the same time that often spreads out data out way too thin since there will be only so a few tokens for any combination.</p>
<p>I’ve seen studies where they’ll put following place of articulation as a random effect in a mixed-effects model. In my opinion, that doesn’t seem methodologically sound because there’s a finite number of possible options, we generally know their effects, and any duplication study would mostly have the same places of articulation. Now, I’m reconsidering this, and I wonder if it might be a good idea to put a combination of previous and following consonant as a random factor (since they interact). While we probably know the effects of these interactions, any replication of the study might not have the same words and therefore might not have the exact same combinations of previous and following consonants. It’s quite common to go the extreme route and just put each word as a random effect since “every word has its own history.”</p>
<p>It’s also important to consider trajectory information more. As I showed above, just looking at one measurement can lead to erroneous claims, and we can get a more fuller picture by looking at the trajectory information. There are statistical methods out there that we can use for trajectories (SS-ANOVA, GAMMs, etc.) but I haven’t quite got the hang of them yet.</p>
<p>Well, I learned a few things by doing this quick study: surrounding consonants affect vowels and trajectories are important. Seems like old news, but it’s nice to have learned this for myself.</p>


</section>

 ]]></description>
  <category>Side Projects</category>
  <category>Phonetics</category>
  <category>Statistics</category>
  <guid>https://new.joeystanley.com/blog/ar-raising/index.html</guid>
  <pubDate>Wed, 27 Sep 2017 04:11:00 GMT</pubDate>
</item>
<item>
  <title>Testing VOT Durations in A Course in Phonetics</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/testing-vot-durations/index.html</link>
  <description><![CDATA[ 




<p>So I’m teaching phonetics and phonology this semester and we’re using Ladefoged &amp; Johnson’s <em>A Course in Phonetics</em> textbook. As I was preparing to teach about stops, I thought it might be a good idea as a homework assignment for students to gather their own data to see if some of these ideas panned out. Here’s my quick study.</p>
<section id="hypotheses" class="level2">
<h2 class="anchored" data-anchor-id="hypotheses">Hypotheses</h2>
<p>The four hypotheses I wanted to test come from Chapter 3 from my 6th edition of <em>A Course in Phonetics</em>:</p>
<ol type="1">
<li><p>Word-initially, /p, t, k/ have longer aspiration than /b, d, g/.</p></li>
<li><p>After onset /s/, /p, t, k/ have about as much aspiration as word-initial /b, d, g/.</p></li>
<li><p>Word-finally, voiced obstruents have an overall longer duration (closure + burst + aspiration) than voiceless obstruents.</p></li>
<li><p>Vowels preceding voiced obstruents are longer than those preceded by voiceless obstruents.</p></li>
</ol>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>Each student was asked to record a friend reading the following words: <em>tack</em>, <em>soap</em>, <em>days</em>, <em>pad</em>, <em>steep</em>, <em>sit</em>, <em>code</em>, <em>tab</em>, <em>bees</em>, <em>scope</em>, <em>dice</em>, <em>goes</em>, <em>bus</em>, <em>seep</em>, <em>cab</em>, <em>spit</em>, <em>gas</em>, <em>peg</em>. I chose these words to maximize onset and coda obstruents in as few words as possible. Vowel quality is assumed to have no effect.</p>
<p>For stops, student measured durations of aspiration and closure; for fricatives it was duration of the fricative itself. This was done in Praat. They have worked with Praat once before in this course, and were taught how to identify boundaries for these, but were otherwise relatively untrained linguistics undergraduates. I provided them with a template spreadsheet to fill out.</p>
<p>I ended up with measurements from 432 words: 18 unique words each from 24 students. I then combined the spreadsheets and wrote up the R code.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="word-initial-stop-aspiration" class="level3">
<h3 class="anchored" data-anchor-id="word-initial-stop-aspiration">Word-initial stop aspiration</h3>
<p>The first hypothesis is one that is commonly taught in intro to linguistics courses: word-initial voiceless stops have aspiration and word-initial voiced stops have little, if any. In my data, this turned out to be the case based on the words <em>pad</em>, <em>peg</em>, <em>tab</em>, <em>tack</em>, <em>cab</em>, <em>code</em>, <em>bees</em>, <em>bus</em>, <em>days</em>, <em>dice</em>, <em>gas</em>, and <em>goes</em>.</p>
<p><img src="https://new.joeystanley.com/blog/testing-vot-durations/word-initial-stops.png" class="img-fluid" style="width:30em"></p>
<p>A mixed-effects regression model that predicts this aspiration with the underlying voicing of the stop as a fixed effect and student and word as random effects suggests that this difference is significant.</p>
<pre><code>summary(lmer(aspiration ~ underlying_voicing + (1|student) + (1|word), data=wi_stops))

Random effects:
 Groups   Name        Variance  Std.Dev.
 student  (Intercept) 1.039e-03 0.03224 
 word     (Intercept) 9.550e-06 0.00309 
 Residual             1.084e-03 0.03293 
Number of obs: 285, groups:  student, 24; word, 12

Fixed effects:
                            Estimate Std. Error t value
(Intercept)                 0.047309   0.007241   6.534
underlying_voicingvoiceless 0.037331   0.004291   8.699</code></pre>
<p>Conclusion: Yep. Based on this data, the model predicts that voiced stops get around 47ms of aspiration while voiceless stops get about 85ms. Cool.</p>
</section>
<section id="stops-following-s" class="level3">
<h3 class="anchored" data-anchor-id="stops-following-s">Stops following /s/</h3>
<p>We learn that voiceless stops following /s/ in the same syllable are not aspirated in English. In our sample, this also proved to be correct based on the same words as above with the addition of <em>spit</em>, <em>steep</em>, and <em>scope</em>.</p>
<p><img src="https://new.joeystanley.com/blog/testing-vot-durations/onset-stops.png" class="img-fluid" style="width:30em"></p>
<p>I ran a mixed-effects regression model like the one described above but with position (=environment) as the main effect and voiced word-initial stops as the reference level.</p>
<pre><code>summary(lmer(aspiration ~ position + (1|student) + (1|word), data=onset_stops))

Random effects:
 Groups   Name        Variance Std.Dev.
 student  (Intercept) 0.001153 0.03395 
 word     (Intercept) 0.000000 0.00000 
 Residual             0.001150 0.03391 
Number of obs: 355, groups:  student, 24; word, 15

Fixed effects:
                                Estimate Std. Error t value
(Intercept)                     0.047309   0.007485   6.321
positionvoiceless word-initial  0.037395   0.004019   9.304
positionfollowing /s/          -0.007779   0.004948  -1.572</code></pre>
<p>We get the same coefficients for word-initial stops. Only this time, we can see how stops followed by /s/ fit in. The model shows that the duration of aspiration was not significantly different from word-initial voiced stops. There’s some indication that the stops following /s/ have even <em>less</em> aspiration, but this didn’t reach significance.</p>
</section>
<section id="duration-of-word-final-stops" class="level3">
<h3 class="anchored" data-anchor-id="duration-of-word-final-stops">Duration of word-final stops</h3>
<p>What I didn’t know before preparing for this class was that the overall duration of word final stops (closure + burst + aspiration) is longer for voiceless obstruents than it is for voiced obstruents. Based on all 18 words (<em>scope</em>, <em>seep</em>, <em>soap</em>, <em>steep</em>, <em>sit</em>, <em>spit</em>, <em>tack</em>, <em>cab</em>, <em>tab</em>, <em>code</em>, <em>pad</em>, <em>peg</em>, <em>bus</em>, <em>dice</em>, <em>gas</em>, <em>bees</em>, <em>days</em>, <em>goes</em>), this was true.</p>
<p><img src="https://new.joeystanley.com/blog/testing-vot-durations/word-final-duration.png" class="img-fluid" style="width:30em"></p>
<p>The difference is small but significant: in a mixed-effects regression model that predicts duration with voicing and manner of articulation as fixed effects and student and word as random effects, the difference reached statistical significance.</p>
<pre><code>summary(lmer(duration ~ underlying_voicing + manner + (1|student) + (1|word), data=wf))

Random effects:
 Groups   Name        Variance  Std.Dev.
 student  (Intercept) 3.002e-03 0.054790
 word     (Intercept) 3.745e-06 0.001935
 Residual             7.550e-03 0.086893
Number of obs: 427, groups:  student, 24; word, 18

Fixed effects:
                          Estimate Std. Error t value
(Intercept)               0.194404   0.012836  15.145
underlying_voicingvoiced -0.035321   0.008540  -4.136
mannerfricative           0.096638   0.008976  10.766
</code></pre>
<p>This model shows that the voiceless stops were about 194ms long, and voiced stops were 159ms (35ms shorter). Furthermore, and I didn’t expect this until I saw the plot, fricatives were generally about 97ms longer than stops. I don’t do a lot of nitty-gritty phonetics work like this too often, especially on consonants, so this was news to me.</p>
</section>
<section id="vowel-length" class="level3">
<h3 class="anchored" data-anchor-id="vowel-length">Vowel length</h3>
<p>Finally, it is pretty well known that vowels before voiced obstruents are said to be longer than vowels before voiceless obstruents. As expected, the data showed this to be the case.</p>
<p><img src="https://new.joeystanley.com/blog/testing-vot-durations/word-final-obstruents-and-vowel-duration.png" class="img-fluid" style="width:30em"></p>
<p>One last regression model, similar to what was done previously, showed not only that this voicing difference is significant but that the manner of articulation mattered as well.</p>
<pre><code>summary(lmer(vowel ~ underlying_voicing + manner + (1|student) + (1|word), data=wf))

Random effects:
 Groups   Name        Variance Std.Dev.
 student  (Intercept) 0.001937 0.04401 
 word     (Intercept) 0.001114 0.03337 
 Residual             0.002274 0.04769 
Number of obs: 431, groups:  student, 24; word, 18

Fixed effects:
                         Estimate Std. Error t value
(Intercept)               0.18905    0.01513  12.495
underlying_voicingvoiced  0.09595    0.01654   5.799
mannerfricative           0.05349    0.01744   3.067</code></pre>
<p>Here, we see that vowels followed by voiceless stops are predicted to have a duration of 189ms. If the following segment is voiced, the model predicts it to actually be 285ms (an increase of 95ms). What stands out is that vowels followed by /g/ were quite a bit shorter. Due to an oversight in my data, there was just one /g/-final word, <em>peg</em>, and it was listed last on the stimulus. No doubt this had an effect. In fact, a more rigorous study that would include more tokens and randomization might find the difference to be even greater, assuming /g/ falls more in like with /b/ and /d/.</p>
<p>I learned something new here as well: fricatives lengthen the vowel even more. The model predicts that vowels preceding fricatives will be 53ms longer than stops with the same voicing. I didn’t know that.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This study was short, unsystematic, and full of methodological issues. Despite its flaws, it shows evidence to support what Ladefoged and Johnson say in <em>A Course in Phonetics</em>. I also learned a few things: in addition to themselves being longer than stops, fricatives cause their preceding vowels to lengthen as well.</p>
<p>Special thanks goes to my Fall 2017 LING 3060 class at the University of Georgia, who bothered their friends with this silly assignment and painstakingly measured durations in software they barely know how to use.</p>


</section>

 ]]></description>
  <category>Side Projects</category>
  <category>Teaching</category>
  <category>Statistics</category>
  <category>Phonetics</category>
  <guid>https://new.joeystanley.com/blog/testing-vot-durations/index.html</guid>
  <pubDate>Fri, 08 Sep 2017 04:02:00 GMT</pubDate>
</item>
<item>
  <title>General Update</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/general-update/index.html</link>
  <description><![CDATA[ 




<p>Because I know I have such a <em>massive</em> following, I thought I’d give an update on my research since it’s been a few months since the last time I wrote.</p>
<section id="publications" class="level1 page-columns page-full">
<h1>Publications</h1>
<p>At the Linguistic Atlas Office, we’re working hard on publishing some of our preliminary results. Currently, I’m on two papers submitted to <em>Proceedings of Meetings on Acoustics</em> that are in various stages of reviewing. I’m excited to see these come out.</p>
<div class="page-columns page-full"><p>I’m also working on some of my own research. I’ve got three manuscripts going right now: one on near-mergers in Washington, one on language change within a speaker’s lifespan, and another on Amazon Mechanical Turk. I still haven’t submitted a paper to an actual journal so mentally this is a big hurdle for me to get over.</p><div class="no-row-height column-margin column-container"><span class="">Edit (September 2023): Lol, none of these manuscripts were even finished.</span></div></div>
</section>
<section id="conferences" class="level1">
<h1>Conferences</h1>
<p>I’m happy to say I’ve been accepted into two conferences that’ll happen over the next few months. The first is a paper called <a href="../../downloads/71007-LCUGA4-UT-slides.pdf">“Consonantal variation in Utah English: What el[t]se is happening[k]?”</a> that I’m doing with Kyle Vanderniet, a fellow grad student here at UGA. Using the MTurk data I collected recently, we focused on just the 14 speakers from Utah and gathered a lot of really interesting tokens of non-standard variants. I’ve also been accepted to present a poster at NWAV on some of my findings in Washington.</p>
<p>I’m also still waiting to hear back from three other conferences: <a href="https://pubs.aip.org/asa/jasa/article/142/4_Supplement/2540/703907/Vowel-mergers-in-the-American-South">ASA</a> and <a href="../../blog/ads2018">two at ADS</a>.</p>
</section>
<section id="teaching" class="level1">
<h1>Teaching</h1>
<p>So I’m teaching for the first time this semester. Because our normal phonetician will be gone, I was asked to teach Phonetics &amp; Phonology. This is pretty cool because normally grad students at UGA don’t get to teach that class. I’ve really enjoyed it so far! I’ve got 29 students who are all linguistics majors or minors. It takes quite a bit of time, but I’m having a great time.</p>
</section>
<section id="r-series" class="level1">
<h1>R Series</h1>
<p>Fortunately, my funding through the DigiLab at the UGA Main Library continues this school year. I’m really excited be giving <a href="../../pages/r-workshops.html">a whole series of workshops on R</a>. Next week I’ll start with just a basic introduction to R. Next month I’ll do a day on <a href="http://ggplot2.tidyverse.org"><code>ggplot2</code></a> and in November will be one on the rest of the <a href="http://www.tidyverse.org"><code>tidyverse</code></a>. I plan on producing some detailed handouts that will be available on this website as a product of these workshops. I’m really looking forward to them.</p>
</section>
<section id="grant" class="level1">
<h1>Grant</h1>
<p>I applied for a small grant a week or so ago that I’m waiting to hear back about. If I get it, it’ll pay for some fieldwork out West I’d like to do. Stay tuned. (Update: I got the grant.)</p>
</section>
<section id="dissertation" class="level1 page-columns page-full">
<h1>Dissertation</h1>
<p>Ah, the dissertation. This <em>should</em> be my primary task right now, and I’m putting as much time as I can into it. I originally wanted to do something on my Washington data. I could do a purely descriptive work and it would make for a fine dissertation. I’ve read through some of those and they’re perfectly good.</p>
<p>The problem with that is if I would do that, it would be of interest only to people interested in Washington English. A very small group of dialectologists. Not that my ultimate goal is to boost the number of citations, but I feel like a dissertation, which is supposed to make a real contribution to the field, should be more than that.</p>
<p>I’ve read a couple other dissertations that are heavily based in dialectology and sociolinguistic fieldwork, but they make a bigger statement. They make some advancement on the study of language change, <em>using</em> the data they’ve collected as an <em>illustration</em>. For example, Ruth Herold’s (1990) UPenn dissertation is based on the <em>cot-caught merger</em> parts of Pennsylvania. Instead of simply describing the data, she makes some very cool statements about how language change works, especially in relation to mergers and generational changes, using the data she collected as evidence. The data is supporting the claim rather than the claim itself.</p>
<div class="page-columns page-full"><p>I’ve got around 180 hours of my own data right now, gathered from lots of sources. All of it has to do with mergers in some way. I feel like I’ve got a dissertation bubbling inside of me relating to vowel mergers. I can use my data as support, illustration, and evidence for some claim that I’d like to make.</p><div class="no-row-height column-margin column-container"><span class="">Update: I stuck with the Washington data for my dissertation, but Betsy Sneller and I did eventually write <a href="https://doi.org/10.1121/10.0016757">something about mergers</a> using the Washington data, so I guess that scratched that itch.</span></div></div>
<p>So I’m working on my prospectus right now and will be pitching it to my committee this semester. I’ll keep you informed.</p>


</section>

 ]]></description>
  <category>West</category>
  <category>Utah</category>
  <category>MTurk</category>
  <category>Research</category>
  <category>Conferences</category>
  <category>Linguistic Atlas</category>
  <category>Pacific Northwest</category>
  <category>Dissertation</category>
  <guid>https://new.joeystanley.com/blog/general-update/index.html</guid>
  <pubDate>Tue, 05 Sep 2017 14:30:00 GMT</pubDate>
</item>
<item>
  <title>Using MTurk</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/using-mturk/index.html</link>
  <description><![CDATA[ 




<p>A few weeks ago, I <a href="../../blog/a-survey-of-western-american-english-using-mturk">wrote</a>about a grant I was awarded where I’ll use Amazon Mechanical Turk (“MTurk”) to collect data from people all across the West. Today, I did a soft launch of the request and already got recordings from five people!</p>
<p>After weeks of carefully wrangling my MTurk request, my Qualtrics survey, and my IRB forms, I finally got it all set up. I’ve had a handful of projects get approved by the IRB, but this one was a little different since it was through MTurk, so I was a little unsure how to go about some things. Luckily, our IRB office was having open houses all through the semester, which were <em>very</em> helpful.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Just got IRB approval on my first try. I'm getting better at this! Huh, so going to the <a href="https://twitter.com/UGAHSO"><span class="citation" data-cites="UGAHSO">@UGAHSO</span></a> open houses helps a ton! Who'da thunk?
</p>
— Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/864932387712630784">May 17, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I decided to do a soft release first. $2500 is a lot of money to just throw into a task all at once and I wanted to make sure things were working out right. So I put in enough for five people do to the task. Within the hour I was getting data sent to me! It was crazy!</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
With MTurk, I'm literally getting data emailed to me throughout the day. Pretty exciting.
</p>
— Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/865311077491462144">May 18, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I got all five in one day with no problem. I’m glad I did the soft release though because there were a couple small snafus that I had to fix. For example, I underestimated how much time it would take people to finish the task, so I’ll raise the amount they’re compensated: I can afford fewer workers that way, but at least I pay them an honest amount.</p>
<p>I’ll spend the next few days making absolutely certain that the task I want them to do is what’s right for this project. But at some point, I’ll pull the trigger and let’er rip. From that point on, all I need to do is approve people’s work (to make sure they get paid) and then just enjoy the hours and hours of recordings showing up in my inbox. What a way to collect data!</p>
<section id="may-22" class="level2">
<h2 class="anchored" data-anchor-id="may-22">May 22</h2>
<p>So this happened:</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Thank you, MTurker, for pointing out that my consent form says the software I ask you download "will be harmful to your computer." <a href="https://twitter.com/hashtag/Typo?src=hash">#Typo</a>
</p>
— Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/866703105798410240">May 22, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</section>
<section id="june-9" class="level2">
<h2 class="anchored" data-anchor-id="june-9">June 9</h2>
<p>Okay, so several weeks have passed, and the data collection phase is drawing to a close. In just a couple of weeks, I was able to get data from almost 200 people. I had some major time constraints on how I could use my money, so I had to find ways to use it quicker. I ended up creating an entirely new task, similar to the first one, with a whole new batch of sentences and words for people to read. A large portion of my participants returned to do the second part, meaning I have around 30 minutes of audio from almost 100 people.</p>
<p>This is an incredible dataset I’ve collected. I don’t know how much audio I have total yet, but it’s well over 50 hours. That’s pretty good for just three weeks.</p>
<p>However, I will be the first to say that it was a rough three weeks. It seems like every hour I was getting data emailed to me, and several times a day I had to sit and catalogue the recordings and speaker metadata, while managing the MTurk tasks. Most of the time, it was relatively straightforward, but some participants needed a little extra attention because of technical difficulties, glitches in the system, or complaints here and there. Luckily, I did this when I wasn’t in classes, because otherwise it would have been impossible.</p>
</section>
<section id="june-20" class="level2">
<h2 class="anchored" data-anchor-id="june-20">June 20</h2>
<p>At last, my data collection has drawn to a close. I ended up with about 212 speakers and 84 hours of data. Not bad. Now comes the daunting task of processing all of this. For every person, if I just want to do a small task that only takes a minute, it’ll take over 3 hours to do it for all speakers! This will take a <em>very</em> long time for me to get through, but from the 2% that I’ve looked at so far, it’s going to be very fruitful corpus.</p>


</section>

 ]]></description>
  <category>Research</category>
  <category>West</category>
  <category>Pacific Northwest</category>
  <category>MTurk</category>
  <guid>https://new.joeystanley.com/blog/using-mturk/index.html</guid>
  <pubDate>Thu, 18 May 2017 14:30:00 GMT</pubDate>
</item>
<item>
  <title>Laboratory Research</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/laboratory-research/index.html</link>
  <description><![CDATA[ 




<p>Recently, I’ve <a href="../../blog/lsa2017">presented</a> on words like <i>pool</i>, <i>pull</i>, and <i>pole</i> and how the difference between them can be really hard to describe, both by me and the non-specialist alike. Based on my findings in Washington, I decided I wanted to dig a little deeper into what these words are like, so I started a study that is less sociolinguistic and more laboratory phonology-based, which is a little unusual for me.</p>
<p>Broadly, I want to look at the phonetics of English vowels before coda laterals. So, after making a list of lots and lots of possible words, cutting them down based on frequency and other factors, I’ve got a decent list of targeted words.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
List of all ~401 monosyllabic English words with a coda lateral, organized by vowel and syllable structure? Check. My favorite? Squelched.
</p>
— Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/861032509534031873">May 7, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I got IRB approval just a little too late into the semester to recruit people and offer them extra credit in courses, so I had to wait a few weeks to get started. Now that Maymester has started, I’ve approached some professors and asked them to offer participation in my research as an extra credit opportunity. I even hand out little business cards after I’ve done my pitch to the class, so they have my contact information—an idea that proved very effective for me in Washington:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/laboratory-research/businesscard.png" class="img-fluid figure-img" style="width:15em"></p>
<figcaption class="figure-caption margin-caption">Recruitment business card</figcaption>
</figure>
</div>
<p>So I’m now meeting with students in the <a href="https://linglab.franklinresearch.uga.edu">linguistics laboratory</a> that we have here at UGA. It’s unfortunately under-utilized but nonetheless very good. Inside the already very quiet recording studio is a tiny booth where the best recordings can be made. I have participants reading a bunch of carefully selected sentences that target key sounds and then taking a quick follow-up survey. It amounts to about 30–40 minutes of speech from each person, which is kind of a lot.</p>
<p>I don’t have a specific goal for how many people I want to get, but I should have 20 by the end of the month and potentially up to 50 by the end of the summer. My only limitation is how much time I can put into this. I’ll do some preliminary analyses on those and see if I need to recalibrate the sentences or maybe collect more data. This will probably be an ongoing project for a while: the IRB and consent forms are purposely pretty open-ended to allow me to modify things where needed without much hassle.</p>
<p>Anyway, it’s been fun being in the lab, and I’m excited to analyze <em>really</em> clean audio for a change.</p>



 ]]></description>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/laboratory-research/index.html</guid>
  <pubDate>Thu, 18 May 2017 02:30:00 GMT</pubDate>
</item>
</channel>
</rss>
