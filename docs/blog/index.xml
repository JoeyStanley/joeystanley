<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Joey Stanley</title>
<link>https://new.joeystanley.com/blog/index.html</link>
<atom:link href="https://new.joeystanley.com/blog/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.433</generator>
<lastBuildDate>Mon, 25 Sep 2023 04:12:00 GMT</lastBuildDate>
<item>
  <title>Website Version 3</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/website-version-3/index.html</link>
  <description><![CDATA[ 




<p>After exactly seven years with my old website, I‚Äôve decided to change it to what you are seeing now.</p>
<section id="what-was-wrong-with-the-old-one" class="level2">
<h2 class="anchored" data-anchor-id="what-was-wrong-with-the-old-one">What was wrong with the old one?</h2>
<p>I built my old website in September 2016. I had a research assistantship at the DigiLab at UGA, and Emily McGinn, the supervisor, suggested I find ways to increase my online presence. I learned some <a href="../../blog/making-a-website-is-fun">web design and CSS skills</a> skills and eventually made Version 1 of my website. That version was essentially the same as what I built in the tutorial I followed, so a few months later I rewrote everything from scratch and made Version 2. (Let‚Äôs be honest though, it‚Äôs still obviously heavily based on the tutorial.) Other than very minor tweaks to a few things, that‚Äôs how my website has been since then.</p>
<p>However, it got a bit unwieldy. The blog was organized just fine, but I also added pages here and there to go along with workshops and other presentations I gave. It got more confusing when I gave the same workshop a second time and had multiple similar pages floating around. Since I didn‚Äôt foresee some of these additions, its growth was reminiscent of unplanned suburban sprawl. For examples, sometimes images were just dumped into a folder, others were better organized. Non-blog pages were hidden and were sometimes a top-level page and other times within a dedicated subdirectory. Each individual addition wasn‚Äôt a big deal, but once I stepped back and looked at it all, it was a mess.</p>
<p>The format of my tutorials wasn‚Äôt consistent either. I have lots of handouts on my website, tucked away here and there. If they were associated with a workshop, they were separate R Markdown files that didn‚Äôt fit in with the rest of the site. Some of my earliest ones are PDFs of Word files!If they weren‚Äôt associated with a workshop, they‚Äôre regular blog posts. But because the site wasn‚Äôt connected to R, I had to do a <em>lot</em> of copying and pasting R Markdown code and careful insertion of images to get those tutorials to look right. In some cases, the extra work made it possible to do things like syntax highlighting in Praat and highlighting specific lines of code. But that was all done by manually inserting HTML tags and updating my CSS.</p>
<p>Also, as careful as I was about my CSS, it wasn‚Äôt perfect. I think there were some issues if like a list had only one element, and there were things with hyperlinks. Some one-off portions of blogs or tutorials sometimes didn‚Äôt look right. I had a disclaimer at the top of every page, something like, ‚ÄúThis website is built from scratch. Pardon the flaws; I am not a web designer.‚Äù Which was a humble brag if anything. But as the site grew I didn‚Äôt want to change the CSS because it might change some blog post from years ago in unexpected ways.</p>
<p>Ultimately, I didn‚Äôt mind the mess because it‚Äôs what made my site unique. But, what made me finally decide to migrate to Quarto was the underlying architecture. It was built using Jekyll, which involves a programming language called Ruby in some way. After seven years I still have no idea what either of those are. I did this because it‚Äôs what the tutorial I followed used. When the site worked, it was great. But sometimes, the Ruby dependencies (called ‚Äúgems‚Äù) would update or break or whatever and I had to google around trying to find a fix. I had no idea what I was doing and it led to a lot of frustrated late nights trying to get my website up and running again.</p>
<p>Then Quarto comes along, which makes it easy to make a blog entirely within R Studio. I have been very familiar with the R world for a while. In 2017, I was an early adopter of <a href="https://www.rstudio.com/products/shiny/">Shiny</a> (at least in linguistics, I think), so I was able to integrate all my html, CSS, and R skills into the <a href="http://lap3.libs.uga.edu/u/jstanley/vowelcharts/">Gazetteer of Southern Vowels</a>. In 2020, I also started dabbling with creating my own R Packages and using the amazing <a href="https://pkgdown.r-lib.org">pkgdown</a> to make dedicated websites for them (see <a href="https://joeystanley.github.io/joeyr/">joeyr</a>, <a href="https://github.com/JoeyStanley/futurevisions">futurevisions</a>, <a href="https://joeystanley.github.io/barktools/">barktools</a>, and <a href="https://joeystanley.github.io/joeysvowels/">joeysvowels</a>). Finally, I have a side project that involves collecting and analyzing data about what hymns are sung in LDS congregations, and in 2023 I decided to build the <a href="hymnstats.joeystanley.com">site</a> entirely in Quarto.</p>
<p>So, I‚Äôve gradually built up to web development in R over the years and Quarto seems like the logical place to migrate to. Plus, it has some features that I‚Äôve always wanted, like scrolling table of contents and a search feature. After some encouragement from folks on Twitter, I decided it‚Äôs time to bite the bullet and go for it.</p>
</section>
<section id="what-does-it-take-to-migrate" class="level2">
<h2 class="anchored" data-anchor-id="what-does-it-take-to-migrate">What does it take to migrate?</h2>
<p>I‚Äôm doing this page by page. Here‚Äôs the order I took:</p>
<ul>
<li>My homepage and any links on it. I didn‚Äôt clean up the linked pages, but at least there weren‚Äôt any dead links.</li>
<li>My blogs.</li>
</ul>
</section>
<section id="things-that-are-the-same" class="level2">
<h2 class="anchored" data-anchor-id="things-that-are-the-same">Things that are the same</h2>
<p>I‚Äôve tried to keep as much of the original structure of the site the same as I could. However, as I migrate</p>
</section>
<section id="changes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="changes">Changes</h2>
<p>Here‚Äôs a list of the changes I‚Äôve made.</p>
<ul>
<li><p>Each blog is now in its own self-contained folder. The previous structure had all posts in a single folder and all images in another folder. This time, the images associated with a blog post are contained within that folder. So, instead of this:</p>
<pre><code>‚îú‚îÄ‚îÄüìÅblog
|  ‚îú‚îÄ‚îÄüìÑblog post 1.md
|  ‚îú‚îÄ‚îÄüìÑblog post 2.md
‚îú‚îÄ‚îÄüìÅimages
|  ‚îú‚îÄ‚îÄüåÖimage1.png
|  ‚îú‚îÄ‚îÄüåÖimage2.png</code></pre>
<p>It‚Äôs now this:</p>
<pre><code>‚îú‚îÄ‚îÄüìÅblog
|  ‚îú‚îÄ‚îÄüìÅblog post 1
|  |  ‚îú‚îÄ‚îÄ  üìÑindex.qmd
|  |  ‚îú‚îÄ‚îÄ  üåÖimage1.png
|  ‚îú‚îÄ‚îÄüìÅblog post 2
|  |  ‚îú‚îÄ‚îÄ  üìÑindex.qmd
|  |  ‚îú‚îÄ‚îÄ  üåÖimage2.png</code></pre>
<p>It shouldn‚Äôt affect any urls to existing blog posts because the url <code>blog/blog post 1</code> in the old format would go to the <code>blog post 1.md</code> file and in the new one it‚Äôll go to the <code>blog post 1</code> directory, which‚Äôll display <code>index.qmd</code> by default. I was concerned about changing the url because I know some people have cited my turorials in published work and I didn‚Äôt want those urls to break. I think this‚Äôll work <em>and</em> it‚Äôll keep the site better organized.</p></li>
</ul>


<div class="no-row-height column-margin column-container"><span class="">By the way, I‚Äôm stealing this way of visualizing file structure directly from <a href="https://github.com/tjmahr/quarto-blog/blob/main/posts/migrating-from-jekyll-to-quarto/index.qmd">TJ Mahr‚Äôs Migrating-to-Quarto page.</a>.</span></div></section>

 ]]></description>
  <category>Meta</category>
  <guid>https://new.joeystanley.com/blog/website-version-3/index.html</guid>
  <pubDate>Mon, 25 Sep 2023 04:12:00 GMT</pubDate>
</item>
<item>
  <title>Kohler Tapes</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/kohler-tapes/index.html</link>
  <description><![CDATA[ 




<p>So, I just acquired a goldmine of data that I can use for linguistic analysis. Sitting in my office are 452 cassette tapes, each containing at least 30 minutes of recorded interviews with an older folks from Heber City, Utah. And that‚Äôs about half of the collection: the other half is with a historian in Midway, Utah. So, I‚Äôm looking at roughly 400‚Äì500 hours of audio. Not sure how I‚Äôm going to process it all, but I wanted to kick off the beginning of this long-term project with a blog post describing the history of the tapes, why I‚Äôm interested in them, and speculations about the future.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/kohler-tapes/kohler_tapes.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">452 tapes sitting on my shelf!</figcaption>
</figure>
</div>
<section id="background" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>I first heard about the tapes a little over three years ago. In January 2018, the LSA annual meeting was in Salt Lake City. Wanting to take advantage of the trip out there, I applied for and received a grant from the University of Georgia to collect audio in Heber City, aiming for multiple generations within a family to track language change over time. I decided on Heber partly because it was a region of Utah that had never been the subject of acoustic (let alone linguistic) analysis, as far as I know. My parents were living there at the time too, so they could hook me up with some potential contacts.</p>
<div class="page-columns page-full"><p>So on the morning of the first day of my fieldwork, the first thing I did was go to the Heber Valley Visitor‚Äôs Center as a way to potentially find some contacts. Literally the first person I talked to told me about a man who had a huge collection of tapes. One person led me to another, and I was talking to an elderly man named Norm Kohler in his nursing home.</p><div class="no-row-height column-margin column-container"><span class="">Side note, it‚Äôs amazing that I heard about this goldmine <em>literally</em> through the first person I talked to while doing fieldwork? Who knew that there‚Äôd be such an amazing collection of audio sitting in someone‚Äôs basement nearby? In fact, there could be lots of collections like these, just collecting dust in people‚Äôs basements. All it takes is to find the right person!</span></div></div>
<p>Norm was a beloved middle school teacher in Heber City in the 1980s and 1990s. As a history project, he had each of his students get a cassette tape and interview a grandparent. I don‚Äôt know what the interview questions were, but I think they mostly concerned life in Heber Valley. He kept all the tapes his students turned in and, over the course of two decades, he ended up with over 1200 interviews! Norm intended to compile them and put together an oral history of the town, but unfortunately was unable to do so. So, just weeks before I met him, he decided it was best to return the tapes to the family members‚Äô of his students and the people they interviewed. So he put an ad in the paper and hundreds of people claimed their tapes and were able to hear their ancestors‚Äô voices, perhaps for the first time.</p>
<p>However, not all the tapes were claimed. I was told a few hundred remained. So, after Norm passed away a few months later, his family held on to them for a while before finally donating them to the Midway Historical Society. (Midway is the town next door to Heber.) Several complications made it difficult for me to get access to the tapes, including outdated contact information on Midway‚Äôs website, the society going on an extended hiatus, me living in Georgia, and then covid. But I did my best to reach out to anyone who might know about where the tapes were being stored.</p>
<p>Finally, on Thursday this week, I was contacted by the historian in custody of the tapes. She asked if I was still interested in them, and I most definitely am! So we had a nice chat about what my goals were for them and what the goals were for the Historical Society, and we think there‚Äôs mutual interest in getting them digitized and transcribed. So, the next day, yesterday, she happened to be in Provo so she dropped off about half of the tapes‚Äî452 of them!‚Äîat my office!</p>
<p>So after three years of following tenuous leads, I finally have the tapes!</p>
</section>
<section id="why-am-i-so-interested" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-am-i-so-interested">Why am I so interested?</h2>
<p>I am a linguist, so why should I care about these tapes? Well, the obvious reason is that it‚Äôs a <em>lot</em> of audio. For my dissertation I analyzed about 40 hours of interviews and that was already a lot of data. This is at least 10 times the amount of audio. In fact, it‚Äôs about the size of the <em>Digital Archive of Southern Speech</em>, a subset of the <em>Linguistic Atlas of the Gulf States</em>, that I spent four years in grad school analyzing. So having access to this much audio is absolutely incredible.</p>
<p>But it‚Äôs not just the amount of audio. There are dozens of oral history projects even in Utah. This particular set is attractive for several reasons:</p>
<ol type="1">
<li><p>The nature of the homework assignment ensured good metadata. A few tapes have already been digitized and they all start off introducing the interviewer (the middle-schooler) and the interviewee, with information like the date of interview, their age, and where they grew up.</p></li>
<li><p>Because these were all students in the same smallish town in Utah the sample will be relatively homogeneous geographically. While it doesn‚Äôt ensure that the interviewees (the grandparents) were from Heber or Heber Valley generally, my guess is that a significant number of them are.</p></li>
<li><p>Typically, interviews happen with a historian or someone that the interviewer is unfamiliar with. In sociolinguistics, it‚Äôs generally accepted that the degree of familiarity with the interviewer can have an influence on a person‚Äôs speech. In all cases with these tapes, the interviewer is a teenager and a grandchild of the interviewee. So that lowers the formality of the situation and will likely mean that the interviewees‚Äô speech will be more casual.</p></li>
<li><p>Heber Valley has been the focus of very little acoustic research. There may be occasional interviews as parts of the Linguistic Atlas Project or the <em>Dictionary of Regional American English</em>, but no study, as far as I know, has focused on Heber. Instead, most research looks at people from Utah Valley and Salt Lake Valley. This collection of interviews will offer a new spot on the map of Utah dialectology and a nice point of comparison between more urban and more rural areas of the state.</p></li>
<li><p>I have virtually no metadata about the interviewees right now, but if their grandchildren were about 14 years old in the 1980s and 1990s, then the speakers in these tapes were born perhaps sometime between 1900 and 1940. There has been some research on the development of Utah English, mostly by David Bowie, but he acknowledges that it was based on public sermons given by upper-class white men. This collection offers a unique look into how other Utahns born around that time talked. And since I have some comparable data from contemporary Heber City residents, I can begin to look at language change in real time.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><span class="">There were about 4500 people living in Heber in the 1980s and 1990s, which means this sample is a significant chunk of the community!</span></div><p>So there are lots of reasons for why I‚Äôm really interested in this collection of tapes. And that‚Äôs on top of the oral history the Midway Historical Society wants to create based on them.</p>
</section>
<section id="looking-ahead" class="level2">
<h2 class="anchored" data-anchor-id="looking-ahead">Looking Ahead</h2>
<p>Luckily, I‚Äôve had some experience working on a project of this size. For four years at the University of Georgia, I was a part of the team that processed the <a href="http://www.lap.uga.edu/Site/DASS.html"><em>Digital Archive of Southern Speech</em></a>, which is a 367-hour subset of the <em>Linguistic Atlas of the Gulf States</em>. So I‚Äôve sat in on transcriber training sessions, seen what kinds of obstacles get in the way of processing, managed thousands of files, and analyzed spreadsheets with a couple million acoustic measurements in them. However, that was only as a graduate student. I‚Äôm sure there‚Äôs a lot that goes on behind the scenes as a PI that I didn‚Äôt see.</p>
<p><em>Transcribers</em>‚ÄîSome back-of-the-envelope calculations suggest that I‚Äôll need a sizable grant to get this all processed. Again, I don‚Äôt have definite numbers for anything, I know my 452 tapes are a little over half of them, so let‚Äôs say there are 700 tapes total. They‚Äôre all at least 30 minutes long and I know many went longer, so if I average say 40 minutes per tape, that‚Äôs 28,000 minutes or roughly 467 hours. I think the the transcribers for <em>DASS</em> averaged about 13 hours per 50 minutes of audio or so, but this audio is newer and I presume Utah transcribers will be more familiar with Utah speakers I think, so I‚Äôll estimate 10 hours of work per tape. That‚Äôs 4670 hours of transcription. At $15 per hour, I‚Äôm looking at about $70,000 in student wages. Obviously, I can‚Äôt get that much coin internally so it sounds like this is only going to happen with an external grant.</p>
<p><em>Grad student workers</em>‚ÄîThat‚Äôs of course assuming that the only wages I‚Äôll need to pay for are transcribers. This might be getting into ‚ÄúIf you give a mouse a cookie‚Äù territory, but it would be nice to have some grad students helping out with the project. At UGA, we had at least four and as many as six grad students involved in the project at a time. There was a lot of overlap between our duties, but very roughly speaking, one managed the transcribers, one managed the spot-checks, one managed the acoustic analysis, and one did miscellaneous duties. We were all involved in analysis, and a few others popped in for a semester or two to do additional analysis or perform other duties. To lighten my load, it would be handy to have perhaps three grad students manage the transcribers, check their work, and do the acoustic analysis. I‚Äôm fuzzy on what costs are associated with RA-ships at BYU, but I do know it‚Äôll add significantly to the total cost of the project.</p>
<p><em>Time</em>‚ÄîHow long will transcriptions take? I‚Äôve done transcriptions and they‚Äôre soul-sucking work. Even when I was highly motivated to process my own dissertation data, that I collected myself, and under a bit of a time crunch, I could barely put in more than about two hours a day. I surely don‚Äôt expect undergraduate transcribers to do more than 10 hours a week. When motivated by money, I‚Äôve seen some at UGA do more, but those students were exceptional. I‚Äôll estimate five hours of work per transcriber per week. So under the assumption of 4670 hours of work total, that‚Äôs 934 transcriber-weeks. If a semester is fifteen weeks, that‚Äôs 62 transcriber-semesters. If I set a goal of getting all the work done in two years (six semesters if you include summers), it would take ten or eleven transcribers to do it in two years. Of course, these are all very rough estimates, but managing several tens of thousands of dollars and almost a dozen workers for two years is not something I expected to do right away!</p>
<p><em>Digitizing</em>‚ÄîRegardless of the cost, number of workers, and time involved, the first step of the process will be digitization. Fortunately, it sounds like the Office of Digital Humanities can take care of that for me! Wow! So my short term goal is to get a batch‚Äîmaybe 30 or 50 tapes‚Äîdone first. While they work on digitizing the next batch, I can get started on listening to the first few minutes of the completed tapes and extracting whatever metadata I can from them. Eventually, all the tapes will be digitized and I can have a more concrete idea of how much audio (and consequently, people, time, hours, and money) I‚Äôm looking at.</p>
<p><em>Metadata</em>‚ÄîAfter digitizing all of them, my next step will be to finish collecting the metadata. It‚Äôll be nice to have a clear picture of birth years, genders, and birthplaces for all 700 or so people. The most likely scenario is that I <em>won‚Äôt</em> get an external grant because they‚Äôre extremely competitive, so I‚Äôll have to prioritize which ones to transcribe first. The Historical Society would like to start with some of the prominent members of the community and descendants of the town‚Äôs founders. I‚Äôd like to find a balance of genders and birth years too, so we‚Äôll probably settle on a subset that satisfies both of our needs. How big? I‚Äôm thinking between 35 and 70 (5% to 10% of the tapes). That‚Äôs a more reasonably-sized project that I could possibly get funded internally. It could provide me at least a beginning look at the speech community which would help seed an external grant.</p>
<p><em>Follow-up project?</em>‚ÄîIn case I just need more data to analyze (ha!) wouldn‚Äôt it be cool to track down some of the tapes that were given away? Presumably, if an ad in the paper is what it took for the families to get them, then an ad might be a good place to start to find them. We‚Äôd digitize the tapes right there for people, give them a copy and return the tape to them of course, but then also add that to the collection for the oral history. I think it would be especially cool to interview those people themselves! That way we can get some contemporary data to compare the tapes to, as well as track change within the family. That‚Äôll have to wait until I get NSF grant number two!</p>
<p><em>Publications</em>‚ÄîWhat‚Äôs the end goal? Well, I‚Äôll obviously start cranking out some papers as soon as a reasonable amount of data has been processed. There is a <em>lot</em> going on in Utah English. Many of the stereotyped features are dying out, so these people may provide good acoustic data for what would otherwise be hard to study phonetically today. But there are also lots of other features that I believe are recent innovations, so if they‚Äôre infrequent or missing from these speakers, it‚Äôll help establish the timing of when they did develop. Even before I had the tapes, I‚Äôve been thinking a full analysis of this collection deserves a book-length treatment. It likely won‚Äôt get done before I‚Äôm up for tenure, but maybe it‚Äôll go towards my application for full professor.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The history of the Kohler Tapes is pretty cool, and I‚Äôm lucky to be a part of the creation of an oral history of Heber City. It‚Äôs so satisfying teaming up with a historical society and finding ways to help the community I‚Äôm studying too. Linguistically, they‚Äôre interesting to me for lots of reasons, but I think everyone benefits from seeing these tapes get processed. As far as how I‚Äôm going to go about processing all of them, I really have no idea what I‚Äôm doing so there will be a lot of learning involved. But I‚Äôm excited to be involved and to have a clear research trajectory for the next decade or so!</p>


</section>

 ]]></description>
  <guid>https://new.joeystanley.com/blog/kohler-tapes/index.html</guid>
  <pubDate>Sat, 13 Feb 2021 21:16:00 GMT</pubDate>
</item>
<item>
  <title>Dissertation</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/dissertation/index.html</link>
  <description><![CDATA[ 




<p>I‚Äôm happy to report that I successfully defended my dissertation today! The <a href="../../downloads/191209-dissertation_defense.pdf">defense</a> was held in the DigiLab (300 Main Library). The study itself is called ‚ÄúVowel Dynamics of the Elsewhere Shift: A sociophonetic analysis of English in Cowlitz County, Washington.‚Äù</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/dissertation/dissertation_defense.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Me with my committee: Chad Howe, Peggy Renwick, and Bill Kretzschmar (Skyping in).</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can download my dissertation <a href="../../downloads/200417-dissertation-revised.pdf">here</a>!</p>
</div>
</div>
<p>The version linked above is a revision that I‚Äôve made after correcting some small typos. Click <a href="https://search.proquest.com/docview/2412335574/398995AF274140D8PQ/1?accountid=4488">here</a> to view the official, submitted version.</p>



 ]]></description>
  <category>Dissertation</category>
  <category>Pacific Northwest</category>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/dissertation/index.html</guid>
  <pubDate>Wed, 04 Dec 2019 13:00:00 GMT</pubDate>
</item>
<item>
  <title>Transcribing a Sociolinguistic Corpus</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/transcribing-a-sociolinguistic-corpus/index.html</link>
  <description><![CDATA[ 




<p>In the summer of 2016, I went to Cowlitz County, Washington to do traditional sociolinguistic interviews. I talked to 54 people and gathered my first audio corpus. It took a lot of preparation beforehand and it took a lot of time in the field. What I could not have expected was the amount of time it would take to transcribe that corpus. Now, two years later, I have finally finished transcriptions.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
I'm DONE! 54 speakers, 46¬Ω hours of audio, 172 hours of work (averaging about 1h50m over like 92 days), producing an audio corpus of 327,000 words! <a href="https://twitter.com/hashtag/amNOLONGERtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amNOLONGERtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017096903878639621?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This come after a lot of work. Since others might be going through the same thing, I thought I‚Äôd share some thoughts on transcribing a sociolinguistic corpus.</p>
<section id="finding-the-motivation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="finding-the-motivation">Finding the motivation</h2>
<div class="page-columns page-full"><p>I think my original goal was to have it all transcribed by the end of 2016. So I gave myself about five months. But then I did the first hour of audio and it took me about 5 hours. Yikes!  At that rate, I estimated it would take about 200 hours of work to finish. I think staring down the barrel of any 200 hour task is a motivation killer. So I put it off.</p><div class="no-row-height column-margin column-container"><span class="">I don‚Äôt know what I expected‚Äîof course it‚Äôs going to take a long time to transcribe!</span></div></div>
<p>I wrote a <a href="../../blog/lots-of-transcribing">blog post</a> nine months later when I had my first wake up call that I needed to get transcriptions done. I talked about some of the struggles I had getting started but mostly made excuses for why I hadn‚Äôt done much. And I got a lot of work done over the next month or so and made it about a quarter of the way through. I remember though just getting burned out after just 10 or 15 minutes of work and would call it a day after half an hour. At that rate, yeah, it‚Äôll take forever.</p>
<p>So I put it off for an entire year. In the meantime I was getting a lot done‚Äîmostly to distract me from the task I inevitably have to do before graduating. For some reason this distraction was in the form of collecting more audio. I got some <a href="../../blog/laboratory-research">laboratory audio</a>, and gathered another corpus using <a href="../../blog/using-mturk">Amazon Mechanical Turk</a>, and in January I went out to Utah to do some more fieldwork. And yet, this audio from 2016 was collecting dust on my computer, just waiting to be analyzed. I think I found that it was easier to collect new data than it was to finish processing the old stuff. Consequently, I had collected something like 150 hours of audio over two years for various projects‚Äîand less than 5% of it was processed.</p>
<p>When I finally defended my prospectus in April, it occurred to me that if I wanted to graduate in 2019, I needed to have data to write about. And the only way to do that was to transcribe that darn audio. So, that was what finally got me to crack down and work at this every day. Even then, it took three months of grinding to finally finish. But I‚Äôm so glad it‚Äôs done.</p>
</section>
<section id="a-rite-of-passage" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="a-rite-of-passage">A rite of passage</h2>
<p>I mentioned as a part of my celebratory tweetstorm that doing this kind of work might be something like a rite of passage for sociolinguists.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
I think putting this much work into a corpus is some sort of rite of passage for sociolinguists. I'm glad I went through it, but ugh, never again.
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017096905564835842?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<div class="page-columns page-full"><p>It seems like a lot of sociolinguists do research on their own corpora, and while the flashy part of statistical analysis, data visualization, or even fieldwork stories are what you see and hear about the most, a significant portion of what we do is the behind-the-scenes tedium on the computer. My university doesn‚Äôt have a huge group of sociolinguists and there‚Äôs no sort of shared corpus that we can use. So if I want to study contemporary spoken English, I was going to have to collect the audio myself. I think would have done fieldwork myself anyway though. I think it was always something I‚Äôve wanted to do. Plus, there‚Äôs this:</p><div class="no-row-height column-margin column-container"><span class="">Yes, the Linguistic Atlas Project has been here since the 80s, but very few of those recordings are transcribed, so they‚Äôre of little use in their current state.</span></div></div>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
Also, shout out to <a href="https://twitter.com/DrDialect?ref_src=twsrc%5Etfw"><span class="citation" data-cites="DrDialect">@DrDialect</span></a>, who I heard say at a Q&amp;A at SECOL in 2015 something like, "the best career move you can do is to create a corpus: you'll be able to analyze it forever." Some of the best advice I've ever heard.
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017096909041885185?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>And from the looks of it, this corpus that I now have is definitely going to last me a while, that‚Äôs for sure!</p>
</section>
<section id="what-software-did-i-use" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="what-software-did-i-use">What software did I use?</h2>
<p>For transcription, I think there are two ways of doing it. The first method is to find some software that will automatically transcribe it for you, and since it‚Äôs not going to be perfect, then spend the time to correct that transcription. I considered doing that, specifically using the transcriber in <a href="http://darla.dartmouth.edu">DARLA</a>. But I found that it took much longer to correct the transcriptions that it would have taken me to just do it by hand. However, DARLA specifically says on their website that their automatic transcriber is not great, so my rate might have been better if I had used a different transcriber. DARLA was what came to mind because it‚Äôs easy to use and free. You might have better luck if you use a more sophisticated transcriber.</p>
<p>The other option therefore was to just do it myself. As far as I can tell, there are two or three main pieces of software you can use. One is <a href="http://trans.sourceforge.net/en/presentation.php">Transcriber</a>. This is one that we use in the Linguistic Atlas Office when we have our undergrads do transcriptions. It‚Äôs free and easy to use. One concern is that it‚Äôs a little bit tricky to get its output to a TextGrid format. The other concern was that I couldn‚Äôt see the spectrogram to accurately place boundaries. Another option is <a href="http://www.mpi.nl/corpus/html/elan/index.html">ELAN</a>, which I hear is fantastic. The only reason I didn‚Äôt use it was frankly because I didn‚Äôt want to take the time to learn a new program.</p>
<p>What I settled on was just plain ol‚Äô Praat. It‚Äôs software that I‚Äôm comfortable with and I‚Äôve used a lot. I can zoom in as close as I want so I can easy skip over stutters or other noise. Plus, I create a TextGrid right there, which is the format I‚Äôm most comfortable working with for scripting purposes. The downside to Praat is that I ended up having to use my trackpad on my laptop more than I wanted to (for scrolling side to side and placing boundaries). I wanted to avoid using my mouse as much as possible because I feel like it hurts my wrist more and I don‚Äôt want carpel tunnel.</p>
<div class="page-columns page-full"><p>Based on my own experience, what I would recommend <em>not</em> doing is hiring out the transcriptions unless you‚Äôre not able to do it yourself. For one, I‚Äôm cheap, and didn‚Äôt want to pay however much per minute of audio. But more importantly, going through my audio a second time gave me a chance to pick up on things that I didn‚Äôt catch or forgot about when I was doing the interviews in person. Things like interesting linguistic phenomena  or passages I may want to quote later. Using the Praat textgrids, I just added a separate tier for my own annotations and could make whatever notes I wanted to about a particular section of audio. I learned so much about my people going through it a second time, and I don‚Äôt think I would have gotten those intuitions about their speech if I had hired it out. Of course, if you need the transcriptions sooner than you can process them or if you‚Äôre not able to do the work yourself, then of course hiring it out might be the better option.</p><div class="no-row-height column-margin column-container"><span class="">I got a token of <em>liketa</em> and two people said <em>I and John</em> instead of <em>John and I</em> which was super cool. I don‚Äôt remember those specifically and would never have caught them if I didn‚Äôt do the transcriptions myself.</span></div></div>
</section>
<section id="the-next-steps" class="level2">
<h2 class="anchored" data-anchor-id="the-next-steps">The next steps</h2>
<p>So while finishing those transcriptions was a monster step, unfortunately the work wasn‚Äôt done.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
Now I've just got to do forced alignment (which includes spell checking) and extract some formants and I'll be ready to go!
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017096904885366784?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<section id="forced-alignment" class="level3">
<h3 class="anchored" data-anchor-id="forced-alignment">Forced alignment</h3>
<p>I‚Äôve been using DARLA for the past few years, but I had some trouble getting the long audio files to process using their web interface. So this gave me a great opportunity to download and install the <a href="https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner">Montreal Forced Aligner</a> on my own computer. Having this in-house provides lots of benefits like processing the files in bulk and quicker turnaround time since I don‚Äôt have to upload the files.</p>
<p>The bad news was that I had to do the spell-checking myself. I completely took for granted that DARLA can handle out-of-dictionary words by guessing their pronunciation. So since the Montreal Forced Aligner doesn‚Äôt do that, I had to check the words myself. When you run it, it‚Äôll produce a list of out-of-dictionary words for you, so all you need to do is add them to the dictionary or correct the spelling in Praat. It seems simple, but it takes a long time. I had at least 20 or 30 typos or new words in every interview, so I probably spent 15 or 20 hours just doing the spell-checking (I think I added over 1000 new dictionary entries too!).</p>
<p>Luckily, all this was made easier with the help of some custom Praat scripts I wrote for this project. One does pre-processing to get the files ready for forced-alignment. It splits the audio and textgrid into two halves (it was easier to process that way), it moves these files into a specific directory, and renames the tiers so that they‚Äôre consistent. As a bonus, it spits out the command that I need to use to run the aligner on those specific files, so all I need to do is copy and paste that into my terminal and it‚Äôll go on its merry way. This was super helpful because typing path names over and over got old real quick.</p>
<p>Once the spell-checking was done and the files were aligned, I had a post-processing script that I used. This one rejoins the two halves into one TextGrid again, adds the new phoneme and word tier to the top of the main TextGrid (so I‚Äôve got the phoneme, word, sentence, and other tiers all in one file), and saves this in that speaker‚Äôs directory on my hard drive. Super handy.</p>
<p>Now ideally, I would go back and hand-check all the boundaries. Maybe one day I‚Äôll have the time to do that, but oh my goodness that‚Äôs not going to happen any time soon.</p>
</section>
<section id="formant-extraction" class="level3">
<h3 class="anchored" data-anchor-id="formant-extraction">Formant extraction</h3>
<p>So keep in mind that all this work, the nearly 200 hours I‚Äôve put into transcribing and force aligning, was mostly just so I could have Praat know where the vowels were in the audio.</p>
<p>So, I modified a couple scripts I wrote to do formant extraction. Of course, I‚Äôve mostly worked with shorter passages of audio (word lists and reading passages and stuff), so what I didn‚Äôt anticipate was that Praat kind of has trouble working with audio longer than about 30 minutes. So I had to modify the script so that it splits the audio into roughly five minute chunks, processes each one individually, and then stitches all the output back together.</p>
<p>And of course, the formant measurements ideally should be handchecked. But again, I just spent way too much time transcribing, so I‚Äôm not about to spend even more time hand-checking these. Not yet at least.</p>
</section>
</section>
<section id="the-end-result-a-giant-spreadsheet" class="level2">
<h2 class="anchored" data-anchor-id="the-end-result-a-giant-spreadsheet">The end result: A giant spreadsheet!</h2>
<p>So what were the main steps here?</p>
<ol type="1">
<li><p>Collect audio.</p></li>
<li><p>Transcription.</p></li>
<li><p>Forced alignment.</p></li>
<li><p>Formant extraction.</p></li>
</ol>
<p>What do I have now? A giant spreadsheet. All this work has been so that I can get a big ol‚Äô spreadsheet that I can then analyze in R. That‚Äôs where I am right now. I‚Äôve got the finalized dataset that I‚Äôll use for my dissertation, so I don‚Äôt even need to open up Praat much anymore, or even plug in my external hard drive. Almost all my work is in R now. But it is quite satisfying to have this monster spreadsheet of my own data.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Transcribing (and the subsequent processing of) a sociolinguistic corpus takes a ton of time, patience, diligence, and determination. My eyesight may have suffered a little bit from staring at the computer, my headphones are a little worn down, my keyboard has had to endure well over a million keystrokes, and my wrists and fingers sure took a hit. But, y‚Äôknow what? It‚Äôs a <em>lot</em> better than it used to be. At least we have tools like forced-alignment, FAVE, and Praat to make our lives easier. But in the end, it is really awesome to have completed this corpus.</p>


</section>

 ]]></description>
  <category>Dissertation</category>
  <category>Methods</category>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/transcribing-a-sociolinguistic-corpus/index.html</guid>
  <pubDate>Mon, 06 Aug 2018 19:10:00 GMT</pubDate>
</item>
<item>
  <title>Testing VOT Durations in A Course in Phonetics</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/testing-english-phonetics/index.html</link>
  <description><![CDATA[ 




<p>So I‚Äôm teaching phonetics and phonology this semester and we‚Äôre using Ladefoged &amp; Johnson‚Äôs <em>A Course in Phonetics</em> textbook. As I was preparing to teach about stops, I thought it might be a good idea as a homework assignment for students to gather their own data to see if some of these ideas panned out. Here‚Äôs my quick study.</p>
<section id="hypotheses" class="level2">
<h2 class="anchored" data-anchor-id="hypotheses">Hypotheses</h2>
<p>The four hypotheses I wanted to test come from Chapter 3 from my 6th edition of <em>A Course in Phonetics</em>:</p>
<ol type="1">
<li><p>Word-initially, /p, t, k/ have longer aspiration than /b, d, g/.</p></li>
<li><p>After onset /s/, /p, t, k/ have about as much aspiration as word-initial /b, d, g/.</p></li>
<li><p>Word-finally, voiced obstruents have an overall longer duration (closure + burst + aspiration) than voiceless obstruents.</p></li>
<li><p>Vowels preceding voiced obstruents are longer than those preceded by voiceless obstruents.</p></li>
</ol>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>Each student was asked to record a friend reading the following words: <em>tack</em>, <em>soap</em>, <em>days</em>, <em>pad</em>, <em>steep</em>, <em>sit</em>, <em>code</em>, <em>tab</em>, <em>bees</em>, <em>scope</em>, <em>dice</em>, <em>goes</em>, <em>bus</em>, <em>seep</em>, <em>cab</em>, <em>spit</em>, <em>gas</em>, <em>peg</em>. I chose these words to maximize onset and coda obstruents in as few words as possible. Vowel quality is assumed to have no effect.</p>
<p>For stops, student measured durations of aspiration and closure; for fricatives it was duration of the fricative itself. This was done in Praat. They have worked with Praat once before in this course, and were taught how to identify boundaries for these, but were otherwise relatively untrained linguistics undergraduates. I provided them with a template spreadsheet to fill out.</p>
<p>I ended up with measurements from 432 words: 18 unique words each from 24 students. I then combined the spreadsheets and wrote up the R code.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="word-initial-stop-aspiration" class="level3">
<h3 class="anchored" data-anchor-id="word-initial-stop-aspiration">Word-initial stop aspiration</h3>
<p>The first hypothesis is one that is commonly taught in intro to linguistics courses: word-initial voiceless stops have aspiration and word-initial voiced stops have little, if any. In my data, this turned out to be the case based on the words <em>pad</em>, <em>peg</em>, <em>tab</em>, <em>tack</em>, <em>cab</em>, <em>code</em>, <em>bees</em>, <em>bus</em>, <em>days</em>, <em>dice</em>, <em>gas</em>, and <em>goes</em>.</p>
<p><img src="https://new.joeystanley.com/blog/testing-english-phonetics/word-initial-stops.png" class="img-fluid" style="width:30em"></p>
<p>A mixed-effects regression model that predicts this aspiration with the underlying voicing of the stop as a fixed effect and student and word as random effects suggests that this difference is significant.</p>
<pre><code>summary(lmer(aspiration ~ underlying_voicing + (1|student) + (1|word), data=wi_stops))

Random effects:
 Groups   Name        Variance  Std.Dev.
 student  (Intercept) 1.039e-03 0.03224 
 word     (Intercept) 9.550e-06 0.00309 
 Residual             1.084e-03 0.03293 
Number of obs: 285, groups:  student, 24; word, 12

Fixed effects:
                            Estimate Std. Error t value
(Intercept)                 0.047309   0.007241   6.534
underlying_voicingvoiceless 0.037331   0.004291   8.699</code></pre>
<p>Conclusion: Yep. Based on this data, the model predicts that voiced stops get around 47ms of aspiration while voiceless stops get about 85ms. Cool.</p>
</section>
<section id="stops-following-s" class="level3">
<h3 class="anchored" data-anchor-id="stops-following-s">Stops following /s/</h3>
<p>We learn that voiceless stops following /s/ in the same syllable are not aspirated in English. In our sample, this also proved to be correct based on the same words as above with the addition of <em>spit</em>, <em>steep</em>, and <em>scope</em>.</p>
<p><img src="https://new.joeystanley.com/blog/testing-english-phonetics/onset-stops.png" class="img-fluid" style="width:30em"></p>
<p>I ran a mixed-effects regression model like the one described above but with position (=environment) as the main effect and voiced word-initial stops as the reference level.</p>
<pre><code>summary(lmer(aspiration ~ position + (1|student) + (1|word), data=onset_stops))

Random effects:
 Groups   Name        Variance Std.Dev.
 student  (Intercept) 0.001153 0.03395 
 word     (Intercept) 0.000000 0.00000 
 Residual             0.001150 0.03391 
Number of obs: 355, groups:  student, 24; word, 15

Fixed effects:
                                Estimate Std. Error t value
(Intercept)                     0.047309   0.007485   6.321
positionvoiceless word-initial  0.037395   0.004019   9.304
positionfollowing /s/          -0.007779   0.004948  -1.572</code></pre>
<p>We get the same coefficients for word-initial stops. Only this time, we can see how stops followed by /s/ fit in. The model shows that the duration of aspiration was not significantly different from word-initial voiced stops. There‚Äôs some indication that the stops following /s/ have even <em>less</em> aspiration, but this didn‚Äôt reach significance.</p>
</section>
<section id="duration-of-word-final-stops" class="level3">
<h3 class="anchored" data-anchor-id="duration-of-word-final-stops">Duration of word-final stops</h3>
<p>What I didn‚Äôt know before preparing for this class was that the overall duration of word final stops (closure + burst + aspiration) is longer for voiceless obstruents than it is for voiced obstruents. Based on all 18 words (<em>scope</em>, <em>seep</em>, <em>soap</em>, <em>steep</em>, <em>sit</em>, <em>spit</em>, <em>tack</em>, <em>cab</em>, <em>tab</em>, <em>code</em>, <em>pad</em>, <em>peg</em>, <em>bus</em>, <em>dice</em>, <em>gas</em>, <em>bees</em>, <em>days</em>, <em>goes</em>), this was true.</p>
<p><img src="https://new.joeystanley.com/blog/testing-english-phonetics/word-final-duration.png" class="img-fluid" style="width:30em"></p>
<p>The difference is small but significant: in a mixed-effects regression model that predicts duration with voicing and manner of articulation as fixed effects and student and word as random effects, the difference reached statistical significance.</p>
<pre><code>summary(lmer(duration ~ underlying_voicing + manner + (1|student) + (1|word), data=wf))

Random effects:
 Groups   Name        Variance  Std.Dev.
 student  (Intercept) 3.002e-03 0.054790
 word     (Intercept) 3.745e-06 0.001935
 Residual             7.550e-03 0.086893
Number of obs: 427, groups:  student, 24; word, 18

Fixed effects:
                          Estimate Std. Error t value
(Intercept)               0.194404   0.012836  15.145
underlying_voicingvoiced -0.035321   0.008540  -4.136
mannerfricative           0.096638   0.008976  10.766
</code></pre>
<p>This model shows that the voiceless stops were about 194ms long, and voiced stops were 159ms (35ms shorter). Furthermore, and I didn‚Äôt expect this until I saw the plot, fricatives were generally about 97ms longer than stops. I don‚Äôt do a lot of nitty-gritty phonetics work like this too often, especially on consonants, so this was news to me.</p>
</section>
<section id="vowel-length" class="level3">
<h3 class="anchored" data-anchor-id="vowel-length">Vowel length</h3>
<p>Finally, it is pretty well known that vowels before voiced obstruents are said to be longer than vowels before voiceless obstruents. As expected, the data showed this to be the case.</p>
<p><img src="https://new.joeystanley.com/blog/testing-english-phonetics/word-final-obstruents-and-vowel-duration.png" class="img-fluid" style="width:30em"></p>
<p>One last regression model, similar to what was done previously, showed not only that this voicing difference is significant but that the manner of articulation mattered as well.</p>
<pre><code>summary(lmer(vowel ~ underlying_voicing + manner + (1|student) + (1|word), data=wf))

Random effects:
 Groups   Name        Variance Std.Dev.
 student  (Intercept) 0.001937 0.04401 
 word     (Intercept) 0.001114 0.03337 
 Residual             0.002274 0.04769 
Number of obs: 431, groups:  student, 24; word, 18

Fixed effects:
                         Estimate Std. Error t value
(Intercept)               0.18905    0.01513  12.495
underlying_voicingvoiced  0.09595    0.01654   5.799
mannerfricative           0.05349    0.01744   3.067</code></pre>
<p>Here, we see that vowels followed by voiceless stops are predicted to have a duration of 189ms. If the following segment is voiced, the model predicts it to actually be 285ms (an increase of 95ms). What stands out is that vowels followed by /g/ were quite a bit shorter. Due to an oversight in my data, there was just one /g/-final word, <em>peg</em>, and it was listed last on the stimulus. No doubt this had an effect. In fact, a more rigorous study that would include more tokens and randomization might find the difference to be even greater, assuming /g/ falls more in like with /b/ and /d/.</p>
<p>I learned something new here as well: fricatives lengthen the vowel even more. The model predicts that vowels preceding fricatives will be 53ms longer than stops with the same voicing. I didn‚Äôt know that.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This study was short, unsystematic, and full of methodological issues. Despite its flaws, it shows evidence to support what Ladefoged and Johnson say in <em>A Course in Phonetics</em>. I also learned a few things: in addition to themselves being longer than stops, fricatives cause their preceding vowels to lengthen as well.</p>
<p>Special thanks goes to my Fall 2017 LING 3060 class at the University of Georgia, who bothered their friends with this silly assignment and painstakingly measured durations in software they barely know how to use.</p>


</section>

 ]]></description>
  <category>Side Projects</category>
  <category>Teaching</category>
  <category>Statistics</category>
  <category>Phonetics</category>
  <guid>https://new.joeystanley.com/blog/testing-english-phonetics/index.html</guid>
  <pubDate>Fri, 08 Sep 2017 04:02:00 GMT</pubDate>
</item>
<item>
  <title>General Update</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/general-update/index.html</link>
  <description><![CDATA[ 




<p>Because I know I have such a <em>massive</em> following, I thought I‚Äôd give an update on my research since it‚Äôs been a few months since the last time I wrote.</p>
<section id="publications" class="level1 page-columns page-full">
<h1>Publications</h1>
<p>At the Linguistic Atlas Office, we‚Äôre working hard on publishing some of our preliminary results. Currently, I‚Äôm on two papers submitted to <em>Proceedings of Meetings on Acoustics</em> that are in various stages of reviewing. I‚Äôm excited to see these come out.</p>
<div class="page-columns page-full"><p>I‚Äôm also working on some of my own research. I‚Äôve got three manuscripts going right now: one on near-mergers in Washington, one on language change within a speaker‚Äôs lifespan, and another on Amazon Mechanical Turk. I still haven‚Äôt submitted a paper to an actual journal so mentally this is a big hurdle for me to get over.</p><div class="no-row-height column-margin column-container"><span class="">Edit (September 2023): Lol, none of these manuscripts were even finished.</span></div></div>
</section>
<section id="conferences" class="level1">
<h1>Conferences</h1>
<p>I‚Äôm happy to say I‚Äôve been accepted into two conferences that‚Äôll happen over the next few months. The first is a paper called <a href="../../downloads/71007-LCUGA4-UT-slides.pdf">‚ÄúConsonantal variation in Utah English: What el[t]se is happening[k]?‚Äù</a> that I‚Äôm doing with Kyle Vanderniet, a fellow grad student here at UGA. Using the MTurk data I collected recently, we focused on just the 14 speakers from Utah and gathered a lot of really interesting tokens of non-standard variants. I‚Äôve also been accepted to present a poster at NWAV on some of my findings in Washington.</p>
<p>I‚Äôm also still waiting to hear back from three other conferences: <a href="https://pubs.aip.org/asa/jasa/article/142/4_Supplement/2540/703907/Vowel-mergers-in-the-American-South">ASA</a> and <a href="../../blog/ads2018.qmd">two at ADS</a>.</p>
</section>
<section id="teaching" class="level1">
<h1>Teaching</h1>
<p>So I‚Äôm teaching for the first time this semester. Because our normal phonetician will be gone, I was asked to teach Phonetics &amp; Phonology. This is pretty cool because normally grad students at UGA don‚Äôt get to teach that class. I‚Äôve really enjoyed it so far! I‚Äôve got 29 students who are all linguistics majors or minors. It takes quite a bit of time, but I‚Äôm having a great time.</p>
</section>
<section id="r-series" class="level1">
<h1>R Series</h1>
<p>Fortunately, my funding through the DigiLab at the UGA Main Library continues this school year. I‚Äôm really excited be giving <a href="../../pages/r-workshops.html">a whole series of workshops on R</a>. Next week I‚Äôll start with just a basic introduction to R. Next month I‚Äôll do a day on <a href="http://ggplot2.tidyverse.org"><code>ggplot2</code></a> and in November will be one on the rest of the <a href="http://www.tidyverse.org"><code>tidyverse</code></a>. I plan on producing some detailed handouts that will be available on this website as a product of these workshops. I‚Äôm really looking forward to them.</p>
</section>
<section id="grant" class="level1">
<h1>Grant</h1>
<p>I applied for a small grant a week or so ago that I‚Äôm waiting to hear back about. If I get it, it‚Äôll pay for some fieldwork out West I‚Äôd like to do. Stay tuned. (Update: I got the grant.)</p>
</section>
<section id="dissertation" class="level1 page-columns page-full">
<h1>Dissertation</h1>
<p>Ah, the dissertation. This <em>should</em> be my primary task right now, and I‚Äôm putting as much time as I can into it. I originally wanted to do something on my Washington data. I could do a purely descriptive work and it would make for a fine dissertation. I‚Äôve read through some of those and they‚Äôre perfectly good.</p>
<p>The problem with that is if I would do that, it would be of interest only to people interested in Washington English. A very small group of dialectologists. Not that my ultimate goal is to boost the number of citations, but I feel like a dissertation, which is supposed to make a real contribution to the field, should be more than that.</p>
<p>I‚Äôve read a couple other dissertations that are heavily based in dialectology and sociolinguistic fieldwork, but they make a bigger statement. They make some advancement on the study of language change, <em>using</em> the data they‚Äôve collected as an <em>illustration</em>. For example, Ruth Herold‚Äôs (1990) UPenn dissertation is based on the <em>cot-caught merger</em> parts of Pennsylvania. Instead of simply describing the data, she makes some very cool statements about how language change works, especially in relation to mergers and generational changes, using the data she collected as evidence. The data is supporting the claim rather than the claim itself.</p>
<div class="page-columns page-full"><p>I‚Äôve got around 180 hours of my own data right now, gathered from lots of sources. All of it has to do with mergers in some way. I feel like I‚Äôve got a dissertation bubbling inside of me relating to vowel mergers. I can use my data as support, illustration, and evidence for some claim that I‚Äôd like to make.</p><div class="no-row-height column-margin column-container"><span class="">Update: I stuck with the Washington data for my dissertation, but Betsy Sneller and I did eventually write <a href="https://doi.org/10.1121/10.0016757">something about mergers</a> using the Washington data, so I guess that scratched that itch.</span></div></div>
<p>So I‚Äôm working on my prospectus right now and will be pitching it to my committee this semester. I‚Äôll keep you informed.</p>


</section>

 ]]></description>
  <category>West</category>
  <category>Utah</category>
  <category>MTurk</category>
  <category>Research</category>
  <category>Conferences</category>
  <category>Linguistic Atlas</category>
  <category>Pacific Northwest</category>
  <category>Dissertation</category>
  <guid>https://new.joeystanley.com/blog/general-update/index.html</guid>
  <pubDate>Tue, 05 Sep 2017 14:30:00 GMT</pubDate>
</item>
<item>
  <title>Using MTurk</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/using-mturk/index.html</link>
  <description><![CDATA[ 




<p>A few weeks ago, I <a href="../../blog/a-survey-of-western-american-english-using-mturk">wrote</a>about a grant I was awarded where I‚Äôll use Amazon Mechanical Turk (‚ÄúMTurk‚Äù) to collect data from people all across the West. Today, I did a soft launch of the request and already got recordings from five people!</p>
<p>After weeks of carefully wrangling my MTurk request, my Qualtrics survey, and my IRB forms, I finally got it all set up. I‚Äôve had a handful of projects get approved by the IRB, but this one was a little different since it was through MTurk, so I was a little unsure how to go about some things. Luckily, our IRB office was having open houses all through the semester, which were <em>very</em> helpful.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Just got IRB approval on my first try. I'm getting better at this! Huh, so going to the <a href="https://twitter.com/UGAHSO"><span class="citation" data-cites="UGAHSO">@UGAHSO</span></a> open houses helps a ton! Who'da thunk?
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/864932387712630784">May 17, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I decided to do a soft release first. $2500 is a lot of money to just throw into a task all at once and I wanted to make sure things were working out right. So I put in enough for five people do to the task. Within the hour I was getting data sent to me! It was crazy!</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
With MTurk, I'm literally getting data emailed to me throughout the day. Pretty exciting.
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/865311077491462144">May 18, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I got all five in one day with no problem. I‚Äôm glad I did the soft release though because there were a couple small snafus that I had to fix. For example, I underestimated how much time it would take people to finish the task, so I‚Äôll raise the amount they‚Äôre compensated: I can afford fewer workers that way, but at least I pay them an honest amount.</p>
<p>I‚Äôll spend the next few days making absolutely certain that the task I want them to do is what‚Äôs right for this project. But at some point, I‚Äôll pull the trigger and let‚Äôer rip. From that point on, all I need to do is approve people‚Äôs work (to make sure they get paid) and then just enjoy the hours and hours of recordings showing up in my inbox. What a way to collect data!</p>
<section id="may-22" class="level2">
<h2 class="anchored" data-anchor-id="may-22">May 22</h2>
<p>So this happened:</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Thank you, MTurker, for pointing out that my consent form says the software I ask you download "will be harmful to your computer." <a href="https://twitter.com/hashtag/Typo?src=hash">#Typo</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/866703105798410240">May 22, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
</section>
<section id="june-9" class="level2">
<h2 class="anchored" data-anchor-id="june-9">June 9</h2>
<p>Okay, so several weeks have passed, and the data collection phase is drawing to a close. In just a couple of weeks, I was able to get data from almost 200 people. I had some major time constraints on how I could use my money, so I had to find ways to use it quicker. I ended up creating an entirely new task, similar to the first one, with a whole new batch of sentences and words for people to read. A large portion of my participants returned to do the second part, meaning I have around 30 minutes of audio from almost 100 people.</p>
<p>This is an incredible dataset I‚Äôve collected. I don‚Äôt know how much audio I have total yet, but it‚Äôs well over 50 hours. That‚Äôs pretty good for just three weeks.</p>
<p>However, I will be the first to say that it was a rough three weeks. It seems like every hour I was getting data emailed to me, and several times a day I had to sit and catalogue the recordings and speaker metadata, while managing the MTurk tasks. Most of the time, it was relatively straightforward, but some participants needed a little extra attention because of technical difficulties, glitches in the system, or complaints here and there. Luckily, I did this when I wasn‚Äôt in classes, because otherwise it would have been impossible.</p>
</section>
<section id="june-20" class="level2">
<h2 class="anchored" data-anchor-id="june-20">June 20</h2>
<p>At last, my data collection has drawn to a close. I ended up with about 212 speakers and 84 hours of data. Not bad. Now comes the daunting task of processing all of this. For every person, if I just want to do a small task that only takes a minute, it‚Äôll take over 3 hours to do it for all speakers! This will take a <em>very</em> long time for me to get through, but from the 2% that I‚Äôve looked at so far, it‚Äôs going to be very fruitful corpus.</p>


</section>

 ]]></description>
  <category>Research</category>
  <category>West</category>
  <category>Pacific Northwest</category>
  <category>MTurk</category>
  <guid>https://new.joeystanley.com/blog/using-mturk/index.html</guid>
  <pubDate>Thu, 18 May 2017 14:30:00 GMT</pubDate>
</item>
<item>
  <title>Laboratory Research</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/laboratory-research/index.html</link>
  <description><![CDATA[ 




<p>Recently, I‚Äôve <a href="../../blog/lsa2017">presented</a> on words like <i>pool</i>, <i>pull</i>, and <i>pole</i> and how the difference between them can be really hard to describe, both by me and the non-specialist alike. Based on my findings in Washington, I decided I wanted to dig a little deeper into what these words are like, so I started a study that is less sociolinguistic and more laboratory phonology-based, which is a little unusual for me.</p>
<p>Broadly, I want to look at the phonetics of English vowels before coda laterals. So, after making a list of lots and lots of possible words, cutting them down based on frequency and other factors, I‚Äôve got a decent list of targeted words.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
List of all ~401 monosyllabic English words with a coda lateral, organized by vowel and syllable structure? Check. My favorite? Squelched.
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/861032509534031873">May 7, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I got IRB approval just a little too late into the semester to recruit people and offer them extra credit in courses, so I had to wait a few weeks to get started. Now that Maymester has started, I‚Äôve approached some professors and asked them to offer participation in my research as an extra credit opportunity. I even hand out little business cards after I‚Äôve done my pitch to the class, so they have my contact information‚Äîan idea that proved very effective for me in Washington:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/laboratory-research/businesscard.png" class="img-fluid figure-img" style="width:15em"></p>
<figcaption class="figure-caption margin-caption">Recruitment business card</figcaption>
</figure>
</div>
<p>So I‚Äôm now meeting with students in the <a href="https://linglab.franklinresearch.uga.edu">linguistics laboratory</a> that we have here at UGA. It‚Äôs unfortunately under-utilized but nonetheless very good. Inside the already very quiet recording studio is a tiny booth where the best recordings can be made. I have participants reading a bunch of carefully selected sentences that target key sounds and then taking a quick follow-up survey. It amounts to about 30‚Äì40 minutes of speech from each person, which is kind of a lot.</p>
<p>I don‚Äôt have a specific goal for how many people I want to get, but I should have 20 by the end of the month and potentially up to 50 by the end of the summer. My only limitation is how much time I can put into this. I‚Äôll do some preliminary analyses on those and see if I need to recalibrate the sentences or maybe collect more data. This will probably be an ongoing project for a while: the IRB and consent forms are purposely pretty open-ended to allow me to modify things where needed without much hassle.</p>
<p>Anyway, it‚Äôs been fun being in the lab, and I‚Äôm excited to analyze <em>really</em> clean audio for a change.</p>



 ]]></description>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/laboratory-research/index.html</guid>
  <pubDate>Thu, 18 May 2017 02:30:00 GMT</pubDate>
</item>
<item>
  <title>Admission to Candidacy</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/admission-to-candidacy/index.html</link>
  <description><![CDATA[ 




<p>This morning I successfully defended my second qualifying paper, ‚ÄúNear-Mergers in Cowlitz County, Washington,‚Äù which means I‚Äôm officially a doctoral candidate! (Okay, actually, a couple forms need to be signed, but that‚Äôs no biggie.) What an important step for me!</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
"Doctoral candidate" just sounds so much cooler than mere "Ph.D.&nbsp;student." <br><br>I like my new title :)
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/860211954090160128">May 4, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>The paper I defended was essentially what I presented at the <a href="../../blog/LSA2017">ADS</a> conference in January and at DiVar in February. I presented some of my data that I collected in Washington State and discussed the <i>Mary-merry-marry</i> merger (or lack thereof) and a collection of mergers involving higher back vowels before coda laterals (<em>pool</em>, <em>pull</em>, <em>pole</em>, and <em>pulp</em>). I got some great feedback from my committee.</p>
<div class="page-columns page-full"><p>What are my plans for the rest of my schooling? I‚Äôm still deciding if I want to finish my dissertation and graduate next year or the year after. I‚Äôll start things up and see how I‚Äôm feeling in six months, but I‚Äôm on the fence. On the one hand, while I will probably be able to secure funding for my fifth year, no matter what it is it‚Äôs probably going to be less pay than a real job, so I‚Äôll want to graduate after my fourth year. On the other hand, giving myself <em>two</em> years to finish will allow me to really put a lot of time into the dissertation, as well as the couple other sizable side projects I have going on, thus beefing up my CV before graduating.</p><div class="no-row-height column-margin column-container"><span class="">Edit: I ended up graduating in 2020 after my sixth year.üò≥</span></div></div>
<p>I‚Äôll have to weigh my research goals against my personal, familial, and financial needs and make some decisions at some point. But for now I‚Äôm just happy to have made that leap to ABD.</p>



 ]]></description>
  <category>Research</category>
  <category>Dissertation</category>
  <guid>https://new.joeystanley.com/blog/admission-to-candidacy/index.html</guid>
  <pubDate>Thu, 04 May 2017 14:30:00 GMT</pubDate>
</item>
<item>
  <title>Lots of Transcribing</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/lots-of-transcribing/index.html</link>
  <description><![CDATA[ 




<p>Last summer, I collected roughly 40 hours of conversation from people in Washington State last year. Not an enormous corpus, but I‚Äôm quite proud of that dataset. Well, my goal was to transcribe one speaker gradually over the course of the year, finishing around now. Well, in 9 months, I‚Äôve done about‚Ä¶ one hour. I realized this week that I really <em>really</em> need to get these done.</p>
<p>Now, I haven‚Äôt just been slacking off doing nothing for the past school year. I‚Äôve been very busy with my research assistantships and with other tasks. I‚Äôve given workshops and seminars here at UGA, and my name has been on a lot of conference presentations in the past few months, even if I haven‚Äôt been able to attend them. I‚Äôve put together this website as well.</p>
<p>I‚Äôve got my second qualifying paper coming up (edit: see my post about it <a href="../../blog/admission-to-candidacy">here</a> and it occurred to me that if I want to graduate in a year, I had better get these recordings transcribed!</p>
<p>Because I am the way I am, I kept track of my transcription rate. I did the first hour or so in about 5 hours. Okay, so at that rate, it‚Äôll take me around 200 hours of work to finish the transcriptions. Okay, so at an hour a day that‚Äôll take me like 8 months. At <em>two</em> hours a day I‚Äôll finish in like August. Yikes. That‚Äôs a lot of work.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
39 hours of audio left. At two hours of transcribing a day I'll finish by‚Ä¶ Thanksgiving. Am I old enough to hire undergrads yet?
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/858046919049977857">April 28, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>So I decided to just get to it. I can calculate and project and write about it all I want, but it‚Äôs not going to get done unless I just go for it. And I‚Äôve got to be the one to do it. Even though there are automatic transcribers out there, I‚Äôd have to double-check their work anyway, and if I need to listen to it all I might as well do it all myself. Plus, it‚Äôs nice to go through them again and listen to linguistic features I didn‚Äôt catch before. And there are a lot!</p>
<hr>
<p>Edit: I‚Äôve decided to tweet every so often to keep track of my progress and to keep myself motivated.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
I put in four hours today and finished an entire speaker, averaging 3.2 minutes of work time for every minute of transcription time. Nice.
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/859227143963897856">May 2, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
I'm 15% finished transcribing. Just finished listening to my interview with a professional radio announcer. Wow. Talk about clear audio.
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/862855005887442944">May 12, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Just hit the 20% mark for transcribing my interviews. I'm currently reliving the one with a guy who worked falling trees for 18 years.
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/864521596924555265">May 16, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
25% done with transcribing. Would have hit it earlier, but I‚Äôve got two other projects going right now, which means more transcribing later‚Ä¶
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/869729263502622720">May 31, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
After a bit of a hiatus, I'm back to transcribing and just hit the 30% mark for my corpus! Hooray!
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/893206602236481537">August 3, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
After another long hiatus, I'm back to transcribing. I just hit the 35% mark of my corpus. I'm averaging around 4.6 hours of work for every hour of audio, and I've got about 26 hours of interviews left. <a href="https://t.co/AyjOL9gIXX">https://t.co/AyjOL9gIXX</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/986324293050134531?ref_src=twsrc%5Etfw">April 17, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Okay I'm now 40% finished transcribing. I've put 78 hours into this already and I've got approximately 105 hours of work left. I'm hoping to be done by July. This is crazy. <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/989190416460341249?ref_src=twsrc%5Etfw">April 25, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
I'm now 45% done with transcription. It'll be really satisfying to heat that 50% mark in a week or so. Only 60 workdays left! <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/991749610992558084?ref_src=twsrc%5Etfw">May 2, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Hooray! I'm HALFWAY done with transcribing these interviews! At slightly more than two hours a day and my current pace of 4.5 hours of work for every hour of audio, I should be done by July. <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/993874814904487937?ref_src=twsrc%5Etfw">May 8, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Just hit 55%. Learning a lot about welding right now. 1022 minutes of interviews left to transcribe, but who's counting anyway? <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/996572117297754112?ref_src=twsrc%5Etfw">May 16, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Okay, so now I'm 60% done. I'm enjoying all the technical terms I'm learning about: welding on the last interview and quilting on this one. Only 65 more hours of work left! <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/998936912252604416?ref_src=twsrc%5Etfw">May 22, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Okay, I'm now 65% done with transcribing my corpus! About 6 more weeks and I'll be finished. <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1001482983503736832?ref_src=twsrc%5Etfw">May 29, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Alright, I just hit the 70% mark. Just 10¬Ω hours of interview left, which should take me about 45 hours to do. In less than a month I'll be done! <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1004788528696496128?ref_src=twsrc%5Etfw">June 7, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Hooray! 75% done! Just finished an interview with a native Washingtonian who totally has existential "it" and "liketa." That was cool. <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1006987475791773696?ref_src=twsrc%5Etfw">June 13, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
It's been a busy week of transcribing, and even though I hit 80% on Tuesday, I just finished out the week at 85%. I did an entire interview yesterday and another one (plus some change) today! <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1012775568268513282?ref_src=twsrc%5Etfw">June 29, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
So apparently, I've been miscalculating my percentage. Turns out I'm 95% done! I only have two more interviews to transcribe! The end is near! <a href="https://twitter.com/hashtag/amTranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amTranscribing</a> <a href="https://twitter.com/hashtag/NotForMuchLonger?src=hash&amp;ref_src=twsrc%5Etfw">#NotForMuchLonger</a> <a href="https://twitter.com/hashtag/CantWaitToStartAnalysis?src=hash&amp;ref_src=twsrc%5Etfw">#CantWaitToStartAnalysis</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1016508276278775808?ref_src=twsrc%5Etfw">July 10, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
I saved the funnest interview for last. This guy's a fourth generation euphonium player and I did trombone before switching to linguistics, so most of the interview is us geeking out on low brass topics. <a href="https://twitter.com/hashtag/amtranscribing?src=hash&amp;ref_src=twsrc%5Etfw">#amtranscribing</a> <a href="https://twitter.com/hashtag/ButIllBeDoneTODAY?src=hash&amp;ref_src=twsrc%5Etfw">#ButIllBeDoneTODAY</a> <a href="https://twitter.com/hashtag/StayTuned?src=hash&amp;ref_src=twsrc%5Etfw">#StayTuned</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/1017054478112051200?ref_src=twsrc%5Etfw">July 11, 2018</a>
</blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>To see my excited tweetstorm for when I finished, see this <a href="../../blog/transcribing-a-sociolinguistic-corpus">new post</a>.</p>



 ]]></description>
  <category>Pacific Northwest</category>
  <category>Research</category>
  <category>Dissertation</category>
  <guid>https://new.joeystanley.com/blog/lots-of-transcribing/index.html</guid>
  <pubDate>Thu, 27 Apr 2017 14:30:00 GMT</pubDate>
</item>
<item>
  <title>A Survey of the Western American English using MTurk</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/a-survey-of-western-american-english-using-mturk/index.html</link>
  <description><![CDATA[ 




<p>I‚Äôm so happy to announce I‚Äôve been selected as a recipient of the UGA Graduate School Innovative and Interdisciplinary Research Grant! This $2,500 grant is part of the Graduate School‚Äôs strategic initiative to support innovation and interdisciplinary in the research being conducted by doctoral students. My project is entitled ‚ÄúA Survey of Western American English using Amazon Mechanic Turk.‚Äù</p>
<div class="page-columns page-full"><p>Amazon Mechanical Turk (‚ÄúMTurk‚Äù) is a crowdsourcing marketplace through Amazon.com where people and business can post tasks for others to perform for a small amount of payment. They are usually menial tasks like completing surveys or data entry. At NWAV last year, Kim, Reddy, Wyschogrod, and Stanford did a presentation on how they cleverly used MTurk to gather recordings of people across the Northeast. After some discussion with some of the authors, I decided to apply for a grant that would pay for the same kind of data collection but targeting the West.</p><div class="no-row-height column-margin column-container"><span class="">Kim, Chaeyoon, Sravana Reddy, Ezra Wyschogrod &amp; James Stanford. 2016. A large-scale online study of dialect variation in the US Northeast: Crowdsourcing with Amazon Mechanical Turk. Paper presented at the New Ways of Analyzing Variation 45, Victoria, BC.</span></div></div>
<p>Compared to other parts of the country, the West has relatively little linguistic research. Some of that is changing, thanks to some of the folks in Stanford, University of Washington, and other universities. But there are large portions of the country like Montana, Wyoming, and other places that have very little written about them. The population isn‚Äôt huge, but there are people there, and those people do speak. So what do they sound like?</p>
<p>This project will hopefully get recordings from possibly up to 500 people in specific western states. This will result in a corpus of over 100 hours of audio and roughly 200,000 words‚Äîa sizable linguistic corpus which I can analyze for many years to come. This will allow a slightly deeper look into some of these places, acting as a launchpad for further research in specific cities.</p>
<p>I‚Äôm really excited to have gotten this grant and to get started. I will post updates as they come. Stay tuned.</p>



 ]]></description>
  <category>Pacific Northwest</category>
  <category>Research</category>
  <category>West</category>
  <category>MTurk</category>
  <guid>https://new.joeystanley.com/blog/a-survey-of-western-american-english-using-mturk/index.html</guid>
  <pubDate>Fri, 21 Apr 2017 14:30:00 GMT</pubDate>
</item>
<item>
  <title>Brother Joseph</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/brother-joseph/index.html</link>
  <description><![CDATA[ 




<p>I had the fun opportunity to be a guest in a podcast today! <em><a href="http://faithpromotingrumors.com">Faith Promoting Rumors</a></em> is a new podcast that my brother and dad started that explores Mormon myths and culture. Having published on an interesting linguistic quirk about Mormon culture‚Äîthe alternation between calling someone either as ‚ÄúBrother Jones‚Äù or as ‚ÄúBob‚Äù‚ÄîI was asked to talk about my research and about this convention in Mormon culture generally.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Listen to the interview <a href="http://faithpromotingrumors.com/12">here</a>!</p>
</div>
</div>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>There is a robust practice of using titles among Mormons. Kids and teenagers are expected to refer to all adults using the appropriate title (usually <em>Brother</em> or <em>Sister</em>, though in some cases <em>Elder</em>, <em>Bishop</em>, or <em>President</em>‚Äîsee below), plus their last name. Adults reciprocate by calling minors by their first name, establishing a clear superiority between kids and adults, though this can be flouted for comedic, sarcastic, or reverential effects.</p>
<p>But between adults the rules are less straightforward. Sometimes adults use first name for each other and other times they use titles. Familiarity is the strongest factor but age and status within the congregation play a role as well. I had a hunch that things like situation, audience, family make up, and place in the social network had to do with it too. This is essentially the same as how some languages have their T-V distinction in pronouns. This is a classic sociolinguistic variable since it doesn‚Äôt appear to be a conscious decision by speakers and it‚Äôs an extremely common linguistic feature.</p>
<p>So what started off as a mild curiosity during Sunday meetings ended up being a term paper in both my sociolinguistics and social network analysis courses, two conference presentations, a spot in the 2016 <a href="http://repository.upenn.edu/pwpl/vol22/iss1/31/">Penn Working Papers in Linguistics</a>, and my first qualifying paper‚ÄîI got a lot of mileage out of that study. I was just getting into quantitative methods, so it‚Äôs a little statistics-heavy, but I did find some interesting results.</p>
</section>
<section id="summary-of-the-study" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-the-study">Summary of the Study</h2>
<p>Essentially, what I did to answer this question was give a survey out to members of my own congregation and ask them to indicate what they would call other members of the congregation in four different situations. I also asked them to tell me how well they knew each person.</p>
<p>As I mention in the podcast, some of the main findings are pretty intuitive. The better someone knows another person, the more likely they are to use their first name. There was more first name among people of the same sex, especially women. Holding all other variables constant, more peripheral members of the social network of the congregation generally use more titles and get called by titles more than the core members. There were some differences between the sexes and the situation: men use more titles for present company while women use more first name in face-to-face situations.</p>
<p>One thing I tested was whether southerners use more titles. It‚Äôs a pretty common stereotype that southerners are more polite and call people ‚ÄúMiss Betty‚Äù more often. It turns out this carries over into Mormon circles as well: southerners generally used titles slightly more than northerners or Utahns.</p>
<p>Surprisingly, age wasn‚Äôt a factor. I thought there would be a clear effect of increased usage of titles for older people, but this didn‚Äôt pan out. What did appear to be the case was that people roughly within a couple years of each other use more titles for each other, but this was more an effect of familiarity than anything else: people are friends with others their age so they use fewer titles.</p>
<p>One of the more interesting findings was that people who have children were called by their titles more than those that didn‚Äôt. I‚Äôve heard anecdotes where unmarried people get titles far less often than married people their same age, and it seems like having kids moves a person up another step in the ‚Äúadult‚Äù category. One of my conclusions was that it seems like a Mormon is truly considered an adult until they are married and have children. Interestingly, the number of kids didn‚Äôt matter, just whether someone <em>had</em> kids. This is explainable by the family-centered religion and culture that Mormons are a part of, and it seems to made manifest in how people address each other‚Äîat least in my Georgia congregation.</p>
</section>
<section id="titles-first-name" class="level2">
<h2 class="anchored" data-anchor-id="titles-first-name">Titles + First Name?</h2>
<p>The title of the episode, ‚ÄúBrother Joseph‚Äù, alludes to the practice that early church leaders had in calling people by a title and their <em>first</em> name. I don‚Äôt know exactly when the change from first to last name happened, but it appears to be sometime in the mid-to-late 1800s. It also might be that certain individuals had this special use of the title: Brother Brigham [Young], Brother Joseph [Smith], Brother John [Taylor], Brother George [Cannon], and Sister Eliza [Snow] were some of the top hits in the mid 1800s. Though Brother [John] Taylor and Brother [George] Cannon were also common in the corpus I looked through. There are a lot of unknowns about forms of address in the early days of Mormonism, but we try to look into it a little bit.</p>
</section>
<section id="other-titles" class="level2">
<h2 class="anchored" data-anchor-id="other-titles">Other Titles</h2>
<p>As we mention in the podcast, there are other titles too that you might hear occasionally. <em>Elder</em> is reserved for male full-time missionaries, whether they be the young guys you might see on the streets or for men in global leadership positions. Women who serve missions retain their generic title of <em>Sister</em>. What‚Äôs interesting about the women though is that in some languages this is a unique title: in Portuguese for example the title is <em>S√≠ster</em> rather than the generic <em>Irm√£</em> (‚Äòsister‚Äô) that other women have.</p>
<p><em>President</em> is for leaders of specifically organized groups of men. This can apply to the president of the church, the leader of the greater local area (what we call ‚Äústakes‚Äù), or the group of 14‚Äì15-year old boys. This is a case when calling a 15-year-old boy ‚ÄúPresident Jones‚Äù is attested in certain circumstances. This title can also be used for the president‚Äôs assistants or ‚Äúcounselors‚Äù, though this applies more to the larger groups and is much less common at the local level.</p>
<p><em>Bishop</em> is for the presiding authority of a congregation. This title, as well as those for full-time missionaries, have a unique position syntactically: they can stand on their own. In other words, it‚Äôs perfectly acceptable to approach to missionaries and say ‚ÄúHey, Sisters!‚Äù or to approach a bishop and say ‚ÄúHey, Bishop‚Äù instead of ‚ÄúHey, Bishop Jones.‚Äù <em>President</em> can be used sometimes in this way though it‚Äôs less common.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>This is an interesting part of Mormonism, and in the podcast we discuss some of the cultural implications of it. Linguistically though I still think there‚Äôs a lot more to be said and I‚Äôd be curious to see other research on this.</p>


</section>

 ]]></description>
  <category>Side Projects</category>
  <guid>https://new.joeystanley.com/blog/brother-joseph/index.html</guid>
  <pubDate>Mon, 10 Apr 2017 19:42:00 GMT</pubDate>
</item>
<item>
  <title>Mount St.¬†Helens and Vowels</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/mount-st-helens-and-vowels/index.html</link>
  <description><![CDATA[ 




<p>Today in our Linguistics Colloquium here at UGA, I got to present on some of my ongoing research on English in a smaller town in Washington. For the past few months I‚Äôve mostly looked at vowel mergers and using lots of statistical tests to show some very subtle changes. Over the past week or so as I‚Äôve prepared for this presentation, I‚Äôve discovered something pretty awesome about my data. And it has to do with Mount St.&nbsp;Helens!</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the slides <a href="../../downloads/170407-Colloquium.pdf">here</a>!</p>
</div>
</div>
<p>In the presentation, I focus on a couple linguistic variables. The first is what linguists call /√¶g/-raising, which is where words like <em>bag</em>, <em>flag</em>, and <em>dragon</em> to sound more like <em>bayg</em>, <em>flayg</em>, or <em>draygon</em>. The other variable is what we call /o/-fronting and /o/-monophthongization, which is where vowel sounds in words like <em>go</em>, <em>snow</em>, or <em>show</em> sound kinda like they would in stereotypical ‚ÄúMinnesohhta‚Äù. These have been studied extensively by researchers in the West regarding Pacific Northwest English and surrounding regions. So, nothing new here.</p>
<p>But looking at the data in relation to speaker age, I noticed a striking pattern: there‚Äôs a clear difference between the speech of people born before 1970 and those born after. I mean really clear. In my sample, /√¶g/ raising virtually disappears after 1970, and /o/ is suddenly diphthongal. /o/ admittedly gradually fronts, so the 1970 date isn‚Äôt quite as drastic in that regard.</p>
<p>So what happened in 1970? Well, not much. But in 1980, Mount St.&nbsp;Helens erupted and seriously affected the logging-based economy of Longview. Up until then, it was easy to find work in the logging industry with only a high school degree, if that. And the salary was relatively good considering it was blue collar work. But when some of the mills started to close, there was a drastic change in the dynamics of the town. Now you need a college education to get a job and even then it‚Äôs not paying well.</p>
<p>So though nothing happened in 1970, those who were born around then were teenagers at the time of this change, and were the first affected by the lack of easy-to-get, high-paying jobs. This marks a paradigm shift in the culture of Longview, and I believe it had to do with the clear changes in the speech. In other words, I think Mount St.&nbsp;Helens played a role in linguistic change in Longview. (The title of the talk was ‚ÄúVolcanic Vocalic Changes‚Äù‚Äîa title I‚Äôm quite proud of!)</p>
<p>This is super exciting for me because up until now most of my work has been phonetic-based and focused on vowel mergers. This the first clearly sociolinguistic project I‚Äôve done‚Äîsomething I‚Äôve been meaning to do this whole time‚Äîand I think the results are cool. It‚Äôs uncharacteristically qualitative and the statistics don‚Äôt play a huge role, which is weird for me. I like this change and I hope I can do more with this research.</p>



 ]]></description>
  <category>Pacific Northwest</category>
  <category>Presentations</category>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/mount-st-helens-and-vowels/index.html</guid>
  <pubDate>Sat, 08 Apr 2017 15:15:00 GMT</pubDate>
</item>
<item>
  <title>SECOL 2017</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/secol2017/index.html</link>
  <description><![CDATA[ 




<p>I was unable to attend this year, but my colleagues presented two papers I was a part of at the 84th Southeastern Conference on Linguistics (<a href="http://conf2017.secol.org">SECOL84</a>) in Charleston, South Carolina.</p>
<p>The first presentation was with Bill Kretzschmar and Katie Kuiper and was called ‚ÄúAutomated Large-Scale Phonetic Analysis: DASS‚Äù wherein we introduce the NSF-funded project‚Äîwith Drs. Kretzschmar and Peggy Renwick as PIs‚Äîthat I am involved in. A PDF of the slide show is are available <a href="../../downloads/170310-SECOL84a-slides.pdf">here</a>.</p>
<p>The second presentation was with Rachel Olsen, Mike Olsen, and Peggy Renwick and was called ‚ÄúTranscribing the Digital Archive of Southern Speech: Methods and Preliminary Analysis‚Äù wherein we talked about the nuts and bolts of how to get a project of this size running. A PDF of the slide show is available <a href="../../downloads/170310-SECOL84b-slides.pdf">here</a>.</p>
<p>This work is part of an ongoing project at the Linguistic Atlas Office at the University of Georgia. We have several hundred hours of recordings from the 1970s of speakers all across the South. We have around 40 undergraduate workers transcribing for us with another couple grad students (including myself) doing the less soul-sucking work. Eventually we‚Äôll have all this data freely available online, but in the meantime we‚Äôre figuring out how to process such scratchy recordings and doing linguistic analysis on it. It‚Äôs been a lot of fun and I‚Äôm glad we were able to show others our work.</p>



 ]]></description>
  <category>Conferences</category>
  <category>Linguistic Atlas</category>
  <category>Presentations</category>
  <category>Research</category>
  <category>Skills</category>
  <guid>https://new.joeystanley.com/blog/secol2017/index.html</guid>
  <pubDate>Thu, 09 Mar 2017 23:12:00 GMT</pubDate>
</item>
<item>
  <title>Updated mvnorm.etest() function</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/updated_mvnorm.etest-function/index.html</link>
  <description><![CDATA[ 




<div class="page-columns page-full"><p>In Levshina‚Äôs <em>How to do Linguistics with R</em>, the function <code>mvnorm.etest()</code> from the <code>energy</code> library is used. This runs what‚Äôs called the ‚ÄúE-statistic (Energy) Test of Multivariate Normality‚Äù which used to test whether multivariate data is normally distributed. This is important because it‚Äôs an assumption that should be met for several statistical tests like MANOVA and for testing statistical significance of a correlation. Well, the code from the book is broken.</p><div class="no-row-height column-margin column-container"><span class="">Levshina, Natalia. 2015. <a href="https://benjamins.com/sites/z.195/">How to do Linguistics with R: Data exploration and statistical analysis</a>. Amsterdam: John Benjamins Publishing Company.</span><span class="">Maria L. Rizzo and Gabor J. Szekely (2016). energy: E-Statistics: Multivariate Inference via the Energy of Data. R package version 1.7-0. <a href="https://cran.r-project.org/web/packages/energy/index.html">https://CRAN.R-project.org/package=energy</a></span></div></div>
<p>I looked into it and it turns out that the book was based on an older version of the <code>energy</code> package (&lt;1.7). But if you‚Äôve updated the package since August 2016 to version 1.7 or later, the code breaks. What happened? Here‚Äôs the old code:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mvnorm.etest</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cbind</span>(x, y))</span></code></pre></div>
<p>While this worked with the old versions, in the newer versions this returns a <em>p</em>-value of ‚ÄúNA‚Äù. This function does some bootstrapping meaning it runs some function on the data over and over some number of times. In the old version of the package, the default was 999 replicates. In the new version there is no default, so you have to specify the number of replicates with the <code>R=999</code> argument:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mvnorm.etest</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cbind</span>(x, y), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">R=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>)</span></code></pre></div>
<p>You can of course change this number to whatever you want, but 999 was the default before so I figure it‚Äôs a good number to keep. Just thought you ought to know.</p>



 ]]></description>
  <category>Statistics</category>
  <category>R</category>
  <category>Skills</category>
  <guid>https://new.joeystanley.com/blog/updated_mvnorm.etest-function/index.html</guid>
  <pubDate>Tue, 28 Feb 2017 23:12:00 GMT</pubDate>
</item>
<item>
  <title>Website Version 2</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/website-version-2/index.html</link>
  <description><![CDATA[ 




<p>Today I finally rolled out a new version of my website! The previous version was great and was an excellent stepping stone into web design, but it was mostly borrowed code. Unsatisfied with some of the way it was designed, I decided to go ahead and just write a new site completely from scratch. It has taken about a month or so to get it going, but I think it‚Äôs a lot better than before.</p>
<section id="new-site" class="level2">
<h2 class="anchored" data-anchor-id="new-site">New Site?</h2>
<p>As I‚Äôve mentioned <a href="../../blog/making-a-website-is-fun">previously</a>, I went through a course on <a href="http://lynda.com">Lynda.com</a>, called ‚ÄúJekyll for Web Designers‚Äù that showed me the basics in how to get that website up. I was still in unfamiliar territory though as I navigated my way around CSS and HTML and any changes I wanted to make to the site were difficult to do. I liked the instructor for that course (James Willliamson) though, so when I looked up some of his other ones and saw that he did several others that would be relevant to me, I decided to go ahead an take them as well.</p>
<p>I first took his ‚ÄúCSS Core Concepts‚Äù where I learned all about CSS and how it works. At a whopping 9 hours long, I knew I was going to get a thorough treatment of CSS. Essentially I learned how to make things look the way I want on a webpage. I learned how to do anything I want to text like change the font, size, and color as well as add things like bold, italic, and small caps. I also learned how to add space around text or between elements on a webpage.</p>
<blockquote class="twitter-tweet blockquote" data-cards="hidden" data-lang="en">
<p lang="en" dir="ltr">
I learned about <a href="https://twitter.com/hashtag/Web?src=hash">#Web</a> on <a href="https://t.co/n13drggo1T">https://t.co/n13drggo1T</a>. I completed CSS: Core Concepts by <a href="https://twitter.com/jameswillweb"><span class="citation" data-cites="jameswillweb">@jameswillweb</span></a> <a href="https://t.co/at3VlMgyPD">https://t.co/at3VlMgyPD</a> via <a href="https://twitter.com/lynda"><span class="citation" data-cites="lynda">@lynda</span></a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/828293046538207234">February 5, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>When I finished that, I continued on to the next course called ‚ÄúCSS Page Layouts‚Äù which I‚Äôm pretty close to finishing. This is more about web design and how to get from a sketch book drawing of a webpage to the screen. Most of the time was in CSS still (as opposed to the HTML) but I learned how to position things on the webpage.</p>
<p>The layout of site I had before was simply copied from the last tutorial in the first course. Since I didn‚Äôt write the HTML or the CSS, I didn‚Äôt know exactly what was going on, so if I wanted to make changes it was really hard to do. Now that I‚Äôve written this new site from scratch, I know exactly what‚Äôs going on in all the webpages and in CSS.</p>
<p>Of course, the sacrifice is that this new site isn‚Äôt quite as clean as the old one was. For example, I know it looks good on my Apple laptop in Safari, but I don‚Äôt know all the code I need to watch out for to make it fully compatible with other browsers, let alone other versions of other browsers. The site also doesn‚Äôt adapt to smaller screens like phones and tablets. I‚Äôm still working on that.</p>
</section>
<section id="so-whats-the-difference" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="so-whats-the-difference">So what‚Äôs the difference?</h2>
<p>I didn‚Äôt do any major changes to the overall structure or the general typographic details that I use in everything I do. The fonts are still Iowan Old Style and Avenir and the background is still ‚Äúwhitesmoke‚Äù (96% white, 4% black). The blue from before was, coincidentally, very similar to the blue I use in my power point slides, but not exactly, so I went ahead and changed it to <em>my</em> blue to match my slides.</p>
<p>Just to give you an idea of what the site looked like before, here are some screenshots. This first one is my home page from my old site:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/website-version-2/site_old_home.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Old site‚Äôs homepage</figcaption>
</figure>
</div>
<p>And this is my new home page:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/website-version-2/site_new_home.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">New site‚Äôs homepage</figcaption>
</figure>
</div>
<p>A couple layout changes. First, I‚Äôve widened it from something like 70% of the screen to a standard 960 pixels. That gave me the option of keeping my ideal width at roughly 66 characters still while giving me room for a sidebar.</p>
<p>I changed the header as well. I felt like it was the weakest part of my page, and I didn‚Äôt like how it looked. I knew I wanted my social media links to be somewhere prominent, and of course the navigation links should be there too. But I wanted to include temporary links to recent presentations so people can download the slideshow if they visit my site, and those didn‚Äôt quite fit up there. And I didn‚Äôt have anywhere to put a photo.</p>
<p>One solution was to have the top just include my name and the navigation buttons, which are now more descriptive. A sidebar could then have my photo, social media, and temporary links to recent presentations. There was also room to put an excerpt of my most recent blog post too, which I figured out how to do dynamically, which is pretty cool.</p>
</section>
<section id="a-new-research-tab" class="level2">
<h2 class="anchored" data-anchor-id="a-new-research-tab">A new Research tab!</h2>
<p>I also added a Research tab. I‚Äôve seen this on lots of other people‚Äôs pages and I thought I‚Äôd include one in my own. I guess I figured everyone would clearly see what my research was by skimming through my CV, but putting it in prose like that makes a lot more sense.</p>
<p>The layout was a blatant copy from the UGA DigiLab <a href="https://digi.uga.edu/projects/">projects tab</a>. I think adding images, even if they‚Äôre simple like mine, contribute a lot to the page.</p>
<p>Eventually, I‚Äôd like to create a separate page for each of these. There I‚Äôd go into more detail on my findings, put a list of publications, and have links to any relevant blog posts. I‚Äôll get there eventually. For now, I‚Äôll just make the the excerpts a little longer.</p>
</section>
<section id="blogi-mean-news" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="blogi-mean-news">Blog‚ÄîI mean, ‚ÄúNews‚Äù</h2>
<p>My blog is now relabeled ‚ÄúNews‚Äù just because it sounds less, well, blog-like. Here‚Äôs the old one:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/website-version-2/site_old_blog.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Old site‚Äôs blog</figcaption>
</figure>
</div>
<p>And here‚Äôs the new one:</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/website-version-2/site_new_blog.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">New site‚Äôs blog</figcaption>
</figure>
</div>
<p>I also redesigned the blog page itself. First off, I didn‚Äôt like that you could only see 4 at a time and that you‚Äôd have to scroll back to see older entries. I don‚Äôt like it that way. I‚Äôve still got the code still there, but it‚Äôll only create a second page once I‚Äôve got 50 posts. I‚Äôve got like 15 for now so I don‚Äôt think it‚Äôll slow anyone down by loading this page.</p>
<p>The rest of the page was completely revamped. To be quite honest, I googled around for great examples of blog layouts and I found <a href="http://johnhenry.ie">this one</a>, which I copied quite a bit of. I think it works well for my site.</p>
<p>Another big thing I didn‚Äôt like about the old layout was that the ‚ÄúArchive‚Äù tab didn‚Äôt make sense as its own page and the label didn‚Äôt really make sense.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/website-version-2/site_old_tags.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Old site‚Äôs ‚Äúarchive‚Äù page</figcaption>
</figure>
</div>
<p>Instead, I moved it to a separate sidebar on the blog. That way it‚Äôs clear that the tags and the blog work together.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/website-version-2/site_new_tags.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">New site‚Äôs ‚Äútags‚Äù page</figcaption>
</figure>
</div>
<p>If I could change one thing on the main ‚Äúnews‚Äù page, it would be that sidebar though. Right now, if you click on a tag, it‚Äôll take you to a page where it lists all the blog posts by category. I don‚Äôt like that separate page. I wanted to do something fancy like you click on a tab and a list of posts will expand down. Turns out I couldn‚Äôt do that without Java scripting and I don‚Äôt know how to do any of that. I also looked into just having pages for each category created on the spot, with blog excerpts instead of just the titles for that one category, but I couldn‚Äôt figure out how to do that.</p>
</section>
<section id="anything-else" class="level2">
<h2 class="anchored" data-anchor-id="anything-else">Anything else?</h2>
<p>Nope. That‚Äôs about it. My CV page has remained essentially the same. I added ‚ÄúResources‚Äù and ‚ÄúTeaching‚Äù tabs, but those are blank for now. I‚Äôll add content to them eventually. There are still a few layout things I need to work on, but I thought I‚Äôd launch the site anyway‚Äîimperfect as it is for now‚Äîbecause it‚Äôs a major improvement over the last one.</p>


</section>

 ]]></description>
  <category>CSS</category>
  <category>Skills</category>
  <category>Meta</category>
  <category>Github</category>
  <category>How-to Guides</category>
  <guid>https://new.joeystanley.com/blog/website-version-2/index.html</guid>
  <pubDate>Sun, 26 Feb 2017 21:29:00 GMT</pubDate>
</item>
<item>
  <title>Excel Workshop</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/excel_workshop/index.html</link>
  <description><![CDATA[ 




<p>Today I had the opportunity to give a workshop in the <a href="https://digi.uga.edu">DigiLab</a> in UGA‚Äôs main library. It was a packed with librarians and grad students from across campus. In just over an hour, I started with the absolute basics and showed more and more tricks that I think would help people with their research projects.</p>
<p>This was the first time I‚Äôve ever given a presentation without powerpoint slides. As I was preparing though, it seemed silly to include detailed descriptions and screenshots when I could just switch over the Excel and show it live. I ended up putting together a handout instead, which had all of the information on it instead. The presentation (and handout) went through the following topics.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Download the handout <a href="../../downloads/170127-ExcelWorkshop.pdf">here</a>!</p>
</div>
</div>
<section id="the-basics" class="level2">
<h2 class="anchored" data-anchor-id="the-basics">The Basics</h2>
<p>I started off by explaining what the differences were between Microsoft Office 2016, 365, and Online. Essentially, Office 2016 is stand-alone software that you buy once and keep forever but doesn‚Äôt upgrade. Office 365 is a subscription service where you have the software as long as you subscribe to it, but it upgrades all the time. Office Online is a free cloud-based version. Through UGA, we can get Office 365 for free, and a heavily reduced priced Office 2016.</p>
<p>I then opened up Excel and covered the absolute basics. Data entry. Moving around the spreadsheet. Cell formatting. Borders. Text formats. I‚Äôm pretty sure everyone there knew this basic stuff, but I thought I‚Äôd cover it anyway because, you never know, there might be someone who‚Äôs never seen it before.</p>
<p>After that, into topics that make it easier to play around with your data. Search and replace, with some extras like matching the entire cell. I also covered sorting and filtering and showed that you can filter multiple columns to get a really specific subsets of your data.</p>
</section>
<section id="pivot-tables" class="level2">
<h2 class="anchored" data-anchor-id="pivot-tables">Pivot Tables</h2>
<p>This is where I wanted to spend the most time. Pivot tables are things that a lot of people had heard of (and from the show of hands, about half the people in the room), but for some reason‚Äîand I don‚Äôt say this to be all high and mighty‚ÄîI have never met <em>anyone</em> who knows how to use them. I learned them in my Intro to Linguistic Computing class with Monte Shelley at BYU and have used them a <em>ton</em> since then, but I guess people just haven‚Äôt had the chance to learn about them.</p>
<p>Pivot tables are dang useful. They can summarize your data in tons of fancy ways. At the very basic level, you can at least see all the unique values in a particular column in your dataset, which is good for checking typos or for copying and pasting into lookup tables (see below). But when you add columns, you can then see how many row of your data frame there are that match the row and column. In the presentation we looked at come census data and saw how many people were from each city within that county. By adding columns, we could see how many men and women there were in each city. We could then add additional columns (like fabricated favorite color data) so that we could see how many men and women liked each color within that city.</p>
<p>There are different ways of viewing the data as well. Instead of the raw count, we looked at how to view the <em>proportion</em> of men to women there were in each town. We switched to a numerical data type (fabricated weight and height data), and were able to see the average weight and height for men and women in each city, as well as the tallest, shortest, heaviest, and lightest man and women in each town. I heard some audible <em>whoa</em>‚Äôs from people as I showed some of this stuff, which was great to hear.</p>
</section>
<section id="functions" class="level2">
<h2 class="anchored" data-anchor-id="functions">Functions</h2>
<p>I then looked at some Excel functions. We started with some basic math, but quickly went into some functions. I showed how some can just stand on their own, like <code>pi()</code>, <code>today()</code>, and <code>now()</code>. We looked at how to reference other cells in functions and how they update automatically. I showed how to create a sequential list of numbers using functions that reference each other. There are some functions like <code>year()</code>, <code>month()</code>, <code>weekday()</code>, and <code>datedif()</code> that work on dates and others like <code>concat()</code>, <code>upper()</code>, <code>lower()</code>, <code>left()</code>, and <code>right()</code> are for manipulating strings. Then there are some that make reference to ranges, like <code>sum()</code> and <code>average()</code>. I showed how the <code>concat()</code> function can be useful to string together last name and first name to create a ‚ÄúLast, First‚Äù column. I know there are much more complicated and useful functions than the ones I covered, but I didn‚Äôt want to intimidate anyone.</p>
<p>We then looked at some conditionals and how they work. As an example, I created a new column in the census data that essentially collapsed the birth state down to two: Washington or not Washington.</p>
</section>
<section id="lookup-tables" class="level2">
<h2 class="anchored" data-anchor-id="lookup-tables">Lookup Tables</h2>
<p>The <code>lookup()</code> function is one that I use all the time, and I wanted to make sure I covered it in the workshop. When preparing for the workshop, I looked at some other site and they all mention that <code>lookup()</code> is essential. What this function can do is basically link together multiple spreadsheets so it starts to act like a database. You set up a table that acts like a dictionary: alphabetic, unique values in one column, with paired information in another. You can then ‚Äúlookup‚Äù some value in this table, and the function will return the information associated with it.</p>
<p>Why is this useful? I use it for two main purposes: a converter, and a collapser. I use it as a converter for things like turning ARPABET representations of vowels into IPA. In one column is the ARPABET pair of letters, and the other column are the IPA symbols. It‚Äôs perfect for that. I use it also to collapse data down to fewer categories. We did this in the workshop by collapsing the 50 states down to 4‚Äì5 regions. We used this lookup table to add a ‚Äúregion‚Äù column to the census data, and then made a pivot table with it. Pretty cool.</p>
<p>For the last part of this section, I showed how to handle the places where <code>lookup()</code> fails, like blank cells or cells not in the dictionary. For blanks, it returns an error, and you can overcome that by wrapping the <code>lookup()</code> function in <code>if(isblank())</code>. But for those pesky typos, <code>lookup()</code> returns the closest value, which I only discovered recently and was not happy with it. I didn‚Äôt have to demonstrate, but in the handout I show that if you do something like <code>=IF(COUNTIF(A1:A5,C2)&gt;0, LOOKUP(C2, A1:A5, B1:B5), "ERROR")</code> it‚Äôll work great.</p>
</section>
<section id="visualizations" class="level2">
<h2 class="anchored" data-anchor-id="visualizations">Visualizations</h2>
<p>Next was how to do quick and dirty visualizations in Excel. I explained briefly (probably too briefly) that not all visualizations work for all kind of data, which I feel is important for people to know. I then showed how to make a bar chart, pie chart, line graph, and scatterplot. I of course used pivot tables to help summarize the data for visualization. I did say though that I don‚Äôt use Excel‚Äôs visualizations for anything because I find them ugly, not customizable enough, and not robust enough to handle what I want to do.</p>
</section>
<section id="bonus-tips-and-tricks" class="level2">
<h2 class="anchored" data-anchor-id="bonus-tips-and-tricks">Bonus Tips and Tricks</h2>
<p>In the last four minutes, I tried to cover some bonus little tips and tricks that I‚Äôve picked up along the way. There are little things like anchoring and freezing/splitting the table. I did show conditional formatting because I use that all the time (my tables look a little psychedelic actually). In the handout I cover how to convert text-to-columns, which can be useful when importing data from somewhere else or for just splitting things up like ‚ÄúLast, First‚Äù into two columns. I covered paste special and how you can transpose, paste multiply, and overwriting functions.</p>
<p>It was a real whirlwind of a presentation but I think some people got a lot out of it. I don‚Äôt know if too many people walked away with any new skills per se, but at least people were exposed to what kinds of things Excel can do, and were given the resources (<em>i.e.</em> this handout) to learn how to do it themselves. I enjoyed giving the presentation, and even though I use R for most of my work nowadays, knowing the ins and outs of Excel sure is useful.</p>
</section>
<section id="downloads" class="level2">
<h2 class="anchored" data-anchor-id="downloads">Downloads</h2>
<p>Again, you can download this handout <a href="../../downloads/170127-ExcelWorkshop.pdf">here</a>. Feel free to also download the three datasets I used: the vowels for <a href="../../data/vowels_oneSpeaker.xlsx">one speaker</a> and the vowels subset of <a href="../../data/vowels_1.xlsx">larger dataset</a>, which both come from the <a href="http://www.lap.uga.edu">Linguistic Atlas of the Gulf States</a>, and the Cowlitz County <a href="../../data/cowlitzData.xlsx">1930 census data</a>, which I gathered myself. Please also visit the accompanying blog post on the <a href="https://digi.uga.edu/news/be-a-data-magician/">DigiLab website</a>.</p>


</section>

 ]]></description>
  <category>How-to Guides</category>
  <category>Presentations</category>
  <category>Skills</category>
  <guid>https://new.joeystanley.com/blog/excel_workshop/index.html</guid>
  <pubDate>Sat, 28 Jan 2017 00:12:00 GMT</pubDate>
</item>
<item>
  <title>Tweeting LSA2017</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/tweeting_LSA2017/index.html</link>
  <description><![CDATA[ 




<p>In addition to the awesome experiences I had overall at the LSA2017 conference (which you can read about <a href="../../blog/lsa2017">here</a>), I made an effort to be active on Twitter during the conference.</p>
<p>I‚Äôve followed conferences in the past (such as LSA and NWAV last year) when I wasn‚Äôt able to attend them, and really enjoy them. I livetweeted the Linguistics Conference at the University of Georgia (LCUGA) in October, which was my first experience as a livetweeter, though I didn‚Äôt do much other than introduce who was presenting next. So this year, not only did I follow the twitter feed, but I also contributed as much as I could myself. Here are some of my more popular tweets:</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
For anyone at <a href="https://twitter.com/hashtag/LSA2017?src=hash">#LSA2017</a>, you should come to the first session of <a href="https://twitter.com/hashtag/ADS2017?src=hash">#ADS2017</a> ("Vowels, vowels, vowels") and hear me talk about Washington State!
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/817121851512160257">January 5, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This one got a surprising amount of traffic (over 800 people saw it on Twitter) for being self promotion, but it‚Äôs because the LSA account retweeted it. I don‚Äôt think they retweet everyone‚Äôs self-advertising tweets though, so I wonder why mine made the cut‚Ä¶ Either way, I didn‚Äôt mind the advertising!</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
Thomas &amp; Kendall: ‚ÄúYou can't get a full sociolinguistic picture of a community by looking at only one kind of variable.‚Äù <a href="https://twitter.com/hashtag/ADS2017?src=hash">#ADS2017</a> <a href="https://twitter.com/hashtag/LSA2017?src=hash">#LSA2017</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/817467349540409344">January 6, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This one was one of many tweets during Erik Thomas and Tyler Kendall‚Äôs presentation, a direct quote from one of their last slides. It‚Äôs a really good quote overall and a lot of people seemed to like it.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
Antieau: Double modals. One Utahn used ‚Äúmight usually would‚Äù. Nice! <a href="https://twitter.com/hashtag/ADS2017?src=hash">#ADS2017</a> <a href="https://twitter.com/hashtag/LSA2017?src=hash">#LSA2017</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/817759261266677761">January 7, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This was one of many things that Lamont Antieau found in the <em>Linguistic Atlas of the Middle Rockies</em>. Having lived in Utah, most of his results were surprising to me, but this one was especially so. Apparently others thought so too.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
<a href="https://twitter.com/BrentPWoo"><span class="citation" data-cites="BrentPWoo">@BrentPWoo</span></a>: "and/or" as a fully lexicalized coordinator with the union set of constraints of on "and" and "or." <a href="https://twitter.com/hashtag/lsa2017?src=hash">#lsa2017</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/817777627314405377">January 7, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>Brent Woo from the University of Washington is doing some really interesting research. We met when he presented at the Linguistics Conference at the University of Georgia in October. He must have some dedicated twitter followers though because anything I tweet at him gets a lot of traffic. I mean, his findings are pretty cool though.</p>
<blockquote class="twitter-tweet blockquote" data-conversation="none" data-lang="en">
<p lang="en" dir="ltr">
Preston: ‚ÄúI grew up in a paint store so I have female-like familiarity with color, and it hasn‚Äôt costed me my masculinity.‚Äù<a href="https://twitter.com/hashtag/ADS2017?src=hash">#ADS2017</a> <a href="https://twitter.com/hashtag/LSA2017?src=hash">#LSA2017</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/817834097888399360">January 7, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This was a random side comment Dennis Preston made after explaining the color scheme in is graphs. Instead of red, green, and blue or something, he used lavender, forest green, chartreuse, and a couple others. After explaining which colors represented which variable, he said that line. Even though it had a typo (hasn‚Äôt costed <em>me</em> my masculinity), a lot of people liked it. I mean, Dennis Preston seems like a pretty funny guy, so this was great.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
After <a href="https://twitter.com/hashtag/LSA2017?src=hash">#LSA2017</a> I feel simultaneously intimidated, inspired, and exhausted. A clear sign that this was a fantastic conference.
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/818213558123134976">January 8, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>At the end of conferences, there‚Äôs a lot of ‚ÄúI‚Äôm so sad the conference is over‚Äù tweets. As I‚Äôve mentioned above, I put as much as I could into the conference, and I did feel exhausted. I was intimidated because seeing what great stuff other grad students are doing I suddenly feel less employable. But I was inspired to do more and better research. This tweet succinctly summed up what my conference experience, and it looks like others felt the same way too.</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
This was my first time &amp; it took more multitasking than expected. I have a newfound respect for tweeters and I appreciate 'em even more now. <a href="https://t.co/1HgUFmHzvV">https://t.co/1HgUFmHzvV</a>
</p>
‚Äî Joey Stanley (<span class="citation" data-cites="joey_stan">@joey_stan</span>) <a href="https://twitter.com/joey_stan/status/818244412417470464">January 8, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>This was part of a small thread that was going. Basically, people were thanking all the livetweeters out there, and I got a mention. I thought I‚Äôd thank the other livetweeters as well, now that I know how hard it is.</p>
<p>All this tweeting paid off though because I think people are noticing me on twitter now, and I think I‚Äôm seen as ‚Äúone of the live-tweeters‚Äù. Not a bad reputation to have. I‚Äôm about to get my hundredth follower, now that I‚Äôve gotten about 10 more since this conference. (I got about 10 just by <em>following</em> NWAV and liking tweets, and another half dozen when I tweeted very basic things at the LCUGA conference.) My goal is for my followers to outnumber the people I follow, but I don‚Äôt know if that‚Äôll happen anytime soon. I‚Äôm not very active on twitter outside of conferences, other than shameless self-promotion. Maybe I should get better at that.</p>
<p>If you‚Äôre interested in getting more into twitter, I‚Äôd recommend this guide:</p>
<blockquote class="twitter-tweet blockquote" data-lang="en">
<p lang="en" dir="ltr">
<a href="https://twitter.com/joey_stan"><span class="citation" data-cites="joey_stan">@joey_stan</span></a> have you seen <a href="https://twitter.com/GretchenAMcC"><span class="citation" data-cites="GretchenAMcC">@GretchenAMcC</span></a>'s excellent guide? <a href="https://t.co/03AjgiGtuq">https://t.co/03AjgiGtuq</a>
</p>
‚Äî Rachael Tatman (<span class="citation" data-cites="rctatman">@rctatman</span>) <a href="https://twitter.com/rctatman/status/818254265974145024">January 9, 2017</a>
</blockquote>
<script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I enjoy being active on twitter for <a href="../../blog/the-importance-of-twitter">lots of reasons</a>, most of them completely selfish, but starting to get a small following is pretty exciting and it‚Äôs a fun group to be a part of.</p>



 ]]></description>
  <category>Conferences</category>
  <category>Twitter</category>
  <guid>https://new.joeystanley.com/blog/tweeting_LSA2017/index.html</guid>
  <pubDate>Sat, 14 Jan 2017 00:12:00 GMT</pubDate>
</item>
<item>
  <title>LSA2017</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/lsa2017/index.html</link>
  <description><![CDATA[ 




<p>Last weekend, I had the opportunity to present at the 2017 Annual Meeting of the American Dialect Society, as well as attend the other meetings of the Linguistic Society of America annual meeting in Austin, TX. There were a lot of really awesome things about the whole thing.</p>
<div class="page-columns page-full"><p>First off, I feel like I had a great experience giving <a href="../../downloads/170105-ADS-slides.pdf">my presentation</a>. I presented Thursday afternoon in the session called ‚ÄúVowels, Vowels, Vowels‚Äù which was chaired by Erik Thomas, and saw other presentations by Charlie Farrington &amp; Tyler Kendall, Matthew Gordon, and Michol Hoffman. There were about 30 people in the room, and I could name about half of them. In fact, while summarizing previous research, I realized that half the people I cited were sitting right there. Afterwards, I had a lot of discussion and great feedback. I couldn‚Äôt have asked for a better experience.</p><div class="no-row-height column-margin column-container"><span class="">As part of the presentation, I showed <a href="../../downloads/170105-ADS-video.mov">this video</a>.</span></div></div>
<p>During the course of the next several days, I made a point to introduce myself to people. Networking is an important part of going to conferences, and I haven‚Äôt really taken that opportunity in the past. So I was able to meet several of the greats and tell them how much I enjoyed some of the things they‚Äôve written. I also met some grad students that have similar interests as me. I feel a lot more connected to other researchers than I did before.</p>
<p>Going into this conference, I knew I wanted to give it everything I had. I got funding from UGA for the first time, both through the Linguistics Program and the Graduate School, and I wanted to make sure the money I received went to a good cause. The conference was busy, but I attended as many presentations as I could. In fact, I hardly had time to eat, and ended up only eating one meal a day during the four days. I actually lost four pounds attending this conference! I also stayed at an Airbnb for the first time, and I didn‚Äôt want to take the bus all the way to the apartment when I knew there were things to do at the conference. I ended up attending 35 presentations and visited about a dozen posters. It made for four very long and busy days, but they were extremely productive.</p>
<p>I also made an effort to be active on Twitter during the conference, but I have a separate blog post about that which you can read <a href="../../blog/tweeting_LSA2017">here</a>.</p>
<p>Overall, a fantastic experience. The best I could have hoped for, and the best conference I‚Äôve been to.</p>



 ]]></description>
  <category>Pacific Northwest</category>
  <category>Conferences</category>
  <category>Research</category>
  <guid>https://new.joeystanley.com/blog/lsa2017/index.html</guid>
  <pubDate>Wed, 11 Jan 2017 00:12:00 GMT</pubDate>
</item>
<item>
  <title>Interactive Guarani Dictionary</title>
  <dc:creator>Joey Stanley</dc:creator>
  <link>https://new.joeystanley.com/blog/interactive-guarani-dictionary/index.html</link>
  <description><![CDATA[ 




<p>The semester is finishing up, and as usual, the most productive week for me is during finals. Not necessarily productive regarding school work or current research projects, but I always rediscover side projects and hobbies. This week I rekindled my interest in Guarani.</p>
<section id="brazil" class="level2">
<h2 class="anchored" data-anchor-id="brazil">Brazil</h2>
<p>I‚Äôve been working on Guarani off and on since 2009. I was living in Campo Grande, Mato Grosso do Sul, Brazil, as a Mormon missionary at the time. Fairly regularly I would meet people that spoke this language called Guarani, and I had friend (a fellow missionary), who had some pedagogical materials that taught Spanish speakers Guarani. So I had to work through the Spanish (I had only been speaking Portuguese for 9 months or at that point), but I was able to decipher some of the basic Guarani morphology and grammar. A while later my dad sent me a copy of the Book of Mormon in Guarani and said I ought to learn what I could. So I sat there with the Guarani, Portuguese, and English translations and would try to figure out new words and morphology.</p>
<p>Again, I was a Mormon missionary at the time, so I didn‚Äôt have a lot of time to spent learning this language. I hadn‚Äôt begun studying linguistics yet, so I had no idea what a non-Indo-European language could possibly be like and there were a few things that had me stumped. I also didn‚Äôt have access to a computer, so I couldn‚Äôt keep track of notes and vocabulary very well. So every couple of weeks I‚Äôd sit there with a dozen sheets of paper spread all over my desk, trying in vain to keep things alphabetized as I added vocabulary and translations. My Brazilian buddies all thought I was insane for trying to learn this language, but I found it to be a LOT of fun.</p>
<p>One of the more frustrating things was that I wanted to see how a single word was used in other contexts. If I was looking through a sentence and there were three Guarani words I didn‚Äôt know, I often had no way of knowing which word corresponded with the meaning in the English sentence. If only I could control+F the book and find the Guarani words in other contexts and figure out the meaning.</p>
</section>
<section id="self-study" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="self-study">Self-study</h2>
<div class="page-columns page-full"><p>After I came back to the United States and went back to college at BYU, I found that there were some books written about Guarani grammar, but they were mostly older ones. I didn‚Äôt know it at the time, but a former Department Chair in the Department of Linguistics and English Language at BYU was Robert W. Blair, who published some Guarani pedagogical material. I found his <em>Guarani Basic Course</em> at the library as well as his student, Charles Graham‚Äôs, <em>Guarani Intermediate Course</em>, and did what I could going through those. There were some other more descriptive grammars of the language written in the mid 20th Century, and I even sat in on a Guarani course for a semester.</p><div class="no-row-height column-margin column-container"><span class="">Yes BYU offers a course in Guarani! The class was taught only every once in a while and was intended for Mormon missionaries who had spent time in Paraguay. The class was taught in Spanish (again‚Äînot a language I‚Äôve studied) by a native Guarani speaker, and was intended to add some formal instruction to people already familiar with the language. I was overwhelmed with other courses so I couldn‚Äôt keep up for more than a few weeks.</span></div></div>
</section>
<section id="translation-program" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="translation-program">Translation Program</h2>
<p>I was in my last year at BYU. I was working as a programmer, creating eBooks for <a href="http://www.wordcruncher.com">WordCruncher</a> and had access to an HTML file of the Guarani Book of Mormon. I had taken a class in Perl already and had gotten pretty proficient through that job. I had also taken Mark Davies‚Äô Corpus Linguistics course. So when I took an NLP course as the capstone to my minor in Linguistic Computing, I decided to write a Guarani translator.</p>
<div class="page-columns page-full"><p>The program worked pretty well and was exactly what I was dreaming of in Brazil. I had paired the Guarani and English text as a ‚Äúparallel corpus‚Äù, meaning each line in one file corresponded to a translated line in the other. What the translator does is it takes an input string (say, <em>mba‚Äôapo</em>) and it displays all the Guarani sentences with that word with the English underneath it. Made it very handy to see how words (or parts of words) were used in other contexts.</p><div class="no-row-height column-margin column-container"><span class="">This corpus might actually be the largest Guarani-English parallel dictionary. It had 329K Guarani words when first wrote the translator, but it‚Äôs now up to 606K after adding some more translated church material. I‚Äôve got another ‚âà250K to add to it, whenever I get the time. I could nearly double it even then if I get access to the Guarani Bible, though I don‚Äôt know if that‚Äôll happen anytime soon. Granted, these are all translated texts from English, and are religious-based, obviously representing a <em>very</em> different style than naturally occurring, spoken Guarani.</span></div></div>
<p>What it then does it is look at all the words in both the English and Guarani sentences with the word, keeps track of their frequencies, then looks at the frequencies for all words in the entire corpus and compares the two. Words that have nothing to do with the translation will occur with roughly the same frequency in the matched sentences as they do in the full corpus. But words that correspond to the same meaning will occur relatively much more often in the matched sentences compared the corpus as a whole. So say the word <em>work</em> appears once every 1000 words in the whole corpus. If it suddenly appears once every 25 words in the matched words, statistically that‚Äôs a big difference, and odds are pretty good that <em>work</em> is a translation for <em>mba‚Äôapo</em> (and it is).</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/interactive-guarani-dictionary/guarani_search.png" class="img-fluid figure-img" style="width:30em"></p>
<figcaption class="figure-caption margin-caption">Guarani Search engine</figcaption>
</figure>
</div>
<p>So using this I could find out which English words correlated with which Guarani words. Not a perfect translator, especially since it didn‚Äôt use any fancy NLP processing, but not bad.</p>
<p>My interest in Guarani, which was mostly about its nasal harmony, verbal morphology, and trying to document the grammar as a whole, started to wane as I started grad school and focused more on sociolinguistics and dialectology. But my reading comprehension is still‚Ä¶ okay let‚Äôs face it, not that great, but I‚Äôm surprised at how much I was able to learn through self-study and a custom computer program.</p>
</section>
<section id="interactive-dictionary" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="interactive-dictionary">Interactive Dictionary</h2>
<p>I think what started this recent resurgence in Guarani was, strangely enough, making this website. I‚Äôve acquired some more HTML and CSS skills and realized that I could make something useful with a web browser. So I dusted off my old files and started something fun.</p>
<p>In just a week I was able to make a pretty useful website (locally hosted only for now) with two main pages. The first is the entire corpus. Unlike what I had before, I could take advantage of the formatting to display useful information. All the words I know are in regular black text, but the words I don‚Äôt know stand out in blue. That makes it easy to figure out which ones I need to learn next. For the words I do know, the roots are underlined, so I can quickly see the base and what morphology is stemming off of it. The interactive part is that if I mouseover the root, a basic definition shows up in the form of a tooltip. So if I‚Äôve forgotten a word, I can very quickly remind myself of what it means. Very handy.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/interactive-guarani-dictionary/guarani_corpus.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Guarani Corpus screenshot</figcaption>
</figure>
</div>
<p>How am I keeping track of what I know and don‚Äôt know? The other page on the site is a dictionary. I usually kept all this stuff in a spreadsheet somewhere, but here I can utilize the formatting to make it look like a real dictionary. I‚Äôve got roots, possible word forms, derivatives, translations, parts of speech, etymology, other notes, and the infrastructure to include example sentences and other metadata. All this is stored on a file on my computer, and when I learn a new word, I just add it to the bottom of the file and a Perl script will take care of alphabetizing it and making sure it looks good for the CSS to take over.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://new.joeystanley.com/blog/interactive-guarani-dictionary/guarani_dictionary.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption margin-caption">Guarani Dictionary screenshot</figcaption>
</figure>
</div>
<p>The result is a slick system where I can quickly see what words I need to learn and I can easily add them to the dictionary. I then run a lightning fast Perl script and refresh my browser, and I‚Äôve got an updated corpus and dictionary.</p>
<p>The system is set up to handle as big of a corpus or dictionary as I‚Äôm willing to feed it. For now, I‚Äôm only a couple paragraphs in and I‚Äôve got over 100 entries in the dictionary. It will take hundreds of hours to go through my entire corpus. But for the first time I‚Äôll be creating a decent Guarani dictionary, which is kinda what I had in mind to do the whole time.</p>


</section>

 ]]></description>
  <category>CSS</category>
  <category>Guarani</category>
  <category>Side Projects</category>
  <guid>https://new.joeystanley.com/blog/interactive-guarani-dictionary/index.html</guid>
  <pubDate>Sat, 10 Dec 2016 18:21:00 GMT</pubDate>
</item>
</channel>
</rss>
