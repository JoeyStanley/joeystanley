{
  "hash": "b7732544b814dcb5e05542a7fa2993e6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Kohler Tapes (Update #3)'\nauthor: \"Joey Stanley\"\ndate: \"2025-06-10\"\noutput: html_document\ncategories:\n  - Kohler Tapes\n  - Research\n  - Utah\nimage: raw_plot.png\n---\n\n\n\n\n\nI am ecstatic to report that I have now have my Kohler Tapes collection processed! In this post, I'll give an update on how I was able to process so much data. I'll also give a really quick demonstration of how to use new-fave and I'll show some of the very first vowel plots from this dataset, even as I see them!\n\n## Background\n\nI've given some explanations of the Kohler Tapes collection [here](/blog/kohler-tapes), [here](/blog/kohler-tapes-update), and [here](/blog/kohler-tapes-update2), so I won't give a full history of the processing here. But, in January 2018, I met Norm Kohler in Heber, Utah. When he was a middle school teacher there in the 80s and 90s, he asked his students interview their grandparents or some other older person in town as part of a local history unit. He collected 1200 tapes and intended to write a compilation of some of these oral narratives. He unfortunately never did and after his passing, the tapes fell into the custody of the Wasatch Historical Society. They knew I was interested in the tapes, so they got into contact and in 2021, I finally got the tapes. \n\nOver the next few months, I carefully photographed and cataloged the 751 tapes. BYU's Office of Digital Humanities digitized them, and I learned I had 631 hours of audio from 806 interviewees from 1986 to 2001. Thanks to a John Topham and Susan Redd Butler BYU Faculty Research Award from the [Charles Redd Center for Western Studies](https://reddcenter.byu.edu), I hired an RA to listen to the first few minutes of each tape to write down any demographic information she could gather. She was then able to find over half the people in in a genealogy database, which we used to fill out the rest of the metadata. With that, I learned that the birth years were from 1892 to 1953 and people were from Wasatch County. \n\n### Transcription\n\nWith that metadata filled out, I had to start the arduous task of transcribing all of these tapes. The problem is it was simply too much for me to do. It took me [172 hours](/blog/transcribing-a-sociolinguistic-corpus/index.html) to transcribe about 46Â½ hours for my dissertation. At that rate, it would take me 2,334 hours to do all of these tapes. Even during my dissertation transcription, I couldn't do more than about two hours of work a day, so it'd end up taking me 1,167 work days, or 233 work weeks or about 5 years. Two hours a day for five years. No way. \n\nWell, I thought maybe I could hire some students. I know that student transcribers are typically a bit slower, so I figured it'd take a team of about 14 student workers two years to get it all done. Not to mention the roughly $100,000 I'd need to pay them. I don't foresee myself securing a grant that big or managing a lab that involved for a while. So the tapes sat untranscribed for a while.\n\nAbout a year ago, a bright new student expressed interest in the project. So applied for an received an Annaley Naegle Redd Assistantship, also awarded by the Redd Center. That paid for that student to begin transcribing a few key tapes. I've already [mentioned](/blog/kohler-tapes-update2/) that I was interested in Wallsburg, so we selected a few tapes from there and from Heber to be the first ones to analyze. \n\nWell so then Fall 2024 happens and I attended a talk by a PhD student in computer science who has a background in linguistics. The talk was basically, \"Here's all the cool stuff that linguists can do with AI.\" I've already [tried using Whisper](/blog/whisper/index.html) to transcribe some of these tape with some success. But most of what my RA had been doing for transcription was correcting those AI-generated transcriptions, and most of that time was adding back in the speech errors and filled pauses that Whisper didn't include. When I spoke to the CS student presenter about this he said, it would certainly be feasible to use AI to transcribe the rest of tapes.\n\nSo, over the next several months, the CS student started working on this. At first, we tried fine-tuning Whisper. He said with about 10 hours of manually transcribed audio from a wide variety of tapes, that'd be enough to get Whisper to perform better on this collection, namely the audio quality, the variety of English, and the speech errors. So, my student transcibed a random 10 minutes from numerous tapes and we used that train Whisper. \n\nIn January 2025, we discovered that it wasn't doing as well as we had hoped. But, the CS student heard about a model called [CrisperWhisper](https://arxiv.org/pdf/2408.16589). It's a fine-tuned version of Whisper that claims to transcribe every spoken word exactly as is, including fillers, pauses, stutters, and false starts. Perfect! So, the CS student started playing with it and we found that the results were quite well done! There is some nuance and complexity that is involved in getting this to work, but I'll save that for another day. The point is, we were now ready to start transcribing all the audio.\n\nOver the next six weeks or so, the CS student used BYU's supercomputer to transcribe the tapes. It took a while because 1) it is a complex task, 2) this is a lot of audio to work with, and 3) other tasks sometimes took priority so we were often bumped down the processing queue. \n\nThe next step for the student was to turn those transcriptions into a format that is suitable for me, namely a Praat TextGrid. What underlies this though was extensive checking to make sure it did a good job. This took most of that semester, partly because this is a back-burner project for this student and I had a busy time teaching so I didn't have much time to help.\n\nBut on May 22, 2025, I finally got my first copy of the TextGrids! There are still some issues to work out, like correcting some things, making sure boundaries don't overlap, converting numerals to text, and most importantly, speaker diarization. But, I'm super excited to have these transcriptions finally! I can now say there are approximately 4.7 million words of transcribed audio in this collection.\n\n### Forced Alignment\n\nWhile the work is mostly done for the CS student, sociophoneticians know that this is just the beginning of the pipeline of data processing. My next task was to use the Montreal Forced Aligner to take those utterance-level transcriptions and turn them into word- and phoneme-level transcriptions. \n\nMy first step was to rethink how I had been running MFA. Normally, I use a custom Praat script to move the files to the MFA folder and clean them up along the way, like removing things that I know will slow MFA down. Once I run MFA, I then have a post-processing script to move them back and to some other cleaning up. This is a one-file-at-a-time kind of task, and while that's fine for all my projects so far, it was not going to work for this one. I have about 1,184 files (751 tapes, many of which have content on side A and side B). And since I know the transcriptions I have are just a first pass, I'll need to run these again at some point (perhaps multiple times). This means that I really needed to find a way to run them all at once.\n\nFortunately, MFA can handle many files at the same time. And all the transcriptions were in one folder already. However, I ran into some trouble and I'm not sure what was wrong. Perhaps it's just my unfamiliarity with MFA. But I could not get it to run on all the files at once. What I eventually did was move all the files to their respective folders and generate the command line prompt to run each one. I then dumped all of those commands into a shell script and had MFA run all of them in succession. (I wrote an R script to automate this entire process.) I'll probably do a blog post about it at some point because I think it worked quite well. So, after a few days of working on this, I eventually was able to process all the files with the click of a button over the course of about 36 hours. \n\n### Formant Extraction\n\nThe next step is to extract formants. I wanted to try [new-fave](https://forced-alignment-and-vowel-extraction.github.io/new-fave/) because 1) it's new, 2) it looks fantastic, 3) it's the only automated way I know of to do all the files. Fortunately, getting that script set up was straightforward. I was going to use the same approach that I did with MFA: generate a shell script to process each file one at a time. I couldn't use FAVE's built-in function for handling lots of files because the file structure on my computer wasn't conducive to it, and that's fine. Again, I'll probably do a tutorial on this soon because it worked really well. \n\nThe tricky part was just the size of the collection. I've learned that reading and writing too much on an external harddrive can cause problems. These files were in some cloud storage, which worked for a while, but it caused some memory problems partway through because files weren't downloading. So I moved the entire collection to my computer's harddrive and continued from there. It was moving slowly, but it was moving. \n\nIt took about five days of non-stop processing on my computer, but I'm thrilled to see that as of 2:04pm today, all the files were processed. I am writing this at 2:11pm, so I'm still in the euphoria of having the data finished. Come with me as I process the files for the first time.\n\n## First look\n\nI'll start by loading `tidyverse` and my own package that has some data processing functions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(joeyr)\nlibrary(geomtextpath)\n```\n:::\n\n\n\nJust yesterday, Joe Fruehwald [posted about the output of new-fave](https://forced-alignment-and-vowel-extraction.github.io/new-fave/usage/outputs/). Let's start with the point data. I'll use `Sys.glob` and to fetch the pathnames of all the files I want to read in, and then I'll use `map` to actually read them in. Unfortunately, I won't be able to show the actual data here because Github had some problems with reading in this much data. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_points_data <- tibble(path = Sys.glob(\"/Users/joeystan/Desktop/KohlerTapes/audio/*/FAVE/*_points.csv\")) |> \n  mutate(data = map(path, read_csv, show_col_types = FALSE)) |> \n  select(-path) |> \n  unnest(data)\n```\n:::\n\n\n\nWhoo! 2.5 million rows of vowel data. That'sâ¦ a lot. ð\n\nI'm going to go ahead and recode FAVE's coding scheme into something I'm more familiar with. Just for this first pass.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfave_code_allophones <- function(.df) {\n  .df |> \n    mutate(phoneme = fct_collapse(label, \n                                  \"FLEECE\"  = c(\"iy\", \"iyF\"),\n                                  \"KIT\"     = \"i\",\n                                  \"FACE\"    = c(\"ey\", \"eyF\"),\n                                  \"DRESS\"   = \"e\",\n                                  \"TRAP\"    = \"ae\",\n                                  \"LOT\"     = \"o\",\n                                  \"THOUGHT\" = \"oh\",\n                                  \"GOAT\"    = c(\"ow\", \"owF\"),\n                                  \"FOOT\"    = \"u\",\n                                  \"GOOSE\"   = c(\"Tuw\", \"uw\"),\n                                  \"STRUT\"   = \"Ê\",\n                                  \"PRICE\"   = c(\"ay\", \"ay0\"),\n                                  \"MOUTH\"   = \"aw\",\n                                  \"CHOICE\"  = \"oy\",\n                                  \"NURSE\"   = \"*hr\",\n                                  \"NEAR\"    = \"iyr\",\n                                  \"START\"   = \"ahr\",\n                                  \"FORCE\"   = \"owr\",\n                                  \"CURE\"    = \"uwr\"), \n           .after = label)\n}\n```\n:::\n\n\n\nI'll then incorporate that into my pipeline of processing to get the data into a format I can work with. For now, I'll just look at stressed, non-r-colored allophones.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoints_processed <- raw_points_data |> \n  \n  # recode FAVE's allophones using the function above\n  fave_code_allophones() |> \n  \n  # subset the data\n  filter(stress == 1,\n         !phoneme %in% c(\"PRICE\", \"MOUTH\", \"CHOICE\", \"NURSE\", \"NEAR\", \"START\", \"FORCE\", \"CURE\")) |>\n  \n  # Use joeyr to further recode allophones\n  code_allophones(.old_col = phoneme, .pre_seg = pre_seg, .fol_seg = fol_seg) |>\n  \n  # select the columns I'll need for now.\n  select(file_name, \n         pre_word, word, fol_word,\n         id, time, dur, prop_time, stress, phoneme, allophone, allophone_environment, pre_seg, fol_seg, context,\n         F1, F2, F3, B1, B2, B3,\n         max_formant) |>\n  arrange(file_name, time)\n```\n:::\n\n\n\nThat brings it down to 1.4 million vowels. Still a ton of data. \n\n### Query the data\n\nLet me pause just to do some quick queries. First, how many files do I have?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(n_files <- length(unique(points_processed$file_name)))\n\n# 1131\n```\n:::\n\n\n\nI think there were about 50 that had some problems. Crazy to think that that's the amount of data I got for my dissertation and now I can just shrug it off!\n\nHow many stressed monophthongs per file per file?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(nrow(points_processed) / n_files)\n\n# 1255.718\n```\n:::\n\n\n\nHow much audio? I could query Praat for this information, but I'll just take a shortcut and get the end time of the last vowel per file. The number is in seconds, so I'll get a minutes version as well.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndurations <- points_processed |> \n  summarize(duration_s = max(time + dur), .by = file_name) |> \n  mutate(duration_m = duration_s/60) |> \n  print()\n```\n:::\n\n\n\nHow much audio is here?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(durations$duration_m)/60\n\n# 571.2587\n```\n:::\n\n\n\nAbout 60 hours short of my earlier estimate. But I know some tapes didn't get processed, so that makes sense.\n\nSo what are some summary stats for the durations?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(durations)\n\n# file_name           duration_s        duration_m     \n# Length:1131        Min.   :  13.15   Min.   : 0.2191  \n# Class :character   1st Qu.:1323.67   1st Qu.:22.0612  \n# Mode  :character   Median :1895.92   Median :31.5987  \n#                    Mean   :1818.33   Mean   :30.3055  \n#                    3rd Qu.:2545.16   3rd Qu.:42.4193  \n#                    Max.   :3724.96   Max.   :62.0827  \n```\n:::\n\n\n\nThe average is about 1,818 seconds or 31.59 minutes. Here's a plot showing the distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(durations, aes(duration_m)) + \n  geom_histogram(binwidth = 1) + \n  scale_x_continuous(breaks = seq(0, 70, 10)) + \n  labs(title = \"Distribution of approximate durations for each file\",\n       x = \"duration (in minutes)\",\n       y = \"number of (sides) of tapes\") + \n  theme_minimal()\n```\n:::\n\n\n\n![](durations.png)\n\nKeep in mind that each filename corresponds to a side of a tape. I was just a little bit too young to really be familiar with tapes, but I guess tapes back then could hold either about 32 or maybe 48 minutes of audio, so it makes sense that most files are around that long.\n\n### Vowel plots\n\nNow the moment of truth. Here's a plot showing the raw F1-F2 measurements for the entire dataset. All 1.4 million points are plotted here. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(points_processed, aes(F2, F1)) + \n  geom_point() + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  labs(title = \"All 1.4 million stressed monophthongs from the Kohler Tapes\") + \n  theme_minimal()\nggsave(\"raw_plot.png\", height = 6, width = 8)\n```\n:::\n\n\n\n![](raw_plot.png)\n\nI think right away, this is a testament to how amazing new-fave is. I've worked with legacy data before, and normally there are tons of datapoints in the extreme bottom left of the vowel space. These are the result of formant tracking errors. And this was using the original FAVE! This dataset appears to be *remarkably* clean, at least when it comes to that. \n\n### Normalizing\n\nBefore I look at too much more, I should normalize the data. Fruehwald has [an extremely useful paper](https://jofrhwld.github.io/dct_normalization/#transforming-dct-coefficients) on how to use DCT to normalize new-fave output. I'll read those DCT coefficients after they've been log-transformed, which is required when doing the Nearey normalization. I'll process them in the same way that I did above, except I will keep all vowels.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndct_coefs <- tibble(path = Sys.glob(\"/Users/joeystan/Desktop/KohlerTapes/audio/*/FAVE/*_logparam.csv\")) |> \n  mutate(data = map(path, read_csv, show_col_types = FALSE)) |> \n  select(-path) |> \n  unnest(data) |> \n  \n  # recode FAVE's allophones using the function above\n  fave_code_allophones() |> \n  \n  # subset the data\n  filter(stress == 1) |>\n  \n  # Use joeyr to further recode allophones\n  code_allophones(.old_col = phoneme, .pre_seg = pre_seg, .fol_seg = fol_seg)\n```\n:::\n\n\n\nOkay, so at first, the millions of rows is intimidating. But we can work with it. \n\nDoing normalization on the DCT coefficients involves jumping over to Python---a new thing for me---but worth it because of all the reasons Fruehwald has explained in his paper. I'll follow the code he has [here](https://jofrhwld.github.io/blog/posts/2024/07/2024-07-19_dct-r/) because this is still new to me, but I'll try to explain as best I can.\n\nI needed to install the `scipy` package, so I did that first. I'll then load it with `reticulate::import` and then save a particular function within that package into an R object called `idct`. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reticulate::py_install(\"scipy\") # in case you need to install it\nscipy <- reticulate::import(\"scipy\")\nidct <- scipy$fft$idct\n```\n:::\n\n\n\nI'll get the average of the DCT parameters by token. Keep in mind that I'm working with full trajectory data, not just single points. I'll then apply the python function, `idct` to those averages. For later analyses, I'll get average smooths by speaker, but for now, I'll just collapse all speakers together into average per allophone.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_smooths <- dct_coefs |>\n  \n  # Step 1: Getting the average of the DCT parameters by allophone.\n  summarise(across(F1:F3, mean),\n            .by = c(param, phoneme, allophone, allophone_environment)) |> \n  \n  # Step 2: Apply `idct` to the averages\n  reframe(across(F1:F3, ~idct(.x, n = 100L, orthogonalize = T, norm = \"forward\")),\n          .by = c(phoneme, allophone, allophone_environment)) |> \n  mutate(prop_time = (row_number()-1)/(n()-1),\n         .by = c(phoneme, allophone, allophone_environment))\n```\n:::\n\n\n\nSweet. That was *quite* fast. Now I can show average trajectories in this dataset.\n\n\n### More vowel plots\n\nI'll start with elsewhere allophones. I'll plot it with `ggtextpath`. [I wasn't a fan](/blog/geomtextpath/) of this package when it first came out, but it's growing on me. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_smooths |> \n  filter(allophone_environment == \"elsewhere\",\n         phoneme != \"NURSE\") |> \n  ggplot(aes(F2, F1, color = allophone)) + \n  # geom_path(arrow = joey_arrow()) + \n  geom_textpath(aes(label = allophone), \n                text_smoothing = 10, hjust = 0,\n                linewidth = 2,\n                arrow = joey_arrow()) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  labs(title = \"Average trajectories for preobstruent vowels in the Kohler Tapes\") + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n```\n:::\n\n\n\n![](elsewheres.png)\n\nLooks like a vowel plot. I won't get into heavy interpretation without further digging into the variation and stuff, but it's noteworthy how diphthongal that <sc>bait</sc> vowel is and how monopthongal that <sc>boot</sc> vowel is. It's also interesting how separated <sc>bot</sc> and <sc>bought</sc> are, but again I'll have to look at the data more closely to say anything. Keep in mind that this plot is just preobstruent data.\n\nLet's look at my favorite class of allophones: prelaterals. I'll use the Wells-inspired labels I came up with [in an earlier blog post](/blog/extending-wells-lexical-sets-to-prelateral-vowels/index.html).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_smooths |> \n  filter(allophone_environment == \"prelateral\") |> \n  ggplot(aes(F2, F1, color = allophone)) + \n  # geom_path(arrow = joey_arrow()) + \n  geom_textpath(aes(label = allophone), \n                text_smoothing = 10, hjust = 0,\n                linewidth = 2,\n                arrow = joey_arrow()) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  labs(title = \"Average trajectories for prelateral monophthongs in the Kohler Tapes\") + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n```\n:::\n\n\n\n![](prelaterals.png)\n\nLots to unpack here, which will undoubtedly happen in future publications. For now, it's noteworthy that the front vowel pairs <sc>zeal-guilt</sc> and <sc>flail-shelf</sc> are not merged as they are for some Utahns today. The back vowels are interesting. Looks like there might be a <sc>mulch-golf</sc> (that is, /Êl/-/Él/, or *cull-call*) merger. And possibly a <sc>jolt-fault</sc> (that is, /ol/-/Él/ or *hole-hall*) merger. Interestingly, <sc>golf</sc> and <sc>fault</sc> are separated, <sc>spool</sc> is not fully backed, and <sc>child</sc> is pretty monophthongal.\n\nNow let's look at prerhotics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_smooths |> \n  filter(allophone_environment == \"prerhotic\") |> \n  ggplot(aes(F2, F1, color = allophone)) + \n  # geom_path(arrow = joey_arrow()) + \n  geom_textpath(aes(label = allophone), \n                text_smoothing = 10, hjust = 0,\n                linewidth = 2,\n                arrow = joey_arrow()) + \n  scale_x_reverse() + \n  scale_y_reverse() + \n  labs(title = \"Average trajectories for prerhotic monophthongs in the Kohler Tapes\") + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n```\n:::\n\n\n\n![](prerhotics.png)\n\nI anticipated some weirdness here. <sc>Cure</sc> is just weird for lots of reasons because, [at least for me](/pages/idiolect/#my-cure-lexical-set), it could have lots of different realizations, and I'm assuming that's true in this dataset too. <sc>Force</sc> and <sc>start</sc> are seemingly straightforward, but there's actually a lot going on here because of the *cord-card* merger. I've already been analyzing that merger in this dataset, so stay tuned for updates on that.\n\n## Conclusion\n\nAnyway, it could take a career to really dive into this data, and I'm happy to finally have something to work with after hearing about it 7Â½ years ago!",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}