{
  "hash": "6dccf06b98fb1054e9a834ac8c09e18b",
  "result": {
    "markdown": "---\nlayout: post\ntitle:  \"Visualizing Jonathan Dowse's Vowels\"\ndate:   2023-12-10\ncategories:\n  - Phonetics\n  - Side Projects\n---\n\n\nI've seen interactive IPA charts where a single person produces all the sounds. But a couple years ago, I was teaching a Phonetics & Phonology course, and I stumbled upon [Jonathan Dowse's IPA extended chart with audio](https://jbdowse.com/ipa/). In this post, I take his vowels and map their formants. \n\nNote that I tweeted about this on Frburary 7, 2022. I don't know what the future of X holds, so I thought I'd make this a more perminant home for this plot. \n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\"><a href=\"https://t.co/r8eoN8Oq2K\">https://t.co/r8eoN8Oq2K</a> has a cool extended IPA chart with audio. So I took formants from the audio and made this. <a href=\"https://t.co/cMxnSWWpR8\">pic.twitter.com/cMxnSWWpR8</a></p>&mdash; Joey Stanley (@joey_stan) <a href=\"https://twitter.com/joey_stan/status/1490806512112918533?ref_src=twsrc%5Etfw\">February 7, 2022</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n## Dowse's IPA charts\n\nDowse's IPA chart is the most detailed one I've ever seen. The consonants include many more places of articulation not normally found on the IPA chart, like linguolabial, alveolo-palatal, rounded velar, low uvular, and aryepiglottal. For each one, he uses all the manners that are physically possible, including ones like aspirated stop, affricate, taps, and trills. And for each manner and place, he has voiced and voiceless variants. Not only is the chart itself interesting to look through with all the diacritics and stuff, but he's got recordings of each consonant in different contexts: [C], [Ca], [aC], and [aCa]. Pretty cool. Further down, he has a whole nother table with \"rarer\" manners, including lateral fricatives, lateral flaps, fricative trills, implosive, and different kinds of ejectives. It's pretty intersting to listen to them. He also has a whole table of clicks at eight places of articulation, six manners, and velar and uvular variants of each. \n\nLook, I don't know enough about phonetics to say whether these are all accurately produced, but it's impressive that Dowse, who does not appear to have much formal training in lingusitics, can produce all these sounds. \n\nToday's post is not about the consonants though; it's about the vowels. His vowel chart is equally extensive. He contrasts five front-to-back distinctions and seven height distinctions, with rounded and unrounded versions of each one. He also has an entirely separate chart showing nasalized versions of all of these.\n\n![Jonathan Dowse's vowel chart](vowels_screenshot.png){width=70%}\n\n\nI *do* know enough about phonetics to be able to look at these vowels. I was curious about how these 70 vowel qualities mapped to the acoustic space. I wanted to see whether these distinctions were all equidistant and whether their distribution in the acoustic space matched this rectangular tabular layout in the chart.\n\n## Data Processing\n\nThe first step was to download the audio, which I did by just clicking on each one and downloading them one at a time. I then processed them using [FastTrack](https://github.com/santiagobarreda/FastTrack). This produces in a spreadsheet for each vowel produced, with measurements and bandwidths for the first three formants (plus some other measurements) every few milliseconds. Here's an example from the high front vowel.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(santoku)\n\nhigh_front <- read_csv(\"./csvs/high_front.csv\")\n```\n:::\n\n\nThis is more information than I need, especially since Dowse tries to say the vowels as monophthongally as he can. But we can take a look at the trajectories in just a sec. \n\nSo, I want to plot the midpoints of all vowels at once. Since each is stored in a separate spreadsheet, I'll use `Sys.glob` to get the paths to all those spreadsheets and `map` the `read_csv` function onto all of those paths. Since the vowel quality is stored in the filename itself (i.e., \"high_front.csv\"), I'll strip away the path and the extension to leave just that filename and use it as the name of the vowel itself. Finally, I'll take all those spreadsheets and combine them into one big one with `bind_rows` and `unnest`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvowels_raw <- tibble(vowel = Sys.glob(\"./csvs/*.csv\")) %>%\n  mutate(data = map(vowel, read_csv, show_col_types = FALSE),\n         vowel = str_remove_all(vowel, \"./csvs/\"),\n         vowel = str_remove_all(vowel, \".csv\")) %>%\n  bind_rows() %>%\n  unnest(data) %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14,772 × 13\n   vowel        time    f1    b1    f2    b2    f3    b3   f1p   f2p   f3p    f0\n   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 high-mid_… 0.0252  478.  90    918. 122.  2683.  324   423   863. 2696.  107 \n 2 high-mid_… 0.0272  476.  77.7  917. 100.  2689.  303.  423.  863. 2696.  107.\n 3 high-mid_… 0.0292  470.  71    914.  82.7 2695.  291.  424.  863. 2697.  107.\n 4 high-mid_… 0.0312  460.  72.9  908.  77.5 2692.  308.  424.  863. 2698.  107.\n 5 high-mid_… 0.0332  451.  78.9  901.  86.2 2680.  337.  425   864. 2700.  107.\n 6 high-mid_… 0.0352  450   79    895.  96.9 2675.  336.  426.  864. 2701.  106.\n 7 high-mid_… 0.0372  453.  74.6  891.  97.4 2685.  308.  427.  865. 2703.  106.\n 8 high-mid_… 0.0392  452.  73.7  887.  92.3 2703.  284.  428.  865. 2705.  106.\n 9 high-mid_… 0.0412  448.  81.5  880   94.6 2723.  280.  429.  866  2708.  106.\n10 high-mid_… 0.0432  446.  89.6  873. 104.  2740.  284.  431.  867. 2710.  106 \n# ℹ 14,762 more rows\n# ℹ 1 more variable: intensity <dbl>\n```\n:::\n:::\n\n\nThis results in a spreadsheet with 14,772 rows, each representing a set of formant measurements at a particular point in time across all 70 recordings. Kind of a lot of data, considering it's only 70 vowels, but that's the kind of resolution FastTrack can give you.\n\nOkay, so for the purposes of the plot, I need to create a spreadsheet that is just the metadata about the vowel itself. In Step 1, I take that monster dataframe and just keep the name of the vowel (i.e. \"high-mid_back-unrounded\") and only keep unique values. I then split that name up into its three parts (`height`, `backness`, and `rounded`) using `separate`. In Step 2, I then modify each one of those a little bit. I turn `rounded` into a boolean instead of a string. Then I turn `height` and `backness` into factors and set what order they should be in. Finally, I create a `marked` column to indicate whether a front vowel is rounded or a back vowel is unrounded---I felt like this might be handy to create a visual of the unmarked vowels. For Step 3, I add the IPA symbols to it. There's no shortcut: I just had to put the symbols in one at a time based on what the chart showed. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nvowels_meta <- vowels_raw %>%\n  \n  # Step 1: split the vowel name up\n  distinct(vowel) %>%\n  separate(vowel, c(\"height\", \"backness\", \"rounded\"), sep = \"_\", fill = \"right\", remove = FALSE) %>%\n  \n  # Step 2: modify those attributes\n  mutate(rounded  = case_when(rounded == \"rounded\" ~ TRUE,\n                             is.na(rounded) ~ FALSE),\n         height   = factor(height,   levels = c(\"high\", \"near-high\", \"high-mid\", \"mid\", \"low-mid\", \"near-low\", \"low\")),\n         backness = factor(backness, levels = c(\"front\", \"near-front\", \"central\", \"near-back\", \"back\")),\n         marked = case_when(backness %in% c(\"near-back\", \"back\") & !rounded ~ TRUE,\n                            backness %in% c(\"front\", \"near-front\", \"central\") & rounded ~  TRUE,\n                            TRUE ~ FALSE)) %>%\n  \n  # Step 3: add IPA\n  arrange(height, backness, rounded) %>%\n    mutate(ipa = c(\"i\", \"y\", \"ï\", \"ÿ\", \"ɨ\", \"u̶\", \"ɯ̈\", \"ü\", \"ɯ\", \"u\",\n                   \"i̞\", \"y̙̞\", \"ɪ\", \"ʏ\", \"ɪ̈\", \"ʊ̈\", \"ɯ̽\", \"ʊ\", \"ɯ̞\", \"u̞\",\n                   \"e\", \"ø\", \"ë\", \"ø̈\", \"ɘ\", \"ɵ\", \"ɤ̈\", \"ö\", \"ɤ\", \"o\",\n                   \"e̞\", \"ø̞\", \"ë̞\", \"ø̞̈\", \"ə\", \"ɵ̘\", \"ɤ̞̈\", \"ö̞\", \"ɤ̞\", \"o̞\",\n                   \"ɛ\", \"œ\", \"ɛ̈\", \"œ̈\", \"ɜ\", \"ɞ\", \"ʌ̈\", \"ɔ̈\", \"ʌ\", \"ɔ\",\n                   \"æ\", \"œ̞\", \"æ̈\", \"ɶ̽\", \"ɐ\", \"ɞ̞\", \"ɑ̽\", \"ɒ̽\", \"ʌ̞\", \"ɔ̞\",\n                   \"a\", \"ɶ\", \"ä\", \"ɶ̈\", \"ɐ̞\", \"ɐ̞̹\", \"ɑ̈\", \"ɒ̈\", \"ɑ\", \"ɒ\")) %>%\n\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 70 × 6\n   vowel                   height backness   rounded marked ipa  \n   <chr>                   <fct>  <fct>      <lgl>   <lgl>  <chr>\n 1 high_front              high   front      FALSE   FALSE  i    \n 2 high_front_rounded      high   front      TRUE    TRUE   y    \n 3 high_near-front         high   near-front FALSE   FALSE  ï    \n 4 high_near-front_rounded high   near-front TRUE    TRUE   ÿ    \n 5 high_central            high   central    FALSE   FALSE  ɨ    \n 6 high_central_rounded    high   central    TRUE    TRUE   u̶    \n 7 high_near-back          high   near-back  FALSE   TRUE   ɯ̈    \n 8 high_near-back_rounded  high   near-back  TRUE    FALSE  ü    \n 9 high_back               high   back       FALSE   TRUE   ɯ    \n10 high_back_rounded       high   back       TRUE    FALSE  u    \n# ℹ 60 more rows\n```\n:::\n:::\n\n\nOkay, so now I have a dataframe that has the name of the vowel from the filename, and then some metadata about that vowel.\n\nNow let's go back and process that acoustic data. I start by taking the raw data and just keeping the formants. Having like 170 timepoints for each vowel is fine, but when visualizing such data, the odds of getting a wonky one are higher and it'll ruin the whole plot. Here's [i]. You can see that most of the formants are pretty stable. But towards the beginning and end, things get weird and they distract from the good data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(high_front, aes(f2, f1, color = time)) + \n  geom_path() + \n  scale_x_reverse() + \n  scale_y_reverse() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWhat I'll do then is take all this high resolution temporal data and make it lower resolution. What I found works is to bin the times into about 10 bins and then take the median within each one. I'll do this by first normalizing the time so that the onset starts at t = 0. That new version of `time` is now `time_diff`. I'll then normalize the time by converting it into percent duration, so that the onset is at 0 and the offset is at 1, with the midpoint at 0.5. That is now in `percent`. This makes it easier to then slice the data into 10 parts using the `santoku::kiru` function. For each of those 10 chunks, I can then get the median F1, F2, and F3 measurements. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhigh_front_summarized <- high_front %>%\n  mutate(time_diff = time - min(time),\n         percent = time_diff / max(time_diff),\n         time_cut = kiru(percent,\n                         breaks = seq(0, 1, 0.1),\n                         labels = 1:10)) %>%\n  summarize(across(c(f1, f2, f3), median), .by = c(time_cut)) %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 4\n   time_cut    f1    f2    f3\n   <fct>    <dbl> <dbl> <dbl>\n 1 1         231. 2343. 3324.\n 2 2         225. 2383. 3275.\n 3 3         216. 2371. 3239.\n 4 4         214. 2385  3227.\n 5 5         216. 2389. 3230.\n 6 6         216. 2371  3204.\n 7 7         217. 2376. 3215.\n 8 8         217. 2371. 3218.\n 9 9         220. 2364  3241.\n10 10        222. 2344. 3191.\n```\n:::\n:::\n\n\nI can plot this new lower-resolution version of the data, and you can see it's much tighter because the extreme outliers were lost. I'll add the original data in gray to provide some context. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(high_front_summarized, aes(f2, f1)) + \n  geom_path(data = high_front, color = \"gray80\") +\n  geom_path() + \n  scale_x_reverse() + \n  scale_y_reverse() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nOkay great. So, that worked for one vowel. Let's do that for all vowels. The code is the same, except I'm grouping things by vowel. This results in `trajs` dataframe (for \"trajectories\"). We'll look at that in just a second. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrajs <- vowels_raw %>%\n  select(vowel, time, f1, f2, f3) %>%\n  mutate(time_diff = time - min(time),\n         percent = time_diff / max(time_diff),\n         time_cut = kiru(percent,\n                         breaks = seq(0, 1, 0.1),\n                         labels = 1:10),\n         .by = vowel) %>%\n  summarize(across(c(f1, f2, f3), median), .by = c(vowel, time_cut)) %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 700 × 5\n   vowel         time_cut    f1    f2    f3\n   <chr>         <fct>    <dbl> <dbl> <dbl>\n 1 high-mid_back 1         448.  872. 2745.\n 2 high-mid_back 2         431.  835. 2747 \n 3 high-mid_back 3         422.  873. 2803.\n 4 high-mid_back 4         422.  874  2834.\n 5 high-mid_back 5         426.  842. 2789.\n 6 high-mid_back 6         416.  844. 2838.\n 7 high-mid_back 7         427.  854. 2914.\n 8 high-mid_back 8         430.  903. 2874.\n 9 high-mid_back 9         404.  966. 3011.\n10 high-mid_back 10        967. 1639. 3047 \n# ℹ 690 more rows\n```\n:::\n:::\n\n\nFor now, let's look at the midpoints. You may have noticed in the high front plot above that the middle 50% or so of the vowel was indeed quite monophthongal with very little formant change. I'll assume that's the case for all the vowels. So I'll take the middle few bins and take the median measurment for each one. That'll give me a new `midpoints` dataset. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmidpoints <- trajs %>%\n  \n  # Get the middle few bins and find the median\n  filter(time_cut %in% 3:7) %>%\n  summarize(across(c(f1, f2, f3), median), .by = vowel) %>%\n  \n  # Add the vowel metadata back in.\n  left_join(vowels_meta, by = \"vowel\") %>%\n  print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 70 × 9\n   vowel                     f1    f2    f3 height backness rounded marked ipa  \n   <chr>                  <dbl> <dbl> <dbl> <fct>  <fct>    <lgl>   <lgl>  <chr>\n 1 high-mid_back           422.  854. 2834. high-… back     FALSE   TRUE   ɤ    \n 2 high-mid_back_rounded   408.  522. 2583. high-… back     TRUE    FALSE  o    \n 3 high-mid_central        414. 1670. 2316. high-… central  FALSE   FALSE  ɘ    \n 4 high-mid_central_roun…  420.  979. 2322. high-… central  TRUE    TRUE   ɵ    \n 5 high-mid_front          416. 2237. 2911. high-… front    FALSE   FALSE  e    \n 6 high-mid_front_rounded  346. 2050. 2596. high-… front    TRUE    TRUE   ø    \n 7 high-mid_near-back      478. 1189. 2550. high-… near-ba… FALSE   TRUE   ɤ̈    \n 8 high-mid_near-back_ro…  413.  728. 2516. high-… near-ba… TRUE    FALSE  ö    \n 9 high-mid_near-front     420. 1940  2476. high-… near-fr… FALSE   FALSE  ë    \n10 high-mid_near-front_r…  403. 1680. 2219. high-… near-fr… TRUE    TRUE   ø̈    \n# ℹ 60 more rows\n```\n:::\n:::\n\n\nOkay, we now have a spreadsheet with reasonably good midpoint measurements for each vowel. \n\n## Plotting midpoints\n\nIt's now time to plot it! Here's just a raw look at the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(midpoints, aes(f2, f1)) + \n    geom_text(aes(label = ipa), size = 5) +\n    scale_x_reverse() + \n    scale_y_reverse() + \n    ggthemes::scale_color_ptol() + \n    labs(x = \"F2\", y = \"F1\") + \n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nOkay, so interesting already because we can see that the overall shape is a trapezoid still and not a square. We can see that the lower back portion of the vowel space is a bit denser than, say, the high front. And there's a bit of a gap in the mid-to-high central portion. \n\nLet's zhuzh this plot up a bit. I'll color the vowels by height. Within each height, I'll connect rounded vowels with a dotted line and unrounded vowels with a solid line. To do that, I'll create a new column that has a unique value for vowel height before I pass it into `ggplot`. I'll color rounded vowels in gray. Finally, I'll add some annotations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmidpoints %>%\n    unite(line_id, height, rounded, remove = FALSE) %>%\n    ggplot(aes(f2, f1, color = height, shape = backness)) + \n    geom_line(aes(group = line_id, linetype = rounded)) + \n    geom_label(aes(label = ipa, fill = rounded), size = 5) +\n    scale_fill_manual(values = c(\"white\", \"gray90\")) +\n    scale_x_reverse() + \n    scale_y_reverse() + \n    ggthemes::scale_color_ptol() + \n    labs(title = \"Acoustic measurements from jbdowse.com/ipa/\",\n         x = \"F2\", y = \"F1\",\n         caption = 'Along the front-to-back dimension, the vowels are \"front\", \"near-front\", \"central\", \"near-back\", and \"back.\"\\nLines connect vowels of the same height and rounding. Dotted lines connect rounded vowels.',) + \n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nOkay, now we're starting to see some things! So, it looks like rounded vowels are pretty consistently further back than their unrounded counterparts. In some cases, drastically so (see [ɯ] compared to [u]). F2 is pretty level across most of the higher vowels. Among the low vowels, the further back they were the higher they were. Here we can better see the clustering in the low back portion of the vowel space. This also gives some nice context for the [Moulton (1968:464)](https://doi.org/10.2307/411713), who says that the fieldworkers for the *Linguistic Atlas of New England* were \"hopelessly and humanly incompetent at transcribing phonetically the low and back vowels they heard from their informants\" (cited in [Johnson 2010:32](https://read.dukeupress.edu/pads/issue/95/1)). Given a spot in the low back portion of the vowel space, there are lots of ways to transcribe it that would come pretty darn close. \n\nLet's pause and just make sure we're on the same page when it comes to mapping acoustics to perception. I'm not saying that Dowse was wrong. I don't make this plot just to point and laugh and say, \"wow, he sure did a terrible job!\" I haven't sat sound and like measured out my perception or anything, but the vowels sound more or less equidistant from each other to me. So, what this really shows is that there's a pretty stark difference between what is perceptually equidistant and what is acoustically equidistant. Perhaps what this is showing is that we can actually hear small distances between vowels in the low back space more than in the high front space. Or perhaps Dowse was a little too ambitious at creating an artificially inflated number of low back distinctions and that we should stick with the trapezoidal shape that the IPA chart has. I don't know. But I'm sure there's some interesting paper from the 70s or something that has been written about this. \n\nLet's clean the vowel chart up a little bit by removing the rounded front vowels and the unrounded back vowels. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmidpoints %>%\n    unite(line_id, height, rounded, remove = FALSE) %>%\n    filter(!marked) %>%\n    ggplot(aes(f2, f1, color = height, shape = backness)) + \n    geom_line(aes(group = line_id, linetype = rounded)) + \n    geom_label(aes(label = ipa), size = 5) +\n    scale_x_reverse() + \n    scale_y_reverse() + \n    ggthemes::scale_color_ptol() + \n    theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nOkay, so this is a little bit sparser I don't know if there's any new insight here, other than the middle of the vowel space really opens up quite a bit. \n\n## Plotting trajectories\n\nSince we have trajectory data, let's plot some of those. I'll have to reshape the data so that all the formant data shows up into a single column. I'll use `pivot_longer` to do that, which you can read more about how it's helpful for such vowel data [here](/blog/reshaping-vowel-formant-data-with-tidyr). If we just look at the high front data, we can see what that looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhigh_front %>%\n  pivot_longer(cols = c(f1, f2, f3), names_to = \"formant\", values_to = \"hz\") %>% \n  ggplot(aes(time, hz, color = formant, group = formant)) + \n  geom_point() + \n  geom_path() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThat wonky data we saw in the F1-F2 plot makes sense now. Let's look at the summarized data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhigh_front_summarized %>%\n  pivot_longer(cols = c(f1, f2, f3), names_to = \"formant\", values_to = \"hz\") %>% \n  ggplot(aes(time_cut, hz, color = formant, group = formant)) + \n  geom_point() + \n  geom_path() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nOkay, so that's cleaner. \n\nLet's plot all the vowels then in this spectrogram-like plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrajs %>%\n  pivot_longer(cols = c(f1, f2, f3), names_to = \"formant\", values_to = \"hz\") %>% \n  unite(traj_id, vowel, formant, remove = FALSE) %>%\n  ggplot(aes(time_cut, hz, color = formant, group = traj_id)) + \n  geom_point() + \n  geom_path() + \n  theme_minimal() + \n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nNothing too surprising here. We've got a lot of lines that are relatively stable. Towards the ends, most formants shift a little bit. Not sure why. A few others have some movement in other places. But, you've got to appreciate Dowse's ability to hold a monophthong.\n\nWe can view this in a traditional F1-F2 plot. Here I've filtered out the edges because they had a lot of really wonky measurements, so this shows between 20% into the duration of the vowel and 70% into the duration of the vowel.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrajs %>%\n  left_join(vowels_meta, by = \"vowel\") %>%\n  filter(time_cut %in% c(2:7)) %>%\n  ggplot(aes(f2, f1, color = height)) + \n  geom_line(aes(group = vowel), arrow = joeyr::joey_arrow()) + \n  scale_x_reverse() +\n  scale_y_reverse() +\n  ggthemes::scale_color_ptol() +\n  labs(title = \"Trajectories from jbdowse.com/ipa/\",\n       x = \"F2\", y = \"F1\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nOverall, you can see that Dowse does a good job at holding a monophthong. The higher vowels are generally pretty monophthongal. The lower the vowel, the more back-gliding it is. Low vowels appear to be less stable in height. \n\n## Conclusion\n\nWhen I found Dowse's vowel chart, I wanted to see what the acoustics were. I think it's pretty enlightening to see how acoustic differences perceptual distances map onto perceptual distances and vice versa. ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}